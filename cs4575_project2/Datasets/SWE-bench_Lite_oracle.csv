instance_id,text,repo,base_commit,problem_statement,hints_text,created_at,patch,test_patch,version,FAIL_TO_PASS,PASS_TO_PASS,environment_setup_commit
sqlfluff__sqlfluff-1625,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
TSQL - L031 incorrectly triggers ""Avoid using aliases in join condition"" when no join present
## Expected Behaviour

Both of these queries should pass, the only difference is the addition of a table alias 'a':

1/ no alias

```
SELECT [hello]
FROM
    mytable
```

2/ same query with alias

```
SELECT a.[hello]
FROM
    mytable AS a
```

## Observed Behaviour

1/ passes
2/ fails with: L031: Avoid using aliases in join condition.

But there is no join condition :-)

## Steps to Reproduce

Lint queries above

## Dialect

TSQL

## Version

sqlfluff 0.6.9
Python 3.6.9

## Configuration

N/A

</issue>
<code>
[start of README.md]
1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)
2 
3 # The SQL Linter for Humans
4 
5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)
6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)
10 
11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)
12 [![Requirements Status](https://img.shields.io/requires/github/sqlfluff/sqlfluff.svg?style=flat-square)](https://requires.io/github/sqlfluff/sqlfluff/requirements/?branch=main)
13 [![CI Tests](https://github.com/sqlfluff/sqlfluff/workflows/CI%20Tests/badge.svg)](https://github.com/sqlfluff/sqlfluff/actions?query=workflow%3A%22CI+Tests%22)
14 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)
15 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)
16 
17 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.
18 
19 ## Dialects Supported
20 
21 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):
22 
23 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition
24 - [BigQuery](https://cloud.google.com/bigquery/)
25 - [Exasol](https://www.exasol.com/)
26 - [Hive](https://hive.apache.org/)
27 - [MySQL](https://www.mysql.com/)
28 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)
29 - [Snowflake](https://www.snowflake.com/)
30 - [SQLite](https://www.sqlite.org/)
31 - [Teradata](https://www.teradata.com/)
32 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)
33 
34 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.
35 
36 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.
37 
38 ## Templates Supported
39 
40 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).
41 
42 **SQLFluff** supports the following templates:
43 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)
44 - [dbt](https://www.getdbt.com/)
45 
46 Again, please raise issues if you wish to support more templating languages/syntaxes.
47 
48 # Getting Started
49 
50 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.
51 
52 ```shell
53 $ pip install sqlfluff
54 $ echo ""  SELECT a  +  b FROM tbl;  "" > test.sql
55 $ sqlfluff lint test.sql
56 == [test.sql] FAIL
57 L:   1 | P:   1 | L003 | Single indentation uses a number of spaces not a multiple of 4
58 L:   1 | P:  14 | L006 | Operators should be surrounded by a single space unless at the start/end of a line
59 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace
60 ```
61 
62 You can also have a play using [**SQLFluff online**](https://online.sqlfluff.com/).
63 
64 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).
65 
66 # Documentation
67 
68 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.
69 
70 # Releases
71 
72 **SQLFluff** is in beta phase - expect the tool to change significantly with potentially non-backwards compatible API and configuration changes in future releases. If you would like to join in please consider [contributing](CONTRIBUTING.md).
73 
74 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).
75 
76 # SQLFluff on Slack
77 
78 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!
79 
80 # SQLFluff on Twitter
81 
82 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.
83 
84 # Contributing
85 
86 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.
87 
88 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).
89 
90 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).
91 
[end of README.md]
[start of src/sqlfluff/rules/L031.py]
1 """"""Implementation of Rule L031.""""""
2 
3 from collections import Counter, defaultdict
4 from typing import Generator, NamedTuple
5 
6 from sqlfluff.core.parser import BaseSegment
7 from sqlfluff.core.rules.base import BaseRule, LintFix, LintResult
8 from sqlfluff.core.rules.doc_decorators import document_fix_compatible
9 
10 
11 @document_fix_compatible
12 class Rule_L031(BaseRule):
13     """"""Avoid table aliases in from clauses and join conditions.
14 
15     | **Anti-pattern**
16     | In this example, alias 'o' is used for the orders table, and 'c' is used for 'customers' table.
17 
18     .. code-block:: sql
19 
20         SELECT
21             COUNT(o.customer_id) as order_amount,
22             c.name
23         FROM orders as o
24         JOIN customers as c on o.id = c.user_id
25 
26 
27     | **Best practice**
28     |  Avoid aliases.
29 
30     .. code-block:: sql
31 
32         SELECT
33             COUNT(orders.customer_id) as order_amount,
34             customers.name
35         FROM orders
36         JOIN customers on orders.id = customers.user_id
37 
38         -- Self-join will not raise issue
39 
40         SELECT
41             table.a,
42             table_alias.b,
43         FROM
44             table
45             LEFT JOIN table AS table_alias ON table.foreign_key = table_alias.foreign_key
46 
47     """"""
48 
49     def _eval(self, segment, **kwargs):
50         """"""Identify aliases in from clause and join conditions.
51 
52         Find base table, table expressions in join, and other expressions in select clause
53         and decide if it's needed to report them.
54         """"""
55         if segment.is_type(""select_statement""):
56             # A buffer for all table expressions in join conditions
57             from_expression_elements = []
58             column_reference_segments = []
59 
60             from_clause_segment = segment.get_child(""from_clause"")
61 
62             if not from_clause_segment:
63                 return None
64 
65             from_expression = from_clause_segment.get_child(""from_expression"")
66             from_expression_element = None
67             if from_expression:
68                 from_expression_element = from_expression.get_child(
69                     ""from_expression_element""
70                 )
71 
72             if not from_expression_element:
73                 return None
74             from_expression_element = from_expression_element.get_child(
75                 ""table_expression""
76             )
77 
78             # Find base table
79             base_table = None
80             if from_expression_element:
81                 base_table = from_expression_element.get_child(""object_reference"")
82 
83             from_clause_index = segment.segments.index(from_clause_segment)
84             from_clause_and_after = segment.segments[from_clause_index:]
85 
86             for clause in from_clause_and_after:
87                 for from_expression_element in clause.recursive_crawl(
88                     ""from_expression_element""
89                 ):
90                     from_expression_elements.append(from_expression_element)
91                 for column_reference in clause.recursive_crawl(""column_reference""):
92                     column_reference_segments.append(column_reference)
93 
94             return (
95                 self._lint_aliases_in_join(
96                     base_table,
97                     from_expression_elements,
98                     column_reference_segments,
99                     segment,
100                 )
101                 or None
102             )
103         return None
104 
105     class TableAliasInfo(NamedTuple):
106         """"""Structure yielded by_filter_table_expressions().""""""
107 
108         table_ref: BaseSegment
109         whitespace_ref: BaseSegment
110         alias_exp_ref: BaseSegment
111         alias_identifier_ref: BaseSegment
112 
113     @classmethod
114     def _filter_table_expressions(
115         cls, base_table, from_expression_elements
116     ) -> Generator[TableAliasInfo, None, None]:
117         for from_expression in from_expression_elements:
118             table_expression = from_expression.get_child(""table_expression"")
119             if not table_expression:
120                 continue
121             table_ref = table_expression.get_child(""object_reference"")
122 
123             # If the from_expression_element has no object_references - skip it
124             # An example case is a lateral flatten, where we have a function segment
125             # instead of a table_reference segment.
126             if not table_ref:
127                 continue
128 
129             # If this is self-join - skip it
130             if (
131                 base_table
132                 and base_table.raw == table_ref.raw
133                 and base_table != table_ref
134             ):
135                 continue
136 
137             whitespace_ref = from_expression.get_child(""whitespace"")
138 
139             # If there's no alias expression - skip it
140             alias_exp_ref = from_expression.get_child(""alias_expression"")
141             if alias_exp_ref is None:
142                 continue
143 
144             alias_identifier_ref = alias_exp_ref.get_child(""identifier"")
145             yield cls.TableAliasInfo(
146                 table_ref, whitespace_ref, alias_exp_ref, alias_identifier_ref
147             )
148 
149     def _lint_aliases_in_join(
150         self, base_table, from_expression_elements, column_reference_segments, segment
151     ):
152         """"""Lint and fix all aliases in joins - except for self-joins.""""""
153         # A buffer to keep any violations.
154         violation_buff = []
155 
156         to_check = list(
157             self._filter_table_expressions(base_table, from_expression_elements)
158         )
159 
160         # How many times does each table appear in the FROM clause?
161         table_counts = Counter(ai.table_ref.raw for ai in to_check)
162 
163         # What is the set of aliases used for each table? (We are mainly
164         # interested in the NUMBER of different aliases used.)
165         table_aliases = defaultdict(set)
166         for ai in to_check:
167             table_aliases[ai.table_ref.raw].add(ai.alias_identifier_ref.raw)
168 
169         # For each aliased table, check whether to keep or remove it.
170         for alias_info in to_check:
171             # If the same table appears more than once in the FROM clause with
172             # different alias names, do not consider removing its aliases.
173             # The aliases may have been introduced simply to make each
174             # occurrence of the table independent within the query.
175             if (
176                 table_counts[alias_info.table_ref.raw] > 1
177                 and len(table_aliases[alias_info.table_ref.raw]) > 1
178             ):
179                 continue
180 
181             select_clause = segment.get_child(""select_clause"")
182 
183             ids_refs = []
184 
185             # Find all references to alias in select clause
186             alias_name = alias_info.alias_identifier_ref.raw
187             for alias_with_column in select_clause.recursive_crawl(""object_reference""):
188                 used_alias_ref = alias_with_column.get_child(""identifier"")
189                 if used_alias_ref and used_alias_ref.raw == alias_name:
190                     ids_refs.append(used_alias_ref)
191 
192             # Find all references to alias in column references
193             for exp_ref in column_reference_segments:
194                 used_alias_ref = exp_ref.get_child(""identifier"")
195                 # exp_ref.get_child('dot') ensures that the column reference includes a table reference
196                 if used_alias_ref.raw == alias_name and exp_ref.get_child(""dot""):
197                     ids_refs.append(used_alias_ref)
198 
199             # Fixes for deleting ` as sth` and for editing references to aliased tables
200             fixes = [
201                 *[
202                     LintFix(""delete"", d)
203                     for d in [alias_info.alias_exp_ref, alias_info.whitespace_ref]
204                 ],
205                 *[
206                     LintFix(""edit"", alias, alias.edit(alias_info.table_ref.raw))
207                     for alias in [alias_info.alias_identifier_ref, *ids_refs]
208                 ],
209             ]
210 
211             violation_buff.append(
212                 LintResult(
213                     anchor=alias_info.alias_identifier_ref,
214                     description=""Avoid using aliases in join condition"",
215                     fixes=fixes,
216                 )
217             )
218 
219         return violation_buff or None
220 
[end of src/sqlfluff/rules/L031.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",sqlfluff/sqlfluff,14e1a23a3166b9a645a16de96f694c77a5d4abb7,"TSQL - L031 incorrectly triggers ""Avoid using aliases in join condition"" when no join present
## Expected Behaviour

Both of these queries should pass, the only difference is the addition of a table alias 'a':

1/ no alias

```
SELECT [hello]
FROM
    mytable
```

2/ same query with alias

```
SELECT a.[hello]
FROM
    mytable AS a
```

## Observed Behaviour

1/ passes
2/ fails with: L031: Avoid using aliases in join condition.

But there is no join condition :-)

## Steps to Reproduce

Lint queries above

## Dialect

TSQL

## Version

sqlfluff 0.6.9
Python 3.6.9

## Configuration

N/A
","Actually, re-reading the docs I think this is the intended behaviour... closing",2021-10-13T11:35:29Z,"<patch>
diff --git a/src/sqlfluff/rules/L031.py b/src/sqlfluff/rules/L031.py
--- a/src/sqlfluff/rules/L031.py
+++ b/src/sqlfluff/rules/L031.py
@@ -211,7 +211,7 @@ def _lint_aliases_in_join(
             violation_buff.append(
                 LintResult(
                     anchor=alias_info.alias_identifier_ref,
-                    description=""Avoid using aliases in join condition"",
+                    description=""Avoid aliases in from clauses and join conditions."",
                     fixes=fixes,
                 )
             )

</patch>","diff --git a/test/cli/commands_test.py b/test/cli/commands_test.py
--- a/test/cli/commands_test.py
+++ b/test/cli/commands_test.py
@@ -49,7 +49,7 @@ def invoke_assert_code(
 expected_output = """"""== [test/fixtures/linter/indentation_error_simple.sql] FAIL
 L:   2 | P:   4 | L003 | Indentation not hanging or a multiple of 4 spaces
 L:   5 | P:  10 | L010 | Keywords must be consistently upper case.
-L:   5 | P:  13 | L031 | Avoid using aliases in join condition
+L:   5 | P:  13 | L031 | Avoid aliases in from clauses and join conditions.
 """"""
 
 
",0.6,"[""test/cli/commands_test.py::test__cli__command_directed""]","[""test/cli/commands_test.py::test__cli__command_dialect"", ""test/cli/commands_test.py::test__cli__command_dialect_legacy"", ""test/cli/commands_test.py::test__cli__command_lint_stdin[command0]"", ""test/cli/commands_test.py::test__cli__command_lint_stdin[command1]"", ""test/cli/commands_test.py::test__cli__command_lint_stdin[command2]"", ""test/cli/commands_test.py::test__cli__command_lint_stdin[command3]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command0]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command1]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command2]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command3]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command4]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command5]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command6]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command7]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command8]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command9]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command10]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command11]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command12]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command13]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command14]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command15]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command16]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command17]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command18]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command19]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command20]"", ""test/cli/commands_test.py::test__cli__command_lint_parse[command21]"", ""test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command0-1]"", ""test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command1-1]"", ""test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode[command2-1]"", ""test/cli/commands_test.py::test__cli__command_lint_warning_explicit_file_ignored"", ""test/cli/commands_test.py::test__cli__command_lint_skip_ignore_files"", ""test/cli/commands_test.py::test__cli__command_versioning"", ""test/cli/commands_test.py::test__cli__command_version"", ""test/cli/commands_test.py::test__cli__command_rules"", ""test/cli/commands_test.py::test__cli__command_dialects"", ""test/cli/commands_test.py::test__cli__command__fix[L001-test/fixtures/linter/indentation_errors.sql]"", ""test/cli/commands_test.py::test__cli__command__fix[L008-test/fixtures/linter/whitespace_errors.sql]"", ""test/cli/commands_test.py::test__cli__command__fix[L008-test/fixtures/linter/indentation_errors.sql]"", ""test/cli/commands_test.py::test__cli__command__fix[L003-test/fixtures/linter/indentation_error_hard.sql]"", ""test/cli/commands_test.py::test__cli__command_fix_stdin[select"", ""test/cli/commands_test.py::test__cli__command_fix_stdin["", ""test/cli/commands_test.py::test__cli__command_fix_stdin[SELECT"", ""test/cli/commands_test.py::test__cli__command_fix_stdin_logging_to_stderr"", ""test/cli/commands_test.py::test__cli__command_fix_stdin_safety"", ""test/cli/commands_test.py::test__cli__command_fix_stdin_error_exit_code[create"", ""test/cli/commands_test.py::test__cli__command_fix_stdin_error_exit_code[select"", ""test/cli/commands_test.py::test__cli__command__fix_no_force[L001-test/fixtures/linter/indentation_errors.sql-y-0-0]"", ""test/cli/commands_test.py::test__cli__command__fix_no_force[L001-test/fixtures/linter/indentation_errors.sql-n-65-1]"", ""test/cli/commands_test.py::test__cli__command_parse_serialize_from_stdin[yaml]"", ""test/cli/commands_test.py::test__cli__command_parse_serialize_from_stdin[json]"", ""test/cli/commands_test.py::test__cli__command_lint_serialize_from_stdin[select"", ""test/cli/commands_test.py::test__cli__command_lint_serialize_from_stdin[SElect"", ""test/cli/commands_test.py::test__cli__command_fail_nice_not_found[command0]"", ""test/cli/commands_test.py::test__cli__command_fail_nice_not_found[command1]"", ""test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[yaml]"", ""test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[json]"", ""test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files[github-annotation]"", ""test/cli/commands_test.py::test__cli__command_lint_serialize_github_annotation"", ""test/cli/commands_test.py::test___main___help"", ""test/cli/commands_test.py::test_encoding[utf-8-ascii]"", ""test/cli/commands_test.py::test_encoding[utf-8-sig-UTF-8-SIG]"", ""test/cli/commands_test.py::test_encoding[utf-32-UTF-32]""]",67023b85c41d23d6c6d69812a41b207c4f8a9331
sqlfluff__sqlfluff-2419,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
Rule L060 could give a specific error message
At the moment rule L060 flags something like this:

```
L:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL' or 'NVL'.
```

Since we likely know the wrong word, it might be nice to actually flag that instead of both `IFNULL` and `NVL` - like most of the other rules do.

That is it should flag this:

```
L:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.
```
 Or this:

```
L:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'.
```

As appropriate.

What do you think @jpy-git ?


</issue>
<code>
[start of README.md]
1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)
2 
3 # The SQL Linter for Humans
4 
5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)
6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)
10 
11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)
12 [![CI Tests](https://github.com/sqlfluff/sqlfluff/workflows/CI%20Tests/badge.svg)](https://github.com/sqlfluff/sqlfluff/actions?query=workflow%3A%22CI+Tests%22)
13 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)
14 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)
15 
16 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.
17 
18 ## Dialects Supported
19 
20 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):
21 
22 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition
23 - [BigQuery](https://cloud.google.com/bigquery/)
24 - [Exasol](https://www.exasol.com/)
25 - [Hive](https://hive.apache.org/)
26 - [MySQL](https://www.mysql.com/)
27 - [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)
28 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)
29 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)
30 - [Snowflake](https://www.snowflake.com/)
31 - [Spark3](https://spark.apache.org/docs/latest/)
32 - [SQLite](https://www.sqlite.org/)
33 - [Teradata](https://www.teradata.com/)
34 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)
35 
36 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.
37 
38 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.
39 
40 ## Templates Supported
41 
42 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).
43 
44 **SQLFluff** supports the following templates:
45 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)
46 - [dbt](https://www.getdbt.com/)
47 
48 Again, please raise issues if you wish to support more templating languages/syntaxes.
49 
50 # Getting Started
51 
52 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.
53 
54 ```shell
55 $ pip install sqlfluff
56 $ echo ""  SELECT a  +  b FROM tbl;  "" > test.sql
57 $ sqlfluff lint test.sql
58 == [test.sql] FAIL
59 L:   1 | P:   1 | L003 | Single indentation uses a number of spaces not a multiple of 4
60 L:   1 | P:  14 | L006 | Operators should be surrounded by a single space unless at the start/end of a line
61 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace
62 ```
63 
64 Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff) or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).
65 
66 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).
67 
68 # Documentation
69 
70 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.
71 
72 # Releases
73 
74 **SQLFluff** is in beta phase - expect the tool to change significantly with potentially non-backwards compatible API and configuration changes in future releases. If you would like to join in please consider [contributing](CONTRIBUTING.md).
75 
76 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).
77 
78 # SQLFluff on Slack
79 
80 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!
81 
82 # SQLFluff on Twitter
83 
84 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.
85 
86 # Contributing
87 
88 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.
89 
90 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).
91 
92 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).
93 
[end of README.md]
[start of src/sqlfluff/rules/L060.py]
1 """"""Implementation of Rule L060.""""""
2 
3 from typing import Optional
4 
5 from sqlfluff.core.parser.segments.raw import CodeSegment
6 from sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext
7 from sqlfluff.core.rules.doc_decorators import document_fix_compatible
8 
9 
10 @document_fix_compatible
11 class Rule_L060(BaseRule):
12     """"""Use ``COALESCE`` instead of ``IFNULL`` or ``NVL``.
13 
14     | **Anti-pattern**
15     | ``IFNULL`` or ``NVL`` are used to fill ``NULL`` values.
16 
17     .. code-block:: sql
18 
19         SELECT ifnull(foo, 0) AS bar,
20         FROM baz;
21 
22         SELECT nvl(foo, 0) AS bar,
23         FROM baz;
24 
25     | **Best practice**
26     | Use ``COALESCE`` instead.
27     | ``COALESCE`` is universally supported,
28     | whereas Redshift doesn't support ``IFNULL``
29     | and BigQuery doesn't support ``NVL``.
30     | Additionally ``COALESCE`` is more flexible
31     | and accepts an arbitrary number of arguments.
32 
33     .. code-block:: sql
34 
35         SELECT coalesce(foo, 0) AS bar,
36         FROM baz;
37 
38     """"""
39 
40     def _eval(self, context: RuleContext) -> Optional[LintResult]:
41         """"""Use ``COALESCE`` instead of ``IFNULL`` or ``NVL``.""""""
42         # We only care about function names.
43         if context.segment.name != ""function_name_identifier"":
44             return None
45 
46         # Only care if the function is ``IFNULL`` or ``NVL``.
47         if context.segment.raw_upper not in {""IFNULL"", ""NVL""}:
48             return None
49 
50         # Create fix to replace ``IFNULL`` or ``NVL`` with ``COALESCE``.
51         fix = LintFix.replace(
52             context.segment,
53             [
54                 CodeSegment(
55                     raw=""COALESCE"",
56                     name=""function_name_identifier"",
57                     type=""function_name_identifier"",
58                 )
59             ],
60         )
61 
62         return LintResult(context.segment, [fix])
63 
[end of src/sqlfluff/rules/L060.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",sqlfluff/sqlfluff,f1dba0e1dd764ae72d67c3d5e1471cf14d3db030,"Rule L060 could give a specific error message
At the moment rule L060 flags something like this:

```
L:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL' or 'NVL'.
```

Since we likely know the wrong word, it might be nice to actually flag that instead of both `IFNULL` and `NVL` - like most of the other rules do.

That is it should flag this:

```
L:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.
```
 Or this:

```
L:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'.
```

As appropriate.

What do you think @jpy-git ?

","@tunetheweb Yeah definitely, should be a pretty quick change ðŸ˜Š",2022-01-22T12:21:52Z,"<patch>
diff --git a/src/sqlfluff/rules/L060.py b/src/sqlfluff/rules/L060.py
--- a/src/sqlfluff/rules/L060.py
+++ b/src/sqlfluff/rules/L060.py
@@ -59,4 +59,8 @@ def _eval(self, context: RuleContext) -> Optional[LintResult]:
             ],
         )
 
-        return LintResult(context.segment, [fix])
+        return LintResult(
+            anchor=context.segment,
+            fixes=[fix],
+            description=f""Use 'COALESCE' instead of '{context.segment.raw_upper}'."",
+        )

</patch>","diff --git a/test/rules/std_L060_test.py b/test/rules/std_L060_test.py
new file mode 100644
--- /dev/null
+++ b/test/rules/std_L060_test.py
@@ -0,0 +1,12 @@
+""""""Tests the python routines within L060.""""""
+import sqlfluff
+
+
+def test__rules__std_L060_raised() -> None:
+    """"""L060 is raised for use of ``IFNULL`` or ``NVL``.""""""
+    sql = ""SELECT\n\tIFNULL(NULL, 100),\n\tNVL(NULL,100);""
+    result = sqlfluff.lint(sql, rules=[""L060""])
+
+    assert len(result) == 2
+    assert result[0][""description""] == ""Use 'COALESCE' instead of 'IFNULL'.""
+    assert result[1][""description""] == ""Use 'COALESCE' instead of 'NVL'.""
",0.8,"[""test/rules/std_L060_test.py::test__rules__std_L060_raised""]",[],a5c4eae4e3e419fe95460c9afd9cf39a35a470c4
sqlfluff__sqlfluff-1733,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
Extra space when first field moved to new line in a WITH statement
Note, the query below uses a `WITH` statement. If I just try to fix the SQL within the CTE, this works fine.

Given the following SQL:

```sql
WITH example AS (
    SELECT my_id,
        other_thing,
        one_more
    FROM
        my_table
)

SELECT *
FROM example
```

## Expected Behaviour

after running `sqlfluff fix` I'd expect (`my_id` gets moved down and indented properly):

```sql
WITH example AS (
    SELECT
        my_id,
        other_thing,
        one_more
    FROM
        my_table
)

SELECT *
FROM example
```

## Observed Behaviour

after running `sqlfluff fix` we get (notice that `my_id` is indented one extra space)

```sql
WITH example AS (
    SELECT
         my_id,
        other_thing,
        one_more
    FROM
        my_table
)

SELECT *
FROM example
```

## Steps to Reproduce

Noted above. Create a file with the initial SQL and fun `sqfluff fix` on it.

## Dialect

Running with default config.

## Version
Include the output of `sqlfluff --version` along with your Python version

sqlfluff, version 0.7.0
Python 3.7.5

## Configuration

Default config.


</issue>
<code>
[start of README.md]
1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)
2 
3 # The SQL Linter for Humans
4 
5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)
6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)
10 
11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)
12 [![Requirements Status](https://img.shields.io/requires/github/sqlfluff/sqlfluff.svg?style=flat-square)](https://requires.io/github/sqlfluff/sqlfluff/requirements/?branch=main)
13 [![CI Tests](https://github.com/sqlfluff/sqlfluff/workflows/CI%20Tests/badge.svg)](https://github.com/sqlfluff/sqlfluff/actions?query=workflow%3A%22CI+Tests%22)
14 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)
15 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)
16 
17 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.
18 
19 ## Dialects Supported
20 
21 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):
22 
23 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition
24 - [BigQuery](https://cloud.google.com/bigquery/)
25 - [Exasol](https://www.exasol.com/)
26 - [Hive](https://hive.apache.org/)
27 - [MySQL](https://www.mysql.com/)
28 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)
29 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)
30 - [Snowflake](https://www.snowflake.com/)
31 - [Spark3](https://spark.apache.org/docs/latest/)
32 - [SQLite](https://www.sqlite.org/)
33 - [Teradata](https://www.teradata.com/)
34 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)
35 
36 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.
37 
38 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.
39 
40 ## Templates Supported
41 
42 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).
43 
44 **SQLFluff** supports the following templates:
45 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)
46 - [dbt](https://www.getdbt.com/)
47 
48 Again, please raise issues if you wish to support more templating languages/syntaxes.
49 
50 # Getting Started
51 
52 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.
53 
54 ```shell
55 $ pip install sqlfluff
56 $ echo ""  SELECT a  +  b FROM tbl;  "" > test.sql
57 $ sqlfluff lint test.sql
58 == [test.sql] FAIL
59 L:   1 | P:   1 | L003 | Single indentation uses a number of spaces not a multiple of 4
60 L:   1 | P:  14 | L006 | Operators should be surrounded by a single space unless at the start/end of a line
61 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace
62 ```
63 
64 You can also have a play using [**SQLFluff online**](https://online.sqlfluff.com/).
65 
66 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).
67 
68 # Documentation
69 
70 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.
71 
72 # Releases
73 
74 **SQLFluff** is in beta phase - expect the tool to change significantly with potentially non-backwards compatible API and configuration changes in future releases. If you would like to join in please consider [contributing](CONTRIBUTING.md).
75 
76 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).
77 
78 # SQLFluff on Slack
79 
80 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!
81 
82 # SQLFluff on Twitter
83 
84 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.
85 
86 # Contributing
87 
88 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.
89 
90 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).
91 
92 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).
93 
[end of README.md]
[start of src/sqlfluff/rules/L039.py]
1 """"""Implementation of Rule L039.""""""
2 from typing import List, Optional
3 
4 from sqlfluff.core.parser import WhitespaceSegment
5 
6 from sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext
7 from sqlfluff.core.rules.doc_decorators import document_fix_compatible
8 
9 
10 @document_fix_compatible
11 class Rule_L039(BaseRule):
12     """"""Unnecessary whitespace found.
13 
14     | **Anti-pattern**
15 
16     .. code-block:: sql
17 
18         SELECT
19             a,        b
20         FROM foo
21 
22     | **Best practice**
23     | Unless an indent or preceding a comment, whitespace should
24     | be a single space.
25 
26     .. code-block:: sql
27 
28         SELECT
29             a, b
30         FROM foo
31     """"""
32 
33     def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:
34         """"""Unnecessary whitespace.""""""
35         # For the given segment, lint whitespace directly within it.
36         prev_newline = True
37         prev_whitespace = None
38         violations = []
39         for seg in context.segment.segments:
40             if seg.is_type(""newline""):
41                 prev_newline = True
42                 prev_whitespace = None
43             elif seg.is_type(""whitespace""):
44                 # This is to avoid indents
45                 if not prev_newline:
46                     prev_whitespace = seg
47                 prev_newline = False
48             elif seg.is_type(""comment""):
49                 prev_newline = False
50                 prev_whitespace = None
51             else:
52                 if prev_whitespace:
53                     if prev_whitespace.raw != "" "":
54                         violations.append(
55                             LintResult(
56                                 anchor=prev_whitespace,
57                                 fixes=[
58                                     LintFix(
59                                         ""edit"",
60                                         prev_whitespace,
61                                         WhitespaceSegment(),
62                                     )
63                                 ],
64                             )
65                         )
66                 prev_newline = False
67                 prev_whitespace = None
68         return violations or None
69 
[end of src/sqlfluff/rules/L039.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",sqlfluff/sqlfluff,a1579a16b1d8913d9d7c7d12add374a290bcc78c,"Extra space when first field moved to new line in a WITH statement
Note, the query below uses a `WITH` statement. If I just try to fix the SQL within the CTE, this works fine.

Given the following SQL:

```sql
WITH example AS (
    SELECT my_id,
        other_thing,
        one_more
    FROM
        my_table
)

SELECT *
FROM example
```

## Expected Behaviour

after running `sqlfluff fix` I'd expect (`my_id` gets moved down and indented properly):

```sql
WITH example AS (
    SELECT
        my_id,
        other_thing,
        one_more
    FROM
        my_table
)

SELECT *
FROM example
```

## Observed Behaviour

after running `sqlfluff fix` we get (notice that `my_id` is indented one extra space)

```sql
WITH example AS (
    SELECT
         my_id,
        other_thing,
        one_more
    FROM
        my_table
)

SELECT *
FROM example
```

## Steps to Reproduce

Noted above. Create a file with the initial SQL and fun `sqfluff fix` on it.

## Dialect

Running with default config.

## Version
Include the output of `sqlfluff --version` along with your Python version

sqlfluff, version 0.7.0
Python 3.7.5

## Configuration

Default config.

","Does running `sqlfluff fix` again correct the SQL?
@tunetheweb yes, yes it does. Is that something that the user is supposed to do (run it multiple times) or is this indeed a bug?
Ideally not, but there are some circumstances where itâ€™s understandable that would happen. This however seems an easy enough example where it should not happen.
This appears to be a combination of rules L036, L003, and L039 not playing nicely together.

The original error is rule L036 and it produces this:

```sql
WITH example AS (
    SELECT
my_id,
        other_thing,
        one_more
    FROM
        my_table
)

SELECT *
FROM example
```

That is, it moves the `my_id` down to the newline but does not even try to fix the indentation.

Then we have another run through and L003 spots the lack of indentation and fixes it by adding the first set of whitespace:

```sql
WITH example AS (
    SELECT
    my_id,
        other_thing,
        one_more
    FROM
        my_table
)

SELECT *
FROM example
```

Then we have another run through and L003 spots that there still isn't enough indentation and fixes it by adding the second set of whitespace:

```sql
WITH example AS (
    SELECT
        my_id,
        other_thing,
        one_more
    FROM
        my_table
)

SELECT *
FROM example
```

At this point we're all good.

However then L039 has a look. It never expects two sets of whitespace following a new line and is specifically coded to only assume one set of spaces (which it normally would be if the other rules hadn't interfered as it would be parsed as one big space), so it think's the second set is too much indentation, so it replaces it with a single space.

Then another run and L003 and the whitespace back in so we end up with two indents, and a single space.

Luckily the fix is easier than that explanation. PR coming up...

",2021-10-22T18:23:33Z,"<patch>
diff --git a/src/sqlfluff/rules/L039.py b/src/sqlfluff/rules/L039.py
--- a/src/sqlfluff/rules/L039.py
+++ b/src/sqlfluff/rules/L039.py
@@ -44,7 +44,9 @@ def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:
                 # This is to avoid indents
                 if not prev_newline:
                     prev_whitespace = seg
-                prev_newline = False
+                # We won't set prev_newline to False, just for whitespace
+                # in case there's multiple indents, inserted by other rule
+                # fixes (see #1713)
             elif seg.is_type(""comment""):
                 prev_newline = False
                 prev_whitespace = None

</patch>","diff --git a/test/rules/std_L003_L036_L039_combo_test.py b/test/rules/std_L003_L036_L039_combo_test.py
new file mode 100644
--- /dev/null
+++ b/test/rules/std_L003_L036_L039_combo_test.py
@@ -0,0 +1,36 @@
+""""""Tests issue #1373 doesn't reoccur.
+
+The combination of L003 (incorrect indentation), L036 (select targets),
+and L039 (unnecessary white space) can result in incorrect indentation.
+""""""
+
+import sqlfluff
+
+
+def test__rules__std_L003_L036_L039():
+    """"""Verify that double indents don't flag L039.""""""
+    sql = """"""
+    WITH example AS (
+        SELECT my_id,
+            other_thing,
+            one_more
+        FROM
+            my_table
+    )
+
+    SELECT *
+    FROM example\n""""""
+    fixed_sql = """"""
+    WITH example AS (
+        SELECT
+            my_id,
+            other_thing,
+            one_more
+        FROM
+            my_table
+    )
+
+    SELECT *
+    FROM example\n""""""
+    result = sqlfluff.fix(sql)
+    assert result == fixed_sql
diff --git a/test/rules/std_L016_L36_combo.py b/test/rules/std_L016_L36_combo_test.py
similarity index 100%
rename from test/rules/std_L016_L36_combo.py
rename to test/rules/std_L016_L36_combo_test.py
",0.6,"[""test/rules/std_L003_L036_L039_combo_test.py::test__rules__std_L003_L036_L039""]","[""test/rules/std_L016_L36_combo_test.py::test__rules__std_L016_L036_long_line_lint"", ""test/rules/std_L016_L36_combo_test.py::test__rules__std_L016_L036_long_line_fix"", ""test/rules/std_L016_L36_combo_test.py::test__rules__std_L016_L036_long_line_fix2""]",67023b85c41d23d6c6d69812a41b207c4f8a9331
sqlfluff__sqlfluff-1517,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
""Dropped elements in sequence matching"" when doubled semicolon
## Expected Behaviour
Frankly, I'm not sure whether it (doubled `;`) should be just ignored or rather some specific rule should be triggered.
## Observed Behaviour
```console
(.venv) ?master ~/prod/_inne/sqlfluff> echo ""select id from tbl;;"" | sqlfluff lint -
Traceback (most recent call last):
  File ""/home/adam/prod/_inne/sqlfluff/.venv/bin/sqlfluff"", line 11, in <module>
    load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')()
  File ""/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py"", line 1137, in __call__
    return self.main(*args, **kwargs)
  File ""/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py"", line 1062, in main
    rv = self.invoke(ctx)
  File ""/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py"", line 1668, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py"", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py"", line 763, in invoke
    return __callback(*args, **kwargs)
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/cli/commands.py"", line 347, in lint
    result = lnt.lint_string_wrapped(sys.stdin.read(), fname=""stdin"")
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py"", line 789, in lint_string_wrapped
    linted_path.add(self.lint_string(string, fname=fname, fix=fix))
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py"", line 668, in lint_string
    parsed = self.parse_string(in_str=in_str, fname=fname, config=config)
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py"", line 607, in parse_string
    return self.parse_rendered(rendered, recurse=recurse)
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py"", line 313, in parse_rendered
    parsed, pvs = cls._parse_tokens(
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py"", line 190, in _parse_tokens
    parsed: Optional[BaseSegment] = parser.parse(
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/parser.py"", line 32, in parse
    parsed = root_segment.parse(parse_context=ctx)
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/segments/base.py"", line 821, in parse
    check_still_complete(segments, m.matched_segments, m.unmatched_segments)
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/helpers.py"", line 30, in check_still_complete
    raise RuntimeError(
RuntimeError: Dropped elements in sequence matching! 'select id from tbl;;' != ';'

```
## Steps to Reproduce
Run 
```console
echo ""select id from tbl;;"" | sqlfluff lint -
```
## Dialect
default (ansi)
## Version
```
sqlfluff, version 0.6.6
Python 3.9.5
```
## Configuration
None


</issue>
<code>
[start of README.md]
1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)
2 
3 # The SQL Linter for Humans
4 
5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)
6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)
10 
11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)
12 [![Requirements Status](https://img.shields.io/requires/github/sqlfluff/sqlfluff.svg?style=flat-square)](https://requires.io/github/sqlfluff/sqlfluff/requirements/?branch=main)
13 [![CI Tests](https://github.com/sqlfluff/sqlfluff/workflows/CI%20Tests/badge.svg)](https://github.com/sqlfluff/sqlfluff/actions?query=workflow%3A%22CI+Tests%22)
14 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)
15 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)
16 
17 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.
18 
19 ## Dialects Supported
20 
21 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):
22 
23 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition
24 - [BigQuery](https://cloud.google.com/bigquery/)
25 - [Exasol](https://www.exasol.com/)
26 - [Hive](https://hive.apache.org/)
27 - [MySQL](https://www.mysql.com/)
28 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)
29 - [Snowflake](https://www.snowflake.com/)
30 - [SQLite](https://www.sqlite.org/)
31 - [Teradata](https://www.teradata.com/)
32 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)
33 
34 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.
35 
36 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.
37 
38 ## Templates Supported
39 
40 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).
41 
42 **SQLFluff** supports the following templates:
43 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)
44 - [dbt](https://www.getdbt.com/)
45 
46 Again, please raise issues if you wish to support more templating languages/syntaxes.
47 
48 # Getting Started
49 
50 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.
51 
52 ```shell
53 $ pip install sqlfluff
54 $ echo ""  SELECT a  +  b FROM tbl;  "" > test.sql
55 $ sqlfluff lint test.sql
56 == [test.sql] FAIL
57 L:   1 | P:   1 | L003 | Single indentation uses a number of spaces not a multiple of 4
58 L:   1 | P:  14 | L006 | Operators should be surrounded by a single space unless at the start/end of a line
59 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace
60 ```
61 
62 You can also have a play using [**SQLFluff online**](https://online.sqlfluff.com/).
63 
64 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).
65 
66 # Documentation
67 
68 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.
69 
70 # Releases
71 
72 **SQLFluff** is in beta phase - expect the tool to change significantly with potentially non-backwards compatible API and configuration changes in future releases. If you would like to join in please consider [contributing](CONTRIBUTING.md).
73 
74 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).
75 
76 # SQLFluff on Slack
77 
78 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!
79 
80 # SQLFluff on Twitter
81 
82 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.
83 
84 # Contributing
85 
86 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.
87 
88 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).
89 
90 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).
91 
[end of README.md]
[start of src/sqlfluff/core/parser/helpers.py]
1 """"""Helpers for the parser module.""""""
2 
3 from typing import Tuple, List, Any, Iterator, TYPE_CHECKING
4 
5 from sqlfluff.core.string_helpers import curtail_string
6 
7 if TYPE_CHECKING:
8     from sqlfluff.core.parser.segments import BaseSegment  # pragma: no cover
9 
10 
11 def join_segments_raw(segments: Tuple[""BaseSegment"", ...]) -> str:
12     """"""Make a string from the joined `raw` attributes of an iterable of segments.""""""
13     return """".join(s.raw for s in segments)
14 
15 
16 def join_segments_raw_curtailed(segments: Tuple[""BaseSegment"", ...], length=20) -> str:
17     """"""Make a string up to a certain length from an iterable of segments.""""""
18     return curtail_string(join_segments_raw(segments), length=length)
19 
20 
21 def check_still_complete(
22     segments_in: Tuple[""BaseSegment"", ...],
23     matched_segments: Tuple[""BaseSegment"", ...],
24     unmatched_segments: Tuple[""BaseSegment"", ...],
25 ) -> bool:
26     """"""Check that the segments in are the same as the segments out.""""""
27     initial_str = join_segments_raw(segments_in)
28     current_str = join_segments_raw(matched_segments + unmatched_segments)
29     if initial_str != current_str:  # pragma: no cover
30         raise RuntimeError(
31             ""Dropped elements in sequence matching! {!r} != {!r}"".format(
32                 initial_str, current_str
33             )
34         )
35     return True
36 
37 
38 def trim_non_code_segments(
39     segments: Tuple[""BaseSegment"", ...]
40 ) -> Tuple[
41     Tuple[""BaseSegment"", ...], Tuple[""BaseSegment"", ...], Tuple[""BaseSegment"", ...]
42 ]:
43     """"""Take segments and split off surrounding non-code segments as appropriate.
44 
45     We use slices to avoid creating too many unnecessary tuples.
46     """"""
47     pre_idx = 0
48     seg_len = len(segments)
49     post_idx = seg_len
50 
51     if segments:
52         seg_len = len(segments)
53 
54         # Trim the start
55         while pre_idx < seg_len and not segments[pre_idx].is_code:
56             pre_idx += 1
57 
58         # Trim the end
59         while post_idx > pre_idx and not segments[post_idx - 1].is_code:
60             post_idx -= 1
61 
62     return segments[:pre_idx], segments[pre_idx:post_idx], segments[post_idx:]
63 
64 
65 def iter_indices(seq: List, val: Any) -> Iterator[int]:
66     """"""Iterate all indices in a list that val occurs at.
67 
68     Args:
69         seq (list): A list to look for indices in.
70         val: What to look for.
71 
72     Yields:
73         int: The index of val in seq.
74 
75     Examples:
76         The function works like str.index() but iterates all
77         the results rather than returning the first.
78 
79         >>> print([i for i in iter_indices([1, 0, 2, 3, 2], 2)])
80         [2, 4]
81     """"""
82     for idx, el in enumerate(seq):
83         if el == val:
84             yield idx
85 
[end of src/sqlfluff/core/parser/helpers.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",sqlfluff/sqlfluff,304a197829f98e7425a46d872ada73176137e5ae,"""Dropped elements in sequence matching"" when doubled semicolon
## Expected Behaviour
Frankly, I'm not sure whether it (doubled `;`) should be just ignored or rather some specific rule should be triggered.
## Observed Behaviour
```console
(.venv) ?master ~/prod/_inne/sqlfluff> echo ""select id from tbl;;"" | sqlfluff lint -
Traceback (most recent call last):
  File ""/home/adam/prod/_inne/sqlfluff/.venv/bin/sqlfluff"", line 11, in <module>
    load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')()
  File ""/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py"", line 1137, in __call__
    return self.main(*args, **kwargs)
  File ""/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py"", line 1062, in main
    rv = self.invoke(ctx)
  File ""/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py"", line 1668, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py"", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py"", line 763, in invoke
    return __callback(*args, **kwargs)
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/cli/commands.py"", line 347, in lint
    result = lnt.lint_string_wrapped(sys.stdin.read(), fname=""stdin"")
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py"", line 789, in lint_string_wrapped
    linted_path.add(self.lint_string(string, fname=fname, fix=fix))
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py"", line 668, in lint_string
    parsed = self.parse_string(in_str=in_str, fname=fname, config=config)
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py"", line 607, in parse_string
    return self.parse_rendered(rendered, recurse=recurse)
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py"", line 313, in parse_rendered
    parsed, pvs = cls._parse_tokens(
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py"", line 190, in _parse_tokens
    parsed: Optional[BaseSegment] = parser.parse(
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/parser.py"", line 32, in parse
    parsed = root_segment.parse(parse_context=ctx)
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/segments/base.py"", line 821, in parse
    check_still_complete(segments, m.matched_segments, m.unmatched_segments)
  File ""/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/helpers.py"", line 30, in check_still_complete
    raise RuntimeError(
RuntimeError: Dropped elements in sequence matching! 'select id from tbl;;' != ';'

```
## Steps to Reproduce
Run 
```console
echo ""select id from tbl;;"" | sqlfluff lint -
```
## Dialect
default (ansi)
## Version
```
sqlfluff, version 0.6.6
Python 3.9.5
```
## Configuration
None

","Sounds similar to #1458 where we should handle ""empty"" statement/files better?
Nope, that's the different issue. I doubt that solving one of them would help in other one. I think both issues should stay, just in the case.
But what do you think @tunetheweb - should it just ignore these `;;` or raise something like `Found unparsable section:`? 
Just tested and in BigQuery it's an error.
Interestingly Oracle is fine with it.

I think it should be raised as `Found unparsable section`.",2021-10-06T07:57:35Z,"<patch>
diff --git a/src/sqlfluff/core/parser/helpers.py b/src/sqlfluff/core/parser/helpers.py
--- a/src/sqlfluff/core/parser/helpers.py
+++ b/src/sqlfluff/core/parser/helpers.py
@@ -2,6 +2,7 @@
 
 from typing import Tuple, List, Any, Iterator, TYPE_CHECKING
 
+from sqlfluff.core.errors import SQLParseError
 from sqlfluff.core.string_helpers import curtail_string
 
 if TYPE_CHECKING:
@@ -26,11 +27,11 @@ def check_still_complete(
     """"""Check that the segments in are the same as the segments out.""""""
     initial_str = join_segments_raw(segments_in)
     current_str = join_segments_raw(matched_segments + unmatched_segments)
-    if initial_str != current_str:  # pragma: no cover
-        raise RuntimeError(
-            ""Dropped elements in sequence matching! {!r} != {!r}"".format(
-                initial_str, current_str
-            )
+
+    if initial_str != current_str:
+        raise SQLParseError(
+            f""Could not parse: {current_str}"",
+            segment=unmatched_segments[0],
         )
     return True
 

</patch>","diff --git a/test/dialects/ansi_test.py b/test/dialects/ansi_test.py
--- a/test/dialects/ansi_test.py
+++ b/test/dialects/ansi_test.py
@@ -3,7 +3,7 @@
 import pytest
 import logging
 
-from sqlfluff.core import FluffConfig, Linter
+from sqlfluff.core import FluffConfig, Linter, SQLParseError
 from sqlfluff.core.parser import Lexer
 
 
@@ -214,3 +214,29 @@ def test__dialect__ansi_parse_indented_joins(sql_string, indented_joins, meta_lo
         idx for idx, raw_seg in enumerate(parsed.tree.iter_raw_seg()) if raw_seg.is_meta
     )
     assert res_meta_locs == meta_loc
+
+
+@pytest.mark.parametrize(
+    ""raw,expected_message"",
+    [
+        ("";;"", ""Line 1, Position 1: Found unparsable section: ';;'""),
+        (""select id from tbl;"", """"),
+        (""select id from tbl;;"", ""Could not parse: ;""),
+        (""select id from tbl;;;;;;"", ""Could not parse: ;;;;;""),
+        (""select id from tbl;select id2 from tbl2;"", """"),
+        (
+            ""select id from tbl;;select id2 from tbl2;"",
+            ""Could not parse: ;select id2 from tbl2;"",
+        ),
+    ],
+)
+def test__dialect__ansi_multiple_semicolons(raw: str, expected_message: str) -> None:
+    """"""Multiple semicolons should be properly handled.""""""
+    lnt = Linter()
+    parsed = lnt.parse_string(raw)
+
+    assert len(parsed.violations) == (1 if expected_message else 0)
+    if expected_message:
+        violation = parsed.violations[0]
+        assert isinstance(violation, SQLParseError)
+        assert violation.desc() == expected_message
",0.6,"[""test/dialects/ansi_test.py::test__dialect__ansi_multiple_semicolons[select""]","[""test/dialects/ansi_test.py::test__dialect__ansi__file_lex[a"", ""test/dialects/ansi_test.py::test__dialect__ansi__file_lex[b.c-res1]"", ""test/dialects/ansi_test.py::test__dialect__ansi__file_lex[abc"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectKeywordSegment-select]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[NakedIdentifierSegment-online_sales]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[BareFunctionSegment-current_timestamp]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[FunctionSegment-current_timestamp()]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[NumericLiteralSegment-1000.0]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-online_sales"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[IntervalExpressionSegment-INTERVAL"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-CASE"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-CAST(ROUND(online_sales"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-name"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment-MIN"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-DATE_ADD(CURRENT_DATE('America/New_York'),"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-my_array[1]]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-my_array[OFFSET(1)]]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-my_array[5:8]]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-4"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-bits[OFFSET(0)]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment-(count_18_24"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-count_18_24"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectStatementSegment-SELECT"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment-t.val/t.id]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment-CAST(num"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment-a.*]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment-a.b.*]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment-a.b.c.*]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ObjectReferenceSegment-a..c.*]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment--some_variable]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment--"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-concat(left(uaid,"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-c"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment-c"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[ExpressionSegment-NULL::INT]"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[SelectClauseElementSegment-NULL::INT"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses[TruncateStatementSegment-TRUNCATE"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_not_match[ObjectReferenceSegment-\\n"", ""test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_not_parse[SELECT"", ""test/dialects/ansi_test.py::test__dialect__ansi_is_whitespace"", ""test/dialects/ansi_test.py::test__dialect__ansi_parse_indented_joins[select"", ""test/dialects/ansi_test.py::test__dialect__ansi_multiple_semicolons[;;-Line""]",67023b85c41d23d6c6d69812a41b207c4f8a9331
sqlfluff__sqlfluff-1763,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
dbt postgres fix command errors with UnicodeEncodeError and also wipes the .sql file
_If this is a parsing or linting issue, please include a minimal SQL example which reproduces the issue, along with the `sqlfluff parse` output, `sqlfluff lint` output and `sqlfluff fix` output when relevant._

## Expected Behaviour
Violation failure notice at a minimum, without wiping the file. Would like a way to ignore the known error at a minimum as --noqa is not getting past this. Actually would expect --noqa to totally ignore this.

## Observed Behaviour
Reported error: `UnicodeEncodeError: 'charmap' codec can't encode character '\u2192' in position 120: character maps to <undefined>`

## Steps to Reproduce
SQL file:
```sql
SELECT
    reacted_table_name_right.descendant_id AS category_id,
    string_agg(redacted_table_name_left.name, ' â†’ ' ORDER BY reacted_table_name_right.generations DESC) AS breadcrumbs -- noqa
FROM {{ ref2('redacted_schema_name', 'redacted_table_name_left') }} AS redacted_table_name_left
INNER JOIN {{ ref2('redacted_schema_name', 'reacted_table_name_right') }} AS reacted_table_name_right
    ON redacted_table_name_left.id = order_issue_category_hierarchies.ancestor_id
GROUP BY reacted_table_name_right.descendant_id
```
Running `sqlfluff fix --ignore templating,parsing,lexing -vvvv` and accepting proposed fixes for linting violations.

## Dialect
`postgres`, with `dbt` templater

## Version
`python 3.7.12`
`sqlfluff 0.7.0`
`sqlfluff-templater-dbt 0.7.0`

## Configuration
I've tried a few, here's one:
```
[sqlfluff]
verbose = 2
dialect = postgres
templater = dbt
exclude_rules = None
output_line_length = 80
runaway_limit = 10
ignore_templated_areas = True
processes = 3
# Comma separated list of file extensions to lint.

# NB: This config will only apply in the root folder.
sql_file_exts = .sql

[sqlfluff:indentation]
indented_joins = False
indented_using_on = True
template_blocks_indent = True

[sqlfluff:templater]
unwrap_wrapped_queries = True

[sqlfluff:templater:jinja]
apply_dbt_builtins = True

[sqlfluff:templater:jinja:macros]
# Macros provided as builtins for dbt projects
dbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}
dbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}
dbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}
dbt_var = {% macro var(variable, default='') %}item{% endmacro %}
dbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}

# Common config across rules
[sqlfluff:rules]
tab_space_size = 4
indent_unit = space
single_table_references = consistent
unquoted_identifiers_policy = all

# L001 - Remove trailing whitespace (fix)
# L002 - Single section of whitespace should not contain both tabs and spaces (fix)
# L003 - Keep consistent indentation (fix)
# L004 - We use 4 spaces for indentation just for completeness (fix)
# L005 - Remove space before commas (fix)
# L006 - Operators (+, -, *, /) will be wrapped by a single space each side (fix)

# L007 - Operators should not be at the end of a line
[sqlfluff:rules:L007]  # Keywords
operator_new_lines = after

# L008 - Always use a single whitespace after a comma (fix)
# L009 - Files will always end with a trailing newline

# L010 - All keywords will use full upper case (fix)
[sqlfluff:rules:L010]  # Keywords
capitalisation_policy = upper

# L011 - Always explicitly alias tables (fix)
[sqlfluff:rules:L011]  # Aliasing
aliasing = explicit

# L012 - Do not have to explicitly alias all columns
[sqlfluff:rules:L012]  # Aliasing
aliasing = explicit

# L013 - Always explicitly alias a column with an expression in it (fix)
[sqlfluff:rules:L013]  # Aliasing
allow_scalar = False

# L014 - Always user full lower case for 'quoted identifiers' -> column refs. without an alias (fix)
[sqlfluff:rules:L014]  # Unquoted identifiers
extended_capitalisation_policy = lower

# L015 - Always remove parenthesis when using DISTINCT to be clear that DISTINCT applies to all columns (fix)

# L016 - Lines should be 120 characters of less. Comment lines should not be ignored (fix)
[sqlfluff:rules:L016]
ignore_comment_lines = False
max_line_length = 120

# L017 - There should not be whitespace between function name and brackets (fix)
# L018 - Always align closing bracket of WITH to the WITH keyword (fix)

# L019 - Always use trailing commas / commas at the end of the line (fix)
[sqlfluff:rules:L019]
comma_style = trailing

# L020 - Table aliases will always be unique per statement
# L021 - Remove any use of ambiguous DISTINCT and GROUP BY combinations. Lean on removing the GROUP BY.
# L022 - Add blank lines after common table expressions (CTE) / WITH.
# L023 - Always add a single whitespace after AS in a WITH clause (fix)

[sqlfluff:rules:L026]
force_enable = False

# L027 - Always add references if more than one referenced table or view is used

[sqlfluff:rules:L028]
force_enable = False

[sqlfluff:rules:L029]  # Keyword identifiers
unquoted_identifiers_policy = aliases

[sqlfluff:rules:L030]  # Function names
capitalisation_policy = upper

# L032 - We prefer use of join keys rather than USING
# L034 - We prefer ordering of columns in select statements as (fix):
# 1. wildcards
# 2. single identifiers
# 3. calculations and aggregates

# L035 - Omit 'else NULL'; it is redundant (fix)
# L036 - Move select targets / identifiers onto new lines each (fix)
# L037 - When using ORDER BY, make the direction explicit (fix)

# L038 - Never use trailing commas at the end of the SELECT clause
[sqlfluff:rules:L038]
select_clause_trailing_comma = forbid

# L039 - Remove unnecessary whitespace (fix)

[sqlfluff:rules:L040]  # Null & Boolean Literals
capitalisation_policy = upper

# L042 - Join clauses should not contain subqueries. Use common tables expressions (CTE) instead.
[sqlfluff:rules:L042]
# By default, allow subqueries in from clauses, but not join clauses.
forbid_subquery_in = join

# L043 - Reduce CASE WHEN conditions to COALESCE (fix)
# L044 - Prefer a known number of columns along the path to the source data
# L045 - Remove unused common tables expressions (CTE) / WITH statements (fix)
# L046 - Jinja tags should have a single whitespace on both sides

# L047 - Use COUNT(*) instead of COUNT(0) or COUNT(1) alternatives (fix)
[sqlfluff:rules:L047]  # Consistent syntax to count all rows
prefer_count_1 = False
prefer_count_0 = False

# L048 - Quoted literals should be surrounded by a single whitespace (fix)
# L049 - Always use IS or IS NOT for comparisons with NULL (fix)
```


</issue>
<code>
[start of README.md]
1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)
2 
3 # The SQL Linter for Humans
4 
5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)
6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)
9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)
10 
11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)
12 [![Requirements Status](https://img.shields.io/requires/github/sqlfluff/sqlfluff.svg?style=flat-square)](https://requires.io/github/sqlfluff/sqlfluff/requirements/?branch=main)
13 [![CI Tests](https://github.com/sqlfluff/sqlfluff/workflows/CI%20Tests/badge.svg)](https://github.com/sqlfluff/sqlfluff/actions?query=workflow%3A%22CI+Tests%22)
14 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)
15 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)
16 
17 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.
18 
19 ## Dialects Supported
20 
21 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):
22 
23 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition
24 - [BigQuery](https://cloud.google.com/bigquery/)
25 - [Exasol](https://www.exasol.com/)
26 - [Hive](https://hive.apache.org/)
27 - [MySQL](https://www.mysql.com/)
28 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)
29 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)
30 - [Snowflake](https://www.snowflake.com/)
31 - [Spark3](https://spark.apache.org/docs/latest/)
32 - [SQLite](https://www.sqlite.org/)
33 - [Teradata](https://www.teradata.com/)
34 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)
35 
36 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.
37 
38 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.
39 
40 ## Templates Supported
41 
42 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).
43 
44 **SQLFluff** supports the following templates:
45 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)
46 - [dbt](https://www.getdbt.com/)
47 
48 Again, please raise issues if you wish to support more templating languages/syntaxes.
49 
50 # Getting Started
51 
52 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.
53 
54 ```shell
55 $ pip install sqlfluff
56 $ echo ""  SELECT a  +  b FROM tbl;  "" > test.sql
57 $ sqlfluff lint test.sql
58 == [test.sql] FAIL
59 L:   1 | P:   1 | L003 | Single indentation uses a number of spaces not a multiple of 4
60 L:   1 | P:  14 | L006 | Operators should be surrounded by a single space unless at the start/end of a line
61 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace
62 ```
63 
64 You can also have a play using [**SQLFluff online**](https://online.sqlfluff.com/).
65 
66 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).
67 
68 # Documentation
69 
70 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.
71 
72 # Releases
73 
74 **SQLFluff** is in beta phase - expect the tool to change significantly with potentially non-backwards compatible API and configuration changes in future releases. If you would like to join in please consider [contributing](CONTRIBUTING.md).
75 
76 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).
77 
78 # SQLFluff on Slack
79 
80 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!
81 
82 # SQLFluff on Twitter
83 
84 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.
85 
86 # Contributing
87 
88 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.
89 
90 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).
91 
92 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).
93 
[end of README.md]
[start of src/sqlfluff/core/linter/linted_file.py]
1 """"""Defines the LintedFile class.
2 
3 This holds linting results for a single file, and also
4 contains all of the routines to apply fixes to that file
5 post linting.
6 """"""
7 
8 import os
9 import logging
10 from typing import (
11     Any,
12     Iterable,
13     List,
14     NamedTuple,
15     Optional,
16     Tuple,
17     Union,
18     cast,
19     Type,
20 )
21 
22 from sqlfluff.core.errors import (
23     SQLBaseError,
24     SQLLintError,
25     CheckTuple,
26 )
27 from sqlfluff.core.string_helpers import findall
28 from sqlfluff.core.templaters import TemplatedFile
29 
30 # Classes needed only for type checking
31 from sqlfluff.core.parser.segments.base import BaseSegment, FixPatch
32 
33 from sqlfluff.core.linter.common import NoQaDirective, EnrichedFixPatch
34 
35 # Instantiate the linter logger
36 linter_logger: logging.Logger = logging.getLogger(""sqlfluff.linter"")
37 
38 
39 class LintedFile(NamedTuple):
40     """"""A class to store the idea of a linted file.""""""
41 
42     path: str
43     violations: List[SQLBaseError]
44     time_dict: dict
45     tree: Optional[BaseSegment]
46     ignore_mask: List[NoQaDirective]
47     templated_file: TemplatedFile
48     encoding: str
49 
50     def check_tuples(self, raise_on_non_linting_violations=True) -> List[CheckTuple]:
51         """"""Make a list of check_tuples.
52 
53         This assumes that all the violations found are
54         linting violations (and therefore implement `check_tuple()`).
55         If they don't then this function raises that error.
56         """"""
57         vs: List[CheckTuple] = []
58         v: SQLLintError
59         for v in self.get_violations():
60             if hasattr(v, ""check_tuple""):
61                 vs.append(v.check_tuple())
62             elif raise_on_non_linting_violations:
63                 raise v
64         return vs
65 
66     def get_violations(
67         self,
68         rules: Optional[Union[str, Tuple[str, ...]]] = None,
69         types: Optional[Union[Type[SQLBaseError], Iterable[Type[SQLBaseError]]]] = None,
70         filter_ignore: bool = True,
71         fixable: bool = None,
72     ) -> list:
73         """"""Get a list of violations, respecting filters and ignore options.
74 
75         Optionally now with filters.
76         """"""
77         violations = self.violations
78         # Filter types
79         if types:
80             # If it's a singular type, make it a single item in a tuple
81             # otherwise coerce to tuple normally so that we can use it with
82             # isinstance.
83             if isinstance(types, type) and issubclass(types, SQLBaseError):
84                 types = (types,)
85             else:
86                 types = tuple(types)  # pragma: no cover TODO?
87             violations = [v for v in violations if isinstance(v, types)]
88         # Filter rules
89         if rules:
90             if isinstance(rules, str):
91                 rules = (rules,)
92             else:
93                 rules = tuple(rules)
94             violations = [v for v in violations if v.rule_code() in rules]
95         # Filter fixable
96         if fixable is not None:
97             # Assume that fixable is true or false if not None
98             violations = [v for v in violations if v.fixable is fixable]
99         # Filter ignorable violations
100         if filter_ignore:
101             violations = [v for v in violations if not v.ignore]
102             # Ignore any rules in the ignore mask
103             if self.ignore_mask:
104                 violations = self.ignore_masked_violations(violations, self.ignore_mask)
105         return violations
106 
107     @staticmethod
108     def _ignore_masked_violations_single_line(
109         violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]
110     ):
111         """"""Returns whether to ignore error for line-specific directives.
112 
113         The ""ignore"" list is assumed to ONLY contain NoQaDirectives with
114         action=None.
115         """"""
116         for ignore in ignore_mask:
117             violations = [
118                 v
119                 for v in violations
120                 if not (
121                     v.line_no == ignore.line_no
122                     and (ignore.rules is None or v.rule_code() in ignore.rules)
123                 )
124             ]
125         return violations
126 
127     @staticmethod
128     def _should_ignore_violation_line_range(
129         line_no: int, ignore_rule: List[NoQaDirective]
130     ):
131         """"""Returns whether to ignore a violation at line_no.""""""
132         # Loop through the NoQaDirectives to find the state of things at
133         # line_no. Assumptions about ""ignore_rule"":
134         # - Contains directives for only ONE RULE, i.e. the rule that was
135         #   violated at line_no
136         # - Sorted in ascending order by line number
137         disable = False
138         for ignore in ignore_rule:
139             if ignore.line_no > line_no:
140                 break
141             disable = ignore.action == ""disable""
142         return disable
143 
144     @classmethod
145     def _ignore_masked_violations_line_range(
146         cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]
147     ):
148         """"""Returns whether to ignore error for line-range directives.
149 
150         The ""ignore"" list is assumed to ONLY contain NoQaDirectives where
151         action is ""enable"" or ""disable"".
152         """"""
153         result = []
154         for v in violations:
155             # Find the directives that affect the violated rule ""v"", either
156             # because they specifically reference it or because they don't
157             # specify a list of rules, thus affecting ALL rules.
158             ignore_rule = sorted(
159                 (
160                     ignore
161                     for ignore in ignore_mask
162                     if not ignore.rules
163                     or (v.rule_code() in cast(Tuple[str, ...], ignore.rules))
164                 ),
165                 key=lambda ignore: ignore.line_no,
166             )
167             # Determine whether to ignore the violation, based on the relevant
168             # enable/disable directives.
169             if not cls._should_ignore_violation_line_range(v.line_no, ignore_rule):
170                 result.append(v)
171         return result
172 
173     @classmethod
174     def ignore_masked_violations(
175         cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]
176     ) -> List[SQLBaseError]:
177         """"""Remove any violations specified by ignore_mask.
178 
179         This involves two steps:
180         1. Filter out violations affected by single-line ""noqa"" directives.
181         2. Filter out violations affected by disable/enable ""noqa"" directives.
182         """"""
183         ignore_specific = [ignore for ignore in ignore_mask if not ignore.action]
184         ignore_range = [ignore for ignore in ignore_mask if ignore.action]
185         violations = cls._ignore_masked_violations_single_line(
186             violations, ignore_specific
187         )
188         violations = cls._ignore_masked_violations_line_range(violations, ignore_range)
189         return violations
190 
191     def num_violations(self, **kwargs) -> int:
192         """"""Count the number of violations.
193 
194         Optionally now with filters.
195         """"""
196         violations = self.get_violations(**kwargs)
197         return len(violations)
198 
199     def is_clean(self) -> bool:
200         """"""Return True if there are no ignorable violations.""""""
201         return not any(self.get_violations(filter_ignore=True))
202 
203     @staticmethod
204     def _log_hints(
205         patch: Union[EnrichedFixPatch, FixPatch], templated_file: TemplatedFile
206     ):
207         """"""Log hints for debugging during patch generation.""""""
208         # This next bit is ALL FOR LOGGING AND DEBUGGING
209         max_log_length = 10
210         if patch.templated_slice.start >= max_log_length:
211             pre_hint = templated_file.templated_str[
212                 patch.templated_slice.start
213                 - max_log_length : patch.templated_slice.start
214             ]
215         else:
216             pre_hint = templated_file.templated_str[: patch.templated_slice.start]
217         if patch.templated_slice.stop + max_log_length < len(
218             templated_file.templated_str
219         ):
220             post_hint = templated_file.templated_str[
221                 patch.templated_slice.stop : patch.templated_slice.stop + max_log_length
222             ]
223         else:
224             post_hint = templated_file.templated_str[patch.templated_slice.stop :]
225         linter_logger.debug(
226             ""        Templated Hint: ...%r <> %r..."", pre_hint, post_hint
227         )
228 
229     def fix_string(self) -> Tuple[Any, bool]:
230         """"""Obtain the changes to a path as a string.
231 
232         We use the source mapping features of TemplatedFile
233         to generate a list of ""patches"" which cover the non
234         templated parts of the file and refer back to the locations
235         in the original file.
236 
237         NB: This is MUCH FASTER than the original approach
238         using difflib in pre 0.4.0.
239 
240         There is an important distinction here between Slices and
241         Segments. A Slice is a portion of a file which is determined
242         by the templater based on which portions of the source file
243         are templated or not, and therefore before Lexing and so is
244         completely dialect agnostic. A Segment is determined by the
245         Lexer from portions of strings after templating.
246         """"""
247         linter_logger.debug(""Original Tree: %r"", self.templated_file.templated_str)
248         assert self.tree
249         linter_logger.debug(""Fixed Tree: %r"", self.tree.raw)
250 
251         # The sliced file is contiguous in the TEMPLATED space.
252         # NB: It has gaps and repeats in the source space.
253         # It's also not the FIXED file either.
254         linter_logger.debug(""### Templated File."")
255         for idx, file_slice in enumerate(self.templated_file.sliced_file):
256             t_str = self.templated_file.templated_str[file_slice.templated_slice]
257             s_str = self.templated_file.source_str[file_slice.source_slice]
258             if t_str == s_str:
259                 linter_logger.debug(
260                     ""    File slice: %s %r [invariant]"", idx, file_slice
261                 )
262             else:
263                 linter_logger.debug(""    File slice: %s %r"", idx, file_slice)
264                 linter_logger.debug(""    \t\t\ttemplated: %r\tsource: %r"", t_str, s_str)
265 
266         original_source = self.templated_file.source_str
267 
268         # Make sure no patches overlap and divide up the source file into slices.
269         # Any Template tags in the source file are off limits.
270         source_only_slices = self.templated_file.source_only_slices()
271 
272         linter_logger.debug(""Source-only slices: %s"", source_only_slices)
273 
274         # Iterate patches, filtering and translating as we go:
275         linter_logger.debug(""### Beginning Patch Iteration."")
276         filtered_source_patches = []
277         dedupe_buffer = []
278         # We use enumerate so that we get an index for each patch. This is entirely
279         # so when debugging logs we can find a given patch again!
280         patch: Union[EnrichedFixPatch, FixPatch]
281         for idx, patch in enumerate(
282             self.tree.iter_patches(templated_str=self.templated_file.templated_str)
283         ):
284             linter_logger.debug(""  %s Yielded patch: %s"", idx, patch)
285             self._log_hints(patch, self.templated_file)
286 
287             # Attempt to convert to source space.
288             try:
289                 source_slice = self.templated_file.templated_slice_to_source_slice(
290                     patch.templated_slice,
291                 )
292             except ValueError:
293                 linter_logger.info(
294                     ""      - Skipping. Source space Value Error. i.e. attempted insertion within templated section.""
295                 )
296                 # If we try and slice within a templated section, then we may fail
297                 # in which case, we should skip this patch.
298                 continue
299 
300             # Check for duplicates
301             dedupe_tuple = (source_slice, patch.fixed_raw)
302             if dedupe_tuple in dedupe_buffer:
303                 linter_logger.info(
304                     ""      - Skipping. Source space Duplicate: %s"", dedupe_tuple
305                 )
306                 continue
307 
308             # We now evaluate patches in the source-space for whether they overlap
309             # or disrupt any templated sections.
310             # The intent here is that unless explicitly stated, a fix should never
311             # disrupt a templated section.
312             # NOTE: We rely here on the patches being sorted.
313             # TODO: Implement a mechanism for doing templated section fixes. For
314             # now it's just not allowed.
315 
316             # Get the affected raw slices.
317             local_raw_slices = self.templated_file.raw_slices_spanning_source_slice(
318                 source_slice
319             )
320             local_type_list = [slc.slice_type for slc in local_raw_slices]
321 
322             enriched_patch = EnrichedFixPatch(
323                 source_slice=source_slice,
324                 templated_slice=patch.templated_slice,
325                 patch_category=patch.patch_category,
326                 fixed_raw=patch.fixed_raw,
327                 templated_str=self.templated_file.templated_str[patch.templated_slice],
328                 source_str=self.templated_file.source_str[source_slice],
329             )
330 
331             # Deal with the easy case of only literals
332             if set(local_type_list) == {""literal""}:
333                 linter_logger.info(
334                     ""      * Keeping patch on literal-only section: %s"", enriched_patch
335                 )
336                 filtered_source_patches.append(enriched_patch)
337                 dedupe_buffer.append(enriched_patch.dedupe_tuple())
338             # Is it a zero length patch.
339             elif (
340                 enriched_patch.source_slice.start == enriched_patch.source_slice.stop
341                 and enriched_patch.source_slice.start == local_raw_slices[0].source_idx
342             ):
343                 linter_logger.info(
344                     ""      * Keeping insertion patch on slice boundary: %s"",
345                     enriched_patch,
346                 )
347                 filtered_source_patches.append(enriched_patch)
348                 dedupe_buffer.append(enriched_patch.dedupe_tuple())
349             # If it's ONLY templated then we should skip it.
350             elif ""literal"" not in local_type_list:
351                 linter_logger.info(
352                     ""      - Skipping patch over templated section: %s"", enriched_patch
353                 )
354             # If we span more than two slices then we should just skip it. Too Hard.
355             elif len(local_raw_slices) > 2:
356                 linter_logger.info(
357                     ""      - Skipping patch over more than two raw slices: %s"",
358                     enriched_patch,
359                 )
360             # If it's an insertion (i.e. the string in the pre-fix template is '') then we
361             # won't be able to place it, so skip.
362             elif not enriched_patch.templated_str:  # pragma: no cover TODO?
363                 linter_logger.info(
364                     ""      - Skipping insertion patch in templated section: %s"",
365                     enriched_patch,
366                 )
367             # If the string from the templated version isn't in the source, then we can't fix it.
368             elif (
369                 enriched_patch.templated_str not in enriched_patch.source_str
370             ):  # pragma: no cover TODO?
371                 linter_logger.info(
372                     ""      - Skipping edit patch on templated content: %s"",
373                     enriched_patch,
374                 )
375             else:
376                 # Identify all the places the string appears in the source content.
377                 positions = list(
378                     findall(enriched_patch.templated_str, enriched_patch.source_str)
379                 )
380                 if len(positions) != 1:
381                     linter_logger.debug(
382                         ""        - Skipping edit patch on non-unique templated content: %s"",
383                         enriched_patch,
384                     )
385                     continue
386                 # We have a single occurrence of the thing we want to patch. This
387                 # means we can use its position to place our patch.
388                 new_source_slice = slice(  # pragma: no cover
389                     enriched_patch.source_slice.start + positions[0],
390                     enriched_patch.source_slice.start
391                     + positions[0]
392                     + len(enriched_patch.templated_str),
393                 )
394                 enriched_patch = EnrichedFixPatch(  # pragma: no cover
395                     source_slice=new_source_slice,
396                     templated_slice=enriched_patch.templated_slice,
397                     patch_category=enriched_patch.patch_category,
398                     fixed_raw=enriched_patch.fixed_raw,
399                     templated_str=enriched_patch.templated_str,
400                     source_str=enriched_patch.source_str,
401                 )
402                 linter_logger.debug(  # pragma: no cover
403                     ""      * Keeping Tricky Case. Positions: %s, New Slice: %s, Patch: %s"",
404                     positions,
405                     new_source_slice,
406                     enriched_patch,
407                 )
408                 filtered_source_patches.append(enriched_patch)  # pragma: no cover
409                 dedupe_buffer.append(enriched_patch.dedupe_tuple())  # pragma: no cover
410                 continue  # pragma: no cover
411 
412         # Sort the patches before building up the file.
413         filtered_source_patches = sorted(
414             filtered_source_patches, key=lambda x: x.source_slice.start
415         )
416         # We now slice up the file using the patches and any source only slices.
417         # This gives us regions to apply changes to.
418         slice_buff = []
419         source_idx = 0
420         for patch in filtered_source_patches:
421             # Are there templated slices at or before the start of this patch?
422             while (
423                 source_only_slices
424                 and source_only_slices[0].source_idx < patch.source_slice.start
425             ):
426                 next_so_slice = source_only_slices.pop(0).source_slice()
427                 # Add a pre-slice before the next templated slices if needed.
428                 if next_so_slice.start > source_idx:
429                     slice_buff.append(slice(source_idx, next_so_slice.start))
430                 # Add the templated slice.
431                 slice_buff.append(next_so_slice)
432                 source_idx = next_so_slice.stop
433 
434             # Is there a gap between current position and this patch?
435             if patch.source_slice.start > source_idx:
436                 # Add a slice up to this patch.
437                 slice_buff.append(slice(source_idx, patch.source_slice.start))
438 
439             # Is this patch covering an area we've already covered?
440             if patch.source_slice.start < source_idx:
441                 linter_logger.info(
442                     ""Skipping overlapping patch at Index %s, Patch: %s"",
443                     source_idx,
444                     patch,
445                 )
446                 # Ignore the patch for now...
447                 continue
448 
449             # Add this patch.
450             slice_buff.append(patch.source_slice)
451             source_idx = patch.source_slice.stop
452         # Add a tail slice.
453         if source_idx < len(self.templated_file.source_str):
454             slice_buff.append(slice(source_idx, len(self.templated_file.source_str)))
455 
456         linter_logger.debug(""Final slice buffer: %s"", slice_buff)
457 
458         # Iterate through the patches, building up the new string.
459         str_buff = """"
460         for source_slice in slice_buff:
461             # Is it one in the patch buffer:
462             for patch in filtered_source_patches:
463                 if patch.source_slice == source_slice:
464                     # Use the patched version
465                     linter_logger.debug(
466                         ""%-30s    %s    %r > %r"",
467                         f""Appending {patch.patch_category} Patch:"",
468                         patch.source_slice,
469                         patch.source_str,
470                         patch.fixed_raw,
471                     )
472                     str_buff += patch.fixed_raw
473                     break
474             else:
475                 # Use the raw string
476                 linter_logger.debug(
477                     ""Appending Raw:                    %s     %r"",
478                     source_slice,
479                     self.templated_file.source_str[source_slice],
480                 )
481                 str_buff += self.templated_file.source_str[source_slice]
482 
483         # The success metric here is whether anything ACTUALLY changed.
484         return str_buff, str_buff != original_source
485 
486     def persist_tree(self, suffix: str = """") -> bool:
487         """"""Persist changes to the given path.""""""
488         write_buff, success = self.fix_string()
489 
490         if success:
491             fname = self.path
492             # If there is a suffix specified, then use it.s
493             if suffix:
494                 root, ext = os.path.splitext(fname)
495                 fname = root + suffix + ext
496             # Actually write the file.
497             with open(fname, ""w"", encoding=self.encoding) as f:
498                 f.write(write_buff)
499         return success
500 
[end of src/sqlfluff/core/linter/linted_file.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",sqlfluff/sqlfluff,a10057635e5b2559293a676486f0b730981f037a,"dbt postgres fix command errors with UnicodeEncodeError and also wipes the .sql file
_If this is a parsing or linting issue, please include a minimal SQL example which reproduces the issue, along with the `sqlfluff parse` output, `sqlfluff lint` output and `sqlfluff fix` output when relevant._

## Expected Behaviour
Violation failure notice at a minimum, without wiping the file. Would like a way to ignore the known error at a minimum as --noqa is not getting past this. Actually would expect --noqa to totally ignore this.

## Observed Behaviour
Reported error: `UnicodeEncodeError: 'charmap' codec can't encode character '\u2192' in position 120: character maps to <undefined>`

## Steps to Reproduce
SQL file:
```sql
SELECT
    reacted_table_name_right.descendant_id AS category_id,
    string_agg(redacted_table_name_left.name, ' â†’ ' ORDER BY reacted_table_name_right.generations DESC) AS breadcrumbs -- noqa
FROM {{ ref2('redacted_schema_name', 'redacted_table_name_left') }} AS redacted_table_name_left
INNER JOIN {{ ref2('redacted_schema_name', 'reacted_table_name_right') }} AS reacted_table_name_right
    ON redacted_table_name_left.id = order_issue_category_hierarchies.ancestor_id
GROUP BY reacted_table_name_right.descendant_id
```
Running `sqlfluff fix --ignore templating,parsing,lexing -vvvv` and accepting proposed fixes for linting violations.

## Dialect
`postgres`, with `dbt` templater

## Version
`python 3.7.12`
`sqlfluff 0.7.0`
`sqlfluff-templater-dbt 0.7.0`

## Configuration
I've tried a few, here's one:
```
[sqlfluff]
verbose = 2
dialect = postgres
templater = dbt
exclude_rules = None
output_line_length = 80
runaway_limit = 10
ignore_templated_areas = True
processes = 3
# Comma separated list of file extensions to lint.

# NB: This config will only apply in the root folder.
sql_file_exts = .sql

[sqlfluff:indentation]
indented_joins = False
indented_using_on = True
template_blocks_indent = True

[sqlfluff:templater]
unwrap_wrapped_queries = True

[sqlfluff:templater:jinja]
apply_dbt_builtins = True

[sqlfluff:templater:jinja:macros]
# Macros provided as builtins for dbt projects
dbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}
dbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}
dbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}
dbt_var = {% macro var(variable, default='') %}item{% endmacro %}
dbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}

# Common config across rules
[sqlfluff:rules]
tab_space_size = 4
indent_unit = space
single_table_references = consistent
unquoted_identifiers_policy = all

# L001 - Remove trailing whitespace (fix)
# L002 - Single section of whitespace should not contain both tabs and spaces (fix)
# L003 - Keep consistent indentation (fix)
# L004 - We use 4 spaces for indentation just for completeness (fix)
# L005 - Remove space before commas (fix)
# L006 - Operators (+, -, *, /) will be wrapped by a single space each side (fix)

# L007 - Operators should not be at the end of a line
[sqlfluff:rules:L007]  # Keywords
operator_new_lines = after

# L008 - Always use a single whitespace after a comma (fix)
# L009 - Files will always end with a trailing newline

# L010 - All keywords will use full upper case (fix)
[sqlfluff:rules:L010]  # Keywords
capitalisation_policy = upper

# L011 - Always explicitly alias tables (fix)
[sqlfluff:rules:L011]  # Aliasing
aliasing = explicit

# L012 - Do not have to explicitly alias all columns
[sqlfluff:rules:L012]  # Aliasing
aliasing = explicit

# L013 - Always explicitly alias a column with an expression in it (fix)
[sqlfluff:rules:L013]  # Aliasing
allow_scalar = False

# L014 - Always user full lower case for 'quoted identifiers' -> column refs. without an alias (fix)
[sqlfluff:rules:L014]  # Unquoted identifiers
extended_capitalisation_policy = lower

# L015 - Always remove parenthesis when using DISTINCT to be clear that DISTINCT applies to all columns (fix)

# L016 - Lines should be 120 characters of less. Comment lines should not be ignored (fix)
[sqlfluff:rules:L016]
ignore_comment_lines = False
max_line_length = 120

# L017 - There should not be whitespace between function name and brackets (fix)
# L018 - Always align closing bracket of WITH to the WITH keyword (fix)

# L019 - Always use trailing commas / commas at the end of the line (fix)
[sqlfluff:rules:L019]
comma_style = trailing

# L020 - Table aliases will always be unique per statement
# L021 - Remove any use of ambiguous DISTINCT and GROUP BY combinations. Lean on removing the GROUP BY.
# L022 - Add blank lines after common table expressions (CTE) / WITH.
# L023 - Always add a single whitespace after AS in a WITH clause (fix)

[sqlfluff:rules:L026]
force_enable = False

# L027 - Always add references if more than one referenced table or view is used

[sqlfluff:rules:L028]
force_enable = False

[sqlfluff:rules:L029]  # Keyword identifiers
unquoted_identifiers_policy = aliases

[sqlfluff:rules:L030]  # Function names
capitalisation_policy = upper

# L032 - We prefer use of join keys rather than USING
# L034 - We prefer ordering of columns in select statements as (fix):
# 1. wildcards
# 2. single identifiers
# 3. calculations and aggregates

# L035 - Omit 'else NULL'; it is redundant (fix)
# L036 - Move select targets / identifiers onto new lines each (fix)
# L037 - When using ORDER BY, make the direction explicit (fix)

# L038 - Never use trailing commas at the end of the SELECT clause
[sqlfluff:rules:L038]
select_clause_trailing_comma = forbid

# L039 - Remove unnecessary whitespace (fix)

[sqlfluff:rules:L040]  # Null & Boolean Literals
capitalisation_policy = upper

# L042 - Join clauses should not contain subqueries. Use common tables expressions (CTE) instead.
[sqlfluff:rules:L042]
# By default, allow subqueries in from clauses, but not join clauses.
forbid_subquery_in = join

# L043 - Reduce CASE WHEN conditions to COALESCE (fix)
# L044 - Prefer a known number of columns along the path to the source data
# L045 - Remove unused common tables expressions (CTE) / WITH statements (fix)
# L046 - Jinja tags should have a single whitespace on both sides

# L047 - Use COUNT(*) instead of COUNT(0) or COUNT(1) alternatives (fix)
[sqlfluff:rules:L047]  # Consistent syntax to count all rows
prefer_count_1 = False
prefer_count_0 = False

# L048 - Quoted literals should be surrounded by a single whitespace (fix)
# L049 - Always use IS or IS NOT for comparisons with NULL (fix)
```

","I get a dbt-related error -- can you provide your project file as well? Also, what operating system are you running this on? I tested a simplified (non-dbt) version of your file on my Mac, and it worked okay.

```
dbt.exceptions.DbtProjectError: Runtime Error
  no dbt_project.yml found at expected path /Users/bhart/dev/sqlfluff/dbt_project.yml
```
Never mind the questions above -- I managed to reproduce the error in a sample dbt project. Taking a look now...
@Tumble17: Have you tried setting the `encoding` parameter in `.sqlfluff`? Do you know what encoding you're using? The default is `autodetect`, and SQLFluff ""thinks"" the file uses ""Windows-1252"" encoding, which I assume is incorrect -- that's why SQLFluff is unable to write out the updated file.
I added this line to the first section of your `.sqlfluff`, and now it seems to work. I'll look into changing the behavior of `sqlfluff fix` so it doesn't erase the file when it fails.

```
encoding = utf-8
```",2021-10-26T17:28:28Z,"<patch>
diff --git a/src/sqlfluff/core/linter/linted_file.py b/src/sqlfluff/core/linter/linted_file.py
--- a/src/sqlfluff/core/linter/linted_file.py
+++ b/src/sqlfluff/core/linter/linted_file.py
@@ -7,6 +7,8 @@
 
 import os
 import logging
+import shutil
+import tempfile
 from typing import (
     Any,
     Iterable,
@@ -493,7 +495,24 @@ def persist_tree(self, suffix: str = """") -> bool:
             if suffix:
                 root, ext = os.path.splitext(fname)
                 fname = root + suffix + ext
-            # Actually write the file.
-            with open(fname, ""w"", encoding=self.encoding) as f:
-                f.write(write_buff)
+            self._safe_create_replace_file(fname, write_buff, self.encoding)
         return success
+
+    @staticmethod
+    def _safe_create_replace_file(fname, write_buff, encoding):
+        # Write to a temporary file first, so in case of encoding or other
+        # issues, we don't delete or corrupt the user's existing file.
+        dirname, basename = os.path.split(fname)
+        with tempfile.NamedTemporaryFile(
+            mode=""w"",
+            encoding=encoding,
+            prefix=basename,
+            dir=dirname,
+            suffix=os.path.splitext(fname)[1],
+            delete=False,
+        ) as tmp:
+            tmp.file.write(write_buff)
+            tmp.flush()
+            os.fsync(tmp.fileno())
+        # Once the temp file is safely written, replace the existing file.
+        shutil.move(tmp.name, fname)

</patch>","diff --git a/test/core/linter_test.py b/test/core/linter_test.py
--- a/test/core/linter_test.py
+++ b/test/core/linter_test.py
@@ -641,3 +641,56 @@ def test__attempt_to_change_templater_warning(caplog):
         assert ""Attempt to set templater to "" in caplog.text
     finally:
         logger.propagate = original_propagate_value
+
+
+@pytest.mark.parametrize(
+    ""case"",
+    [
+        dict(
+            name=""utf8_create"",
+            fname=""test.sql"",
+            encoding=""utf-8"",
+            existing=None,
+            update=""def"",
+            expected=""def"",
+        ),
+        dict(
+            name=""utf8_update"",
+            fname=""test.sql"",
+            encoding=""utf-8"",
+            existing=""abc"",
+            update=""def"",
+            expected=""def"",
+        ),
+        dict(
+            name=""utf8_special_char"",
+            fname=""test.sql"",
+            encoding=""utf-8"",
+            existing=""abc"",
+            update=""â†’"",  # Special utf-8 character
+            expected=""â†’"",
+        ),
+        dict(
+            name=""incorrect_encoding"",
+            fname=""test.sql"",
+            encoding=""Windows-1252"",
+            existing=""abc"",
+            update=""â†’"",  # Not valid in Windows-1252
+            expected=""abc"",  # File should be unchanged
+        ),
+    ],
+    ids=lambda case: case[""name""],
+)
+def test_safe_create_replace_file(case, tmp_path):
+    """"""Test creating or updating .sql files, various content and encoding.""""""
+    p = tmp_path / case[""fname""]
+    if case[""existing""]:
+        p.write_text(case[""existing""])
+    try:
+        linter.LintedFile._safe_create_replace_file(
+            str(p), case[""update""], case[""encoding""]
+        )
+    except:  # noqa: E722
+        pass
+    actual = p.read_text(encoding=case[""encoding""])
+    assert case[""expected""] == actual
",0.6,"[""test/core/linter_test.py::test_safe_create_replace_file[utf8_create]"", ""test/core/linter_test.py::test_safe_create_replace_file[utf8_update]"", ""test/core/linter_test.py::test_safe_create_replace_file[utf8_special_char]""]","[""test/core/linter_test.py::test__linter__path_from_paths__dir"", ""test/core/linter_test.py::test__linter__path_from_paths__default"", ""test/core/linter_test.py::test__linter__path_from_paths__exts"", ""test/core/linter_test.py::test__linter__path_from_paths__file"", ""test/core/linter_test.py::test__linter__path_from_paths__not_exist"", ""test/core/linter_test.py::test__linter__path_from_paths__not_exist_ignore"", ""test/core/linter_test.py::test__linter__path_from_paths__explicit_ignore"", ""test/core/linter_test.py::test__linter__path_from_paths__dot"", ""test/core/linter_test.py::test__linter__path_from_paths__ignore[test/fixtures/linter/sqlfluffignore]"", ""test/core/linter_test.py::test__linter__path_from_paths__ignore[test/fixtures/linter/sqlfluffignore/]"", ""test/core/linter_test.py::test__linter__path_from_paths__ignore[test/fixtures/linter/sqlfluffignore/.]"", ""test/core/linter_test.py::test__linter__lint_string_vs_file[test/fixtures/linter/indentation_errors.sql]"", ""test/core/linter_test.py::test__linter__lint_string_vs_file[test/fixtures/linter/whitespace_errors.sql]"", ""test/core/linter_test.py::test__linter__get_violations_filter_rules[None-7]"", ""test/core/linter_test.py::test__linter__get_violations_filter_rules[L010-2]"", ""test/core/linter_test.py::test__linter__get_violations_filter_rules[rules2-2]"", ""test/core/linter_test.py::test__linter__linting_result__sum_dicts"", ""test/core/linter_test.py::test__linter__linting_result__combine_dicts"", ""test/core/linter_test.py::test__linter__linting_result_check_tuples_by_path[False-list]"", ""test/core/linter_test.py::test__linter__linting_result_check_tuples_by_path[True-dict]"", ""test/core/linter_test.py::test__linter__linting_result_get_violations[1]"", ""test/core/linter_test.py::test__linter__linting_result_get_violations[2]"", ""test/core/linter_test.py::test__linter__linting_parallel_thread[False]"", ""test/core/linter_test.py::test__linter__linting_parallel_thread[True]"", ""test/core/linter_test.py::test_lint_path_parallel_wrapper_exception"", ""test/core/linter_test.py::test__linter__linting_unexpected_error_handled_gracefully"", ""test/core/linter_test.py::test__linter__raises_malformed_noqa"", ""test/core/linter_test.py::test__linter__empty_file"", ""test/core/linter_test.py::test__linter__mask_templated_violations[True-check_tuples0]"", ""test/core/linter_test.py::test__linter__mask_templated_violations[False-check_tuples1]"", ""test/core/linter_test.py::test__linter__encoding[test/fixtures/linter/encoding-utf-8.sql-autodetect-False]"", ""test/core/linter_test.py::test__linter__encoding[test/fixtures/linter/encoding-utf-8-sig.sql-autodetect-False]"", ""test/core/linter_test.py::test__linter__encoding[test/fixtures/linter/encoding-utf-8.sql-utf-8-False]"", ""test/core/linter_test.py::test__linter__encoding[test/fixtures/linter/encoding-utf-8-sig.sql-utf-8-True]"", ""test/core/linter_test.py::test__linter__encoding[test/fixtures/linter/encoding-utf-8.sql-utf-8-sig-False]"", ""test/core/linter_test.py::test__linter__encoding[test/fixtures/linter/encoding-utf-8-sig.sql-utf-8-sig-False]"", ""test/core/linter_test.py::test_parse_noqa[-None]"", ""test/core/linter_test.py::test_parse_noqa[noqa-expected1]"", ""test/core/linter_test.py::test_parse_noqa[noqa?-SQLParseError]"", ""test/core/linter_test.py::test_parse_noqa[noqa:-expected3]"", ""test/core/linter_test.py::test_parse_noqa[noqa:L001,L002-expected4]"", ""test/core/linter_test.py::test_parse_noqa[noqa:"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_no_ignore]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_ignore_specific_line]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_ignore_different_specific_line]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_ignore_different_specific_rule]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_ignore_enable_this_range]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_ignore_disable_this_range]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_line_1_ignore_disable_specific_2_3]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_line_2_ignore_disable_specific_2_3]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_line_3_ignore_disable_specific_2_3]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_line_4_ignore_disable_specific_2_3]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_line_1_ignore_disable_all_2_3]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_line_2_ignore_disable_all_2_3]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_line_3_ignore_disable_all_2_3]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[1_violation_line_4_ignore_disable_all_2_3]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[4_violations_two_types_disable_specific_enable_all]"", ""test/core/linter_test.py::test_linted_file_ignore_masked_violations[4_violations_two_types_disable_all_enable_specific]"", ""test/core/linter_test.py::test_linter_noqa"", ""test/core/linter_test.py::test_linter_noqa_with_templating"", ""test/core/linter_test.py::test_delayed_exception"", ""test/core/linter_test.py::test__attempt_to_change_templater_warning"", ""test/core/linter_test.py::test_safe_create_replace_file[incorrect_encoding]""]",67023b85c41d23d6c6d69812a41b207c4f8a9331
marshmallow-code__marshmallow-1359,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
3.0: DateTime fields cannot be used as inner field for List or Tuple fields
Between releases 3.0.0rc8 and 3.0.0rc9, `DateTime` fields have started throwing an error when being instantiated as inner fields of container fields like `List` or `Tuple`. The snippet below works in <=3.0.0rc8 and throws the error below in >=3.0.0rc9 (and, worryingly, 3.0.0):

```python
from marshmallow import fields, Schema

class MySchema(Schema):
    times = fields.List(fields.DateTime())

s = MySchema()
```

Traceback:
```
Traceback (most recent call last):
  File ""test-mm.py"", line 8, in <module>
    s = MySchema()
  File ""/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py"", line 383, in __init__
    self.fields = self._init_fields()
  File ""/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py"", line 913, in _init_fields
    self._bind_field(field_name, field_obj)
  File ""/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py"", line 969, in _bind_field
    field_obj._bind_to_schema(field_name, self)
  File ""/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py"", line 636, in _bind_to_schema
    self.inner._bind_to_schema(field_name, self)
  File ""/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py"", line 1117, in _bind_to_schema
    or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)
AttributeError: 'List' object has no attribute 'opts'
```

It seems like it's treating the parent field as a Schema without checking that it is indeed a schema, so the `schema.opts` statement fails as fields don't have an `opts` attribute.

</issue>
<code>
[start of README.rst]
1 ********************************************
2 marshmallow: simplified object serialization
3 ********************************************
4 
5 .. image:: https://badgen.net/pypi/v/marshmallow
6     :target: https://pypi.org/project/marshmallow/
7     :alt: Latest version
8 
9 .. image:: https://dev.azure.com/sloria/sloria/_apis/build/status/marshmallow-code.marshmallow?branchName=dev
10     :target: https://dev.azure.com/sloria/sloria/_build/latest?definitionId=5&branchName=dev
11     :alt: Build status
12 
13 .. image:: https://readthedocs.org/projects/marshmallow/badge/
14    :target: https://marshmallow.readthedocs.io/
15    :alt: Documentation
16    
17 .. image:: https://badgen.net/badge/code%20style/black/000
18     :target: https://github.com/ambv/black
19     :alt: code style: black
20 
21 
22 **marshmallow** is an ORM/ODM/framework-agnostic library for converting complex datatypes, such as objects, to and from native Python datatypes.
23 
24 .. code-block:: python
25 
26     from datetime import date
27     from marshmallow import Schema, fields, pprint
28 
29 
30     class ArtistSchema(Schema):
31         name = fields.Str()
32 
33 
34     class AlbumSchema(Schema):
35         title = fields.Str()
36         release_date = fields.Date()
37         artist = fields.Nested(ArtistSchema())
38 
39 
40     bowie = dict(name=""David Bowie"")
41     album = dict(artist=bowie, title=""Hunky Dory"", release_date=date(1971, 12, 17))
42 
43     schema = AlbumSchema()
44     result = schema.dump(album)
45     pprint(result, indent=2)
46     # { 'artist': {'name': 'David Bowie'},
47     #   'release_date': '1971-12-17',
48     #   'title': 'Hunky Dory'}
49 
50 
51 In short, marshmallow schemas can be used to:
52 
53 - **Validate** input data.
54 - **Deserialize** input data to app-level objects.
55 - **Serialize** app-level objects to primitive Python types. The serialized objects can then be rendered to standard formats such as JSON for use in an HTTP API.
56 
57 Get It Now
58 ==========
59 
60 ::
61 
62     $ pip install -U marshmallow
63 
64 
65 Documentation
66 =============
67 
68 Full documentation is available at https://marshmallow.readthedocs.io/ .
69 
70 Requirements
71 ============
72 
73 - Python >= 3.5
74 
75 Ecosystem
76 =========
77 
78 A list of marshmallow-related libraries can be found at the GitHub wiki here:
79 
80 https://github.com/marshmallow-code/marshmallow/wiki/Ecosystem
81 
82 Credits
83 =======
84 
85 Contributors
86 ------------
87 
88 This project exists thanks to all the people who contribute.
89 
90 You're highly encouraged to participate in marshmallow's development.
91 Check out the `Contributing Guidelines <https://marshmallow.readthedocs.io/en/latest/contributing.html>`_ to see
92 how you can help.
93 
94 Thank you to all who have already contributed to marshmallow!
95 
96 .. image:: https://opencollective.com/marshmallow/contributors.svg?width=890&button=false
97     :target: https://marshmallow.readthedocs.io/en/latest/authors.html
98     :alt: Contributors
99 
100 Backers
101 -------
102 
103 If you find marshmallow useful, please consider supporting the team with
104 a donation. Your donation helps move marshmallow forward.
105 
106 Thank you to all our backers! [`Become a backer`_]
107 
108 .. _`Become a backer`: https://opencollective.com/marshmallow#backer
109 
110 .. image:: https://opencollective.com/marshmallow/backers.svg?width=890
111     :target: https://opencollective.com/marshmallow#backers
112     :alt: Backers
113 
114 Sponsors
115 --------
116 
117 Support this project by becoming a sponsor (or ask your company to support this project by becoming a sponsor).
118 Your logo will show up here with a link to your website. [`Become a sponsor`_]
119 
120 .. _`Become a sponsor`: https://opencollective.com/marshmallow#sponsor
121 
122 .. image:: https://opencollective.com/marshmallow/sponsor/0/avatar.svg
123     :target: https://opencollective.com/marshmallow/sponsor/0/website
124     :alt: Sponsors
125 
126 .. image:: https://opencollective.com/static/images/become_sponsor.svg
127     :target: https://opencollective.com/marshmallow#sponsor
128     :alt: Become a sponsor
129 
130 
131 Professional Support
132 ====================
133 
134 Professionally-supported marshmallow is now available through the
135 `Tidelift Subscription <https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=pypi-marshmallow&utm_medium=readme>`_.
136 
137 Tidelift gives software development teams a single source for purchasing and maintaining their software,
138 with professional-grade assurances from the experts who know it best,
139 while seamlessly integrating with existing tools. [`Get professional support`_]
140 
141 .. _`Get professional support`: https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=marshmallow&utm_medium=referral&utm_campaign=github
142 
143 .. image:: https://user-images.githubusercontent.com/2379650/45126032-50b69880-b13f-11e8-9c2c-abd16c433495.png
144     :target: https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=pypi-marshmallow&utm_medium=readme
145     :alt: Get supported marshmallow with Tidelift
146 
147 Security Contact Information
148 ============================
149 
150 To report a security vulnerability, please use the
151 `Tidelift security contact <https://tidelift.com/security>`_.
152 Tidelift will coordinate the fix and disclosure.
153 
154 Project Links
155 =============
156 
157 - Docs: https://marshmallow.readthedocs.io/
158 - Changelog: https://marshmallow.readthedocs.io/en/latest/changelog.html
159 - PyPI: https://pypi.python.org/pypi/marshmallow
160 - Issues: https://github.com/marshmallow-code/marshmallow/issues
161 - Donate: https://opencollective.com/marshmallow
162 
163 License
164 =======
165 
166 MIT licensed. See the bundled `LICENSE <https://github.com/marshmallow-code/marshmallow/blob/dev/LICENSE>`_ file for more details.
167 
[end of README.rst]
[start of src/marshmallow/fields.py]
1 """"""Field classes for various types of data.""""""
2 
3 import collections
4 import copy
5 import datetime as dt
6 import numbers
7 import uuid
8 import decimal
9 import math
10 import warnings
11 from collections.abc import Mapping as _Mapping
12 
13 from marshmallow import validate, utils, class_registry
14 from marshmallow.base import FieldABC, SchemaABC
15 from marshmallow.utils import (
16     is_collection,
17     missing as missing_,
18     resolve_field_instance,
19     is_aware,
20 )
21 from marshmallow.exceptions import (
22     ValidationError,
23     StringNotCollectionError,
24     FieldInstanceResolutionError,
25 )
26 from marshmallow.validate import Validator, Length
27 
28 __all__ = [
29     ""Field"",
30     ""Raw"",
31     ""Nested"",
32     ""Mapping"",
33     ""Dict"",
34     ""List"",
35     ""Tuple"",
36     ""String"",
37     ""UUID"",
38     ""Number"",
39     ""Integer"",
40     ""Decimal"",
41     ""Boolean"",
42     ""Float"",
43     ""DateTime"",
44     ""NaiveDateTime"",
45     ""AwareDateTime"",
46     ""Time"",
47     ""Date"",
48     ""TimeDelta"",
49     ""Url"",
50     ""URL"",
51     ""Email"",
52     ""Method"",
53     ""Function"",
54     ""Str"",
55     ""Bool"",
56     ""Int"",
57     ""Constant"",
58     ""Pluck"",
59 ]
60 
61 MISSING_ERROR_MESSAGE = (
62     ""ValidationError raised by `{class_name}`, but error key `{key}` does ""
63     ""not exist in the `error_messages` dictionary.""
64 )
65 
66 
67 class Field(FieldABC):
68     """"""Basic field from which other fields should extend. It applies no
69     formatting by default, and should only be used in cases where
70     data does not need to be formatted before being serialized or deserialized.
71     On error, the name of the field will be returned.
72 
73     :param default: If set, this value will be used during serialization if the input value
74         is missing. If not set, the field will be excluded from the serialized output if the
75         input value is missing. May be a value or a callable.
76     :param missing: Default deserialization value for the field if the field is not
77         found in the input data. May be a value or a callable.
78     :param str data_key: The name of the dict key in the external representation, i.e.
79         the input of `load` and the output of `dump`.
80         If `None`, the key will match the name of the field.
81     :param str attribute: The name of the attribute to get the value from when serializing.
82         If `None`, assumes the attribute has the same name as the field.
83         Note: This should only be used for very specific use cases such as
84         outputting multiple fields for a single attribute. In most cases,
85         you should use ``data_key`` instead.
86     :param callable validate: Validator or collection of validators that are called
87         during deserialization. Validator takes a field's input value as
88         its only parameter and returns a boolean.
89         If it returns `False`, an :exc:`ValidationError` is raised.
90     :param required: Raise a :exc:`ValidationError` if the field value
91         is not supplied during deserialization.
92     :param allow_none: Set this to `True` if `None` should be considered a valid value during
93         validation/deserialization. If ``missing=None`` and ``allow_none`` is unset,
94         will default to ``True``. Otherwise, the default is ``False``.
95     :param bool load_only: If `True` skip this field during serialization, otherwise
96         its value will be present in the serialized data.
97     :param bool dump_only: If `True` skip this field during deserialization, otherwise
98         its value will be present in the deserialized object. In the context of an
99         HTTP API, this effectively marks the field as ""read-only"".
100     :param dict error_messages: Overrides for `Field.default_error_messages`.
101     :param metadata: Extra arguments to be stored as metadata.
102 
103     .. versionchanged:: 2.0.0
104         Removed `error` parameter. Use ``error_messages`` instead.
105 
106     .. versionchanged:: 2.0.0
107         Added `allow_none` parameter, which makes validation/deserialization of `None`
108         consistent across fields.
109 
110     .. versionchanged:: 2.0.0
111         Added `load_only` and `dump_only` parameters, which allow field skipping
112         during the (de)serialization process.
113 
114     .. versionchanged:: 2.0.0
115         Added `missing` parameter, which indicates the value for a field if the field
116         is not found during deserialization.
117 
118     .. versionchanged:: 2.0.0
119         ``default`` value is only used if explicitly set. Otherwise, missing values
120         inputs are excluded from serialized output.
121 
122     .. versionchanged:: 3.0.0b8
123         Add ``data_key`` parameter for the specifying the key in the input and
124         output data. This parameter replaced both ``load_from`` and ``dump_to``.
125     """"""
126 
127     # Some fields, such as Method fields and Function fields, are not expected
128     #  to exist as attributes on the objects to serialize. Set this to False
129     #  for those fields
130     _CHECK_ATTRIBUTE = True
131     _creation_index = 0  # Used for sorting
132 
133     #: Default error messages for various kinds of errors. The keys in this dictionary
134     #: are passed to `Field.fail`. The values are error messages passed to
135     #: :exc:`marshmallow.exceptions.ValidationError`.
136     default_error_messages = {
137         ""required"": ""Missing data for required field."",
138         ""null"": ""Field may not be null."",
139         ""validator_failed"": ""Invalid value."",
140     }
141 
142     def __init__(
143         self,
144         *,
145         default=missing_,
146         missing=missing_,
147         data_key=None,
148         attribute=None,
149         validate=None,
150         required=False,
151         allow_none=None,
152         load_only=False,
153         dump_only=False,
154         error_messages=None,
155         **metadata
156     ):
157         self.default = default
158         self.attribute = attribute
159         self.data_key = data_key
160         self.validate = validate
161         if utils.is_iterable_but_not_string(validate):
162             if not utils.is_generator(validate):
163                 self.validators = validate
164             else:
165                 self.validators = list(validate)
166         elif callable(validate):
167             self.validators = [validate]
168         elif validate is None:
169             self.validators = []
170         else:
171             raise ValueError(
172                 ""The 'validate' parameter must be a callable ""
173                 ""or a collection of callables.""
174             )
175 
176         # If missing=None, None should be considered valid by default
177         if allow_none is None:
178             if missing is None:
179                 self.allow_none = True
180             else:
181                 self.allow_none = False
182         else:
183             self.allow_none = allow_none
184         self.load_only = load_only
185         self.dump_only = dump_only
186         if required is True and missing is not missing_:
187             raise ValueError(""'missing' must not be set for required fields."")
188         self.required = required
189         self.missing = missing
190         self.metadata = metadata
191         self._creation_index = Field._creation_index
192         Field._creation_index += 1
193 
194         # Collect default error message from self and parent classes
195         messages = {}
196         for cls in reversed(self.__class__.__mro__):
197             messages.update(getattr(cls, ""default_error_messages"", {}))
198         messages.update(error_messages or {})
199         self.error_messages = messages
200 
201     def __repr__(self):
202         return (
203             ""<fields.{ClassName}(default={self.default!r}, ""
204             ""attribute={self.attribute!r}, ""
205             ""validate={self.validate}, required={self.required}, ""
206             ""load_only={self.load_only}, dump_only={self.dump_only}, ""
207             ""missing={self.missing}, allow_none={self.allow_none}, ""
208             ""error_messages={self.error_messages})>"".format(
209                 ClassName=self.__class__.__name__, self=self
210             )
211         )
212 
213     def __deepcopy__(self, memo):
214         return copy.copy(self)
215 
216     def get_value(self, obj, attr, accessor=None, default=missing_):
217         """"""Return the value for a given key from an object.
218 
219         :param object obj: The object to get the value from.
220         :param str attr: The attribute/key in `obj` to get the value from.
221         :param callable accessor: A callable used to retrieve the value of `attr` from
222             the object `obj`. Defaults to `marshmallow.utils.get_value`.
223         """"""
224         # NOTE: Use getattr instead of direct attribute access here so that
225         # subclasses aren't required to define `attribute` member
226         attribute = getattr(self, ""attribute"", None)
227         accessor_func = accessor or utils.get_value
228         check_key = attr if attribute is None else attribute
229         return accessor_func(obj, check_key, default)
230 
231     def _validate(self, value):
232         """"""Perform validation on ``value``. Raise a :exc:`ValidationError` if validation
233         does not succeed.
234         """"""
235         errors = []
236         kwargs = {}
237         for validator in self.validators:
238             try:
239                 r = validator(value)
240                 if not isinstance(validator, Validator) and r is False:
241                     raise self.make_error(""validator_failed"")
242             except ValidationError as err:
243                 kwargs.update(err.kwargs)
244                 if isinstance(err.messages, dict):
245                     errors.append(err.messages)
246                 else:
247                     errors.extend(err.messages)
248         if errors:
249             raise ValidationError(errors, **kwargs)
250 
251     def make_error(self, key: str, **kwargs) -> ValidationError:
252         """"""Helper method to make a `ValidationError` with an error message
253         from ``self.error_messages``.
254         """"""
255         try:
256             msg = self.error_messages[key]
257         except KeyError as error:
258             class_name = self.__class__.__name__
259             msg = MISSING_ERROR_MESSAGE.format(class_name=class_name, key=key)
260             raise AssertionError(msg) from error
261         if isinstance(msg, (str, bytes)):
262             msg = msg.format(**kwargs)
263         return ValidationError(msg)
264 
265     def fail(self, key: str, **kwargs):
266         """"""Helper method that raises a `ValidationError` with an error message
267         from ``self.error_messages``.
268 
269         .. deprecated:: 3.0.0
270             Use `make_error <marshmallow.fields.Field.make_error>` instead.
271         """"""
272         warnings.warn(
273             '`Field.fail` is deprecated. Use `raise self.make_error(""{}"", ...)` instead.'.format(
274                 key
275             ),
276             DeprecationWarning,
277         )
278         raise self.make_error(key=key, **kwargs)
279 
280     def _validate_missing(self, value):
281         """"""Validate missing values. Raise a :exc:`ValidationError` if
282         `value` should be considered missing.
283         """"""
284         if value is missing_:
285             if hasattr(self, ""required"") and self.required:
286                 raise self.make_error(""required"")
287         if value is None:
288             if hasattr(self, ""allow_none"") and self.allow_none is not True:
289                 raise self.make_error(""null"")
290 
291     def serialize(self, attr, obj, accessor=None, **kwargs):
292         """"""Pulls the value for the given key from the object, applies the
293         field's formatting and returns the result.
294 
295         :param str attr: The attribute/key to get from the object.
296         :param str obj: The object to access the attribute/key from.
297         :param callable accessor: Function used to access values from ``obj``.
298         :param dict kwargs: Field-specific keyword arguments.
299         """"""
300         if self._CHECK_ATTRIBUTE:
301             value = self.get_value(obj, attr, accessor=accessor)
302             if value is missing_ and hasattr(self, ""default""):
303                 default = self.default
304                 value = default() if callable(default) else default
305             if value is missing_:
306                 return value
307         else:
308             value = None
309         return self._serialize(value, attr, obj, **kwargs)
310 
311     def deserialize(self, value, attr=None, data=None, **kwargs):
312         """"""Deserialize ``value``.
313 
314         :param value: The value to deserialize.
315         :param str attr: The attribute/key in `data` to deserialize.
316         :param dict data: The raw input data passed to `Schema.load`.
317         :param dict kwargs: Field-specific keyword arguments.
318         :raise ValidationError: If an invalid value is passed or if a required value
319             is missing.
320         """"""
321         # Validate required fields, deserialize, then validate
322         # deserialized value
323         self._validate_missing(value)
324         if value is missing_:
325             _miss = self.missing
326             return _miss() if callable(_miss) else _miss
327         if getattr(self, ""allow_none"", False) is True and value is None:
328             return None
329         output = self._deserialize(value, attr, data, **kwargs)
330         self._validate(output)
331         return output
332 
333     # Methods for concrete classes to override.
334 
335     def _bind_to_schema(self, field_name, schema):
336         """"""Update field with values from its parent schema. Called by
337         :meth:`Schema._bind_field <marshmallow.Schema._bind_field>`.
338 
339         :param str field_name: Field name set in schema.
340         :param Schema schema: Parent schema.
341         """"""
342         self.parent = self.parent or schema
343         self.name = self.name or field_name
344 
345     def _serialize(self, value, attr, obj, **kwargs):
346         """"""Serializes ``value`` to a basic Python datatype. Noop by default.
347         Concrete :class:`Field` classes should implement this method.
348 
349         Example: ::
350 
351             class TitleCase(Field):
352                 def _serialize(self, value, attr, obj, **kwargs):
353                     if not value:
354                         return ''
355                     return str(value).title()
356 
357         :param value: The value to be serialized.
358         :param str attr: The attribute or key on the object to be serialized.
359         :param object obj: The object the value was pulled from.
360         :param dict kwargs: Field-specific keyword arguments.
361         :return: The serialized value
362         """"""
363         return value
364 
365     def _deserialize(self, value, attr, data, **kwargs):
366         """"""Deserialize value. Concrete :class:`Field` classes should implement this method.
367 
368         :param value: The value to be deserialized.
369         :param str attr: The attribute/key in `data` to be deserialized.
370         :param dict data: The raw input data passed to the `Schema.load`.
371         :param dict kwargs: Field-specific keyword arguments.
372         :raise ValidationError: In case of formatting or validation failure.
373         :return: The deserialized value.
374 
375         .. versionchanged:: 2.0.0
376             Added ``attr`` and ``data`` parameters.
377 
378         .. versionchanged:: 3.0.0
379             Added ``**kwargs`` to signature.
380         """"""
381         return value
382 
383     # Properties
384 
385     @property
386     def context(self):
387         """"""The context dictionary for the parent :class:`Schema`.""""""
388         return self.parent.context
389 
390     @property
391     def root(self):
392         """"""Reference to the `Schema` that this field belongs to even if it is buried in a
393         container field (e.g. `List`).
394         Return `None` for unbound fields.
395         """"""
396         ret = self
397         while hasattr(ret, ""parent""):
398             ret = ret.parent
399         return ret if isinstance(ret, SchemaABC) else None
400 
401 
402 class Raw(Field):
403     """"""Field that applies no formatting or validation.""""""
404 
405     pass
406 
407 
408 class Nested(Field):
409     """"""Allows you to nest a :class:`Schema <marshmallow.Schema>`
410     inside a field.
411 
412     Examples: ::
413 
414         user = fields.Nested(UserSchema)
415         user2 = fields.Nested('UserSchema')  # Equivalent to above
416         collaborators = fields.Nested(UserSchema, many=True, only=('id',))
417         parent = fields.Nested('self')
418 
419     When passing a `Schema <marshmallow.Schema>` instance as the first argument,
420     the instance's ``exclude``, ``only``, and ``many`` attributes will be respected.
421 
422     Therefore, when passing the ``exclude``, ``only``, or ``many`` arguments to `fields.Nested`,
423     you should pass a `Schema <marshmallow.Schema>` class (not an instance) as the first argument.
424 
425     ::
426 
427         # Yes
428         author = fields.Nested(UserSchema, only=('id', 'name'))
429 
430         # No
431         author = fields.Nested(UserSchema(), only=('id', 'name'))
432 
433     :param Schema nested: The Schema class or class name (string)
434         to nest, or ``""self""`` to nest the :class:`Schema` within itself.
435     :param tuple exclude: A list or tuple of fields to exclude.
436     :param only: A list or tuple of fields to marshal. If `None`, all fields are marshalled.
437         This parameter takes precedence over ``exclude``.
438     :param bool many: Whether the field is a collection of objects.
439     :param unknown: Whether to exclude, include, or raise an error for unknown
440         fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.
441     :param kwargs: The same keyword arguments that :class:`Field` receives.
442     """"""
443 
444     default_error_messages = {""type"": ""Invalid type.""}
445 
446     def __init__(
447         self, nested, *, default=missing_, exclude=tuple(), only=None, **kwargs
448     ):
449         # Raise error if only or exclude is passed as string, not list of strings
450         if only is not None and not is_collection(only):
451             raise StringNotCollectionError('""only"" should be a collection of strings.')
452         if exclude is not None and not is_collection(exclude):
453             raise StringNotCollectionError(
454                 '""exclude"" should be a collection of strings.'
455             )
456         self.nested = nested
457         self.only = only
458         self.exclude = exclude
459         self.many = kwargs.get(""many"", False)
460         self.unknown = kwargs.get(""unknown"")
461         self._schema = None  # Cached Schema instance
462         super().__init__(default=default, **kwargs)
463 
464     @property
465     def schema(self):
466         """"""The nested Schema object.
467 
468         .. versionchanged:: 1.0.0
469             Renamed from `serializer` to `schema`.
470         """"""
471         if not self._schema:
472             # Inherit context from parent.
473             context = getattr(self.parent, ""context"", {})
474             if isinstance(self.nested, SchemaABC):
475                 self._schema = self.nested
476                 self._schema.context.update(context)
477             else:
478                 if isinstance(self.nested, type) and issubclass(self.nested, SchemaABC):
479                     schema_class = self.nested
480                 elif not isinstance(self.nested, (str, bytes)):
481                     raise ValueError(
482                         ""Nested fields must be passed a ""
483                         ""Schema, not {}."".format(self.nested.__class__)
484                     )
485                 elif self.nested == ""self"":
486                     ret = self
487                     while not isinstance(ret, SchemaABC):
488                         ret = ret.parent
489                     schema_class = ret.__class__
490                 else:
491                     schema_class = class_registry.get_class(self.nested)
492                 self._schema = schema_class(
493                     many=self.many,
494                     only=self.only,
495                     exclude=self.exclude,
496                     context=context,
497                     load_only=self._nested_normalized_option(""load_only""),
498                     dump_only=self._nested_normalized_option(""dump_only""),
499                 )
500         return self._schema
501 
502     def _nested_normalized_option(self, option_name):
503         nested_field = ""%s."" % self.name
504         return [
505             field.split(nested_field, 1)[1]
506             for field in getattr(self.root, option_name, set())
507             if field.startswith(nested_field)
508         ]
509 
510     def _serialize(self, nested_obj, attr, obj, many=False, **kwargs):
511         # Load up the schema first. This allows a RegistryError to be raised
512         # if an invalid schema name was passed
513         schema = self.schema
514         if nested_obj is None:
515             return None
516         return schema.dump(nested_obj, many=self.many or many)
517 
518     def _test_collection(self, value, many=False):
519         many = self.many or many
520         if many and not utils.is_collection(value):
521             raise self.make_error(""type"", input=value, type=value.__class__.__name__)
522 
523     def _load(self, value, data, partial=None, many=False):
524         try:
525             valid_data = self.schema.load(
526                 value, unknown=self.unknown, partial=partial, many=self.many or many
527             )
528         except ValidationError as error:
529             raise ValidationError(
530                 error.messages, valid_data=error.valid_data
531             ) from error
532         return valid_data
533 
534     def _deserialize(self, value, attr, data, partial=None, many=False, **kwargs):
535         """"""Same as :meth:`Field._deserialize` with additional ``partial`` argument.
536 
537         :param bool|tuple partial: For nested schemas, the ``partial``
538             parameter passed to `Schema.load`.
539 
540         .. versionchanged:: 3.0.0
541             Add ``partial`` parameter.
542         """"""
543         self._test_collection(value, many=many)
544         return self._load(value, data, partial=partial, many=many)
545 
546 
547 class Pluck(Nested):
548     """"""Allows you to replace nested data with one of the data's fields.
549 
550     Example: ::
551 
552         from marshmallow import Schema, fields
553 
554         class ArtistSchema(Schema):
555             id = fields.Int()
556             name = fields.Str()
557 
558         class AlbumSchema(Schema):
559             artist = fields.Pluck(ArtistSchema, 'id')
560 
561 
562         in_data = {'artist': 42}
563         loaded = AlbumSchema().load(in_data) # => {'artist': {'id': 42}}
564         dumped = AlbumSchema().dump(loaded)  # => {'artist': 42}
565 
566     :param Schema nested: The Schema class or class name (string)
567         to nest, or ``""self""`` to nest the :class:`Schema` within itself.
568     :param str field_name: The key to pluck a value from.
569     :param kwargs: The same keyword arguments that :class:`Nested` receives.
570     """"""
571 
572     def __init__(self, nested, field_name, **kwargs):
573         super().__init__(nested, only=(field_name,), **kwargs)
574         self.field_name = field_name
575 
576     @property
577     def _field_data_key(self):
578         only_field = self.schema.fields[self.field_name]
579         return only_field.data_key or self.field_name
580 
581     def _serialize(self, nested_obj, attr, obj, **kwargs):
582         ret = super()._serialize(nested_obj, attr, obj, **kwargs)
583         if ret is None:
584             return None
585         if self.many:
586             return utils.pluck(ret, key=self._field_data_key)
587         return ret[self._field_data_key]
588 
589     def _deserialize(self, value, attr, data, partial=None, **kwargs):
590         self._test_collection(value)
591         if self.many:
592             value = [{self._field_data_key: v} for v in value]
593         else:
594             value = {self._field_data_key: value}
595         return self._load(value, data, partial=partial)
596 
597 
598 class List(Field):
599     """"""A list field, composed with another `Field` class or
600     instance.
601 
602     Example: ::
603 
604         numbers = fields.List(fields.Float())
605 
606     :param Field cls_or_instance: A field class or instance.
607     :param bool default: Default value for serialization.
608     :param kwargs: The same keyword arguments that :class:`Field` receives.
609 
610     .. versionchanged:: 2.0.0
611         The ``allow_none`` parameter now applies to deserialization and
612         has the same semantics as the other fields.
613 
614     .. versionchanged:: 3.0.0rc9
615         Does not serialize scalar values to single-item lists.
616     """"""
617 
618     default_error_messages = {""invalid"": ""Not a valid list.""}
619 
620     def __init__(self, cls_or_instance, **kwargs):
621         super().__init__(**kwargs)
622         try:
623             self.inner = resolve_field_instance(cls_or_instance)
624         except FieldInstanceResolutionError as error:
625             raise ValueError(
626                 ""The list elements must be a subclass or instance of ""
627                 ""marshmallow.base.FieldABC.""
628             ) from error
629         if isinstance(self.inner, Nested):
630             self.only = self.inner.only
631             self.exclude = self.inner.exclude
632 
633     def _bind_to_schema(self, field_name, schema):
634         super()._bind_to_schema(field_name, schema)
635         self.inner = copy.deepcopy(self.inner)
636         self.inner._bind_to_schema(field_name, self)
637         if isinstance(self.inner, Nested):
638             self.inner.only = self.only
639             self.inner.exclude = self.exclude
640 
641     def _serialize(self, value, attr, obj, **kwargs):
642         if value is None:
643             return None
644         # Optimize dumping a list of Nested objects by calling dump(many=True)
645         if isinstance(self.inner, Nested) and not self.inner.many:
646             return self.inner._serialize(value, attr, obj, many=True, **kwargs)
647         return [self.inner._serialize(each, attr, obj, **kwargs) for each in value]
648 
649     def _deserialize(self, value, attr, data, **kwargs):
650         if not utils.is_collection(value):
651             raise self.make_error(""invalid"")
652         # Optimize loading a list of Nested objects by calling load(many=True)
653         if isinstance(self.inner, Nested) and not self.inner.many:
654             return self.inner.deserialize(value, many=True, **kwargs)
655 
656         result = []
657         errors = {}
658         for idx, each in enumerate(value):
659             try:
660                 result.append(self.inner.deserialize(each, **kwargs))
661             except ValidationError as error:
662                 if error.valid_data is not None:
663                     result.append(error.valid_data)
664                 errors.update({idx: error.messages})
665         if errors:
666             raise ValidationError(errors, valid_data=result)
667         return result
668 
669 
670 class Tuple(Field):
671     """"""A tuple field, composed of a fixed number of other `Field` classes or
672     instances
673 
674     Example: ::
675 
676         row = Tuple((fields.String(), fields.Integer(), fields.Float()))
677 
678     .. note::
679         Because of the structured nature of `collections.namedtuple` and
680         `typing.NamedTuple`, using a Schema within a Nested field for them is
681         more appropriate than using a `Tuple` field.
682 
683     :param Iterable[Field] tuple_fields: An iterable of field classes or
684         instances.
685     :param kwargs: The same keyword arguments that :class:`Field` receives.
686 
687     .. versionadded:: 3.0.0rc4
688     """"""
689 
690     default_error_messages = {""invalid"": ""Not a valid tuple.""}
691 
692     def __init__(self, tuple_fields, *args, **kwargs):
693         super().__init__(*args, **kwargs)
694         if not utils.is_collection(tuple_fields):
695             raise ValueError(
696                 ""tuple_fields must be an iterable of Field classes or "" ""instances.""
697             )
698 
699         try:
700             self.tuple_fields = [
701                 resolve_field_instance(cls_or_instance)
702                 for cls_or_instance in tuple_fields
703             ]
704         except FieldInstanceResolutionError as error:
705             raise ValueError(
706                 'Elements of ""tuple_fields"" must be subclasses or '
707                 ""instances of marshmallow.base.FieldABC.""
708             ) from error
709 
710         self.validate_length = Length(equal=len(self.tuple_fields))
711 
712     def _bind_to_schema(self, field_name, schema):
713         super()._bind_to_schema(field_name, schema)
714         new_tuple_fields = []
715         for field in self.tuple_fields:
716             field = copy.deepcopy(field)
717             field._bind_to_schema(field_name, self)
718             new_tuple_fields.append(field)
719 
720         self.tuple_fields = new_tuple_fields
721 
722     def _serialize(self, value, attr, obj, **kwargs):
723         if value is None:
724             return None
725 
726         return tuple(
727             field._serialize(each, attr, obj, **kwargs)
728             for field, each in zip(self.tuple_fields, value)
729         )
730 
731     def _deserialize(self, value, attr, data, **kwargs):
732         if not utils.is_collection(value):
733             raise self.make_error(""invalid"")
734 
735         self.validate_length(value)
736 
737         result = []
738         errors = {}
739 
740         for idx, (field, each) in enumerate(zip(self.tuple_fields, value)):
741             try:
742                 result.append(field.deserialize(each, **kwargs))
743             except ValidationError as error:
744                 if error.valid_data is not None:
745                     result.append(error.valid_data)
746                 errors.update({idx: error.messages})
747         if errors:
748             raise ValidationError(errors, valid_data=result)
749 
750         return tuple(result)
751 
752 
753 class String(Field):
754     """"""A string field.
755 
756     :param kwargs: The same keyword arguments that :class:`Field` receives.
757     """"""
758 
759     default_error_messages = {
760         ""invalid"": ""Not a valid string."",
761         ""invalid_utf8"": ""Not a valid utf-8 string."",
762     }
763 
764     def _serialize(self, value, attr, obj, **kwargs):
765         if value is None:
766             return None
767         return utils.ensure_text_type(value)
768 
769     def _deserialize(self, value, attr, data, **kwargs):
770         if not isinstance(value, (str, bytes)):
771             raise self.make_error(""invalid"")
772         try:
773             return utils.ensure_text_type(value)
774         except UnicodeDecodeError as error:
775             raise self.make_error(""invalid_utf8"") from error
776 
777 
778 class UUID(String):
779     """"""A UUID field.""""""
780 
781     default_error_messages = {""invalid_uuid"": ""Not a valid UUID.""}
782 
783     def _validated(self, value):
784         """"""Format the value or raise a :exc:`ValidationError` if an error occurs.""""""
785         if value is None:
786             return None
787         if isinstance(value, uuid.UUID):
788             return value
789         try:
790             if isinstance(value, bytes) and len(value) == 16:
791                 return uuid.UUID(bytes=value)
792             else:
793                 return uuid.UUID(value)
794         except (ValueError, AttributeError, TypeError) as error:
795             raise self.make_error(""invalid_uuid"") from error
796 
797     def _serialize(self, value, attr, obj, **kwargs):
798         val = str(value) if value is not None else None
799         return super()._serialize(val, attr, obj, **kwargs)
800 
801     def _deserialize(self, value, attr, data, **kwargs):
802         return self._validated(value)
803 
804 
805 class Number(Field):
806     """"""Base class for number fields.
807 
808     :param bool as_string: If True, format the serialized value as a string.
809     :param kwargs: The same keyword arguments that :class:`Field` receives.
810     """"""
811 
812     num_type = float
813     default_error_messages = {
814         ""invalid"": ""Not a valid number."",
815         ""too_large"": ""Number too large."",
816     }
817 
818     def __init__(self, *, as_string=False, **kwargs):
819         self.as_string = as_string
820         super().__init__(**kwargs)
821 
822     def _format_num(self, value):
823         """"""Return the number value for value, given this field's `num_type`.""""""
824         return self.num_type(value)
825 
826     def _validated(self, value):
827         """"""Format the value or raise a :exc:`ValidationError` if an error occurs.""""""
828         if value is None:
829             return None
830         # (value is True or value is False) is ~5x faster than isinstance(value, bool)
831         if value is True or value is False:
832             raise self.make_error(""invalid"", input=value)
833         try:
834             return self._format_num(value)
835         except (TypeError, ValueError) as error:
836             raise self.make_error(""invalid"", input=value) from error
837         except OverflowError as error:
838             raise self.make_error(""too_large"", input=value) from error
839 
840     def _to_string(self, value):
841         return str(value)
842 
843     def _serialize(self, value, attr, obj, **kwargs):
844         """"""Return a string if `self.as_string=True`, otherwise return this field's `num_type`.""""""
845         if value is None:
846             return None
847         ret = self._format_num(value)
848         return self._to_string(ret) if self.as_string else ret
849 
850     def _deserialize(self, value, attr, data, **kwargs):
851         return self._validated(value)
852 
853 
854 class Integer(Number):
855     """"""An integer field.
856 
857     :param kwargs: The same keyword arguments that :class:`Number` receives.
858     """"""
859 
860     num_type = int
861     default_error_messages = {""invalid"": ""Not a valid integer.""}
862 
863     def __init__(self, *, strict=False, **kwargs):
864         self.strict = strict
865         super().__init__(**kwargs)
866 
867     # override Number
868     def _validated(self, value):
869         if self.strict:
870             if isinstance(value, numbers.Number) and isinstance(
871                 value, numbers.Integral
872             ):
873                 return super()._validated(value)
874             raise self.make_error(""invalid"", input=value)
875         return super()._validated(value)
876 
877 
878 class Float(Number):
879     """"""A double as an IEEE-754 double precision string.
880 
881     :param bool allow_nan: If `True`, `NaN`, `Infinity` and `-Infinity` are allowed,
882         even though they are illegal according to the JSON specification.
883     :param bool as_string: If True, format the value as a string.
884     :param kwargs: The same keyword arguments that :class:`Number` receives.
885     """"""
886 
887     num_type = float
888     default_error_messages = {
889         ""special"": ""Special numeric values (nan or infinity) are not permitted.""
890     }
891 
892     def __init__(self, *, allow_nan=False, as_string=False, **kwargs):
893         self.allow_nan = allow_nan
894         super().__init__(as_string=as_string, **kwargs)
895 
896     def _validated(self, value):
897         num = super()._validated(value)
898         if self.allow_nan is False:
899             if math.isnan(num) or num == float(""inf"") or num == float(""-inf""):
900                 raise self.make_error(""special"")
901         return num
902 
903 
904 class Decimal(Number):
905     """"""A field that (de)serializes to the Python ``decimal.Decimal`` type.
906     It's safe to use when dealing with money values, percentages, ratios
907     or other numbers where precision is critical.
908 
909     .. warning::
910 
911         This field serializes to a `decimal.Decimal` object by default. If you need
912         to render your data as JSON, keep in mind that the `json` module from the
913         standard library does not encode `decimal.Decimal`. Therefore, you must use
914         a JSON library that can handle decimals, such as `simplejson`, or serialize
915         to a string by passing ``as_string=True``.
916 
917     .. warning::
918 
919         If a JSON `float` value is passed to this field for deserialization it will
920         first be cast to its corresponding `string` value before being deserialized
921         to a `decimal.Decimal` object. The default `__str__` implementation of the
922         built-in Python `float` type may apply a destructive transformation upon
923         its input data and therefore cannot be relied upon to preserve precision.
924         To avoid this, you can instead pass a JSON `string` to be deserialized
925         directly.
926 
927     :param int places: How many decimal places to quantize the value. If `None`, does
928         not quantize the value.
929     :param rounding: How to round the value during quantize, for example
930         `decimal.ROUND_UP`. If None, uses the rounding value from
931         the current thread's context.
932     :param bool allow_nan: If `True`, `NaN`, `Infinity` and `-Infinity` are allowed,
933         even though they are illegal according to the JSON specification.
934     :param bool as_string: If True, serialize to a string instead of a Python
935         `decimal.Decimal` type.
936     :param kwargs: The same keyword arguments that :class:`Number` receives.
937 
938     .. versionadded:: 1.2.0
939     """"""
940 
941     num_type = decimal.Decimal
942 
943     default_error_messages = {
944         ""special"": ""Special numeric values (nan or infinity) are not permitted.""
945     }
946 
947     def __init__(
948         self, places=None, rounding=None, *, allow_nan=False, as_string=False, **kwargs
949     ):
950         self.places = (
951             decimal.Decimal((0, (1,), -places)) if places is not None else None
952         )
953         self.rounding = rounding
954         self.allow_nan = allow_nan
955         super().__init__(as_string=as_string, **kwargs)
956 
957     # override Number
958     def _format_num(self, value):
959         num = decimal.Decimal(str(value))
960         if self.allow_nan:
961             if num.is_nan():
962                 return decimal.Decimal(""NaN"")  # avoid sNaN, -sNaN and -NaN
963         if self.places is not None and num.is_finite():
964             num = num.quantize(self.places, rounding=self.rounding)
965         return num
966 
967     # override Number
968     def _validated(self, value):
969         try:
970             num = super()._validated(value)
971         except decimal.InvalidOperation as error:
972             raise self.make_error(""invalid"") from error
973         if not self.allow_nan and (num.is_nan() or num.is_infinite()):
974             raise self.make_error(""special"")
975         return num
976 
977     # override Number
978     def _to_string(self, value):
979         return format(value, ""f"")
980 
981 
982 class Boolean(Field):
983     """"""A boolean field.
984 
985     :param set truthy: Values that will (de)serialize to `True`. If an empty
986         set, any non-falsy value will deserialize to `True`. If `None`,
987         `marshmallow.fields.Boolean.truthy` will be used.
988     :param set falsy: Values that will (de)serialize to `False`. If `None`,
989         `marshmallow.fields.Boolean.falsy` will be used.
990     :param kwargs: The same keyword arguments that :class:`Field` receives.
991     """"""
992 
993     #: Default truthy values.
994     truthy = {
995         ""t"",
996         ""T"",
997         ""true"",
998         ""True"",
999         ""TRUE"",
1000         ""on"",
1001         ""On"",
1002         ""ON"",
1003         ""y"",
1004         ""Y"",
1005         ""yes"",
1006         ""Yes"",
1007         ""YES"",
1008         ""1"",
1009         1,
1010         True,
1011     }
1012     #: Default falsy values.
1013     falsy = {
1014         ""f"",
1015         ""F"",
1016         ""false"",
1017         ""False"",
1018         ""FALSE"",
1019         ""off"",
1020         ""Off"",
1021         ""OFF"",
1022         ""n"",
1023         ""N"",
1024         ""no"",
1025         ""No"",
1026         ""NO"",
1027         ""0"",
1028         0,
1029         0.0,
1030         False,
1031     }
1032 
1033     default_error_messages = {""invalid"": ""Not a valid boolean.""}
1034 
1035     def __init__(self, *, truthy=None, falsy=None, **kwargs):
1036         super().__init__(**kwargs)
1037 
1038         if truthy is not None:
1039             self.truthy = set(truthy)
1040         if falsy is not None:
1041             self.falsy = set(falsy)
1042 
1043     def _serialize(self, value, attr, obj, **kwargs):
1044         if value is None:
1045             return None
1046         elif value in self.truthy:
1047             return True
1048         elif value in self.falsy:
1049             return False
1050 
1051         return bool(value)
1052 
1053     def _deserialize(self, value, attr, data, **kwargs):
1054         if not self.truthy:
1055             return bool(value)
1056         else:
1057             try:
1058                 if value in self.truthy:
1059                     return True
1060                 elif value in self.falsy:
1061                     return False
1062             except TypeError as error:
1063                 raise self.make_error(""invalid"", input=value) from error
1064         raise self.make_error(""invalid"", input=value)
1065 
1066 
1067 class DateTime(Field):
1068     """"""A formatted datetime string.
1069 
1070     Example: ``'2014-12-22T03:12:58.019077+00:00'``
1071 
1072     :param str format: Either ``""rfc""`` (for RFC822), ``""iso""`` (for ISO8601),
1073         or a date format string. If `None`, defaults to ""iso"".
1074     :param kwargs: The same keyword arguments that :class:`Field` receives.
1075 
1076     .. versionchanged:: 3.0.0rc9
1077         Does not modify timezone information on (de)serialization.
1078     """"""
1079 
1080     SERIALIZATION_FUNCS = {
1081         ""iso"": utils.isoformat,
1082         ""iso8601"": utils.isoformat,
1083         ""rfc"": utils.rfcformat,
1084         ""rfc822"": utils.rfcformat,
1085     }
1086 
1087     DESERIALIZATION_FUNCS = {
1088         ""iso"": utils.from_iso_datetime,
1089         ""iso8601"": utils.from_iso_datetime,
1090         ""rfc"": utils.from_rfc,
1091         ""rfc822"": utils.from_rfc,
1092     }
1093 
1094     DEFAULT_FORMAT = ""iso""
1095 
1096     OBJ_TYPE = ""datetime""
1097 
1098     SCHEMA_OPTS_VAR_NAME = ""datetimeformat""
1099 
1100     default_error_messages = {
1101         ""invalid"": ""Not a valid {obj_type}."",
1102         ""invalid_awareness"": ""Not a valid {awareness} {obj_type}."",
1103         ""format"": '""{input}"" cannot be formatted as a {obj_type}.',
1104     }
1105 
1106     def __init__(self, format=None, **kwargs):
1107         super().__init__(**kwargs)
1108         # Allow this to be None. It may be set later in the ``_serialize``
1109         # or ``_deserialize`` methods. This allows a Schema to dynamically set the
1110         # format, e.g. from a Meta option
1111         self.format = format
1112 
1113     def _bind_to_schema(self, field_name, schema):
1114         super()._bind_to_schema(field_name, schema)
1115         self.format = (
1116             self.format
1117             or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)
1118             or self.DEFAULT_FORMAT
1119         )
1120 
1121     def _serialize(self, value, attr, obj, **kwargs):
1122         if value is None:
1123             return None
1124         data_format = self.format or self.DEFAULT_FORMAT
1125         format_func = self.SERIALIZATION_FUNCS.get(data_format)
1126         if format_func:
1127             return format_func(value)
1128         else:
1129             return value.strftime(data_format)
1130 
1131     def _deserialize(self, value, attr, data, **kwargs):
1132         if not value:  # Falsy values, e.g. '', None, [] are not valid
1133             raise self.make_error(""invalid"", input=value, obj_type=self.OBJ_TYPE)
1134         data_format = self.format or self.DEFAULT_FORMAT
1135         func = self.DESERIALIZATION_FUNCS.get(data_format)
1136         if func:
1137             try:
1138                 return func(value)
1139             except (TypeError, AttributeError, ValueError) as error:
1140                 raise self.make_error(
1141                     ""invalid"", input=value, obj_type=self.OBJ_TYPE
1142                 ) from error
1143         else:
1144             try:
1145                 return self._make_object_from_format(value, data_format)
1146             except (TypeError, AttributeError, ValueError) as error:
1147                 raise self.make_error(
1148                     ""invalid"", input=value, obj_type=self.OBJ_TYPE
1149                 ) from error
1150 
1151     @staticmethod
1152     def _make_object_from_format(value, data_format):
1153         return dt.datetime.strptime(value, data_format)
1154 
1155 
1156 class NaiveDateTime(DateTime):
1157     """"""A formatted naive datetime string.
1158 
1159     :param str format: See :class:`DateTime`.
1160     :param timezone timezone: Used on deserialization. If `None`,
1161         aware datetimes are rejected. If not `None`, aware datetimes are
1162         converted to this timezone before their timezone information is
1163         removed.
1164     :param kwargs: The same keyword arguments that :class:`Field` receives.
1165 
1166     .. versionadded:: 3.0.0rc9
1167     """"""
1168 
1169     AWARENESS = ""naive""
1170 
1171     def __init__(self, format=None, *, timezone=None, **kwargs):
1172         super().__init__(format=format, **kwargs)
1173         self.timezone = timezone
1174 
1175     def _deserialize(self, value, attr, data, **kwargs):
1176         ret = super()._deserialize(value, attr, data, **kwargs)
1177         if is_aware(ret):
1178             if self.timezone is None:
1179                 raise self.make_error(
1180                     ""invalid_awareness"",
1181                     awareness=self.AWARENESS,
1182                     obj_type=self.OBJ_TYPE,
1183                 )
1184             ret = ret.astimezone(self.timezone).replace(tzinfo=None)
1185         return ret
1186 
1187 
1188 class AwareDateTime(DateTime):
1189     """"""A formatted aware datetime string.
1190 
1191     :param str format: See :class:`DateTime`.
1192     :param timezone default_timezone: Used on deserialization. If `None`, naive
1193         datetimes are rejected. If not `None`, naive datetimes are set this
1194         timezone.
1195     :param kwargs: The same keyword arguments that :class:`Field` receives.
1196 
1197     .. versionadded:: 3.0.0rc9
1198     """"""
1199 
1200     AWARENESS = ""aware""
1201 
1202     def __init__(self, format=None, *, default_timezone=None, **kwargs):
1203         super().__init__(format=format, **kwargs)
1204         self.default_timezone = default_timezone
1205 
1206     def _deserialize(self, value, attr, data, **kwargs):
1207         ret = super()._deserialize(value, attr, data, **kwargs)
1208         if not is_aware(ret):
1209             if self.default_timezone is None:
1210                 raise self.make_error(
1211                     ""invalid_awareness"",
1212                     awareness=self.AWARENESS,
1213                     obj_type=self.OBJ_TYPE,
1214                 )
1215             ret = ret.replace(tzinfo=self.default_timezone)
1216         return ret
1217 
1218 
1219 class Time(Field):
1220     """"""ISO8601-formatted time string.
1221 
1222     :param kwargs: The same keyword arguments that :class:`Field` receives.
1223     """"""
1224 
1225     default_error_messages = {
1226         ""invalid"": ""Not a valid time."",
1227         ""format"": '""{input}"" cannot be formatted as a time.',
1228     }
1229 
1230     def _serialize(self, value, attr, obj, **kwargs):
1231         if value is None:
1232             return None
1233         ret = value.isoformat()
1234         if value.microsecond:
1235             return ret[:15]
1236         return ret
1237 
1238     def _deserialize(self, value, attr, data, **kwargs):
1239         """"""Deserialize an ISO8601-formatted time to a :class:`datetime.time` object.""""""
1240         if not value:  # falsy values are invalid
1241             raise self.make_error(""invalid"")
1242         try:
1243             return utils.from_iso_time(value)
1244         except (AttributeError, TypeError, ValueError) as error:
1245             raise self.make_error(""invalid"") from error
1246 
1247 
1248 class Date(DateTime):
1249     """"""ISO8601-formatted date string.
1250 
1251     :param format: Either ``""iso""`` (for ISO8601) or a date format string.
1252         If `None`, defaults to ""iso"".
1253     :param kwargs: The same keyword arguments that :class:`Field` receives.
1254     """"""
1255 
1256     default_error_messages = {
1257         ""invalid"": ""Not a valid date."",
1258         ""format"": '""{input}"" cannot be formatted as a date.',
1259     }
1260 
1261     SERIALIZATION_FUNCS = {""iso"": utils.to_iso_date, ""iso8601"": utils.to_iso_date}
1262 
1263     DESERIALIZATION_FUNCS = {""iso"": utils.from_iso_date, ""iso8601"": utils.from_iso_date}
1264 
1265     DEFAULT_FORMAT = ""iso""
1266 
1267     OBJ_TYPE = ""date""
1268 
1269     SCHEMA_OPTS_VAR_NAME = ""dateformat""
1270 
1271     @staticmethod
1272     def _make_object_from_format(value, data_format):
1273         return dt.datetime.strptime(value, data_format).date()
1274 
1275 
1276 class TimeDelta(Field):
1277     """"""A field that (de)serializes a :class:`datetime.timedelta` object to an
1278     integer and vice versa. The integer can represent the number of days,
1279     seconds or microseconds.
1280 
1281     :param str precision: Influences how the integer is interpreted during
1282         (de)serialization. Must be 'days', 'seconds', 'microseconds',
1283         'milliseconds', 'minutes', 'hours' or 'weeks'.
1284     :param kwargs: The same keyword arguments that :class:`Field` receives.
1285 
1286     .. versionchanged:: 2.0.0
1287         Always serializes to an integer value to avoid rounding errors.
1288         Add `precision` parameter.
1289     """"""
1290 
1291     DAYS = ""days""
1292     SECONDS = ""seconds""
1293     MICROSECONDS = ""microseconds""
1294     MILLISECONDS = ""milliseconds""
1295     MINUTES = ""minutes""
1296     HOURS = ""hours""
1297     WEEKS = ""weeks""
1298 
1299     default_error_messages = {
1300         ""invalid"": ""Not a valid period of time."",
1301         ""format"": ""{input!r} cannot be formatted as a timedelta."",
1302     }
1303 
1304     def __init__(self, precision=SECONDS, **kwargs):
1305         precision = precision.lower()
1306         units = (
1307             self.DAYS,
1308             self.SECONDS,
1309             self.MICROSECONDS,
1310             self.MILLISECONDS,
1311             self.MINUTES,
1312             self.HOURS,
1313             self.WEEKS,
1314         )
1315 
1316         if precision not in units:
1317             msg = 'The precision must be {} or ""{}"".'.format(
1318                 "", "".join(['""{}""'.format(each) for each in units[:-1]]), units[-1]
1319             )
1320             raise ValueError(msg)
1321 
1322         self.precision = precision
1323         super().__init__(**kwargs)
1324 
1325     def _serialize(self, value, attr, obj, **kwargs):
1326         if value is None:
1327             return None
1328         base_unit = dt.timedelta(**{self.precision: 1})
1329         return int(value.total_seconds() / base_unit.total_seconds())
1330 
1331     def _deserialize(self, value, attr, data, **kwargs):
1332         try:
1333             value = int(value)
1334         except (TypeError, ValueError) as error:
1335             raise self.make_error(""invalid"") from error
1336 
1337         kwargs = {self.precision: value}
1338 
1339         try:
1340             return dt.timedelta(**kwargs)
1341         except OverflowError as error:
1342             raise self.make_error(""invalid"") from error
1343 
1344 
1345 class Mapping(Field):
1346     """"""An abstract class for objects with key-value pairs.
1347 
1348     :param Field keys: A field class or instance for dict keys.
1349     :param Field values: A field class or instance for dict values.
1350     :param kwargs: The same keyword arguments that :class:`Field` receives.
1351 
1352     .. note::
1353         When the structure of nested data is not known, you may omit the
1354         `keys` and `values` arguments to prevent content validation.
1355 
1356     .. versionadded:: 3.0.0rc4
1357     """"""
1358 
1359     mapping_type = dict
1360     default_error_messages = {""invalid"": ""Not a valid mapping type.""}
1361 
1362     def __init__(self, keys=None, values=None, **kwargs):
1363         super().__init__(**kwargs)
1364         if keys is None:
1365             self.key_field = None
1366         else:
1367             try:
1368                 self.key_field = resolve_field_instance(keys)
1369             except FieldInstanceResolutionError as error:
1370                 raise ValueError(
1371                     '""keys"" must be a subclass or instance of '
1372                     ""marshmallow.base.FieldABC.""
1373                 ) from error
1374 
1375         if values is None:
1376             self.value_field = None
1377         else:
1378             try:
1379                 self.value_field = resolve_field_instance(values)
1380             except FieldInstanceResolutionError as error:
1381                 raise ValueError(
1382                     '""values"" must be a subclass or instance of '
1383                     ""marshmallow.base.FieldABC.""
1384                 ) from error
1385             if isinstance(self.value_field, Nested):
1386                 self.only = self.value_field.only
1387                 self.exclude = self.value_field.exclude
1388 
1389     def _bind_to_schema(self, field_name, schema):
1390         super()._bind_to_schema(field_name, schema)
1391         if self.value_field:
1392             self.value_field = copy.deepcopy(self.value_field)
1393             self.value_field._bind_to_schema(field_name, self)
1394         if isinstance(self.value_field, Nested):
1395             self.value_field.only = self.only
1396             self.value_field.exclude = self.exclude
1397         if self.key_field:
1398             self.key_field = copy.deepcopy(self.key_field)
1399             self.key_field._bind_to_schema(field_name, self)
1400 
1401     def _serialize(self, value, attr, obj, **kwargs):
1402         if value is None:
1403             return None
1404         if not self.value_field and not self.key_field:
1405             return value
1406 
1407         # Â Serialize keys
1408         if self.key_field is None:
1409             keys = {k: k for k in value.keys()}
1410         else:
1411             keys = {
1412                 k: self.key_field._serialize(k, None, None, **kwargs)
1413                 for k in value.keys()
1414             }
1415 
1416         # Â Serialize values
1417         result = self.mapping_type()
1418         if self.value_field is None:
1419             for k, v in value.items():
1420                 if k in keys:
1421                     result[keys[k]] = v
1422         else:
1423             for k, v in value.items():
1424                 result[keys[k]] = self.value_field._serialize(v, None, None, **kwargs)
1425 
1426         return result
1427 
1428     def _deserialize(self, value, attr, data, **kwargs):
1429         if not isinstance(value, _Mapping):
1430             raise self.make_error(""invalid"")
1431         if not self.value_field and not self.key_field:
1432             return value
1433 
1434         errors = collections.defaultdict(dict)
1435 
1436         # Â Deserialize keys
1437         if self.key_field is None:
1438             keys = {k: k for k in value.keys()}
1439         else:
1440             keys = {}
1441             for key in value.keys():
1442                 try:
1443                     keys[key] = self.key_field.deserialize(key, **kwargs)
1444                 except ValidationError as error:
1445                     errors[key][""key""] = error.messages
1446 
1447         # Â Deserialize values
1448         result = self.mapping_type()
1449         if self.value_field is None:
1450             for k, v in value.items():
1451                 if k in keys:
1452                     result[keys[k]] = v
1453         else:
1454             for key, val in value.items():
1455                 try:
1456                     deser_val = self.value_field.deserialize(val, **kwargs)
1457                 except ValidationError as error:
1458                     errors[key][""value""] = error.messages
1459                     if error.valid_data is not None and key in keys:
1460                         result[keys[key]] = error.valid_data
1461                 else:
1462                     if key in keys:
1463                         result[keys[key]] = deser_val
1464 
1465         if errors:
1466             raise ValidationError(errors, valid_data=result)
1467 
1468         return result
1469 
1470 
1471 class Dict(Mapping):
1472     """"""A dict field. Supports dicts and dict-like objects. Extends
1473     Mapping with dict as the mapping_type.
1474 
1475     Example: ::
1476 
1477         numbers = fields.Dict(keys=fields.Str(), values=fields.Float())
1478 
1479     :param kwargs: The same keyword arguments that :class:`Mapping` receives.
1480 
1481     .. versionadded:: 2.1.0
1482     """"""
1483 
1484     mapping_type = dict
1485 
1486 
1487 class Url(String):
1488     """"""A validated URL field. Validation occurs during both serialization and
1489     deserialization.
1490 
1491     :param default: Default value for the field if the attribute is not set.
1492     :param str attribute: The name of the attribute to get the value from. If
1493         `None`, assumes the attribute has the same name as the field.
1494     :param bool relative: Whether to allow relative URLs.
1495     :param bool require_tld: Whether to reject non-FQDN hostnames.
1496     :param kwargs: The same keyword arguments that :class:`String` receives.
1497     """"""
1498 
1499     default_error_messages = {""invalid"": ""Not a valid URL.""}
1500 
1501     def __init__(self, *, relative=False, schemes=None, require_tld=True, **kwargs):
1502         super().__init__(**kwargs)
1503 
1504         self.relative = relative
1505         self.require_tld = require_tld
1506         # Insert validation into self.validators so that multiple errors can be
1507         # stored.
1508         self.validators.insert(
1509             0,
1510             validate.URL(
1511                 relative=self.relative,
1512                 schemes=schemes,
1513                 require_tld=self.require_tld,
1514                 error=self.error_messages[""invalid""],
1515             ),
1516         )
1517 
1518 
1519 class Email(String):
1520     """"""A validated email field. Validation occurs during both serialization and
1521     deserialization.
1522 
1523     :param args: The same positional arguments that :class:`String` receives.
1524     :param kwargs: The same keyword arguments that :class:`String` receives.
1525     """"""
1526 
1527     default_error_messages = {""invalid"": ""Not a valid email address.""}
1528 
1529     def __init__(self, *args, **kwargs):
1530         super().__init__(*args, **kwargs)
1531         # Insert validation into self.validators so that multiple errors can be
1532         # stored.
1533         self.validators.insert(0, validate.Email(error=self.error_messages[""invalid""]))
1534 
1535 
1536 class Method(Field):
1537     """"""A field that takes the value returned by a `Schema` method.
1538 
1539     :param str serialize: The name of the Schema method from which
1540         to retrieve the value. The method must take an argument ``obj``
1541         (in addition to self) that is the object to be serialized.
1542     :param str deserialize: Optional name of the Schema method for deserializing
1543         a value The method must take a single argument ``value``, which is the
1544         value to deserialize.
1545 
1546     .. versionchanged:: 2.0.0
1547         Removed optional ``context`` parameter on methods. Use ``self.context`` instead.
1548 
1549     .. versionchanged:: 2.3.0
1550         Deprecated ``method_name`` parameter in favor of ``serialize`` and allow
1551         ``serialize`` to not be passed at all.
1552 
1553     .. versionchanged:: 3.0.0
1554         Removed ``method_name`` parameter.
1555     """"""
1556 
1557     _CHECK_ATTRIBUTE = False
1558 
1559     def __init__(self, serialize=None, deserialize=None, **kwargs):
1560         # Set dump_only and load_only based on arguments
1561         kwargs[""dump_only""] = bool(serialize) and not bool(deserialize)
1562         kwargs[""load_only""] = bool(deserialize) and not bool(serialize)
1563         super().__init__(**kwargs)
1564         self.serialize_method_name = serialize
1565         self.deserialize_method_name = deserialize
1566 
1567     def _serialize(self, value, attr, obj, **kwargs):
1568         if not self.serialize_method_name:
1569             return missing_
1570 
1571         method = utils.callable_or_raise(
1572             getattr(self.parent, self.serialize_method_name, None)
1573         )
1574         return method(obj)
1575 
1576     def _deserialize(self, value, attr, data, **kwargs):
1577         if self.deserialize_method_name:
1578             method = utils.callable_or_raise(
1579                 getattr(self.parent, self.deserialize_method_name, None)
1580             )
1581             return method(value)
1582         return value
1583 
1584 
1585 class Function(Field):
1586     """"""A field that takes the value returned by a function.
1587 
1588     :param callable serialize: A callable from which to retrieve the value.
1589         The function must take a single argument ``obj`` which is the object
1590         to be serialized. It can also optionally take a ``context`` argument,
1591         which is a dictionary of context variables passed to the serializer.
1592         If no callable is provided then the ```load_only``` flag will be set
1593         to True.
1594     :param callable deserialize: A callable from which to retrieve the value.
1595         The function must take a single argument ``value`` which is the value
1596         to be deserialized. It can also optionally take a ``context`` argument,
1597         which is a dictionary of context variables passed to the deserializer.
1598         If no callable is provided then ```value``` will be passed through
1599         unchanged.
1600 
1601     .. versionchanged:: 2.3.0
1602         Deprecated ``func`` parameter in favor of ``serialize``.
1603 
1604     .. versionchanged:: 3.0.0a1
1605         Removed ``func`` parameter.
1606     """"""
1607 
1608     _CHECK_ATTRIBUTE = False
1609 
1610     def __init__(self, serialize=None, deserialize=None, **kwargs):
1611         # Set dump_only and load_only based on arguments
1612         kwargs[""dump_only""] = bool(serialize) and not bool(deserialize)
1613         kwargs[""load_only""] = bool(deserialize) and not bool(serialize)
1614         super().__init__(**kwargs)
1615         self.serialize_func = serialize and utils.callable_or_raise(serialize)
1616         self.deserialize_func = deserialize and utils.callable_or_raise(deserialize)
1617 
1618     def _serialize(self, value, attr, obj, **kwargs):
1619         return self._call_or_raise(self.serialize_func, obj, attr)
1620 
1621     def _deserialize(self, value, attr, data, **kwargs):
1622         if self.deserialize_func:
1623             return self._call_or_raise(self.deserialize_func, value, attr)
1624         return value
1625 
1626     def _call_or_raise(self, func, value, attr):
1627         if len(utils.get_func_args(func)) > 1:
1628             if self.parent.context is None:
1629                 msg = ""No context available for Function field {!r}"".format(attr)
1630                 raise ValidationError(msg)
1631             return func(value, self.parent.context)
1632         else:
1633             return func(value)
1634 
1635 
1636 class Constant(Field):
1637     """"""A field that (de)serializes to a preset constant.  If you only want the
1638     constant added for serialization or deserialization, you should use
1639     ``dump_only=True`` or ``load_only=True`` respectively.
1640 
1641     :param constant: The constant to return for the field attribute.
1642 
1643     .. versionadded:: 2.0.0
1644     """"""
1645 
1646     _CHECK_ATTRIBUTE = False
1647 
1648     def __init__(self, constant, **kwargs):
1649         super().__init__(**kwargs)
1650         self.constant = constant
1651         self.missing = constant
1652         self.default = constant
1653 
1654     def _serialize(self, value, *args, **kwargs):
1655         return self.constant
1656 
1657     def _deserialize(self, value, *args, **kwargs):
1658         return self.constant
1659 
1660 
1661 class Inferred(Field):
1662     """"""A field that infers how to serialize, based on the value type.
1663 
1664     .. warning::
1665 
1666         This class is treated as private API.
1667         Users should not need to use this class directly.
1668     """"""
1669 
1670     def __init__(self):
1671         super().__init__()
1672         # We memoize the fields to avoid creating and binding new fields
1673         # every time on serialization.
1674         self._field_cache = {}
1675 
1676     def _serialize(self, value, attr, obj, **kwargs):
1677         field_cls = self.root.TYPE_MAPPING.get(type(value))
1678         if field_cls is None:
1679             field = super()
1680         else:
1681             field = self._field_cache.get(field_cls)
1682             if field is None:
1683                 field = field_cls()
1684                 field._bind_to_schema(self.name, self.parent)
1685                 self._field_cache[field_cls] = field
1686         return field._serialize(value, attr, obj, **kwargs)
1687 
1688 
1689 # Aliases
1690 URL = Url
1691 Str = String
1692 Bool = Boolean
1693 Int = Integer
1694 
[end of src/marshmallow/fields.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",marshmallow-code/marshmallow,b40a0f4e33823e6d0f341f7e8684e359a99060d1,"3.0: DateTime fields cannot be used as inner field for List or Tuple fields
Between releases 3.0.0rc8 and 3.0.0rc9, `DateTime` fields have started throwing an error when being instantiated as inner fields of container fields like `List` or `Tuple`. The snippet below works in <=3.0.0rc8 and throws the error below in >=3.0.0rc9 (and, worryingly, 3.0.0):

```python
from marshmallow import fields, Schema

class MySchema(Schema):
    times = fields.List(fields.DateTime())

s = MySchema()
```

Traceback:
```
Traceback (most recent call last):
  File ""test-mm.py"", line 8, in <module>
    s = MySchema()
  File ""/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py"", line 383, in __init__
    self.fields = self._init_fields()
  File ""/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py"", line 913, in _init_fields
    self._bind_field(field_name, field_obj)
  File ""/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py"", line 969, in _bind_field
    field_obj._bind_to_schema(field_name, self)
  File ""/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py"", line 636, in _bind_to_schema
    self.inner._bind_to_schema(field_name, self)
  File ""/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py"", line 1117, in _bind_to_schema
    or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)
AttributeError: 'List' object has no attribute 'opts'
```

It seems like it's treating the parent field as a Schema without checking that it is indeed a schema, so the `schema.opts` statement fails as fields don't have an `opts` attribute.
","Thanks for reporting. I don't think I'll have time to look into this until the weekend. Would you like to send a PR? 
I'm afraid I don't have any time either, and I don't really have enough context on the `_bind_to_schema` process to make sure I'm not breaking stuff.
OK, no problem. @lafrech Will you have a chance to look into this?
I've found the patch below to fix the minimal example above, but I'm not really sure what it's missing out on or how to test it properly:
```patch
diff --git a/src/marshmallow/fields.py b/src/marshmallow/fields.py
index 0b18e7d..700732e 100644
--- a/src/marshmallow/fields.py
+++ b/src/marshmallow/fields.py
@@ -1114,7 +1114,7 @@ class DateTime(Field):
         super()._bind_to_schema(field_name, schema)
         self.format = (
             self.format
-            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)
+            or getattr(getattr(schema, ""opts"", None), self.SCHEMA_OPTS_VAR_NAME, None)
             or self.DEFAULT_FORMAT
         )
```
    git difftool 3.0.0rc8 3.0.0rc9 src/marshmallow/fields.py

When reworking container stuff, I changed

```py
        self.inner.parent = self
        self.inner.name = field_name
```
into

```py
        self.inner._bind_to_schema(field_name, self)
```

AFAIR, I did this merely to avoid duplication. On second thought, I think it was the right thing to do, not only for duplication but to actually bind inner fields to the `Schema`.

Reverting this avoids the error but the inner field's `_bind_to_schema` method is not called so I'm not sure it is desirable.

I think we really mean to call that method, not only in this case but also generally.

Changing

```py
            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)
```

into

```py
            or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)
```

might be a better fix. Can anyone confirm (@sloria, @deckar01)?

The fix in https://github.com/marshmallow-code/marshmallow/issues/1357#issuecomment-523465528 removes the error but also the feature: `DateTime` fields buried into container fields won't respect the format set in the `Schema`.

I didn't double-check that but AFAIU, the change I mentioned above (in container stuff rework) was the right thing to do. The feature was already broken (format set in `Schema` not respected if `DateTime` field in container field) and that's just one of the issues that may arise due to the inner field not being bound to the `Schema`. But I may be wrong.
On quick glance, your analysis and fix look correct @lafrech 
Let's do that, then.

Not much time either. The first who gets the time can do it.

For the non-reg tests :

1/ a test that checks the format set in the schema is respected if the `DateTime` field is in a container field

2/ a set of tests asserting the `_bind_to_schema` method of inner fields `List`, `Dict`, `Tuple` is called from container fields (we can use `DateTime` with the same test case for that)

Perhaps 1/ is useless if 2/ is done.",2019-08-21T15:45:13Z,"<patch>
diff --git a/src/marshmallow/fields.py b/src/marshmallow/fields.py
--- a/src/marshmallow/fields.py
+++ b/src/marshmallow/fields.py
@@ -1114,7 +1114,7 @@ def _bind_to_schema(self, field_name, schema):
         super()._bind_to_schema(field_name, schema)
         self.format = (
             self.format
-            or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)
+            or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)
             or self.DEFAULT_FORMAT
         )
 

</patch>","diff --git a/tests/test_fields.py b/tests/test_fields.py
--- a/tests/test_fields.py
+++ b/tests/test_fields.py
@@ -169,6 +169,20 @@ class OtherSchema(MySchema):
         assert schema2.fields[""foo""].key_field.root == schema2
         assert schema2.fields[""foo""].value_field.root == schema2
 
+    # Regression test for https://github.com/marshmallow-code/marshmallow/issues/1357
+    def test_datetime_list_inner_format(self, schema):
+        class MySchema(Schema):
+            foo = fields.List(fields.DateTime())
+            bar = fields.Tuple((fields.DateTime(),))
+
+            class Meta:
+                datetimeformat = ""iso8601""
+                dateformat = ""iso8601""
+
+        schema = MySchema()
+        assert schema.fields[""foo""].inner.format == ""iso8601""
+        assert schema.fields[""bar""].tuple_fields[0].format == ""iso8601""
+
 
 class TestMetadata:
     @pytest.mark.parametrize(""FieldClass"", ALL_FIELDS)
",3.0,"[""tests/test_fields.py::TestParentAndName::test_datetime_list_inner_format""]","[""tests/test_fields.py::test_field_aliases[Integer-Integer]"", ""tests/test_fields.py::test_field_aliases[String-String]"", ""tests/test_fields.py::test_field_aliases[Boolean-Boolean]"", ""tests/test_fields.py::test_field_aliases[Url-Url]"", ""tests/test_fields.py::TestField::test_repr"", ""tests/test_fields.py::TestField::test_error_raised_if_uncallable_validator_passed"", ""tests/test_fields.py::TestField::test_error_raised_if_missing_is_set_on_required_field"", ""tests/test_fields.py::TestField::test_custom_field_receives_attr_and_obj"", ""tests/test_fields.py::TestField::test_custom_field_receives_data_key_if_set"", ""tests/test_fields.py::TestField::test_custom_field_follows_data_key_if_set"", ""tests/test_fields.py::TestParentAndName::test_simple_field_parent_and_name"", ""tests/test_fields.py::TestParentAndName::test_unbound_field_root_returns_none"", ""tests/test_fields.py::TestParentAndName::test_list_field_inner_parent_and_name"", ""tests/test_fields.py::TestParentAndName::test_tuple_field_inner_parent_and_name"", ""tests/test_fields.py::TestParentAndName::test_mapping_field_inner_parent_and_name"", ""tests/test_fields.py::TestParentAndName::test_simple_field_root"", ""tests/test_fields.py::TestParentAndName::test_list_field_inner_root"", ""tests/test_fields.py::TestParentAndName::test_tuple_field_inner_root"", ""tests/test_fields.py::TestParentAndName::test_list_root_inheritance"", ""tests/test_fields.py::TestParentAndName::test_dict_root_inheritance"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[String]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[Integer]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[Boolean]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[Float]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[Number]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[DateTime]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[Time]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[Date]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[TimeDelta]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[Dict]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[Url]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[Email]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[UUID]"", ""tests/test_fields.py::TestMetadata::test_extra_metadata_may_be_added_to_field[Decimal]"", ""tests/test_fields.py::TestErrorMessages::test_default_error_messages_get_merged_with_parent_error_messages_cstm_msg"", ""tests/test_fields.py::TestErrorMessages::test_default_error_messages_get_merged_with_parent_error_messages"", ""tests/test_fields.py::TestErrorMessages::test_make_error[required-Missing"", ""tests/test_fields.py::TestErrorMessages::test_make_error[null-Field"", ""tests/test_fields.py::TestErrorMessages::test_make_error[custom-Custom"", ""tests/test_fields.py::TestErrorMessages::test_make_error[validator_failed-Invalid"", ""tests/test_fields.py::TestErrorMessages::test_fail[required-Missing"", ""tests/test_fields.py::TestErrorMessages::test_fail[null-Field"", ""tests/test_fields.py::TestErrorMessages::test_fail[custom-Custom"", ""tests/test_fields.py::TestErrorMessages::test_fail[validator_failed-Invalid"", ""tests/test_fields.py::TestErrorMessages::test_make_error_key_doesnt_exist"", ""tests/test_fields.py::TestNestedField::test_nested_only_and_exclude_as_string[only]"", ""tests/test_fields.py::TestNestedField::test_nested_only_and_exclude_as_string[exclude]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[None-exclude]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[None-include]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[None-raise]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[exclude-exclude]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[exclude-include]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[exclude-raise]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[include-exclude]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[include-include]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[include-raise]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[raise-exclude]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[raise-include]"", ""tests/test_fields.py::TestNestedField::test_nested_unknown_override[raise-raise]"", ""tests/test_fields.py::TestListNested::test_list_nested_only_exclude_dump_only_load_only_propagated_to_nested[only]"", ""tests/test_fields.py::TestListNested::test_list_nested_only_exclude_dump_only_load_only_propagated_to_nested[exclude]"", ""tests/test_fields.py::TestListNested::test_list_nested_only_exclude_dump_only_load_only_propagated_to_nested[dump_only]"", ""tests/test_fields.py::TestListNested::test_list_nested_only_exclude_dump_only_load_only_propagated_to_nested[load_only]"", ""tests/test_fields.py::TestListNested::test_list_nested_only_and_exclude_merged_with_nested[only-expected0]"", ""tests/test_fields.py::TestListNested::test_list_nested_only_and_exclude_merged_with_nested[exclude-expected1]"", ""tests/test_fields.py::TestListNested::test_list_nested_partial_propagated_to_nested"", ""tests/test_fields.py::TestTupleNested::test_tuple_nested_only_exclude_dump_only_load_only_propagated_to_nested[dump_only]"", ""tests/test_fields.py::TestTupleNested::test_tuple_nested_only_exclude_dump_only_load_only_propagated_to_nested[load_only]"", ""tests/test_fields.py::TestTupleNested::test_tuple_nested_partial_propagated_to_nested"", ""tests/test_fields.py::TestDictNested::test_dict_nested_only_exclude_dump_only_load_only_propagated_to_nested[only]"", ""tests/test_fields.py::TestDictNested::test_dict_nested_only_exclude_dump_only_load_only_propagated_to_nested[exclude]"", ""tests/test_fields.py::TestDictNested::test_dict_nested_only_exclude_dump_only_load_only_propagated_to_nested[dump_only]"", ""tests/test_fields.py::TestDictNested::test_dict_nested_only_exclude_dump_only_load_only_propagated_to_nested[load_only]"", ""tests/test_fields.py::TestDictNested::test_dict_nested_only_and_exclude_merged_with_nested[only-expected0]"", ""tests/test_fields.py::TestDictNested::test_dict_nested_only_and_exclude_merged_with_nested[exclude-expected1]"", ""tests/test_fields.py::TestDictNested::test_dict_nested_partial_propagated_to_nested""]",8b3a32614fd4a74e93e9a63a042e74c1fea34466
marshmallow-code__marshmallow-1343,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
[version 2.20.0] TypeError: 'NoneType' object is not subscriptable
After update from version 2.19.5 to 2.20.0 I got error for code like:

```python
from marshmallow import Schema, fields, validates


class Bar(Schema):
    value = fields.String()

    @validates('value')  # <- issue here
    def validate_value(self, value):
        pass


class Foo(Schema):
    bar = fields.Nested(Bar)


sch = Foo()

sch.validate({
    'bar': 'invalid',
})
```

```
Traceback (most recent call last):
  File ""/_/bug_mschema.py"", line 19, in <module>
    'bar': 'invalid',
  File ""/_/env/lib/python3.7/site-packages/marshmallow/schema.py"", line 628, in validate
    _, errors = self._do_load(data, many, partial=partial, postprocess=False)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/schema.py"", line 670, in _do_load
    index_errors=self.opts.index_errors,
  File ""/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py"", line 292, in deserialize
    index=(index if index_errors else None)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py"", line 65, in call_and_store
    value = getter_func(data)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py"", line 285, in <lambda>
    data
  File ""/_/env/lib/python3.7/site-packages/marshmallow/fields.py"", line 265, in deserialize
    output = self._deserialize(value, attr, data)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/fields.py"", line 465, in _deserialize
    data, errors = self.schema.load(value)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/schema.py"", line 588, in load
    result, errors = self._do_load(data, many, partial=partial, postprocess=True)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/schema.py"", line 674, in _do_load
    self._invoke_field_validators(unmarshal, data=result, many=many)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/schema.py"", line 894, in _invoke_field_validators
    value = data[field_obj.attribute or field_name]
TypeError: 'NoneType' object is not subscriptable
```

</issue>
<code>
[start of README.rst]
1 ********************************************
2 marshmallow: simplified object serialization
3 ********************************************
4 
5 .. image:: https://badge.fury.io/py/marshmallow.svg
6     :target: http://badge.fury.io/py/marshmallow
7     :alt: Latest version
8 
9 .. image:: https://dev.azure.com/sloria/sloria/_apis/build/status/marshmallow-code.marshmallow?branchName=2.x-line
10     :target: https://dev.azure.com/sloria/sloria/_build/latest?definitionId=5&branchName=2.x-line
11     :alt: Build status
12 
13 .. image:: https://readthedocs.org/projects/marshmallow/badge/
14    :target: https://marshmallow.readthedocs.io/
15    :alt: Documentation
16 
17 **marshmallow** is an ORM/ODM/framework-agnostic library for converting complex datatypes, such as objects, to and from native Python datatypes.
18 
19 .. code-block:: python
20 
21     from datetime import date
22     from marshmallow import Schema, fields, pprint
23 
24     class ArtistSchema(Schema):
25         name = fields.Str()
26 
27     class AlbumSchema(Schema):
28         title = fields.Str()
29         release_date = fields.Date()
30         artist = fields.Nested(ArtistSchema())
31 
32     bowie = dict(name='David Bowie')
33     album = dict(artist=bowie, title='Hunky Dory', release_date=date(1971, 12, 17))
34 
35     schema = AlbumSchema()
36     result = schema.dump(album)
37     pprint(result.data, indent=2)
38     # { 'artist': {'name': 'David Bowie'},
39     #   'release_date': '1971-12-17',
40     #   'title': 'Hunky Dory'}
41 
42 
43 In short, marshmallow schemas can be used to:
44 
45 - **Validate** input data.
46 - **Deserialize** input data to app-level objects.
47 - **Serialize** app-level objects to primitive Python types. The serialized objects can then be rendered to standard formats such as JSON for use in an HTTP API.
48 
49 Get It Now
50 ==========
51 
52 ::
53 
54     $ pip install -U marshmallow
55 
56 
57 Documentation
58 =============
59 
60 Full documentation is available at http://marshmallow.readthedocs.io/ .
61 
62 Requirements
63 ============
64 
65 - Python >= 2.7 or >= 3.4
66 
67 marshmallow has no external dependencies outside of the Python standard library, although `python-dateutil <https://pypi.python.org/pypi/python-dateutil>`_ is recommended for robust datetime deserialization.
68 
69 
70 Ecosystem
71 =========
72 
73 A list of marshmallow-related libraries can be found at the GitHub wiki here:
74 
75 https://github.com/marshmallow-code/marshmallow/wiki/Ecosystem
76 
77 Credits
78 =======
79 
80 Contributors
81 ------------
82 
83 This project exists thanks to all the people who contribute.
84 
85 You're highly encouraged to participate in marshmallow's development. 
86 Check out the `Contributing Guidelines <https://marshmallow.readthedocs.io/en/latest/contributing.html>`_ to see
87 how you can help.
88 
89 Thank you to all who have already contributed to marshmallow!
90 
91 .. image:: https://opencollective.com/marshmallow/contributors.svg?width=890&button=false
92     :target: https://marshmallow.readthedocs.io/en/latest/authors.html
93     :alt: Contributors
94 
95 Backers
96 -------
97 
98 If you find marshmallow useful, please consider supporting the team with
99 a donation. Your donation helps move marshmallow forward. 
100 
101 Thank you to all our backers! [`Become a backer`_]
102 
103 .. _`Become a backer`: https://opencollective.com/marshmallow#backer
104 
105 .. image:: https://opencollective.com/marshmallow/backers.svg?width=890
106     :target: https://opencollective.com/marshmallow#backers
107     :alt: Backers
108 
109 Sponsors
110 --------
111 
112 Support this project by becoming a sponsor (or ask your company to support this project by becoming a sponsor).
113 Your logo will show up here with a link to your website. [`Become a sponsor`_]
114 
115 .. _`Become a sponsor`: https://opencollective.com/marshmallow#sponsor
116 
117 .. image:: https://opencollective.com/marshmallow/sponsor/0/avatar.svg
118     :target: https://opencollective.com/marshmallow/sponsor/0/website
119     :alt: Sponsors
120 
121 
122 Professional Support
123 ====================
124 
125 Professionally-supported marshmallow is now available through the 
126 `Tidelift Subscription <https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=pypi-marshmallow&utm_medium=readme>`_.
127 
128 Tidelift gives software development teams a single source for purchasing and maintaining their software, 
129 with professional-grade assurances from the experts who know it best,
130 while seamlessly integrating with existing tools. [`Get professional support`_]
131 
132 .. _`Get professional support`: https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=marshmallow&utm_medium=referral&utm_campaign=github
133 
134 .. image:: https://user-images.githubusercontent.com/2379650/45126032-50b69880-b13f-11e8-9c2c-abd16c433495.png
135     :target: https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=pypi-marshmallow&utm_medium=readme
136     :alt: Get supported marshmallow with Tidelift
137 
138 Security Contact Information
139 ============================
140 
141 To report a security vulnerability, please use the
142 `Tidelift security contact <https://tidelift.com/security>`_.
143 Tidelift will coordinate the fix and disclosure.
144 
145 Project Links
146 =============
147 
148 - Docs: http://marshmallow.readthedocs.io/
149 - Changelog: http://marshmallow.readthedocs.io/en/latest/changelog.html
150 - PyPI: https://pypi.python.org/pypi/marshmallow
151 - Issues: https://github.com/marshmallow-code/marshmallow/issues
152 - Donate: https://opencollective.com/marshmallow
153 
154 License
155 =======
156 
157 MIT licensed. See the bundled `LICENSE <https://github.com/marshmallow-code/marshmallow/blob/dev/LICENSE>`_ file for more details.
158 
[end of README.rst]
[start of src/marshmallow/schema.py]
1 # -*- coding: utf-8 -*-
2 """"""The :class:`Schema` class, including its metaclass and options (class Meta).""""""
3 from __future__ import absolute_import, unicode_literals
4 
5 from collections import defaultdict, namedtuple
6 import copy
7 import datetime as dt
8 import decimal
9 import inspect
10 import json
11 import uuid
12 import warnings
13 import functools
14 
15 from marshmallow import base, fields, utils, class_registry, marshalling
16 from marshmallow.compat import (with_metaclass, iteritems, text_type,
17                                 binary_type, Mapping, OrderedDict)
18 from marshmallow.exceptions import ValidationError
19 from marshmallow.orderedset import OrderedSet
20 from marshmallow.decorators import (PRE_DUMP, POST_DUMP, PRE_LOAD, POST_LOAD,
21                                     VALIDATES, VALIDATES_SCHEMA)
22 from marshmallow.utils import missing
23 from marshmallow.warnings import RemovedInMarshmallow3Warning, ChangedInMarshmallow3Warning
24 
25 
26 #: Return type of :meth:`Schema.dump` including serialized data and errors
27 MarshalResult = namedtuple('MarshalResult', ['data', 'errors'])
28 #: Return type of :meth:`Schema.load`, including deserialized data and errors
29 UnmarshalResult = namedtuple('UnmarshalResult', ['data', 'errors'])
30 
31 def _get_fields(attrs, field_class, pop=False, ordered=False):
32     """"""Get fields from a class. If ordered=True, fields will sorted by creation index.
33 
34     :param attrs: Mapping of class attributes
35     :param type field_class: Base field class
36     :param bool pop: Remove matching fields
37     """"""
38     getter = getattr(attrs, 'pop' if pop else 'get')
39     fields = [
40         (field_name, getter(field_name))
41         for field_name, field_value in list(iteritems(attrs))
42         if utils.is_instance_or_subclass(field_value, field_class)
43     ]
44     if ordered:
45         return sorted(
46             fields,
47             key=lambda pair: pair[1]._creation_index,
48         )
49     else:
50         return fields
51 
52 # This function allows Schemas to inherit from non-Schema classes and ensures
53 #   inheritance according to the MRO
54 def _get_fields_by_mro(klass, field_class, ordered=False):
55     """"""Collect fields from a class, following its method resolution order. The
56     class itself is excluded from the search; only its parents are checked. Get
57     fields from ``_declared_fields`` if available, else use ``__dict__``.
58 
59     :param type klass: Class whose fields to retrieve
60     :param type field_class: Base field class
61     """"""
62     mro = inspect.getmro(klass)
63     # Loop over mro in reverse to maintain correct order of fields
64     return sum(
65         (
66             _get_fields(
67                 getattr(base, '_declared_fields', base.__dict__),
68                 field_class,
69                 ordered=ordered
70             )
71             for base in mro[:0:-1]
72         ),
73         [],
74     )
75 
76 
77 class SchemaMeta(type):
78     """"""Metaclass for the Schema class. Binds the declared fields to
79     a ``_declared_fields`` attribute, which is a dictionary mapping attribute
80     names to field objects. Also sets the ``opts`` class attribute, which is
81     the Schema class's ``class Meta`` options.
82     """"""
83 
84     def __new__(mcs, name, bases, attrs):
85         meta = attrs.get('Meta')
86         ordered = getattr(meta, 'ordered', False)
87         if not ordered:
88             # Inherit 'ordered' option
89             # Warning: We loop through bases instead of MRO because we don't
90             # yet have access to the class object
91             # (i.e. can't call super before we have fields)
92             for base_ in bases:
93                 if hasattr(base_, 'Meta') and hasattr(base_.Meta, 'ordered'):
94                     ordered = base_.Meta.ordered
95                     break
96             else:
97                 ordered = False
98         cls_fields = _get_fields(attrs, base.FieldABC, pop=True, ordered=ordered)
99         klass = super(SchemaMeta, mcs).__new__(mcs, name, bases, attrs)
100         inherited_fields = _get_fields_by_mro(klass, base.FieldABC, ordered=ordered)
101 
102         # Use getattr rather than attrs['Meta'] so that we get inheritance for free
103         meta = getattr(klass, 'Meta')
104         # Set klass.opts in __new__ rather than __init__ so that it is accessible in
105         # get_declared_fields
106         klass.opts = klass.OPTIONS_CLASS(meta)
107         # Pass the inherited `ordered` into opts
108         klass.opts.ordered = ordered
109         # Add fields specifid in the `include` class Meta option
110         cls_fields += list(klass.opts.include.items())
111 
112         dict_cls = OrderedDict if ordered else dict
113         # Assign _declared_fields on class
114         klass._declared_fields = mcs.get_declared_fields(
115             klass=klass,
116             cls_fields=cls_fields,
117             inherited_fields=inherited_fields,
118             dict_cls=dict_cls
119         )
120         return klass
121 
122     @classmethod
123     def get_declared_fields(mcs, klass, cls_fields, inherited_fields, dict_cls):
124         """"""Returns a dictionary of field_name => `Field` pairs declard on the class.
125         This is exposed mainly so that plugins can add additional fields, e.g. fields
126         computed from class Meta options.
127 
128         :param type klass: The class object.
129         :param dict cls_fields: The fields declared on the class, including those added
130             by the ``include`` class Meta option.
131         :param dict inherited_fileds: Inherited fields.
132         :param type dict_class: Either `dict` or `OrderedDict`, depending on the whether
133             the user specified `ordered=True`.
134         """"""
135         return dict_cls(inherited_fields + cls_fields)
136 
137     # NOTE: self is the class object
138     def __init__(self, name, bases, attrs):
139         super(SchemaMeta, self).__init__(name, bases, attrs)
140         if name:
141             class_registry.register(name, self)
142         self._resolve_processors()
143 
144     def _resolve_processors(self):
145         """"""Add in the decorated processors
146 
147         By doing this after constructing the class, we let standard inheritance
148         do all the hard work.
149         """"""
150         mro = inspect.getmro(self)
151         self._has_processors = False
152         self.__processors__ = defaultdict(list)
153         for attr_name in dir(self):
154             # Need to look up the actual descriptor, not whatever might be
155             # bound to the class. This needs to come from the __dict__ of the
156             # declaring class.
157             for parent in mro:
158                 try:
159                     attr = parent.__dict__[attr_name]
160                 except KeyError:
161                     continue
162                 else:
163                     break
164             else:
165                 # In case we didn't find the attribute and didn't break above.
166                 # We should never hit this - it's just here for completeness
167                 # to exclude the possibility of attr being undefined.
168                 continue
169 
170             try:
171                 processor_tags = attr.__marshmallow_tags__
172             except AttributeError:
173                 continue
174 
175             self._has_processors = bool(processor_tags)
176             for tag in processor_tags:
177                 # Use name here so we can get the bound method later, in case
178                 # the processor was a descriptor or something.
179                 self.__processors__[tag].append(attr_name)
180 
181 
182 class SchemaOpts(object):
183     """"""class Meta options for the :class:`Schema`. Defines defaults.""""""
184 
185     def __init__(self, meta):
186         self.fields = getattr(meta, 'fields', ())
187         if not isinstance(self.fields, (list, tuple)):
188             raise ValueError(""`fields` option must be a list or tuple."")
189         self.additional = getattr(meta, 'additional', ())
190         if not isinstance(self.additional, (list, tuple)):
191             raise ValueError(""`additional` option must be a list or tuple."")
192         if self.fields and self.additional:
193             raise ValueError(""Cannot set both `fields` and `additional` options""
194                             "" for the same Schema."")
195         self.exclude = getattr(meta, 'exclude', ())
196         if not isinstance(self.exclude, (list, tuple)):
197             raise ValueError(""`exclude` must be a list or tuple."")
198         self.strict = getattr(meta, 'strict', False)
199         if hasattr(meta, 'dateformat'):
200             warnings.warn(
201                 ""The dateformat option is renamed to datetimeformat in marshmallow 3."",
202                 ChangedInMarshmallow3Warning
203             )
204         self.dateformat = getattr(meta, 'dateformat', None)
205         if hasattr(meta, 'json_module'):
206             warnings.warn(
207                 ""The json_module option is renamed to render_module in marshmallow 3."",
208                 ChangedInMarshmallow3Warning
209             )
210         self.json_module = getattr(meta, 'json_module', json)
211         if hasattr(meta, 'skip_missing'):
212             warnings.warn(
213                 'The skip_missing option is no longer necessary. Missing inputs passed to '
214                 'Schema.dump will be excluded from the serialized output by default.',
215                 UserWarning
216             )
217         self.ordered = getattr(meta, 'ordered', False)
218         self.index_errors = getattr(meta, 'index_errors', True)
219         self.include = getattr(meta, 'include', {})
220         self.load_only = getattr(meta, 'load_only', ())
221         self.dump_only = getattr(meta, 'dump_only', ())
222 
223 
224 class BaseSchema(base.SchemaABC):
225     """"""Base schema class with which to define custom schemas.
226 
227     Example usage:
228 
229     .. code-block:: python
230 
231         import datetime as dt
232         from marshmallow import Schema, fields
233 
234         class Album(object):
235             def __init__(self, title, release_date):
236                 self.title = title
237                 self.release_date = release_date
238 
239         class AlbumSchema(Schema):
240             title = fields.Str()
241             release_date = fields.Date()
242 
243         # Or, equivalently
244         class AlbumSchema2(Schema):
245             class Meta:
246                 fields = (""title"", ""release_date"")
247 
248         album = Album(""Beggars Banquet"", dt.date(1968, 12, 6))
249         schema = AlbumSchema()
250         data, errors = schema.dump(album)
251         data  # {'release_date': '1968-12-06', 'title': 'Beggars Banquet'}
252 
253     :param dict extra: A dict of extra attributes to bind to the serialized result.
254     :param tuple|list only: Whitelist of fields to select when instantiating the Schema.
255         If None, all fields are used.
256         Nested fields can be represented with dot delimiters.
257     :param tuple|list exclude: Blacklist of fields to exclude when instantiating the Schema.
258         If a field appears in both `only` and `exclude`, it is not used.
259         Nested fields can be represented with dot delimiters.
260     :param str prefix: Optional prefix that will be prepended to all the
261         serialized field names.
262     :param bool strict: If `True`, raise errors if invalid data are passed in
263         instead of failing silently and storing the errors.
264     :param bool many: Should be set to `True` if ``obj`` is a collection
265         so that the object will be serialized to a list.
266     :param dict context: Optional context passed to :class:`fields.Method` and
267         :class:`fields.Function` fields.
268     :param tuple|list load_only: Fields to skip during serialization (write-only fields)
269     :param tuple|list dump_only: Fields to skip during deserialization (read-only fields)
270     :param bool|tuple partial: Whether to ignore missing fields. If its value
271         is an iterable, only missing fields listed in that iterable will be
272         ignored.
273 
274     .. versionchanged:: 2.0.0
275         `__validators__`, `__preprocessors__`, and `__data_handlers__` are removed in favor of
276         `marshmallow.decorators.validates_schema`,
277         `marshmallow.decorators.pre_load` and `marshmallow.decorators.post_dump`.
278         `__accessor__` and `__error_handler__` are deprecated. Implement the
279         `handle_error` and `get_attribute` methods instead.
280         """"""
281     TYPE_MAPPING = {
282         text_type: fields.String,
283         binary_type: fields.String,
284         dt.datetime: fields.DateTime,
285         float: fields.Float,
286         bool: fields.Boolean,
287         tuple: fields.Raw,
288         list: fields.Raw,
289         set: fields.Raw,
290         int: fields.Integer,
291         uuid.UUID: fields.UUID,
292         dt.time: fields.Time,
293         dt.date: fields.Date,
294         dt.timedelta: fields.TimeDelta,
295         decimal.Decimal: fields.Decimal,
296     }
297 
298     OPTIONS_CLASS = SchemaOpts
299 
300     #: DEPRECATED: Custom error handler function. May be `None`.
301     __error_handler__ = None
302     #: DEPRECATED: Function used to get values of an object.
303     __accessor__ = None
304 
305     class Meta(object):
306         """"""Options object for a Schema.
307 
308         Example usage: ::
309 
310             class Meta:
311                 fields = (""id"", ""email"", ""date_created"")
312                 exclude = (""password"", ""secret_attribute"")
313 
314         Available options:
315 
316         - ``fields``: Tuple or list of fields to include in the serialized result.
317         - ``additional``: Tuple or list of fields to include *in addition* to the
318             explicitly declared fields. ``additional`` and ``fields`` are
319             mutually-exclusive options.
320         - ``include``: Dictionary of additional fields to include in the schema. It is
321             usually better to define fields as class variables, but you may need to
322             use this option, e.g., if your fields are Python keywords. May be an
323             `OrderedDict`.
324         - ``exclude``: Tuple or list of fields to exclude in the serialized result.
325             Nested fields can be represented with dot delimiters.
326         - ``dateformat``: Date format for all DateTime fields that do not have their
327             date format explicitly specified.
328         - ``strict``: If `True`, raise errors during marshalling rather than
329             storing them.
330         - ``json_module``: JSON module to use for `loads` and `dumps`.
331             Defaults to the ``json`` module in the stdlib.
332         - ``ordered``: If `True`, order serialization output according to the
333             order in which fields were declared. Output of `Schema.dump` will be a
334             `collections.OrderedDict`.
335         - ``index_errors``: If `True`, errors dictionaries will include the index
336             of invalid items in a collection.
337         - ``load_only``: Tuple or list of fields to exclude from serialized results.
338         - ``dump_only``: Tuple or list of fields to exclude from deserialization
339         """"""
340         pass
341 
342     def __init__(self, extra=None, only=None, exclude=(), prefix='', strict=None,
343                  many=False, context=None, load_only=(), dump_only=(),
344                  partial=False):
345         # copy declared fields from metaclass
346         self.declared_fields = copy.deepcopy(self._declared_fields)
347         self.many = many
348         self.only = only
349         self.exclude = set(self.opts.exclude) | set(exclude)
350         if prefix:
351             warnings.warn(
352                 'The `prefix` argument is deprecated. Use a post_dump '
353                 'method to insert a prefix instead.',
354                 RemovedInMarshmallow3Warning
355             )
356         self.prefix = prefix
357         self.strict = strict if strict is not None else self.opts.strict
358         self.ordered = self.opts.ordered
359         self.load_only = set(load_only) or set(self.opts.load_only)
360         self.dump_only = set(dump_only) or set(self.opts.dump_only)
361         self.partial = partial
362         #: Dictionary mapping field_names -> :class:`Field` objects
363         self.fields = self.dict_class()
364         if extra:
365             warnings.warn(
366                 'The `extra` argument is deprecated. Use a post_dump '
367                 'method to add additional data instead.',
368                 RemovedInMarshmallow3Warning
369             )
370         self.extra = extra
371         self.context = context or {}
372         self._normalize_nested_options()
373         self._types_seen = set()
374         self._update_fields(many=many)
375 
376     def __repr__(self):
377         return '<{ClassName}(many={self.many}, strict={self.strict})>'.format(
378             ClassName=self.__class__.__name__, self=self
379         )
380 
381     def _postprocess(self, data, many, obj):
382         if self.extra:
383             if many:
384                 for each in data:
385                     each.update(self.extra)
386             else:
387                 data.update(self.extra)
388         return data
389 
390     @property
391     def dict_class(self):
392         return OrderedDict if self.ordered else dict
393 
394     @property
395     def set_class(self):
396         return OrderedSet if self.ordered else set
397 
398     ##### Override-able methods #####
399 
400     def handle_error(self, error, data):
401         """"""Custom error handler function for the schema.
402 
403         :param ValidationError error: The `ValidationError` raised during (de)serialization.
404         :param data: The original input data.
405 
406         .. versionadded:: 2.0.0
407         """"""
408         pass
409 
410     def get_attribute(self, attr, obj, default):
411         """"""Defines how to pull values from an object to serialize.
412 
413         .. versionadded:: 2.0.0
414         """"""
415         return utils.get_value(attr, obj, default)
416 
417     ##### Handler decorators (deprecated) #####
418 
419     @classmethod
420     def error_handler(cls, func):
421         """"""Decorator that registers an error handler function for the schema.
422         The function receives the :class:`Schema` instance, a dictionary of errors,
423         and the serialized object (if serializing data) or data dictionary (if
424         deserializing data) as arguments.
425 
426         Example: ::
427 
428             class UserSchema(Schema):
429                 email = fields.Email()
430 
431             @UserSchema.error_handler
432             def handle_errors(schema, errors, obj):
433                 raise ValueError('An error occurred while marshalling {}'.format(obj))
434 
435             user = User(email='invalid')
436             UserSchema().dump(user)  # => raises ValueError
437             UserSchema().load({'email': 'bademail'})  # raises ValueError
438 
439         .. versionadded:: 0.7.0
440         .. deprecated:: 2.0.0
441             Set the ``error_handler`` class Meta option instead.
442         """"""
443         warnings.warn(
444             'Schema.error_handler is deprecated. Set the error_handler class Meta option '
445             'instead.', category=DeprecationWarning
446         )
447         cls.__error_handler__ = func
448         return func
449 
450     @classmethod
451     def accessor(cls, func):
452         """"""Decorator that registers a function for pulling values from an object
453         to serialize. The function receives the :class:`Schema` instance, the
454         ``key`` of the value to get, the ``obj`` to serialize, and an optional
455         ``default`` value.
456 
457         .. deprecated:: 2.0.0
458             Set the ``error_handler`` class Meta option instead.
459         """"""
460         warnings.warn(
461             'Schema.accessor is deprecated. Set the accessor class Meta option '
462             'instead.', category=DeprecationWarning
463         )
464         cls.__accessor__ = func
465         return func
466 
467     ##### Serialization/Deserialization API #####
468 
469     def dump(self, obj, many=None, update_fields=True, **kwargs):
470         """"""Serialize an object to native Python data types according to this
471         Schema's fields.
472 
473         :param obj: The object to serialize.
474         :param bool many: Whether to serialize `obj` as a collection. If `None`, the value
475             for `self.many` is used.
476         :param bool update_fields: Whether to update the schema's field classes. Typically
477             set to `True`, but may be `False` when serializing a homogenous collection.
478             This parameter is used by `fields.Nested` to avoid multiple updates.
479         :return: A tuple of the form (``data``, ``errors``)
480         :rtype: `MarshalResult`, a `collections.namedtuple`
481 
482         .. versionadded:: 1.0.0
483         """"""
484         # Callable marshalling object
485         marshal = marshalling.Marshaller(prefix=self.prefix)
486         errors = {}
487         many = self.many if many is None else bool(many)
488         if many and utils.is_iterable_but_not_string(obj):
489             obj = list(obj)
490 
491         if self._has_processors:
492             try:
493                 processed_obj = self._invoke_dump_processors(
494                     PRE_DUMP,
495                     obj,
496                     many,
497                     original_data=obj)
498             except ValidationError as error:
499                 errors = error.normalized_messages()
500                 result = None
501         else:
502             processed_obj = obj
503 
504         if not errors:
505             if update_fields:
506                 obj_type = type(processed_obj)
507                 if obj_type not in self._types_seen:
508                     self._update_fields(processed_obj, many=many)
509                     if not isinstance(processed_obj, Mapping):
510                         self._types_seen.add(obj_type)
511 
512             try:
513                 preresult = marshal(
514                     processed_obj,
515                     self.fields,
516                     many=many,
517                     # TODO: Remove self.__accessor__ in a later release
518                     accessor=self.get_attribute or self.__accessor__,
519                     dict_class=self.dict_class,
520                     index_errors=self.opts.index_errors,
521                     **kwargs
522                 )
523             except ValidationError as error:
524                 errors = marshal.errors
525                 preresult = error.data
526 
527             result = self._postprocess(preresult, many, obj=obj)
528 
529         if not errors and self._has_processors:
530             try:
531                 result = self._invoke_dump_processors(
532                     POST_DUMP,
533                     result,
534                     many,
535                     original_data=obj)
536             except ValidationError as error:
537                 errors = error.normalized_messages()
538         if errors:
539             # TODO: Remove self.__error_handler__ in a later release
540             if self.__error_handler__ and callable(self.__error_handler__):
541                 self.__error_handler__(errors, obj)
542             exc = ValidationError(
543                 errors,
544                 field_names=marshal.error_field_names,
545                 fields=marshal.error_fields,
546                 data=obj,
547                 **marshal.error_kwargs
548             )
549             self.handle_error(exc, obj)
550             if self.strict:
551                 raise exc
552 
553         return MarshalResult(result, errors)
554 
555     def dumps(self, obj, many=None, update_fields=True, *args, **kwargs):
556         """"""Same as :meth:`dump`, except return a JSON-encoded string.
557 
558         :param obj: The object to serialize.
559         :param bool many: Whether to serialize `obj` as a collection. If `None`, the value
560             for `self.many` is used.
561         :param bool update_fields: Whether to update the schema's field classes. Typically
562             set to `True`, but may be `False` when serializing a homogenous collection.
563             This parameter is used by `fields.Nested` to avoid multiple updates.
564         :return: A tuple of the form (``data``, ``errors``)
565         :rtype: `MarshalResult`, a `collections.namedtuple`
566 
567         .. versionadded:: 1.0.0
568         """"""
569         deserialized, errors = self.dump(obj, many=many, update_fields=update_fields)
570         ret = self.opts.json_module.dumps(deserialized, *args, **kwargs)
571         return MarshalResult(ret, errors)
572 
573     def load(self, data, many=None, partial=None):
574         """"""Deserialize a data structure to an object defined by this Schema's
575         fields and :meth:`make_object`.
576 
577         :param dict data: The data to deserialize.
578         :param bool many: Whether to deserialize `data` as a collection. If `None`, the
579             value for `self.many` is used.
580         :param bool|tuple partial: Whether to ignore missing fields. If `None`,
581             the value for `self.partial` is used. If its value is an iterable,
582             only missing fields listed in that iterable will be ignored.
583         :return: A tuple of the form (``data``, ``errors``)
584         :rtype: `UnmarshalResult`, a `collections.namedtuple`
585 
586         .. versionadded:: 1.0.0
587         """"""
588         result, errors = self._do_load(data, many, partial=partial, postprocess=True)
589         return UnmarshalResult(data=result, errors=errors)
590 
591     def loads(self, json_data, many=None, *args, **kwargs):
592         """"""Same as :meth:`load`, except it takes a JSON string as input.
593 
594         :param str json_data: A JSON string of the data to deserialize.
595         :param bool many: Whether to deserialize `obj` as a collection. If `None`, the
596             value for `self.many` is used.
597         :param bool|tuple partial: Whether to ignore missing fields. If `None`,
598             the value for `self.partial` is used. If its value is an iterable,
599             only missing fields listed in that iterable will be ignored.
600         :return: A tuple of the form (``data``, ``errors``)
601         :rtype: `UnmarshalResult`, a `collections.namedtuple`
602 
603         .. versionadded:: 1.0.0
604         """"""
605         # TODO: This avoids breaking backward compatibility if people were
606         # passing in positional args after `many` for use by `json.loads`, but
607         # ideally we shouldn't have to do this.
608         partial = kwargs.pop('partial', None)
609 
610         data = self.opts.json_module.loads(json_data, *args, **kwargs)
611         return self.load(data, many=many, partial=partial)
612 
613     def validate(self, data, many=None, partial=None):
614         """"""Validate `data` against the schema, returning a dictionary of
615         validation errors.
616 
617         :param dict data: The data to validate.
618         :param bool many: Whether to validate `data` as a collection. If `None`, the
619             value for `self.many` is used.
620         :param bool|tuple partial: Whether to ignore missing fields. If `None`,
621             the value for `self.partial` is used. If its value is an iterable,
622             only missing fields listed in that iterable will be ignored.
623         :return: A dictionary of validation errors.
624         :rtype: dict
625 
626         .. versionadded:: 1.1.0
627         """"""
628         _, errors = self._do_load(data, many, partial=partial, postprocess=False)
629         return errors
630 
631     ##### Private Helpers #####
632 
633     def _do_load(self, data, many=None, partial=None, postprocess=True):
634         """"""Deserialize `data`, returning the deserialized result and a dictonary of
635         validation errors.
636 
637         :param data: The data to deserialize.
638         :param bool many: Whether to deserialize `data` as a collection. If `None`, the
639             value for `self.many` is used.
640         :param bool|tuple partial: Whether to validate required fields. If its value is an iterable,
641             only fields listed in that iterable will be ignored will be allowed missing.
642             If `True`, all fields will be allowed missing.
643             If `None`, the value for `self.partial` is used.
644         :param bool postprocess: Whether to run post_load methods..
645         :return: A tuple of the form (`data`, `errors`)
646         """"""
647         # Callable unmarshalling object
648         unmarshal = marshalling.Unmarshaller()
649         errors = {}
650         many = self.many if many is None else bool(many)
651         if partial is None:
652             partial = self.partial
653         try:
654             processed_data = self._invoke_load_processors(
655                 PRE_LOAD,
656                 data,
657                 many,
658                 original_data=data)
659         except ValidationError as err:
660             errors = err.normalized_messages()
661             result = None
662         if not errors:
663             try:
664                 result = unmarshal(
665                     processed_data,
666                     self.fields,
667                     many=many,
668                     partial=partial,
669                     dict_class=self.dict_class,
670                     index_errors=self.opts.index_errors,
671                 )
672             except ValidationError as error:
673                 result = error.data
674             self._invoke_field_validators(unmarshal, data=result, many=many)
675             errors = unmarshal.errors
676             field_errors = bool(errors)
677             # Run schema-level migration
678             try:
679                 self._invoke_validators(unmarshal, pass_many=True, data=result, original_data=data,
680                                         many=many, field_errors=field_errors)
681             except ValidationError as err:
682                 errors.update(err.messages)
683             try:
684                 self._invoke_validators(unmarshal, pass_many=False, data=result, original_data=data,
685                                         many=many, field_errors=field_errors)
686             except ValidationError as err:
687                 errors.update(err.messages)
688         # Run post processors
689         if not errors and postprocess:
690             try:
691                 result = self._invoke_load_processors(
692                     POST_LOAD,
693                     result,
694                     many,
695                     original_data=data)
696             except ValidationError as err:
697                 errors = err.normalized_messages()
698         if errors:
699             # TODO: Remove self.__error_handler__ in a later release
700             if self.__error_handler__ and callable(self.__error_handler__):
701                 self.__error_handler__(errors, data)
702             exc = ValidationError(
703                 errors,
704                 field_names=unmarshal.error_field_names,
705                 fields=unmarshal.error_fields,
706                 data=data,
707                 **unmarshal.error_kwargs
708             )
709             self.handle_error(exc, data)
710             if self.strict:
711                 raise exc
712 
713         return result, errors
714 
715     def _normalize_nested_options(self):
716         """"""Apply then flatten nested schema options""""""
717         if self.only is not None:
718             # Apply the only option to nested fields.
719             self.__apply_nested_option('only', self.only, 'intersection')
720             # Remove the child field names from the only option.
721             self.only = self.set_class(
722                 [field.split('.', 1)[0] for field in self.only],
723             )
724         if self.exclude:
725             # Apply the exclude option to nested fields.
726             self.__apply_nested_option('exclude', self.exclude, 'union')
727             # Remove the parent field names from the exclude option.
728             self.exclude = self.set_class(
729                 [field for field in self.exclude if '.' not in field],
730             )
731 
732     def __apply_nested_option(self, option_name, field_names, set_operation):
733         """"""Apply nested options to nested fields""""""
734         # Split nested field names on the first dot.
735         nested_fields = [name.split('.', 1) for name in field_names if '.' in name]
736         # Partition the nested field names by parent field.
737         nested_options = defaultdict(list)
738         for parent, nested_names in nested_fields:
739             nested_options[parent].append(nested_names)
740         # Apply the nested field options.
741         for key, options in iter(nested_options.items()):
742             new_options = self.set_class(options)
743             original_options = getattr(self.declared_fields[key], option_name, ())
744             if original_options:
745                 if set_operation == 'union':
746                     new_options |= self.set_class(original_options)
747                 if set_operation == 'intersection':
748                     new_options &= self.set_class(original_options)
749             setattr(self.declared_fields[key], option_name, new_options)
750 
751     def _update_fields(self, obj=None, many=False):
752         """"""Update fields based on the passed in object.""""""
753         if self.only is not None:
754             # Return only fields specified in only option
755             if self.opts.fields:
756                 field_names = self.set_class(self.opts.fields) & self.set_class(self.only)
757             else:
758                 field_names = self.set_class(self.only)
759         elif self.opts.fields:
760             # Return fields specified in fields option
761             field_names = self.set_class(self.opts.fields)
762         elif self.opts.additional:
763             # Return declared fields + additional fields
764             field_names = (self.set_class(self.declared_fields.keys()) |
765                             self.set_class(self.opts.additional))
766         else:
767             field_names = self.set_class(self.declared_fields.keys())
768 
769         # If ""exclude"" option or param is specified, remove those fields
770         field_names -= self.exclude
771         ret = self.__filter_fields(field_names, obj, many=many)
772         # Set parents
773         self.__set_field_attrs(ret)
774         self.fields = ret
775         return self.fields
776 
777     def on_bind_field(self, field_name, field_obj):
778         """"""Hook to modify a field when it is bound to the `Schema`. No-op by default.""""""
779         return None
780 
781     def __set_field_attrs(self, fields_dict):
782         """"""Bind fields to the schema, setting any necessary attributes
783         on the fields (e.g. parent and name).
784 
785         Also set field load_only and dump_only values if field_name was
786         specified in ``class Meta``.
787         """"""
788         for field_name, field_obj in iteritems(fields_dict):
789             try:
790                 if field_name in self.load_only:
791                     field_obj.load_only = True
792                 if field_name in self.dump_only:
793                     field_obj.dump_only = True
794                 field_obj._add_to_schema(field_name, self)
795                 self.on_bind_field(field_name, field_obj)
796             except TypeError:
797                 # field declared as a class, not an instance
798                 if (isinstance(field_obj, type) and
799                         issubclass(field_obj, base.FieldABC)):
800                     msg = ('Field for ""{0}"" must be declared as a '
801                            'Field instance, not a class. '
802                            'Did you mean ""fields.{1}()""?'
803                            .format(field_name, field_obj.__name__))
804                     raise TypeError(msg)
805         return fields_dict
806 
807     def __filter_fields(self, field_names, obj, many=False):
808         """"""Return only those field_name:field_obj pairs specified by
809         ``field_names``.
810 
811         :param set field_names: Field names to include in the final
812             return dictionary.
813         :param object|Mapping|list obj The object to base filtered fields on.
814         :returns: An dict of field_name:field_obj pairs.
815         """"""
816         if obj and many:
817             try:  # list
818                 obj = obj[0]
819             except IndexError:  # Nothing to serialize
820                 return dict((k, v) for k, v in self.declared_fields.items() if k in field_names)
821         ret = self.dict_class()
822         for key in field_names:
823             if key in self.declared_fields:
824                 ret[key] = self.declared_fields[key]
825             else:  # Implicit field creation (class Meta 'fields' or 'additional')
826                 if obj:
827                     attribute_type = None
828                     try:
829                         if isinstance(obj, Mapping):
830                             attribute_type = type(obj[key])
831                         else:
832                             attribute_type = type(getattr(obj, key))
833                     except (AttributeError, KeyError) as err:
834                         err_type = type(err)
835                         raise err_type(
836                             '""{0}"" is not a valid field for {1}.'.format(key, obj))
837                     field_obj = self.TYPE_MAPPING.get(attribute_type, fields.Field)()
838                 else:  # Object is None
839                     field_obj = fields.Field()
840                 # map key -> field (default to Raw)
841                 ret[key] = field_obj
842         return ret
843 
844     def _invoke_dump_processors(self, tag_name, data, many, original_data=None):
845         # The pass_many post-dump processors may do things like add an envelope, so
846         # invoke those after invoking the non-pass_many processors which will expect
847         # to get a list of items.
848         data = self._invoke_processors(tag_name, pass_many=False,
849             data=data, many=many, original_data=original_data)
850         data = self._invoke_processors(tag_name, pass_many=True,
851             data=data, many=many, original_data=original_data)
852         return data
853 
854     def _invoke_load_processors(self, tag_name, data, many, original_data=None):
855         # This has to invert the order of the dump processors, so run the pass_many
856         # processors first.
857         data = self._invoke_processors(tag_name, pass_many=True,
858             data=data, many=many, original_data=original_data)
859         data = self._invoke_processors(tag_name, pass_many=False,
860             data=data, many=many, original_data=original_data)
861         return data
862 
863     def _invoke_field_validators(self, unmarshal, data, many):
864         for attr_name in self.__processors__[(VALIDATES, False)]:
865             validator = getattr(self, attr_name)
866             validator_kwargs = validator.__marshmallow_kwargs__[(VALIDATES, False)]
867             field_name = validator_kwargs['field_name']
868 
869             try:
870                 field_obj = self.fields[field_name]
871             except KeyError:
872                 if field_name in self.declared_fields:
873                     continue
874                 raise ValueError('""{0}"" field does not exist.'.format(field_name))
875 
876             if many:
877                 for idx, item in enumerate(data):
878                     try:
879                         value = item[field_obj.attribute or field_name]
880                     except KeyError:
881                         pass
882                     else:
883                         validated_value = unmarshal.call_and_store(
884                             getter_func=validator,
885                             data=value,
886                             field_name=field_obj.load_from or field_name,
887                             field_obj=field_obj,
888                             index=(idx if self.opts.index_errors else None)
889                         )
890                         if validated_value is missing:
891                             data[idx].pop(field_name, None)
892             else:
893                 try:
894                     value = data[field_obj.attribute or field_name]
895                 except KeyError:
896                     pass
897                 else:
898                     validated_value = unmarshal.call_and_store(
899                         getter_func=validator,
900                         data=value,
901                         field_name=field_obj.load_from or field_name,
902                         field_obj=field_obj
903                     )
904                     if validated_value is missing:
905                         data.pop(field_name, None)
906 
907     def _invoke_validators(
908             self, unmarshal, pass_many, data, original_data, many, field_errors=False):
909         errors = {}
910         for attr_name in self.__processors__[(VALIDATES_SCHEMA, pass_many)]:
911             validator = getattr(self, attr_name)
912             validator_kwargs = validator.__marshmallow_kwargs__[(VALIDATES_SCHEMA, pass_many)]
913             pass_original = validator_kwargs.get('pass_original', False)
914 
915             skip_on_field_errors = validator_kwargs['skip_on_field_errors']
916             if skip_on_field_errors and field_errors:
917                 continue
918 
919             if pass_many:
920                 validator = functools.partial(validator, many=many)
921             if many and not pass_many:
922                 for idx, item in enumerate(data):
923                     try:
924                         unmarshal.run_validator(validator,
925                                                 item, original_data, self.fields, many=many,
926                                                 index=idx, pass_original=pass_original)
927                     except ValidationError as err:
928                         errors.update(err.messages)
929             else:
930                 try:
931                     unmarshal.run_validator(validator,
932                                             data, original_data, self.fields, many=many,
933                                             pass_original=pass_original)
934                 except ValidationError as err:
935                     errors.update(err.messages)
936         if errors:
937             raise ValidationError(errors)
938         return None
939 
940     def _invoke_processors(self, tag_name, pass_many, data, many, original_data=None):
941         for attr_name in self.__processors__[(tag_name, pass_many)]:
942             # This will be a bound method.
943             processor = getattr(self, attr_name)
944 
945             processor_kwargs = processor.__marshmallow_kwargs__[(tag_name, pass_many)]
946             pass_original = processor_kwargs.get('pass_original', False)
947 
948             if pass_many:
949                 if pass_original:
950                     data = utils.if_none(processor(data, many, original_data), data)
951                 else:
952                     data = utils.if_none(processor(data, many), data)
953             elif many:
954                 if pass_original:
955                     data = [utils.if_none(processor(item, original_data), item)
956                             for item in data]
957                 else:
958                     data = [utils.if_none(processor(item), item) for item in data]
959             else:
960                 if pass_original:
961                     data = utils.if_none(processor(data, original_data), data)
962                 else:
963                     data = utils.if_none(processor(data), data)
964         return data
965 
966 
967 class Schema(with_metaclass(SchemaMeta, BaseSchema)):
968     __doc__ = BaseSchema.__doc__
969 
[end of src/marshmallow/schema.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",marshmallow-code/marshmallow,2be2d83a1a9a6d3d9b85804f3ab545cecc409bb0,"[version 2.20.0] TypeError: 'NoneType' object is not subscriptable
After update from version 2.19.5 to 2.20.0 I got error for code like:

```python
from marshmallow import Schema, fields, validates


class Bar(Schema):
    value = fields.String()

    @validates('value')  # <- issue here
    def validate_value(self, value):
        pass


class Foo(Schema):
    bar = fields.Nested(Bar)


sch = Foo()

sch.validate({
    'bar': 'invalid',
})
```

```
Traceback (most recent call last):
  File ""/_/bug_mschema.py"", line 19, in <module>
    'bar': 'invalid',
  File ""/_/env/lib/python3.7/site-packages/marshmallow/schema.py"", line 628, in validate
    _, errors = self._do_load(data, many, partial=partial, postprocess=False)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/schema.py"", line 670, in _do_load
    index_errors=self.opts.index_errors,
  File ""/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py"", line 292, in deserialize
    index=(index if index_errors else None)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py"", line 65, in call_and_store
    value = getter_func(data)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py"", line 285, in <lambda>
    data
  File ""/_/env/lib/python3.7/site-packages/marshmallow/fields.py"", line 265, in deserialize
    output = self._deserialize(value, attr, data)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/fields.py"", line 465, in _deserialize
    data, errors = self.schema.load(value)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/schema.py"", line 588, in load
    result, errors = self._do_load(data, many, partial=partial, postprocess=True)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/schema.py"", line 674, in _do_load
    self._invoke_field_validators(unmarshal, data=result, many=many)
  File ""/_/env/lib/python3.7/site-packages/marshmallow/schema.py"", line 894, in _invoke_field_validators
    value = data[field_obj.attribute or field_name]
TypeError: 'NoneType' object is not subscriptable
```
",Thanks for reporting. I was able to reproduce this on 2.20.0. This is likely a regression from https://github.com/marshmallow-code/marshmallow/pull/1323 . I don't have time to look into it now. Would appreciate a PR.,2019-08-13T04:36:01Z,"<patch>
diff --git a/src/marshmallow/schema.py b/src/marshmallow/schema.py
--- a/src/marshmallow/schema.py
+++ b/src/marshmallow/schema.py
@@ -877,7 +877,7 @@ def _invoke_field_validators(self, unmarshal, data, many):
                 for idx, item in enumerate(data):
                     try:
                         value = item[field_obj.attribute or field_name]
-                    except KeyError:
+                    except (KeyError, TypeError):
                         pass
                     else:
                         validated_value = unmarshal.call_and_store(
@@ -892,7 +892,7 @@ def _invoke_field_validators(self, unmarshal, data, many):
             else:
                 try:
                     value = data[field_obj.attribute or field_name]
-                except KeyError:
+                except (KeyError, TypeError):
                     pass
                 else:
                     validated_value = unmarshal.call_and_store(

</patch>","diff --git a/tests/test_marshalling.py b/tests/test_marshalling.py
--- a/tests/test_marshalling.py
+++ b/tests/test_marshalling.py
@@ -2,7 +2,7 @@
 
 import pytest
 
-from marshmallow import fields, Schema
+from marshmallow import fields, Schema, validates
 from marshmallow.marshalling import Marshaller, Unmarshaller, missing
 from marshmallow.exceptions import ValidationError
 
@@ -283,3 +283,24 @@ class TestSchema(Schema):
 
             assert result is None
             assert excinfo.value.messages == {'foo': {'_schema': ['Invalid input type.']}}
+
+    # Regression test for https://github.com/marshmallow-code/marshmallow/issues/1342
+    def test_deserialize_wrong_nested_type_with_validates_method(self, unmarshal):
+        class TestSchema(Schema):
+            value = fields.String()
+
+            @validates('value')
+            def validate_value(self, value):
+                pass
+
+        data = {
+            'foo': 'not what we need'
+        }
+        fields_dict = {
+            'foo': fields.Nested(TestSchema, required=True)
+        }
+        with pytest.raises(ValidationError) as excinfo:
+            result = unmarshal.deserialize(data, fields_dict)
+
+            assert result is None
+            assert excinfo.value.messages == {'foo': {'_schema': ['Invalid input type.']}}
",2.20,"[""tests/test_marshalling.py::TestUnmarshaller::test_deserialize_wrong_nested_type_with_validates_method""]","[""tests/test_marshalling.py::test_missing_is_falsy"", ""tests/test_marshalling.py::TestMarshaller::test_prefix"", ""tests/test_marshalling.py::TestMarshaller::test_marshalling_generator"", ""tests/test_marshalling.py::TestMarshaller::test_default_to_missing"", ""tests/test_marshalling.py::TestMarshaller::test_serialize_fields_with_load_only_param"", ""tests/test_marshalling.py::TestMarshaller::test_missing_data_are_skipped"", ""tests/test_marshalling.py::TestMarshaller::test_serialize_with_load_only_doesnt_validate"", ""tests/test_marshalling.py::TestMarshaller::test_serialize_fields_with_dump_to_param"", ""tests/test_marshalling.py::TestMarshaller::test_serialize_fields_with_dump_to_and_prefix_params"", ""tests/test_marshalling.py::TestMarshaller::test_stores_indices_of_errors_when_many_equals_true"", ""tests/test_marshalling.py::TestMarshaller::test_doesnt_store_errors_when_index_errors_equals_false"", ""tests/test_marshalling.py::TestUnmarshaller::test_extra_data_is_ignored"", ""tests/test_marshalling.py::TestUnmarshaller::test_stores_errors"", ""tests/test_marshalling.py::TestUnmarshaller::test_stores_indices_of_errors_when_many_equals_true"", ""tests/test_marshalling.py::TestUnmarshaller::test_doesnt_store_errors_when_index_errors_equals_false"", ""tests/test_marshalling.py::TestUnmarshaller::test_deserialize"", ""tests/test_marshalling.py::TestUnmarshaller::test_extra_fields"", ""tests/test_marshalling.py::TestUnmarshaller::test_deserialize_many"", ""tests/test_marshalling.py::TestUnmarshaller::test_deserialize_stores_errors"", ""tests/test_marshalling.py::TestUnmarshaller::test_deserialize_fields_with_attribute_param"", ""tests/test_marshalling.py::TestUnmarshaller::test_deserialize_fields_with_load_from_param"", ""tests/test_marshalling.py::TestUnmarshaller::test_deserialize_fields_with_dump_only_param"", ""tests/test_marshalling.py::TestUnmarshaller::test_deserialize_wrong_type_root_data"", ""tests/test_marshalling.py::TestUnmarshaller::test_deserialize_wrong_type_nested_data""]",7015fc4333a2f32cd58c3465296e834acd4496ff
pvlib__pvlib-python-1707,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
regression: iam.physical returns nan for aoi > 90Â° when n = 1
**Describe the bug**
For pvlib==0.9.5, when n = 1 (no reflection) and aoi > 90Â°, we get nan as result.

**To Reproduce**
```python
import pvlib
pvlib.iam.physical(aoi=100, n=1)
```
returns `nan`.

**Expected behavior**
The result should be `0`, as it was for pvlib <= 0.9.4.


**Versions:**
 - ``pvlib.__version__``: '0.9.5'
 - ``pandas.__version__``:  '1.5.3'
 - python: 3.10.4


</issue>
<code>
[start of README.md]
1 <img src=""docs/sphinx/source/_images/pvlib_logo_horiz.png"" width=""600"">
2 
3 <table>
4 <tr>
5   <td>Latest Release</td>
6   <td>
7     <a href=""https://pypi.org/project/pvlib/"">
8     <img src=""https://img.shields.io/pypi/v/pvlib.svg"" alt=""latest release"" />
9     </a>
10     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
11     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/version.svg"" />
12     </a>
13     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
14     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg"" />
15     </a>
16 </tr>
17 <tr>
18   <td>License</td>
19   <td>
20     <a href=""https://github.com/pvlib/pvlib-python/blob/main/LICENSE"">
21     <img src=""https://img.shields.io/pypi/l/pvlib.svg"" alt=""license"" />
22     </a>
23 </td>
24 </tr>
25 <tr>
26   <td>Build Status</td>
27   <td>
28     <a href=""http://pvlib-python.readthedocs.org/en/stable/"">
29     <img src=""https://readthedocs.org/projects/pvlib-python/badge/?version=stable"" alt=""documentation build status"" />
30     </a>
31     <a href=""https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml?query=branch%3Amain"">
32       <img src=""https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml/badge.svg?branch=main"" alt=""GitHub Actions Testing Status"" />
33     </a>
34     <a href=""https://codecov.io/gh/pvlib/pvlib-python"">
35     <img src=""https://codecov.io/gh/pvlib/pvlib-python/branch/main/graph/badge.svg"" alt=""codecov coverage"" />
36     </a>
37   </td>
38 </tr>
39 <tr>
40   <td>Benchmarks</td>
41   <td>
42     <a href=""https://pvlib.github.io/pvlib-benchmarks/"">
43     <img src=""https://img.shields.io/badge/benchmarks-asv-lightgrey"" />
44     </a>
45   </td>
46 </tr>
47 <tr>
48   <td>Publications</td>
49   <td>
50     <a href=""https://doi.org/10.5281/zenodo.593284"">
51     <img src=""https://zenodo.org/badge/DOI/10.5281/zenodo.593284.svg"" alt=""zenodo reference"">
52     </a>
53     <a href=""http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1"">
54     <img src=""http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg"" alt=""JOSS reference"" />
55     </a>
56   </td>
57 </tr>
58 <tr>
59   <td>Downloads</td>
60   <td>
61     <a href=""https://pypi.org/project/pvlib/"">
62     <img src=""https://img.shields.io/pypi/dm/pvlib"" alt=""PyPI downloads"" />
63     </a>
64     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
65     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg"" alt=""conda-forge downloads"" />
66     </a>
67   </td>
68 </tr>
69 </table>
70 
71 
72 pvlib python is a community supported tool that provides a set of
73 functions and classes for simulating the performance of photovoltaic
74 energy systems. pvlib python was originally ported from the PVLIB MATLAB
75 toolbox developed at Sandia National Laboratories and it implements many
76 of the models and methods developed at the Labs. More information on
77 Sandia Labs PV performance modeling programs can be found at
78 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,
79 but operate independently of it.
80 
81 
82 Documentation
83 =============
84 
85 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/),
86 including an [FAQ](http://pvlib-python.readthedocs.io/en/stable/user_guide/faq.html) page.
87 
88 Installation
89 ============
90 
91 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.
92 Please see the [Installation page](https://pvlib-python.readthedocs.io/en/stable/user_guide/installation.html) of the documentation for complete instructions.
93 
94 
95 Contributing
96 ============
97 
98 We need your help to make pvlib-python a great tool!
99 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.
100 The long-term success of pvlib-python requires substantial community support.
101 
102 
103 Citing
104 ======
105 
106 If you use pvlib-python in a published work, please cite:
107 
108   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.
109   ""pvlib python: a python package for modeling solar energy systems.""
110   Journal of Open Source Software, 3(29), 884, (2018).
111   https://doi.org/10.21105/joss.00884
112 
113 Please also cite the DOI corresponding to the specific version of
114 pvlib-python that you used. pvlib-python DOIs are listed at
115 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)
116 
117 If you use pvlib-python in a commercial or publicly-available application, please
118 consider displaying one of the ""powered by pvlib"" logos:
119 
120 <img src=""docs/sphinx/source/_images/pvlib_powered_logo_vert.png"" width=""300""><img src=""docs/sphinx/source/_images/pvlib_powered_logo_horiz.png"" width=""300"">
121 
122 Getting support
123 ===============
124 
125 pvlib usage questions can be asked on
126 [Stack Overflow](http://stackoverflow.com) and tagged with
127 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.
128 
129 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)
130 is used for discussing various topics of interest to the pvlib-python
131 community. We also make new version announcements on the google group.
132 
133 If you suspect that you may have discovered a bug or if you'd like to
134 change something about pvlib, then please make an issue on our
135 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).
136 
137 
138 
139 License
140 =======
141 
142 BSD 3-clause.
143 
144 
145 NumFOCUS
146 ========
147 
148 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)
149 
150 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)
151 
[end of README.md]
[start of pvlib/iam.py]
1 r""""""
2 The ``iam`` module contains functions that implement models for the incidence
3 angle modifier (IAM). The IAM quantifies the fraction of direct irradiance on
4 a module's front surface that is transmitted through the module materials to
5 the cells. Stated differently, the quantity 1 - IAM is the fraction of direct
6 irradiance that is reflected away or absorbed by the module's front materials.
7 IAM is typically a function of the angle of incidence (AOI) of the direct
8 irradiance to the module's surface.
9 """"""
10 
11 import numpy as np
12 import pandas as pd
13 import functools
14 from pvlib.tools import cosd, sind
15 
16 # a dict of required parameter names for each IAM model
17 # keys are the function names for the IAM models
18 _IAM_MODEL_PARAMS = {
19     'ashrae': {'b'},
20     'physical': {'n', 'K', 'L'},
21     'martin_ruiz': {'a_r'},
22     'sapm': {'B0', 'B1', 'B2', 'B3', 'B4', 'B5'},
23     'interp': set()
24 }
25 
26 
27 def ashrae(aoi, b=0.05):
28     r""""""
29     Determine the incidence angle modifier using the ASHRAE transmission
30     model.
31 
32     The ASHRAE (American Society of Heating, Refrigeration, and Air
33     Conditioning Engineers) transmission model is developed in
34     [1]_, and in [2]_. The model has been used in software such as PVSyst [3]_.
35 
36     Parameters
37     ----------
38     aoi : numeric
39         The angle of incidence (AOI) between the module normal vector and the
40         sun-beam vector in degrees. Angles of nan will result in nan.
41 
42     b : float, default 0.05
43         A parameter to adjust the incidence angle modifier as a function of
44         angle of incidence. Typical values are on the order of 0.05 [3].
45 
46     Returns
47     -------
48     iam : numeric
49         The incident angle modifier (IAM). Returns zero for all abs(aoi) >= 90
50         and for all ``iam`` values that would be less than 0.
51 
52     Notes
53     -----
54     The incidence angle modifier is calculated as
55 
56     .. math::
57 
58         IAM = 1 - b (\sec(aoi) - 1)
59 
60     As AOI approaches 90 degrees, the model yields negative values for IAM;
61     negative IAM values are set to zero in this implementation.
62 
63     References
64     ----------
65     .. [1] Souka A.F., Safwat H.H., ""Determination of the optimum
66        orientations for the double exposure flat-plate collector and its
67        reflections"". Solar Energy vol .10, pp 170-174. 1966.
68 
69     .. [2] ASHRAE standard 93-77
70 
71     .. [3] PVsyst Contextual Help.
72        https://files.pvsyst.com/help/index.html?iam_loss.htm retrieved on
73        October 14, 2019
74 
75     See Also
76     --------
77     pvlib.iam.physical
78     pvlib.iam.martin_ruiz
79     pvlib.iam.interp
80     """"""
81 
82     iam = 1 - b * (1 / np.cos(np.radians(aoi)) - 1)
83     aoi_gte_90 = np.full_like(aoi, False, dtype='bool')
84     np.greater_equal(np.abs(aoi), 90, where=~np.isnan(aoi), out=aoi_gte_90)
85     iam = np.where(aoi_gte_90, 0, iam)
86     iam = np.maximum(0, iam)
87 
88     if isinstance(aoi, pd.Series):
89         iam = pd.Series(iam, index=aoi.index)
90 
91     return iam
92 
93 
94 def physical(aoi, n=1.526, K=4.0, L=0.002, *, n_ar=None):
95     r""""""
96     Determine the incidence angle modifier using refractive index ``n``,
97     extinction coefficient ``K``, glazing thickness ``L`` and refractive
98     index ``n_ar`` of an optional anti-reflective coating.
99 
100     ``iam.physical`` calculates the incidence angle modifier as described in
101     [1]_, Section 3, with additional support of an anti-reflective coating.
102     The calculation is based on a physical model of reflections, absorption,
103     and transmission through a transparent cover.
104 
105     Parameters
106     ----------
107     aoi : numeric
108         The angle of incidence between the module normal vector and the
109         sun-beam vector in degrees. Angles of nan will result in nan.
110 
111     n : numeric, default 1.526
112         The effective index of refraction (unitless). Reference [1]_
113         indicates that a value of 1.526 is acceptable for glass.
114 
115     K : numeric, default 4.0
116         The glazing extinction coefficient in units of 1/meters.
117         Reference [1] indicates that a value of 4 is reasonable for
118         ""water white"" glass.
119 
120     L : numeric, default 0.002
121         The glazing thickness in units of meters. Reference [1]_
122         indicates that 0.002 meters (2 mm) is reasonable for most
123         glass-covered PV panels.
124 
125     n_ar : numeric, optional
126         The effective index of refraction of the anti-reflective (AR) coating
127         (unitless). If n_ar is None (default), no AR coating is applied.
128         A typical value for the effective index of an AR coating is 1.29.
129 
130     Returns
131     -------
132     iam : numeric
133         The incident angle modifier
134 
135     Notes
136     -----
137     The pvlib python authors believe that Eqn. 14 in [1]_ is
138     incorrect, which presents :math:`\theta_{r} = \arcsin(n \sin(AOI))`.
139     Here, :math:`\theta_{r} = \arcsin(1/n \times \sin(AOI))`
140 
141     References
142     ----------
143     .. [1] W. De Soto et al., ""Improvement and validation of a model for
144        photovoltaic array performance"", Solar Energy, vol 80, pp. 78-88,
145        2006.
146 
147     .. [2] Duffie, John A. & Beckman, William A.. (2006). Solar Engineering
148        of Thermal Processes, third edition. [Books24x7 version] Available
149        from http://common.books24x7.com/toc.aspx?bookid=17160.
150 
151     See Also
152     --------
153     pvlib.iam.martin_ruiz
154     pvlib.iam.ashrae
155     pvlib.iam.interp
156     pvlib.iam.sapm
157     """"""
158     n1, n3 = 1, n
159     if n_ar is None or np.allclose(n_ar, n1):
160         # no AR coating
161         n2 = n
162     else:
163         n2 = n_ar
164 
165     # incidence angle
166     costheta = np.maximum(0, cosd(aoi))  # always >= 0
167     sintheta = np.sqrt(1 - costheta**2)  # always >= 0
168     n1costheta1 = n1 * costheta
169     n2costheta1 = n2 * costheta
170 
171     # refraction angle of first interface
172     sintheta = n1 / n2 * sintheta
173     costheta = np.sqrt(1 - sintheta**2)
174     n1costheta2 = n1 * costheta
175     n2costheta2 = n2 * costheta
176 
177     # reflectance of s-, p-polarized, and normal light by the first interface
178     rho12_s = ((n1costheta1 - n2costheta2) / (n1costheta1 + n2costheta2)) ** 2
179     rho12_p = ((n1costheta2 - n2costheta1) / (n1costheta2 + n2costheta1)) ** 2
180     rho12_0 = ((n1 - n2) / (n1 + n2)) ** 2
181 
182     # transmittance through the first interface
183     tau_s = 1 - rho12_s
184     tau_p = 1 - rho12_p
185     tau_0 = 1 - rho12_0
186 
187     if not np.allclose(n3, n2):  # AR coated glass
188         n3costheta2 = n3 * costheta
189         # refraction angle of second interface
190         sintheta = n2 / n3 * sintheta
191         costheta = np.sqrt(1 - sintheta**2)
192         n2costheta3 = n2 * costheta
193         n3costheta3 = n3 * costheta
194 
195         # reflectance by the second interface
196         rho23_s = (
197             (n2costheta2 - n3costheta3) / (n2costheta2 + n3costheta3)
198         ) ** 2
199         rho23_p = (
200             (n2costheta3 - n3costheta2) / (n2costheta3 + n3costheta2)
201         ) ** 2
202         rho23_0 = ((n2 - n3) / (n2 + n3)) ** 2
203 
204         # transmittance through the coating, including internal reflections
205         # 1 + rho23*rho12 + (rho23*rho12)^2 + ... = 1/(1 - rho23*rho12)
206         tau_s *= (1 - rho23_s) / (1 - rho23_s * rho12_s)
207         tau_p *= (1 - rho23_p) / (1 - rho23_p * rho12_p)
208         tau_0 *= (1 - rho23_0) / (1 - rho23_0 * rho12_0)
209 
210     # transmittance after absorption in the glass
211     tau_s *= np.exp(-K * L / costheta)
212     tau_p *= np.exp(-K * L / costheta)
213     tau_0 *= np.exp(-K * L)
214 
215     # incidence angle modifier
216     iam = (tau_s + tau_p) / 2 / tau_0
217 
218     return iam
219 
220 
221 def martin_ruiz(aoi, a_r=0.16):
222     r'''
223     Determine the incidence angle modifier (IAM) using the Martin
224     and Ruiz incident angle model.
225 
226     Parameters
227     ----------
228     aoi : numeric, degrees
229         The angle of incidence between the module normal vector and the
230         sun-beam vector in degrees.
231 
232     a_r : numeric
233         The angular losses coefficient described in equation 3 of [1]_.
234         This is an empirical dimensionless parameter. Values of ``a_r`` are
235         generally on the order of 0.08 to 0.25 for flat-plate PV modules.
236 
237     Returns
238     -------
239     iam : numeric
240         The incident angle modifier(s)
241 
242     Notes
243     -----
244     `martin_ruiz` calculates the incidence angle modifier (IAM) as described in
245     [1]_. The information required is the incident angle (AOI) and the angular
246     losses coefficient (a_r). Note that [1]_ has a corrigendum [2]_ which
247     clarifies a mix-up of 'alpha's and 'a's in the former.
248 
249     The incident angle modifier is defined as
250 
251     .. math::
252 
253        IAM = \frac{1 - \exp(-\frac{\cos(aoi)}{a_r})}
254        {1 - \exp(\frac{-1}{a_r})}
255 
256     which is presented as :math:`AL(\alpha) = 1 - IAM` in equation 4 of [1]_,
257     with :math:`\alpha` representing the angle of incidence AOI. Thus IAM = 1
258     at AOI = 0, and IAM = 0 at AOI = 90.  This equation is only valid for
259     -90 <= aoi <= 90, therefore `iam` is constrained to 0.0 outside this
260     interval.
261 
262     References
263     ----------
264     .. [1] N. Martin and J. M. Ruiz, ""Calculation of the PV modules angular
265        losses under field conditions by means of an analytical model"", Solar
266        Energy Materials & Solar Cells, vol. 70, pp. 25-38, 2001.
267 
268     .. [2] N. Martin and J. M. Ruiz, ""Corrigendum to 'Calculation of the PV
269        modules angular losses under field conditions by means of an
270        analytical model'"", Solar Energy Materials & Solar Cells, vol. 110,
271        pp. 154, 2013.
272 
273     See Also
274     --------
275     pvlib.iam.martin_ruiz_diffuse
276     pvlib.iam.physical
277     pvlib.iam.ashrae
278     pvlib.iam.interp
279     pvlib.iam.sapm
280     '''
281     # Contributed by Anton Driesse (@adriesse), PV Performance Labs. July, 2019
282 
283     aoi_input = aoi
284 
285     aoi = np.asanyarray(aoi)
286     a_r = np.asanyarray(a_r)
287 
288     if np.any(np.less_equal(a_r, 0)):
289         raise ValueError(""The parameter 'a_r' cannot be zero or negative."")
290 
291     with np.errstate(invalid='ignore'):
292         iam = (1 - np.exp(-cosd(aoi) / a_r)) / (1 - np.exp(-1 / a_r))
293         iam = np.where(np.abs(aoi) >= 90.0, 0.0, iam)
294 
295     if isinstance(aoi_input, pd.Series):
296         iam = pd.Series(iam, index=aoi_input.index)
297 
298     return iam
299 
300 
301 def martin_ruiz_diffuse(surface_tilt, a_r=0.16, c1=0.4244, c2=None):
302     '''
303     Determine the incidence angle modifiers (iam) for diffuse sky and
304     ground-reflected irradiance using the Martin and Ruiz incident angle model.
305 
306     Parameters
307     ----------
308     surface_tilt: float or array-like, default 0
309         Surface tilt angles in decimal degrees.
310         The tilt angle is defined as degrees from horizontal
311         (e.g. surface facing up = 0, surface facing horizon = 90)
312         surface_tilt must be in the range [0, 180]
313 
314     a_r : numeric
315         The angular losses coefficient described in equation 3 of [1]_.
316         This is an empirical dimensionless parameter. Values of a_r are
317         generally on the order of 0.08 to 0.25 for flat-plate PV modules.
318         a_r must be greater than zero.
319 
320     c1 : float
321         First fitting parameter for the expressions that approximate the
322         integral of diffuse irradiance coming from different directions.
323         c1 is given as the constant 4 / 3 / pi (0.4244) in [1]_.
324 
325     c2 : float
326         Second fitting parameter for the expressions that approximate the
327         integral of diffuse irradiance coming from different directions.
328         If c2 is None, it will be calculated according to the linear
329         relationship given in [3]_.
330 
331     Returns
332     -------
333     iam_sky : numeric
334         The incident angle modifier for sky diffuse
335 
336     iam_ground : numeric
337         The incident angle modifier for ground-reflected diffuse
338 
339     Notes
340     -----
341     Sky and ground modifiers are complementary: iam_sky for tilt = 30 is
342     equal to iam_ground for tilt = 180 - 30.  For vertical surfaces,
343     tilt = 90, the two factors are equal.
344 
345     References
346     ----------
347     .. [1] N. Martin and J. M. Ruiz, ""Calculation of the PV modules angular
348        losses under field conditions by means of an analytical model"", Solar
349        Energy Materials & Solar Cells, vol. 70, pp. 25-38, 2001.
350 
351     .. [2] N. Martin and J. M. Ruiz, ""Corrigendum to 'Calculation of the PV
352        modules angular losses under field conditions by means of an
353        analytical model'"", Solar Energy Materials & Solar Cells, vol. 110,
354        pp. 154, 2013.
355 
356     .. [3] ""IEC 61853-3 Photovoltaic (PV) module performance testing and energy
357        rating - Part 3: Energy rating of PV modules"". IEC, Geneva, 2018.
358 
359     See Also
360     --------
361     pvlib.iam.martin_ruiz
362     pvlib.iam.physical
363     pvlib.iam.ashrae
364     pvlib.iam.interp
365     pvlib.iam.sapm
366     '''
367     # Contributed by Anton Driesse (@adriesse), PV Performance Labs. Oct. 2019
368 
369     if isinstance(surface_tilt, pd.Series):
370         out_index = surface_tilt.index
371     else:
372         out_index = None
373 
374     surface_tilt = np.asanyarray(surface_tilt)
375 
376     # avoid undefined results for horizontal or upside-down surfaces
377     zeroang = 1e-06
378 
379     surface_tilt = np.where(surface_tilt == 0, zeroang, surface_tilt)
380     surface_tilt = np.where(surface_tilt == 180, 180 - zeroang, surface_tilt)
381 
382     if c2 is None:
383         # This equation is from [3] Sect. 7.2
384         c2 = 0.5 * a_r - 0.154
385 
386     beta = np.radians(surface_tilt)
387     sin = np.sin
388     pi = np.pi
389     cos = np.cos
390 
391     # avoid RuntimeWarnings for <, sin, and cos with nan
392     with np.errstate(invalid='ignore'):
393         # because sin(pi) isn't exactly zero
394         sin_beta = np.where(surface_tilt < 90, sin(beta), sin(pi - beta))
395 
396         trig_term_sky = sin_beta + (pi - beta - sin_beta) / (1 + cos(beta))
397         trig_term_gnd = sin_beta +      (beta - sin_beta) / (1 - cos(beta))  # noqa: E222 E261 E501
398 
399     iam_sky = 1 - np.exp(-(c1 + c2 * trig_term_sky) * trig_term_sky / a_r)
400     iam_gnd = 1 - np.exp(-(c1 + c2 * trig_term_gnd) * trig_term_gnd / a_r)
401 
402     if out_index is not None:
403         iam_sky = pd.Series(iam_sky, index=out_index, name='iam_sky')
404         iam_gnd = pd.Series(iam_gnd, index=out_index, name='iam_ground')
405 
406     return iam_sky, iam_gnd
407 
408 
409 def interp(aoi, theta_ref, iam_ref, method='linear', normalize=True):
410     r'''
411     Determine the incidence angle modifier (IAM) by interpolating a set of
412     reference values, which are usually measured values.
413 
414     Parameters
415     ----------
416     aoi : numeric
417         The angle of incidence between the module normal vector and the
418         sun-beam vector [degrees].
419 
420     theta_ref : numeric
421         Vector of angles at which the IAM is known [degrees].
422 
423     iam_ref : numeric
424         IAM values for each angle in ``theta_ref`` [unitless].
425 
426     method : str, default 'linear'
427         Specifies the interpolation method.
428         Useful options are: 'linear', 'quadratic', 'cubic'.
429         See scipy.interpolate.interp1d for more options.
430 
431     normalize : boolean, default True
432         When true, the interpolated values are divided by the interpolated
433         value at zero degrees.  This ensures that ``iam=1.0`` at normal
434         incidence.
435 
436     Returns
437     -------
438     iam : numeric
439         The incident angle modifier(s) [unitless]
440 
441     Notes
442     -----
443     ``theta_ref`` must have two or more points and may span any range of
444     angles. Typically there will be a dozen or more points in the range 0-90
445     degrees. Beyond the range of ``theta_ref``, IAM values are extrapolated,
446     but constrained to be non-negative.
447 
448     The sign of ``aoi`` is ignored; only the magnitude is used.
449 
450     See Also
451     --------
452     pvlib.iam.physical
453     pvlib.iam.ashrae
454     pvlib.iam.martin_ruiz
455     pvlib.iam.sapm
456     '''
457     # Contributed by Anton Driesse (@adriesse), PV Performance Labs. July, 2019
458 
459     from scipy.interpolate import interp1d
460 
461     # Scipy doesn't give the clearest feedback, so check number of points here.
462     MIN_REF_VALS = {'linear': 2, 'quadratic': 3, 'cubic': 4, 1: 2, 2: 3, 3: 4}
463 
464     if len(theta_ref) < MIN_REF_VALS.get(method, 2):
465         raise ValueError(""Too few reference points defined ""
466                          ""for interpolation method '%s'."" % method)
467 
468     if np.any(np.less(iam_ref, 0)):
469         raise ValueError(""Negative value(s) found in 'iam_ref'. ""
470                          ""This is not physically possible."")
471 
472     interpolator = interp1d(theta_ref, iam_ref, kind=method,
473                             fill_value='extrapolate')
474     aoi_input = aoi
475 
476     aoi = np.asanyarray(aoi)
477     aoi = np.abs(aoi)
478     iam = interpolator(aoi)
479     iam = np.clip(iam, 0, None)
480 
481     if normalize:
482         iam /= interpolator(0)
483 
484     if isinstance(aoi_input, pd.Series):
485         iam = pd.Series(iam, index=aoi_input.index)
486 
487     return iam
488 
489 
490 def sapm(aoi, module, upper=None):
491     r""""""
492     Determine the incidence angle modifier (IAM) using the SAPM model.
493 
494     Parameters
495     ----------
496     aoi : numeric
497         Angle of incidence in degrees. Negative input angles will return
498         zeros.
499 
500     module : dict-like
501         A dict or Series with the SAPM IAM model parameters.
502         See the :py:func:`sapm` notes section for more details.
503 
504     upper : None or float, default None
505         Upper limit on the results.
506 
507     Returns
508     -------
509     iam : numeric
510         The SAPM angle of incidence loss coefficient, termed F2 in [1]_.
511 
512     Notes
513     -----
514     The SAPM [1]_ traditionally does not define an upper limit on the AOI
515     loss function and values slightly exceeding 1 may exist for moderate
516     angles of incidence (15-40 degrees). However, users may consider
517     imposing an upper limit of 1.
518 
519     References
520     ----------
521     .. [1] King, D. et al, 2004, ""Sandia Photovoltaic Array Performance
522        Model"", SAND Report 3535, Sandia National Laboratories, Albuquerque,
523        NM.
524 
525     .. [2] B.H. King et al, ""Procedure to Determine Coefficients for the
526        Sandia Array Performance Model (SAPM),"" SAND2016-5284, Sandia
527        National Laboratories (2016).
528 
529     .. [3] B.H. King et al, ""Recent Advancements in Outdoor Measurement
530        Techniques for Angle of Incidence Effects,"" 42nd IEEE PVSC (2015).
531        DOI: 10.1109/PVSC.2015.7355849
532 
533     See Also
534     --------
535     pvlib.iam.physical
536     pvlib.iam.ashrae
537     pvlib.iam.martin_ruiz
538     pvlib.iam.interp
539     """"""
540 
541     aoi_coeff = [module['B5'], module['B4'], module['B3'], module['B2'],
542                  module['B1'], module['B0']]
543 
544     iam = np.polyval(aoi_coeff, aoi)
545     iam = np.clip(iam, 0, upper)
546     # nan tolerant masking
547     aoi_lt_0 = np.full_like(aoi, False, dtype='bool')
548     np.less(aoi, 0, where=~np.isnan(aoi), out=aoi_lt_0)
549     iam = np.where(aoi_lt_0, 0, iam)
550 
551     if isinstance(aoi, pd.Series):
552         iam = pd.Series(iam, aoi.index)
553 
554     return iam
555 
556 
557 def marion_diffuse(model, surface_tilt, **kwargs):
558     """"""
559     Determine diffuse irradiance incidence angle modifiers using Marion's
560     method of integrating over solid angle.
561 
562     Parameters
563     ----------
564     model : str
565         The IAM function to evaluate across solid angle. Must be one of
566         `'ashrae', 'physical', 'martin_ruiz', 'sapm', 'schlick'`.
567 
568     surface_tilt : numeric
569         Surface tilt angles in decimal degrees.
570         The tilt angle is defined as degrees from horizontal
571         (e.g. surface facing up = 0, surface facing horizon = 90).
572 
573     **kwargs
574         Extra parameters passed to the IAM function.
575 
576     Returns
577     -------
578     iam : dict
579         IAM values for each type of diffuse irradiance:
580 
581             * 'sky': radiation from the sky dome (zenith <= 90)
582             * 'horizon': radiation from the region of the sky near the horizon
583               (89.5 <= zenith <= 90)
584             * 'ground': radiation reflected from the ground (zenith >= 90)
585 
586         See [1]_ for a detailed description of each class.
587 
588     See Also
589     --------
590     pvlib.iam.marion_integrate
591 
592     References
593     ----------
594     .. [1] B. Marion ""Numerical method for angle-of-incidence correction
595        factors for diffuse radiation incident photovoltaic modules"",
596        Solar Energy, Volume 147, Pages 344-348. 2017.
597        DOI: 10.1016/j.solener.2017.03.027
598 
599     Examples
600     --------
601     >>> marion_diffuse('physical', surface_tilt=20)
602     {'sky': 0.9539178294437575,
603      'horizon': 0.7652650139134007,
604      'ground': 0.6387140117795903}
605 
606     >>> marion_diffuse('ashrae', [20, 30], b=0.04)
607     {'sky': array([0.96748999, 0.96938408]),
608      'horizon': array([0.86478428, 0.91825792]),
609      'ground': array([0.77004435, 0.8522436 ])}
610     """"""
611 
612     models = {
613         'physical': physical,
614         'ashrae': ashrae,
615         'sapm': sapm,
616         'martin_ruiz': martin_ruiz,
617         'schlick': schlick,
618     }
619 
620     try:
621         iam_model = models[model]
622     except KeyError:
623         raise ValueError('model must be one of: ' + str(list(models.keys())))
624 
625     iam_function = functools.partial(iam_model, **kwargs)
626     iam = {}
627     for region in ['sky', 'horizon', 'ground']:
628         iam[region] = marion_integrate(iam_function, surface_tilt, region)
629 
630     return iam
631 
632 
633 def marion_integrate(function, surface_tilt, region, num=None):
634     """"""
635     Integrate an incidence angle modifier (IAM) function over solid angle
636     to determine a diffuse irradiance correction factor using Marion's method.
637 
638     This lower-level function actually performs the IAM integration for the
639     specified solid angle region.
640 
641     Parameters
642     ----------
643     function : callable(aoi)
644         The IAM function to evaluate across solid angle. The function must
645         be vectorized and take only one parameter, the angle of incidence in
646         degrees.
647 
648     surface_tilt : numeric
649         Surface tilt angles in decimal degrees.
650         The tilt angle is defined as degrees from horizontal
651         (e.g. surface facing up = 0, surface facing horizon = 90).
652 
653     region : {'sky', 'horizon', 'ground'}
654         The region to integrate over. Must be one of:
655 
656             * 'sky': radiation from the sky dome (zenith <= 90)
657             * 'horizon': radiation from the region of the sky near the horizon
658               (89.5 <= zenith <= 90)
659             * 'ground': radiation reflected from the ground (zenith >= 90)
660 
661         See [1]_ for a detailed description of each class.
662 
663     num : int, optional
664         The number of increments in the zenith integration.
665         If not specified, N will follow the values used in [1]_:
666 
667             * 'sky' or 'ground': num = 180
668             * 'horizon': num = 1800
669 
670     Returns
671     -------
672     iam : numeric
673         AOI diffuse correction factor for the specified region.
674 
675     See Also
676     --------
677     pvlib.iam.marion_diffuse
678 
679     References
680     ----------
681     .. [1] B. Marion ""Numerical method for angle-of-incidence correction
682        factors for diffuse radiation incident photovoltaic modules"",
683        Solar Energy, Volume 147, Pages 344-348. 2017.
684        DOI: 10.1016/j.solener.2017.03.027
685 
686     Examples
687     --------
688     >>> marion_integrate(pvlib.iam.ashrae, 20, 'sky')
689     0.9596085829811408
690 
691     >>> from functools import partial
692     >>> f = partial(pvlib.iam.physical, n=1.3)
693     >>> marion_integrate(f, [20, 30], 'sky')
694     array([0.96225034, 0.9653219 ])
695     """"""
696 
697     if num is None:
698         if region in ['sky', 'ground']:
699             num = 180
700         elif region == 'horizon':
701             num = 1800
702         else:
703             raise ValueError(f'Invalid region: {region}')
704 
705     beta = np.radians(surface_tilt)
706     if isinstance(beta, pd.Series):
707         # convert Series to np array for broadcasting later
708         beta = beta.values
709     ai = np.pi/num  # angular increment
710 
711     phi_range = np.linspace(0, np.pi, num, endpoint=False)
712     psi_range = np.linspace(0, 2*np.pi, 2*num, endpoint=False)
713 
714     # the pseudocode in [1] do these checks at the end, but it's
715     # faster to do this criteria check up front instead of later.
716     if region == 'sky':
717         mask = phi_range + ai <= np.pi/2
718     elif region == 'horizon':
719         lo = 89.5 * np.pi/180
720         hi = np.pi/2
721         mask = (lo <= phi_range) & (phi_range + ai <= hi)
722     elif region == 'ground':
723         mask = (phi_range >= np.pi/2)
724     else:
725         raise ValueError(f'Invalid region: {region}')
726     phi_range = phi_range[mask]
727 
728     # fast Cartesian product of phi and psi
729     angles = np.array(np.meshgrid(phi_range, psi_range)).T.reshape(-1, 2)
730     # index with single-element lists to maintain 2nd dimension so that
731     # these angle arrays broadcast across the beta array
732     phi_1 = angles[:, [0]]
733     psi_1 = angles[:, [1]]
734     phi_2 = phi_1 + ai
735     # psi_2 = psi_1 + ai  # not needed
736     phi_avg = phi_1 + 0.5*ai
737     psi_avg = psi_1 + 0.5*ai
738     term_1 = np.cos(beta) * np.cos(phi_avg)
739     # The AOI formula includes a term based on the difference between
740     # panel azimuth and the photon azimuth, but because we assume each class
741     # of diffuse irradiance is isotropic and we are integrating over all
742     # angles, it doesn't matter what panel azimuth we choose (i.e., the
743     # system is rotationally invariant).  So we choose gamma to be zero so
744     # that we can omit it from the cos(psi_avg) term.
745     # Marion's paper mentions this in the Section 3 pseudocode:
746     # ""set gamma to pi (or any value between 0 and 2pi)""
747     term_2 = np.sin(beta) * np.sin(phi_avg) * np.cos(psi_avg)
748     cosaoi = term_1 + term_2
749     aoi = np.arccos(cosaoi)
750     # simplify Eq 8, (psi_2 - psi_1) is always ai
751     dAs = ai * (np.cos(phi_1) - np.cos(phi_2))
752     cosaoi_dAs = cosaoi * dAs
753     # apply the final AOI check, zeroing out non-passing points
754     mask = aoi < np.pi/2
755     cosaoi_dAs = np.where(mask, cosaoi_dAs, 0)
756     numerator = np.sum(function(np.degrees(aoi)) * cosaoi_dAs, axis=0)
757     denominator = np.sum(cosaoi_dAs, axis=0)
758 
759     with np.errstate(invalid='ignore'):
760         # in some cases, no points pass the criteria
761         # (e.g. region='ground', surface_tilt=0), so we override the division
762         # by zero to set Fd=0.  Also, preserve nans in beta.
763         Fd = np.where((denominator != 0) | ~np.isfinite(beta),
764                       numerator / denominator,
765                       0)
766 
767     # preserve input type
768     if np.isscalar(surface_tilt):
769         Fd = Fd.item()
770     elif isinstance(surface_tilt, pd.Series):
771         Fd = pd.Series(Fd, surface_tilt.index)
772 
773     return Fd
774 
775 
776 def schlick(aoi):
777     """"""
778     Determine incidence angle modifier (IAM) for direct irradiance using the
779     Schlick approximation to the Fresnel equations.
780 
781     The Schlick approximation was proposed in [1]_ as a computationally
782     efficient alternative to computing the Fresnel factor in computer
783     graphics contexts.  This implementation is a normalized form of the
784     equation in [1]_ so that it can be used as a PV IAM model.
785     Unlike other IAM models, this model has no ability to describe
786     different reflection profiles.
787 
788     In PV contexts, the Schlick approximation has been used as an analytically
789     integrable alternative to the Fresnel equations for estimating IAM
790     for diffuse irradiance [2]_.
791 
792     Parameters
793     ----------
794     aoi : numeric
795         The angle of incidence (AOI) between the module normal vector and the
796         sun-beam vector. Angles of nan will result in nan. [degrees]
797 
798     Returns
799     -------
800     iam : numeric
801         The incident angle modifier.
802 
803     References
804     ----------
805     .. [1] Schlick, C. An inexpensive BRDF model for physically-based
806        rendering. Computer graphics forum 13 (1994).
807 
808     .. [2] Xie, Y., M. Sengupta, A. Habte, A. Andreas, ""The 'Fresnel Equations'
809        for Diffuse radiation on Inclined photovoltaic Surfaces (FEDIS)"",
810        Renewable and Sustainable Energy Reviews, vol. 161, 112362. June 2022.
811        :doi:`10.1016/j.rser.2022.112362`
812 
813     See Also
814     --------
815     pvlib.iam.schlick_diffuse
816     """"""
817     iam = 1 - (1 - cosd(aoi)) ** 5
818     iam = np.where(np.abs(aoi) >= 90.0, 0.0, iam)
819 
820     # preserve input type
821     if np.isscalar(aoi):
822         iam = iam.item()
823     elif isinstance(aoi, pd.Series):
824         iam = pd.Series(iam, aoi.index)
825 
826     return iam
827 
828 
829 def schlick_diffuse(surface_tilt):
830     """"""
831     Determine the incidence angle modifiers (IAM) for diffuse sky and
832     ground-reflected irradiance on a tilted surface using the Schlick
833     incident angle model.
834 
835     The diffuse iam values are calculated using an analytical integration
836     of the Schlick equation [1]_ over the portion of an isotropic sky and
837     isotropic foreground that is visible from the tilted surface [2]_.
838 
839     Parameters
840     ----------
841     surface_tilt : numeric
842         Surface tilt angle measured from horizontal (e.g. surface facing
843         up = 0, surface facing horizon = 90). [degrees]
844 
845     Returns
846     -------
847     iam_sky : numeric
848         The incident angle modifier for sky diffuse.
849 
850     iam_ground : numeric
851         The incident angle modifier for ground-reflected diffuse.
852 
853     References
854     ----------
855     .. [1] Schlick, C. An inexpensive BRDF model for physically-based
856        rendering. Computer graphics forum 13 (1994).
857 
858     .. [2] Xie, Y., M. Sengupta, A. Habte, A. Andreas, ""The 'Fresnel Equations'
859        for Diffuse radiation on Inclined photovoltaic Surfaces (FEDIS)"",
860        Renewable and Sustainable Energy Reviews, vol. 161, 112362. June 2022.
861        :doi:`10.1016/j.rser.2022.112362`
862 
863     See Also
864     --------
865     pvlib.iam.schlick
866     """"""
867     # these calculations are as in [2]_, but with the refractive index
868     # weighting coefficient w set to 1.0 (so it is omitted)
869 
870     # relative transmittance of sky diffuse radiation by PV cover:
871     cosB = cosd(surface_tilt)
872     sinB = sind(surface_tilt)
873     cuk = (2 / (np.pi * (1 + cosB))) * (
874         (30/7)*np.pi - (160/21)*np.radians(surface_tilt) - (10/3)*np.pi*cosB
875         + (160/21)*cosB*sinB - (5/3)*np.pi*cosB*sinB**2 + (20/7)*cosB*sinB**3
876         - (5/16)*np.pi*cosB*sinB**4 + (16/105)*cosB*sinB**5
877     )  # Eq 4 in [2]
878 
879     # relative transmittance of ground-reflected radiation by PV cover:
880     with np.errstate(divide='ignore', invalid='ignore'):  # Eq 6 in [2]
881         cug = 40 / (21 * (1 - cosB)) - (1 + cosB) / (1 - cosB) * cuk
882 
883     cug = np.where(surface_tilt < 1e-6, 0, cug)
884 
885     # respect input types:
886     if np.isscalar(surface_tilt):
887         cuk = cuk.item()
888         cug = cug.item()
889     elif isinstance(surface_tilt, pd.Series):
890         cuk = pd.Series(cuk, surface_tilt.index)
891         cug = pd.Series(cug, surface_tilt.index)
892 
893     return cuk, cug
894 
[end of pvlib/iam.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pvlib/pvlib-python,40e9e978c170bdde4eeee1547729417665dbc34c,"regression: iam.physical returns nan for aoi > 90Â° when n = 1
**Describe the bug**
For pvlib==0.9.5, when n = 1 (no reflection) and aoi > 90Â°, we get nan as result.

**To Reproduce**
```python
import pvlib
pvlib.iam.physical(aoi=100, n=1)
```
returns `nan`.

**Expected behavior**
The result should be `0`, as it was for pvlib <= 0.9.4.


**Versions:**
 - ``pvlib.__version__``: '0.9.5'
 - ``pandas.__version__``:  '1.5.3'
 - python: 3.10.4

",,2023-03-24T10:46:42Z,"<patch>
diff --git a/pvlib/iam.py b/pvlib/iam.py
--- a/pvlib/iam.py
+++ b/pvlib/iam.py
@@ -175,8 +175,12 @@ def physical(aoi, n=1.526, K=4.0, L=0.002, *, n_ar=None):
     n2costheta2 = n2 * costheta
 
     # reflectance of s-, p-polarized, and normal light by the first interface
-    rho12_s = ((n1costheta1 - n2costheta2) / (n1costheta1 + n2costheta2)) ** 2
-    rho12_p = ((n1costheta2 - n2costheta1) / (n1costheta2 + n2costheta1)) ** 2
+    with np.errstate(divide='ignore', invalid='ignore'):
+        rho12_s = \
+            ((n1costheta1 - n2costheta2) / (n1costheta1 + n2costheta2)) ** 2
+        rho12_p = \
+            ((n1costheta2 - n2costheta1) / (n1costheta2 + n2costheta1)) ** 2
+
     rho12_0 = ((n1 - n2) / (n1 + n2)) ** 2
 
     # transmittance through the first interface
@@ -208,13 +212,22 @@ def physical(aoi, n=1.526, K=4.0, L=0.002, *, n_ar=None):
         tau_0 *= (1 - rho23_0) / (1 - rho23_0 * rho12_0)
 
     # transmittance after absorption in the glass
-    tau_s *= np.exp(-K * L / costheta)
-    tau_p *= np.exp(-K * L / costheta)
+    with np.errstate(divide='ignore', invalid='ignore'):
+        tau_s *= np.exp(-K * L / costheta)
+        tau_p *= np.exp(-K * L / costheta)
+
     tau_0 *= np.exp(-K * L)
 
     # incidence angle modifier
     iam = (tau_s + tau_p) / 2 / tau_0
 
+    # for light coming from behind the plane, none can enter the module
+    # when n2 > 1, this is already the case
+    if np.isclose(n2, 1).any():
+        iam = np.where(aoi >= 90, 0, iam)
+        if isinstance(aoi, pd.Series):
+            iam = pd.Series(iam, index=aoi.index)
+
     return iam
 
 

</patch>","diff --git a/pvlib/tests/test_iam.py b/pvlib/tests/test_iam.py
--- a/pvlib/tests/test_iam.py
+++ b/pvlib/tests/test_iam.py
@@ -51,6 +51,18 @@ def test_physical():
     assert_series_equal(iam, expected)
 
 
+def test_physical_n1_L0():
+    aoi = np.array([0, 22.5, 45, 67.5, 90, 100, np.nan])
+    expected = np.array([1, 1, 1, 1, 0, 0, np.nan])
+    iam = _iam.physical(aoi, n=1, L=0)
+    assert_allclose(iam, expected, equal_nan=True)
+
+    aoi = pd.Series(aoi)
+    expected = pd.Series(expected)
+    iam = _iam.physical(aoi, n=1, L=0)
+    assert_series_equal(iam, expected)
+
+
 def test_physical_ar():
     aoi = np.array([0, 22.5, 45, 67.5, 90, 100, np.nan])
     expected = np.array([1, 0.99944171, 0.9917463, 0.91506158, 0, 0, np.nan])
",0.9,"[""pvlib/tests/test_iam.py::test_physical_n1_L0""]","[""pvlib/tests/test_iam.py::test_ashrae"", ""pvlib/tests/test_iam.py::test_ashrae_scalar"", ""pvlib/tests/test_iam.py::test_physical"", ""pvlib/tests/test_iam.py::test_physical_ar"", ""pvlib/tests/test_iam.py::test_physical_noar"", ""pvlib/tests/test_iam.py::test_physical_scalar"", ""pvlib/tests/test_iam.py::test_martin_ruiz"", ""pvlib/tests/test_iam.py::test_martin_ruiz_exception"", ""pvlib/tests/test_iam.py::test_martin_ruiz_diffuse"", ""pvlib/tests/test_iam.py::test_iam_interp"", ""pvlib/tests/test_iam.py::test_sapm[45-0.9975036250000002]"", ""pvlib/tests/test_iam.py::test_sapm[aoi1-expected1]"", ""pvlib/tests/test_iam.py::test_sapm[aoi2-expected2]"", ""pvlib/tests/test_iam.py::test_sapm_limits"", ""pvlib/tests/test_iam.py::test_marion_diffuse_model"", ""pvlib/tests/test_iam.py::test_marion_diffuse_kwargs"", ""pvlib/tests/test_iam.py::test_marion_diffuse_invalid"", ""pvlib/tests/test_iam.py::test_marion_integrate_scalar[sky-180-0.9596085829811408]"", ""pvlib/tests/test_iam.py::test_marion_integrate_scalar[horizon-1800-0.8329070417832541]"", ""pvlib/tests/test_iam.py::test_marion_integrate_scalar[ground-180-0.719823559106309]"", ""pvlib/tests/test_iam.py::test_marion_integrate_list[sky-180-expected0]"", ""pvlib/tests/test_iam.py::test_marion_integrate_list[horizon-1800-expected1]"", ""pvlib/tests/test_iam.py::test_marion_integrate_list[ground-180-expected2]"", ""pvlib/tests/test_iam.py::test_marion_integrate_series[sky-180-expected0]"", ""pvlib/tests/test_iam.py::test_marion_integrate_series[horizon-1800-expected1]"", ""pvlib/tests/test_iam.py::test_marion_integrate_series[ground-180-expected2]"", ""pvlib/tests/test_iam.py::test_marion_integrate_ground_flat"", ""pvlib/tests/test_iam.py::test_marion_integrate_invalid"", ""pvlib/tests/test_iam.py::test_schlick"", ""pvlib/tests/test_iam.py::test_schlick_diffuse""]",6072e0982c3c0236f532ddfa48fbf461180d834e
pvlib__pvlib-python-1072,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
temperature.fuentes errors when given tz-aware inputs on pandas>=1.0.0
**Describe the bug**
When the weather timeseries inputs to `temperature.fuentes` have tz-aware index, an internal call to `np.diff(index)` returns an array of `Timedelta` objects instead of an array of nanosecond ints, throwing an error immediately after.  The error only happens when using pandas>=1.0.0; using 0.25.3 runs successfully, but emits the warning:

```
  /home/kevin/anaconda3/envs/pvlib-dev/lib/python3.7/site-packages/numpy/lib/function_base.py:1243: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.
  	To accept the future behavior, pass 'dtype=object'.
  	To keep the old behavior, pass 'dtype=""datetime64[ns]""'.
    a = asanyarray(a)
```

**To Reproduce**
```python
In [1]: import pvlib
   ...: import pandas as pd
   ...: 
   ...: index_naive = pd.date_range('2019-01-01', freq='h', periods=3)
   ...: 
   ...: kwargs = {
   ...:     'poa_global': pd.Series(1000, index_naive),
   ...:     'temp_air': pd.Series(20, index_naive),
   ...:     'wind_speed': pd.Series(1, index_naive),
   ...:     'noct_installed': 45
   ...: }
   ...: 

In [2]: print(pvlib.temperature.fuentes(**kwargs))
2019-01-01 00:00:00    47.85
2019-01-01 01:00:00    50.85
2019-01-01 02:00:00    50.85
Freq: H, Name: tmod, dtype: float64

In [3]: kwargs['poa_global'].index = index_naive.tz_localize('UTC')
   ...: print(pvlib.temperature.fuentes(**kwargs))
   ...: 
Traceback (most recent call last):

  File ""<ipython-input-3-ff99badadc91>"", line 2, in <module>
    print(pvlib.temperature.fuentes(**kwargs))

  File ""/home/kevin/anaconda3/lib/python3.7/site-packages/pvlib/temperature.py"", line 602, in fuentes
    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60

TypeError: float() argument must be a string or a number, not 'Timedelta'
```

**Expected behavior**
`temperature.fuentes` should work with both tz-naive and tz-aware inputs.


**Versions:**
 - ``pvlib.__version__``: 0.8.0
 - ``pandas.__version__``: 1.0.0+
 - python: 3.7.4 (default, Aug 13 2019, 20:35:49) \n[GCC 7.3.0]



</issue>
<code>
[start of README.md]
1 <img src=""docs/sphinx/source/_images/pvlib_logo_horiz.png"" width=""600"">
2 
3 <table>
4 <tr>
5   <td>Latest Release</td>
6   <td>
7     <a href=""https://pypi.org/project/pvlib/"">
8     <img src=""https://img.shields.io/pypi/v/pvlib.svg"" alt=""latest release"" />
9     </a>
10     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
11     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/version.svg"" />
12     </a>
13     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
14     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg"" />
15     </a>
16 </tr>
17 <tr>
18   <td>License</td>
19   <td>
20     <a href=""https://github.com/pvlib/pvlib-python/blob/master/LICENSE"">
21     <img src=""https://img.shields.io/pypi/l/pvlib.svg"" alt=""license"" />
22     </a>
23 </td>
24 </tr>
25 <tr>
26   <td>Build Status</td>
27   <td>
28     <a href=""http://pvlib-python.readthedocs.org/en/stable/"">
29     <img src=""https://readthedocs.org/projects/pvlib-python/badge/?version=stable"" alt=""documentation build status"" />
30     </a>
31     <a href=""https://dev.azure.com/solararbiter/pvlib%20python/_build/latest?definitionId=4&branchName=master"">
32       <img src=""https://dev.azure.com/solararbiter/pvlib%20python/_apis/build/status/pvlib.pvlib-python?branchName=master"" alt=""Azure Pipelines build status"" />
33     </a>
34   </td>
35 </tr>
36 <tr>
37   <td>Code Quality</td>
38  Â <td>
39     <a href=""https://lgtm.com/projects/g/pvlib/pvlib-python/context:python"">
40     <img src=""https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18"" alt=""lgtm quality grade"" />
41     </a>
42     <a href=""https://lgtm.com/projects/g/pvlib/pvlib-python/alerts"">
43     <img src=""https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18"" alt=""lgtm alters"" />
44     </a>
45   </td>
46 </tr>
47 <tr>
48   <td>Coverage</td>
49  Â <td>
50     <a href=""https://coveralls.io/r/pvlib/pvlib-python"">
51     <img src=""https://img.shields.io/coveralls/pvlib/pvlib-python.svg"" alt=""coveralls coverage"" />
52     </a>
53     <a href=""https://codecov.io/gh/pvlib/pvlib-python"">
54     <img src=""https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg"" alt=""codecov coverage"" />
55     </a>
56   </td>
57 </tr>
58 <tr>
59   <td>Publications</td>
60   <td>
61     <a href=""https://doi.org/10.5281/zenodo.3762635"">
62     <img src=""https://zenodo.org/badge/DOI/10.5281/zenodo.3762635.svg"" alt=""zenodo reference"">
63     </a>
64     <a href=""http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1"">
65     <img src=""http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg"" alt=""JOSS reference"" />
66     </a>
67   </td>
68 </tr>
69 <tr>
70   <td>Downloads</td>
71   <td>
72     <a href=""https://pypi.org/project/pvlib/"">
73     <img src=""https://img.shields.io/pypi/dm/pvlib"" alt=""PyPI downloads"" />
74     </a>
75     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
76     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg"" alt=""conda-forge downloads"" />
77     </a>
78   </td>
79 </tr>
80 </table>
81 
82 
83 pvlib python is a community supported tool that provides a set of
84 functions and classes for simulating the performance of photovoltaic
85 energy systems. pvlib python was originally ported from the PVLIB MATLAB
86 toolbox developed at Sandia National Laboratories and it implements many
87 of the models and methods developed at the Labs. More information on
88 Sandia Labs PV performance modeling programs can be found at
89 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,
90 but operate independently of it.
91 
92 
93 Documentation
94 =============
95 
96 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/).
97 
98 
99 Installation
100 ============
101 
102 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.
103 Please see the [Installation page](http://pvlib-python.readthedocs.io/en/stable/installation.html) of the documentation for complete instructions.
104 
105 
106 Contributing
107 ============
108 
109 We need your help to make pvlib-python a great tool!
110 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.
111 The long-term success of pvlib-python requires substantial community support.
112 
113 
114 License
115 =======
116 
117 BSD 3-clause
118 
119 
120 Getting support
121 ===============
122 
123 pvlib usage questions can be asked on
124 [Stack Overflow](http://stackoverflow.com) and tagged with
125 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.
126 
127 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)
128 is used for discussing various topics of interest to the pvlib-python
129 community. We also make new version announcements on the google group.
130 
131 If you suspect that you may have discovered a bug or if you'd like to
132 change something about pvlib, then please make an issue on our
133 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).
134 
135 
136 Citing
137 ======
138 
139 If you use pvlib-python in a published work, please cite:
140 
141   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.
142   ""pvlib python: a python package for modeling solar energy systems.""
143   Journal of Open Source Software, 3(29), 884, (2018).
144   https://doi.org/10.21105/joss.00884
145 
146 Please also cite the DOI corresponding to the specific version of
147 pvlib-python that you used. pvlib-python DOIs are listed at
148 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)
149 
150 NumFOCUS
151 ========
152 
153 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)
154 
155 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)
156 
[end of README.md]
[start of pvlib/temperature.py]
1 """"""
2 The ``temperature`` module contains functions for modeling temperature of
3 PV modules and cells.
4 """"""
5 
6 import numpy as np
7 import pandas as pd
8 from pvlib.tools import sind
9 
10 TEMPERATURE_MODEL_PARAMETERS = {
11     'sapm': {
12         'open_rack_glass_glass': {'a': -3.47, 'b': -.0594, 'deltaT': 3},
13         'close_mount_glass_glass': {'a': -2.98, 'b': -.0471, 'deltaT': 1},
14         'open_rack_glass_polymer': {'a': -3.56, 'b': -.0750, 'deltaT': 3},
15         'insulated_back_glass_polymer': {'a': -2.81, 'b': -.0455, 'deltaT': 0},
16     },
17     'pvsyst': {'freestanding': {'u_c': 29.0, 'u_v': 0},
18                'insulated': {'u_c': 15.0, 'u_v': 0}}
19 }
20 """"""Dictionary of temperature parameters organized by model.
21 
22 There are keys for each model at the top level. Currently there are two models,
23 ``'sapm'`` for the Sandia Array Performance Model, and ``'pvsyst'``. Each model
24 has a dictionary of configurations; a value is itself a dictionary containing
25 model parameters. Retrieve parameters by indexing the model and configuration
26 by name. Note: the keys are lower-cased and case sensitive.
27 
28 Example
29 -------
30 Retrieve the open rack glass-polymer configuration for SAPM::
31 
32     from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS
33     temperature_model_parameters = (
34         TEMPERATURE_MODEL_PARAMETERS['sapm']['open_rack_glass_polymer'])
35     # {'a': -3.56, 'b': -0.075, 'deltaT': 3}
36 """"""
37 
38 
39 def _temperature_model_params(model, parameter_set):
40     try:
41         params = TEMPERATURE_MODEL_PARAMETERS[model]
42         return params[parameter_set]
43     except KeyError:
44         msg = ('{} is not a named set of parameters for the {} cell'
45                ' temperature model.'
46                ' See pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS'
47                ' for names'.format(parameter_set, model))
48         raise KeyError(msg)
49 
50 
51 def sapm_cell(poa_global, temp_air, wind_speed, a, b, deltaT,
52               irrad_ref=1000):
53     r'''
54     Calculate cell temperature per the Sandia Array Performance Model.
55 
56     See [1]_ for details on the Sandia Array Performance Model.
57 
58     Parameters
59     ----------
60     poa_global : numeric
61         Total incident irradiance [W/m^2].
62 
63     temp_air : numeric
64         Ambient dry bulb temperature [C].
65 
66     wind_speed : numeric
67         Wind speed at a height of 10 meters [m/s].
68 
69     a : float
70         Parameter :math:`a` in :eq:`sapm1`.
71 
72     b : float
73         Parameter :math:`b` in :eq:`sapm1`.
74 
75     deltaT : float
76         Parameter :math:`\Delta T` in :eq:`sapm2` [C].
77 
78     irrad_ref : float, default 1000
79         Reference irradiance, parameter :math:`E_{0}` in
80         :eq:`sapm2` [W/m^2].
81 
82     Returns
83     -------
84     numeric, values in degrees C.
85 
86     Notes
87     -----
88     The model for cell temperature :math:`T_{C}` is given by a pair of
89     equations (Eq. 11 and 12 in [1]_).
90 
91     .. math::
92        :label: sapm1
93 
94        T_{m} = E \times \exp (a + b \times WS) + T_{a}
95 
96     .. math::
97        :label: sapm2
98 
99        T_{C} = T_{m} + \frac{E}{E_{0}} \Delta T
100 
101     The module back surface temperature :math:`T_{m}` is implemented in
102     :py:func:`~pvlib.temperature.sapm_module`.
103 
104     Inputs to the model are plane-of-array irradiance :math:`E` (W/m2) and
105     ambient air temperature :math:`T_{a}` (C). Model parameters depend both on
106     the module construction and its mounting. Parameter sets are provided in
107     [1]_ for representative modules and mounting, and are coded for convenience
108     in :data:`~pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS`.
109 
110     +---------------+----------------+-------+---------+---------------------+
111     | Module        | Mounting       | a     | b       | :math:`\Delta T [C]`|
112     +===============+================+=======+=========+=====================+
113     | glass/glass   | open rack      | -3.47 | -0.0594 | 3                   |
114     +---------------+----------------+-------+---------+---------------------+
115     | glass/glass   | close roof     | -2.98 | -0.0471 | 1                   |
116     +---------------+----------------+-------+---------+---------------------+
117     | glass/polymer | open rack      | -3.56 | -0.075  | 3                   |
118     +---------------+----------------+-------+---------+---------------------+
119     | glass/polymer | insulated back | -2.81 | -0.0455 | 0                   |
120     +---------------+----------------+-------+---------+---------------------+
121 
122     References
123     ----------
124     .. [1] King, D. et al, 2004, ""Sandia Photovoltaic Array Performance
125        Model"", SAND Report 3535, Sandia National Laboratories, Albuquerque,
126        NM.
127 
128     See also
129     --------
130     sapm_cell_from_module
131     sapm_module
132 
133     Examples
134     --------
135     >>> from pvlib.temperature import sapm_cell, TEMPERATURE_MODEL_PARAMETERS
136     >>> params = TEMPERATURE_MODEL_PARAMETERS['sapm']['open_rack_glass_glass']
137     >>> sapm_cell(1000, 10, 0, **params)
138     44.11703066106086
139     '''
140     module_temperature = sapm_module(poa_global, temp_air, wind_speed,
141                                      a, b)
142     return sapm_cell_from_module(module_temperature, poa_global, deltaT,
143                                  irrad_ref)
144 
145 
146 def sapm_module(poa_global, temp_air, wind_speed, a, b):
147     r'''
148     Calculate module back surface temperature per the Sandia Array
149     Performance Model.
150 
151     See [1]_ for details on the Sandia Array Performance Model.
152 
153     Parameters
154     ----------
155     poa_global : numeric
156         Total incident irradiance [W/m^2].
157 
158     temp_air : numeric
159         Ambient dry bulb temperature [C].
160 
161     wind_speed : numeric
162         Wind speed at a height of 10 meters [m/s].
163 
164     a : float
165         Parameter :math:`a` in :eq:`sapm1mod`.
166 
167     b : float
168         Parameter :math:`b` in :eq:`sapm1mod`.
169 
170     Returns
171     -------
172     numeric, values in degrees C.
173 
174     Notes
175     -----
176     The model for module temperature :math:`T_{m}` is given by Eq. 11 in [1]_.
177 
178     .. math::
179        :label: sapm1mod
180 
181        T_{m} = E \times \exp (a + b \times WS) + T_{a}
182 
183     Inputs to the model are plane-of-array irradiance :math:`E` (W/m2) and
184     ambient air temperature :math:`T_{a}` (C). Model outputs are surface
185     temperature at the back of the module :math:`T_{m}` and cell temperature
186     :math:`T_{C}`. Model parameters depend both on the module construction and
187     its mounting. Parameter sets are provided in [1]_ for representative
188     modules and mounting, and are coded for convenience in
189     :data:`~pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS`.
190 
191     +---------------+----------------+-------+---------+---------------------+
192     | Module        | Mounting       | a     | b       | :math:`\Delta T [C]`|
193     +===============+================+=======+=========+=====================+
194     | glass/glass   | open rack      | -3.47 | -0.0594 | 3                   |
195     +---------------+----------------+-------+---------+---------------------+
196     | glass/glass   | close roof     | -2.98 | -0.0471 | 1                   |
197     +---------------+----------------+-------+---------+---------------------+
198     | glass/polymer | open rack      | -3.56 | -0.075  | 3                   |
199     +---------------+----------------+-------+---------+---------------------+
200     | glass/polymer | insulated back | -2.81 | -0.0455 | 0                   |
201     +---------------+----------------+-------+---------+---------------------+
202 
203     References
204     ----------
205     .. [1] King, D. et al, 2004, ""Sandia Photovoltaic Array Performance
206        Model"", SAND Report 3535, Sandia National Laboratories, Albuquerque,
207        NM.
208 
209     See also
210     --------
211     sapm_cell
212     sapm_cell_from_module
213     '''
214     return poa_global * np.exp(a + b * wind_speed) + temp_air
215 
216 
217 def sapm_cell_from_module(module_temperature, poa_global, deltaT,
218                           irrad_ref=1000):
219     r'''
220     Calculate cell temperature from module temperature using the Sandia Array
221     Performance Model.
222 
223     See [1]_ for details on the Sandia Array Performance Model.
224 
225     Parameters
226     ----------
227     module_temperature : numeric
228         Temperature of back of module surface [C].
229 
230     poa_global : numeric
231         Total incident irradiance [W/m^2].
232 
233     deltaT : float
234         Parameter :math:`\Delta T` in :eq:`sapm2_cell_from_mod` [C].
235 
236     irrad_ref : float, default 1000
237         Reference irradiance, parameter :math:`E_{0}` in
238         :eq:`sapm2` [W/m^2].
239 
240     Returns
241     -------
242     numeric, values in degrees C.
243 
244     Notes
245     -----
246     The model for cell temperature :math:`T_{C}` is given by Eq. 12 in [1]_.
247 
248     .. math::
249        :label: sapm2_cell_from_mod
250 
251        T_{C} = T_{m} + \frac{E}{E_{0}} \Delta T
252 
253     The module back surface temperature :math:`T_{m}` is implemented in
254     :py:func:`~pvlib.temperature.sapm_module`.
255 
256     Model parameters depend both on the module construction and its mounting.
257     Parameter sets are provided in [1]_ for representative modules and
258     mounting, and are coded for convenience in
259     :data:`~pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS`.
260 
261     +---------------+----------------+-------+---------+---------------------+
262     | Module        | Mounting       | a     | b       | :math:`\Delta T [C]`|
263     +===============+================+=======+=========+=====================+
264     | glass/glass   | open rack      | -3.47 | -0.0594 | 3                   |
265     +---------------+----------------+-------+---------+---------------------+
266     | glass/glass   | close roof     | -2.98 | -0.0471 | 1                   |
267     +---------------+----------------+-------+---------+---------------------+
268     | glass/polymer | open rack      | -3.56 | -0.075  | 3                   |
269     +---------------+----------------+-------+---------+---------------------+
270     | glass/polymer | insulated back | -2.81 | -0.0455 | 0                   |
271     +---------------+----------------+-------+---------+---------------------+
272 
273     References
274     ----------
275     .. [1] King, D. et al, 2004, ""Sandia Photovoltaic Array Performance
276        Model"", SAND Report 3535, Sandia National Laboratories, Albuquerque,
277        NM.
278 
279     See also
280     --------
281     sapm_cell
282     sapm_module
283     '''
284     return module_temperature + (poa_global / irrad_ref) * deltaT
285 
286 
287 def pvsyst_cell(poa_global, temp_air, wind_speed=1.0, u_c=29.0, u_v=0.0,
288                 eta_m=0.1, alpha_absorption=0.9):
289     r""""""
290     Calculate cell temperature using an empirical heat loss factor model
291     as implemented in PVsyst.
292 
293     Parameters
294     ----------
295     poa_global : numeric
296         Total incident irradiance [W/m^2].
297 
298     temp_air : numeric
299         Ambient dry bulb temperature [C].
300 
301     wind_speed : numeric, default 1.0
302         Wind speed in m/s measured at the same height for which the wind loss
303         factor was determined.  The default value 1.0 m/2 is the wind
304         speed at module height used to determine NOCT. [m/s]
305 
306     u_c : float, default 29.0
307         Combined heat loss factor coefficient. The default value is
308         representative of freestanding modules with the rear surfaces exposed
309         to open air (e.g., rack mounted). Parameter :math:`U_{c}` in
310         :eq:`pvsyst`.
311         :math:`\left[\frac{\text{W}/{\text{m}^2}}{\text{C}}\right]`
312 
313     u_v : float, default 0.0
314         Combined heat loss factor influenced by wind. Parameter :math:`U_{v}`
315         in :eq:`pvsyst`.
316         :math:`\left[ \frac{\text{W}/\text{m}^2}{\text{C}\ \left( \text{m/s} \right)} \right]`
317 
318     eta_m : numeric, default 0.1
319         Module external efficiency as a fraction, i.e., DC power / poa_global.
320         Parameter :math:`\eta_{m}` in :eq:`pvsyst`.
321 
322     alpha_absorption : numeric, default 0.9
323         Absorption coefficient. Parameter :math:`\alpha` in :eq:`pvsyst`.
324 
325     Returns
326     -------
327     numeric, values in degrees Celsius
328 
329     Notes
330     -----
331     The Pvsyst model for cell temperature :math:`T_{C}` is given by
332 
333     .. math::
334        :label: pvsyst
335 
336         T_{C} = T_{a} + \frac{\alpha E (1 - \eta_{m})}{U_{c} + U_{v} \times WS}
337 
338     Inputs to the model are plane-of-array irradiance :math:`E` (W/m2), ambient
339     air temperature :math:`T_{a}` (C) and wind speed :math:`WS` (m/s). Model
340     output is cell temperature :math:`T_{C}`. Model parameters depend both on
341     the module construction and its mounting. Parameters are provided in
342     [1]_ for open (freestanding) and close (insulated) mounting configurations,
343     , and are coded for convenience in
344     :data:`~pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS`. The heat loss
345     factors provided represent the combined effect of convection, radiation and
346     conduction, and their values are experimentally determined.
347 
348     +--------------+---------------+---------------+
349     | Mounting     | :math:`U_{c}` | :math:`U_{v}` |
350     +==============+===============+===============+
351     | freestanding | 29.0          | 0.0           |
352     +--------------+---------------+---------------+
353     | insulated    | 15.0          | 0.0           |
354     +--------------+---------------+---------------+
355 
356     References
357     ----------
358     .. [1] ""PVsyst 6 Help"", Files.pvsyst.com, 2018. [Online]. Available:
359        http://files.pvsyst.com/help/index.html. [Accessed: 10- Dec- 2018].
360 
361     .. [2] Faiman, D. (2008). ""Assessing the outdoor operating temperature of
362        photovoltaic modules."" Progress in Photovoltaics 16(4): 307-315.
363 
364     Examples
365     --------
366     >>> from pvlib.temperature import pvsyst_cell, TEMPERATURE_MODEL_PARAMETERS
367     >>> params = TEMPERATURE_MODEL_PARAMETERS['pvsyst']['freestanding']
368     >>> pvsyst_cell(1000, 10, **params)
369     37.93103448275862
370     """"""
371 
372     total_loss_factor = u_c + u_v * wind_speed
373     heat_input = poa_global * alpha_absorption * (1 - eta_m)
374     temp_difference = heat_input / total_loss_factor
375     return temp_air + temp_difference
376 
377 
378 def faiman(poa_global, temp_air, wind_speed=1.0, u0=25.0, u1=6.84):
379     r'''
380     Calculate cell or module temperature using the Faiman model.  The Faiman
381     model uses an empirical heat loss factor model [1]_ and is adopted in the
382     IEC 61853 standards [2]_ and [3]_.
383 
384     Usage of this model in the IEC 61853 standard does not distinguish
385     between cell and module temperature.
386 
387     Parameters
388     ----------
389     poa_global : numeric
390         Total incident irradiance [W/m^2].
391 
392     temp_air : numeric
393         Ambient dry bulb temperature [C].
394 
395     wind_speed : numeric, default 1.0
396         Wind speed in m/s measured at the same height for which the wind loss
397         factor was determined.  The default value 1.0 m/s is the wind
398         speed at module height used to determine NOCT. [m/s]
399 
400     u0 : numeric, default 25.0
401         Combined heat loss factor coefficient. The default value is one
402         determined by Faiman for 7 silicon modules.
403         :math:`\left[\frac{\text{W}/{\text{m}^2}}{\text{C}}\right]`
404 
405     u1 : numeric, default 6.84
406         Combined heat loss factor influenced by wind. The default value is one
407         determined by Faiman for 7 silicon modules.
408         :math:`\left[ \frac{\text{W}/\text{m}^2}{\text{C}\ \left( \text{m/s} \right)} \right]`
409 
410     Returns
411     -------
412     numeric, values in degrees Celsius
413 
414     Notes
415     -----
416     All arguments may be scalars or vectors. If multiple arguments
417     are vectors they must be the same length.
418 
419     References
420     ----------
421     .. [1] Faiman, D. (2008). ""Assessing the outdoor operating temperature of
422        photovoltaic modules."" Progress in Photovoltaics 16(4): 307-315.
423 
424     .. [2] ""IEC 61853-2 Photovoltaic (PV) module performance testing and energy
425        rating - Part 2: Spectral responsivity, incidence angle and module
426        operating temperature measurements"". IEC, Geneva, 2018.
427 
428     .. [3] ""IEC 61853-3 Photovoltaic (PV) module performance testing and energy
429        rating - Part 3: Energy rating of PV modules"". IEC, Geneva, 2018.
430 
431     '''
432     # Contributed by Anton Driesse (@adriesse), PV Performance Labs. Dec., 2019
433 
434     # The following lines may seem odd since u0 & u1 are probably scalar,
435     # but it serves an indirect and easy way of allowing lists and
436     # tuples for the other function arguments.
437     u0 = np.asanyarray(u0)
438     u1 = np.asanyarray(u1)
439 
440     total_loss_factor = u0 + u1 * wind_speed
441     heat_input = poa_global
442     temp_difference = heat_input / total_loss_factor
443     return temp_air + temp_difference
444 
445 
446 def _fuentes_hconv(tave, windmod, tinoct, temp_delta, xlen, tilt,
447                    check_reynold):
448     # Calculate the convective coefficient as in Fuentes 1987 -- a mixture of
449     # free, laminar, and turbulent convection.
450     densair = 0.003484 * 101325.0 / tave  # density
451     visair = 0.24237e-6 * tave**0.76 / densair  # kinematic viscosity
452     condair = 2.1695e-4 * tave**0.84  # thermal conductivity
453     reynold = windmod * xlen / visair
454     # the boundary between laminar and turbulent is modeled as an abrupt
455     # change at Re = 1.2e5:
456     if check_reynold and reynold > 1.2e5:
457         # turbulent convection
458         hforce = 0.0282 / reynold**0.2 * densair * windmod * 1007 / 0.71**0.4
459     else:
460         # laminar convection
461         hforce = 0.8600 / reynold**0.5 * densair * windmod * 1007 / 0.71**0.67
462     # free convection via Grashof number
463     # NB: Fuentes hardwires sind(tilt) as 0.5 for tilt=30
464     grashof = 9.8 / tave * temp_delta * xlen**3 / visair**2 * sind(tilt)
465     # product of Nusselt number and (k/l)
466     hfree = 0.21 * (grashof * 0.71)**0.32 * condair / xlen
467     # combine free and forced components
468     hconv = (hfree**3 + hforce**3)**(1/3)
469     return hconv
470 
471 
472 def _hydraulic_diameter(width, height):
473     # calculate the hydraulic diameter of a rectangle
474     return 2 * (width * height) / (width + height)
475 
476 
477 def fuentes(poa_global, temp_air, wind_speed, noct_installed, module_height=5,
478             wind_height=9.144, emissivity=0.84, absorption=0.83,
479             surface_tilt=30, module_width=0.31579, module_length=1.2):
480     """"""
481     Calculate cell or module temperature using the Fuentes model.
482 
483     The Fuentes model is a first-principles heat transfer energy balance
484     model [1]_ that is used in PVWatts for cell temperature modeling [2]_.
485 
486     Parameters
487     ----------
488     poa_global : pandas Series
489         Total incident irradiance [W/m^2]
490 
491     temp_air : pandas Series
492         Ambient dry bulb temperature [C]
493 
494     wind_speed : pandas Series
495         Wind speed [m/s]
496 
497     noct_installed : float
498         The ""installed"" nominal operating cell temperature as defined in [1]_.
499         PVWatts assumes this value to be 45 C for rack-mounted arrays and
500         49 C for roof mount systems with restricted air flow around the
501         module.  [C]
502 
503     module_height : float, default 5.0
504         The height above ground of the center of the module. The PVWatts
505         default is 5.0 [m]
506 
507     wind_height : float, default 9.144
508         The height above ground at which ``wind_speed`` is measured. The
509         PVWatts defauls is 9.144 [m]
510 
511     emissivity : float, default 0.84
512         The effectiveness of the module at radiating thermal energy. [unitless]
513 
514     absorption : float, default 0.83
515         The fraction of incident irradiance that is converted to thermal
516         energy in the module. [unitless]
517 
518     surface_tilt : float, default 30
519         Module tilt from horizontal. If not provided, the default value
520         of 30 degrees from [1]_ and [2]_ is used. [degrees]
521 
522     module_width : float, default 0.31579
523         Module width. The default value of 0.31579 meters in combination with
524         the default `module_length` gives a hydraulic diameter of 0.5 as
525         assumed in [1]_ and [2]_. [m]
526 
527     module_length : float, default 1.2
528         Module length. The default value of 1.2 meters in combination with
529         the default `module_width` gives a hydraulic diameter of 0.5 as
530         assumed in [1]_ and [2]_. [m]
531 
532     Returns
533     -------
534     temperature_cell : pandas Series
535         The modeled cell temperature [C]
536 
537     Notes
538     -----
539     This function returns slightly different values from PVWatts at night
540     and just after dawn. This is because the SAM SSC assumes that module
541     temperature equals ambient temperature when irradiance is zero so it can
542     skip the heat balance calculation at night.
543 
544     References
545     ----------
546     .. [1] Fuentes, M. K., 1987, ""A Simplifed Thermal Model for Flat-Plate
547            Photovoltaic Arrays"", SAND85-0330, Sandia National Laboratories,
548            Albuquerque NM.
549            http://prod.sandia.gov/techlib/access-control.cgi/1985/850330.pdf
550     .. [2] Dobos, A. P., 2014, ""PVWatts Version 5 Manual"", NREL/TP-6A20-62641,
551            National Renewable Energy Laboratory, Golden CO.
552            doi:10.2172/1158421.
553     """"""
554     # ported from the FORTRAN77 code provided in Appendix A of Fuentes 1987;
555     # nearly all variable names are kept the same for ease of comparison.
556 
557     boltz = 5.669e-8
558     emiss = emissivity
559     absorp = absorption
560     xlen = _hydraulic_diameter(module_width, module_length)
561     # cap0 has units of [J / (m^2 K)], equal to mass per unit area times
562     # specific heat of the module.
563     cap0 = 11000
564     tinoct = noct_installed + 273.15
565 
566     # convective coefficient of top surface of module at NOCT
567     windmod = 1.0
568     tave = (tinoct + 293.15) / 2
569     hconv = _fuentes_hconv(tave, windmod, tinoct, tinoct - 293.15, xlen,
570                            surface_tilt, False)
571 
572     # determine the ground temperature ratio and the ratio of the total
573     # convection to the top side convection
574     hground = emiss * boltz * (tinoct**2 + 293.15**2) * (tinoct + 293.15)
575     backrat = (
576         absorp * 800.0
577         - emiss * boltz * (tinoct**4 - 282.21**4)
578         - hconv * (tinoct - 293.15)
579     ) / ((hground + hconv) * (tinoct - 293.15))
580     tground = (tinoct**4 - backrat * (tinoct**4 - 293.15**4))**0.25
581     tground = np.clip(tground, 293.15, tinoct)
582 
583     tgrat = (tground - 293.15) / (tinoct - 293.15)
584     convrat = (absorp * 800 - emiss * boltz * (
585         2 * tinoct**4 - 282.21**4 - tground**4)) / (hconv * (tinoct - 293.15))
586 
587     # adjust the capacitance (thermal mass) of the module based on the INOCT.
588     # It is a function of INOCT because high INOCT implies thermal coupling
589     # with the racking (e.g. roofmount), so the thermal mass is increased.
590     # `cap` has units J/(m^2 C) -- see Table 3, Equations 26 & 27
591     cap = cap0
592     if tinoct > 321.15:
593         cap = cap * (1 + (tinoct - 321.15) / 12)
594 
595     # iterate through timeseries inputs
596     sun0 = 0
597     tmod0 = 293.15
598 
599     # n.b. the way Fuentes calculates the first timedelta makes it seem like
600     # the value doesn't matter -- rather than recreate it here, just assume
601     # it's the same as the second timedelta:
602     timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60
603     timedelta_hours = np.append([timedelta_hours[0]], timedelta_hours)
604 
605     tamb_array = temp_air + 273.15
606     sun_array = poa_global * absorp
607 
608     # Two of the calculations are easily vectorized, so precalculate them:
609     # sky temperature -- Equation 24
610     tsky_array = 0.68 * (0.0552 * tamb_array**1.5) + 0.32 * tamb_array
611     # wind speed at module height -- Equation 22
612     # not sure why the 1e-4 factor is included -- maybe the equations don't
613     # behave well if wind == 0?
614     windmod_array = wind_speed * (module_height/wind_height)**0.2 + 1e-4
615 
616     tmod0 = 293.15
617     tmod_array = np.zeros_like(poa_global)
618 
619     iterator = zip(tamb_array, sun_array, windmod_array, tsky_array,
620                    timedelta_hours)
621     for i, (tamb, sun, windmod, tsky, dtime) in enumerate(iterator):
622         # solve the heat transfer equation, iterating because the heat loss
623         # terms depend on tmod. NB Fuentes doesn't show that 10 iterations is
624         # sufficient for convergence.
625         tmod = tmod0
626         for j in range(10):
627             # overall convective coefficient
628             tave = (tmod + tamb) / 2
629             hconv = convrat * _fuentes_hconv(tave, windmod, tinoct,
630                                              abs(tmod-tamb), xlen,
631                                              surface_tilt, True)
632             # sky radiation coefficient (Equation 3)
633             hsky = emiss * boltz * (tmod**2 + tsky**2) * (tmod + tsky)
634             # ground radiation coeffieicient (Equation 4)
635             tground = tamb + tgrat * (tmod - tamb)
636             hground = emiss * boltz * (tmod**2 + tground**2) * (tmod + tground)
637             # thermal lag -- Equation 8
638             eigen = - (hconv + hsky + hground) / cap * dtime * 3600
639             # not sure why this check is done, maybe as a speed optimization?
640             if eigen > -10:
641                 ex = np.exp(eigen)
642             else:
643                 ex = 0
644             # Equation 7 -- note that `sun` and `sun0` already account for
645             # absorption (alpha)
646             tmod = tmod0 * ex + (
647                 (1 - ex) * (
648                     hconv * tamb
649                     + hsky * tsky
650                     + hground * tground
651                     + sun0
652                     + (sun - sun0) / eigen
653                 ) + sun - sun0
654             ) / (hconv + hsky + hground)
655         tmod_array[i] = tmod
656         tmod0 = tmod
657         sun0 = sun
658 
659     return pd.Series(tmod_array - 273.15, index=poa_global.index, name='tmod')
660 
[end of pvlib/temperature.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pvlib/pvlib-python,04a523fafbd61bc2e49420963b84ed8e2bd1b3cf,"temperature.fuentes errors when given tz-aware inputs on pandas>=1.0.0
**Describe the bug**
When the weather timeseries inputs to `temperature.fuentes` have tz-aware index, an internal call to `np.diff(index)` returns an array of `Timedelta` objects instead of an array of nanosecond ints, throwing an error immediately after.  The error only happens when using pandas>=1.0.0; using 0.25.3 runs successfully, but emits the warning:

```
  /home/kevin/anaconda3/envs/pvlib-dev/lib/python3.7/site-packages/numpy/lib/function_base.py:1243: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.
  	To accept the future behavior, pass 'dtype=object'.
  	To keep the old behavior, pass 'dtype=""datetime64[ns]""'.
    a = asanyarray(a)
```

**To Reproduce**
```python
In [1]: import pvlib
   ...: import pandas as pd
   ...: 
   ...: index_naive = pd.date_range('2019-01-01', freq='h', periods=3)
   ...: 
   ...: kwargs = {
   ...:     'poa_global': pd.Series(1000, index_naive),
   ...:     'temp_air': pd.Series(20, index_naive),
   ...:     'wind_speed': pd.Series(1, index_naive),
   ...:     'noct_installed': 45
   ...: }
   ...: 

In [2]: print(pvlib.temperature.fuentes(**kwargs))
2019-01-01 00:00:00    47.85
2019-01-01 01:00:00    50.85
2019-01-01 02:00:00    50.85
Freq: H, Name: tmod, dtype: float64

In [3]: kwargs['poa_global'].index = index_naive.tz_localize('UTC')
   ...: print(pvlib.temperature.fuentes(**kwargs))
   ...: 
Traceback (most recent call last):

  File ""<ipython-input-3-ff99badadc91>"", line 2, in <module>
    print(pvlib.temperature.fuentes(**kwargs))

  File ""/home/kevin/anaconda3/lib/python3.7/site-packages/pvlib/temperature.py"", line 602, in fuentes
    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60

TypeError: float() argument must be a string or a number, not 'Timedelta'
```

**Expected behavior**
`temperature.fuentes` should work with both tz-naive and tz-aware inputs.


**Versions:**
 - ``pvlib.__version__``: 0.8.0
 - ``pandas.__version__``: 1.0.0+
 - python: 3.7.4 (default, Aug 13 2019, 20:35:49) \n[GCC 7.3.0]


",,2020-10-01T00:53:14Z,"<patch>
diff --git a/pvlib/temperature.py b/pvlib/temperature.py
--- a/pvlib/temperature.py
+++ b/pvlib/temperature.py
@@ -599,8 +599,9 @@ def fuentes(poa_global, temp_air, wind_speed, noct_installed, module_height=5,
     # n.b. the way Fuentes calculates the first timedelta makes it seem like
     # the value doesn't matter -- rather than recreate it here, just assume
     # it's the same as the second timedelta:
-    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60
-    timedelta_hours = np.append([timedelta_hours[0]], timedelta_hours)
+    timedelta_seconds = poa_global.index.to_series().diff().dt.total_seconds()
+    timedelta_hours = timedelta_seconds / 3600
+    timedelta_hours.iloc[0] = timedelta_hours.iloc[1]
 
     tamb_array = temp_air + 273.15
     sun_array = poa_global * absorp

</patch>","diff --git a/pvlib/tests/test_temperature.py b/pvlib/tests/test_temperature.py
--- a/pvlib/tests/test_temperature.py
+++ b/pvlib/tests/test_temperature.py
@@ -190,3 +190,17 @@ def test_fuentes(filename, inoct):
     night_difference = expected_tcell[is_night] - actual_tcell[is_night]
     assert night_difference.max() < 6
     assert night_difference.min() > 0
+
+
+@pytest.mark.parametrize('tz', [None, 'Etc/GMT+5'])
+def test_fuentes_timezone(tz):
+    index = pd.date_range('2019-01-01', freq='h', periods=3, tz=tz)
+
+    df = pd.DataFrame({'poa_global': 1000, 'temp_air': 20, 'wind_speed': 1},
+                      index)
+
+    out = temperature.fuentes(df['poa_global'], df['temp_air'],
+                              df['wind_speed'], noct_installed=45)
+
+    assert_series_equal(out, pd.Series([47.85, 50.85, 50.85], index=index,
+                                       name='tmod'))
",0.7,"[""pvlib/tests/test_temperature.py::test_fuentes_timezone[Etc/GMT+5]""]","[""pvlib/tests/test_temperature.py::test_sapm_cell"", ""pvlib/tests/test_temperature.py::test_sapm_module"", ""pvlib/tests/test_temperature.py::test_sapm_cell_from_module"", ""pvlib/tests/test_temperature.py::test_sapm_ndarray"", ""pvlib/tests/test_temperature.py::test_sapm_series"", ""pvlib/tests/test_temperature.py::test_pvsyst_cell_default"", ""pvlib/tests/test_temperature.py::test_pvsyst_cell_kwargs"", ""pvlib/tests/test_temperature.py::test_pvsyst_cell_ndarray"", ""pvlib/tests/test_temperature.py::test_pvsyst_cell_series"", ""pvlib/tests/test_temperature.py::test_faiman_default"", ""pvlib/tests/test_temperature.py::test_faiman_kwargs"", ""pvlib/tests/test_temperature.py::test_faiman_list"", ""pvlib/tests/test_temperature.py::test_faiman_ndarray"", ""pvlib/tests/test_temperature.py::test_faiman_series"", ""pvlib/tests/test_temperature.py::test__temperature_model_params"", ""pvlib/tests/test_temperature.py::test_fuentes[pvwatts_8760_rackmount.csv-45]"", ""pvlib/tests/test_temperature.py::test_fuentes[pvwatts_8760_roofmount.csv-49]"", ""pvlib/tests/test_temperature.py::test_fuentes_timezone[None]""]",6e5148f59c5050e8f7a0084b7ae39e93b80f72e6
pvlib__pvlib-python-1606,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
golden-section search fails when upper and lower bounds are equal
**Describe the bug**
I was using pvlib for sometime now and until now I was always passing a big dataframe containing readings of a long period. Because of some changes in our software architecture, I need to pass the weather readings as a single reading (a dataframe with only one row) and I noticed that for readings that GHI-DHI are zero pvlib fails to calculate the output and returns below error while the same code executes correctly with weather information that has non-zero GHI-DHI:
```python
import os
import pathlib
import time
import json
from datetime import datetime
from time import mktime, gmtime

import pandas as pd

from pvlib import pvsystem
from pvlib import location as pvlocation
from pvlib import modelchain
from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS as PARAMS # not used -- to remove
from pvlib.bifacial.pvfactors import pvfactors_timeseries
from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS

class PV:
    def pv_transform_time(self, val):
        # tt = gmtime(val / 1000)
        tt = gmtime(val)
        dd = datetime.fromtimestamp(mktime(tt))
        timestamp = pd.Timestamp(dd)
        return timestamp

    def __init__(self, model: str, inverter: str, latitude: float, longitude: float, **kwargs):
        # super().__init__(**kwargs)

        temperature_model_parameters = TEMPERATURE_MODEL_PARAMETERS[""sapm""][
            ""open_rack_glass_glass""
        ]
        # Load the database of CEC module model parameters
        modules = pvsystem.retrieve_sam(""cecmod"")
        # Load the database of CEC inverter model parameters
        inverters = pvsystem.retrieve_sam(""cecinverter"")


        # A bare bone PV simulator

        # Load the database of CEC module model parameters
        modules = pvsystem.retrieve_sam('cecmod')
        inverters = pvsystem.retrieve_sam('cecinverter')
        module_parameters = modules[model]
        inverter_parameters = inverters[inverter]

        location = pvlocation.Location(latitude=latitude, longitude=longitude)
        system = pvsystem.PVSystem(module_parameters=module_parameters, inverter_parameters=inverter_parameters, temperature_model_parameters=temperature_model_parameters)
        self.modelchain = modelchain.ModelChain(system, location, aoi_model='no_loss', spectral_model=""no_loss"")

    def process(self, data):
        weather = pd.read_json(data)
        # print(f""raw_weather: {weather}"")
        weather.drop('time.1', axis=1, inplace=True)
        weather['time'] = pd.to_datetime(weather['time']).map(datetime.timestamp) # --> this works for the new process_weather code and also the old weather file
        weather[""time""] = weather[""time""].apply(self.pv_transform_time)
        weather.index = weather[""time""]
        # print(f""weather: {weather}"")
        # print(weather.dtypes)
        # print(weather['ghi'][0])
        # print(type(weather['ghi'][0]))

        # simulate
        self.modelchain.run_model(weather)
        # print(self.modelchain.results.ac.to_frame().to_json())
        print(self.modelchain.results.ac)


# good data
good_data = ""{\""time\"":{\""12\"":\""2010-01-01 13:30:00+00:00\""},\""ghi\"":{\""12\"":36},\""dhi\"":{\""12\"":36},\""dni\"":{\""12\"":0},\""Tamb\"":{\""12\"":8.0},\""WindVel\"":{\""12\"":5.0},\""WindDir\"":{\""12\"":270},\""time.1\"":{\""12\"":\""2010-01-01 13:30:00+00:00\""}}""

# data that causes error
data = ""{\""time\"":{\""4\"":\""2010-01-01 05:30:00+00:00\""},\""ghi\"":{\""4\"":0},\""dhi\"":{\""4\"":0},\""dni\"":{\""4\"":0},\""Tamb\"":{\""4\"":8.0},\""WindVel\"":{\""4\"":4.0},\""WindDir\"":{\""4\"":240},\""time.1\"":{\""4\"":\""2010-01-01 05:30:00+00:00\""}}""
p1 = PV(model=""Trina_Solar_TSM_300DEG5C_07_II_"", inverter=""ABB__MICRO_0_25_I_OUTD_US_208__208V_"", latitude=51.204483, longitude=5.265472)
p1.process(good_data)
print(""====="")
p1.process(data)
```
Error:
```log
$ python3 ./tmp-pv.py 
time
2010-01-01 13:30:00    7.825527
dtype: float64
=====
/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py:340: RuntimeWarning: divide by zero encountered in divide
  np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))
Traceback (most recent call last):
  File ""/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py"", line 88, in <module>
    p1.process(data)
  File ""/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py"", line 75, in process
    self.modelchain.run_model(weather)
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py"", line 1770, in run_model
    self._run_from_effective_irrad(weather)
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py"", line 1858, in _run_from_effective_irrad
    self.dc_model()
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py"", line 790, in cec
    return self._singlediode(self.system.calcparams_cec)
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py"", line 772, in _singlediode
    self.results.dc = tuple(itertools.starmap(
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py"", line 931, in singlediode
    return singlediode(photocurrent, saturation_current,
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py"", line 2826, in singlediode
    out = _singlediode._lambertw(
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/singlediode.py"", line 651, in _lambertw
    p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14,
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py"", line 364, in _golden_sect_DataFrame
    raise Exception(""Iterations exceeded maximum. Check that func"",
Exception: ('Iterations exceeded maximum. Check that func', ' is not NaN in (lower, upper)')
```

I have to mention that for now the workaround that I am using is to pass the weather data as a dataframe with two rows, the first row is a good weather data that pvlib can process and the second row is the incoming weather reading (I can also post that code if you want).

**Expected behavior**
PVlib should have consistent behavior and regardless of GHI-DHI readings.

**Versions:**
```python
>>> import pvlib
>>> import pandas
>>> pvlib.__version__
'0.9.1'
>>> pandas.__version__
'1.4.3'
``` 
 - python: 3.10.6
- OS: Ubuntu 22.04.1 LTS

</issue>
<code>
[start of README.md]
1 <img src=""docs/sphinx/source/_images/pvlib_logo_horiz.png"" width=""600"">
2 
3 <table>
4 <tr>
5   <td>Latest Release</td>
6   <td>
7     <a href=""https://pypi.org/project/pvlib/"">
8     <img src=""https://img.shields.io/pypi/v/pvlib.svg"" alt=""latest release"" />
9     </a>
10     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
11     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/version.svg"" />
12     </a>
13     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
14     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg"" />
15     </a>
16 </tr>
17 <tr>
18   <td>License</td>
19   <td>
20     <a href=""https://github.com/pvlib/pvlib-python/blob/master/LICENSE"">
21     <img src=""https://img.shields.io/pypi/l/pvlib.svg"" alt=""license"" />
22     </a>
23 </td>
24 </tr>
25 <tr>
26   <td>Build Status</td>
27   <td>
28     <a href=""http://pvlib-python.readthedocs.org/en/stable/"">
29     <img src=""https://readthedocs.org/projects/pvlib-python/badge/?version=stable"" alt=""documentation build status"" />
30     </a>
31     <a href=""https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml?query=branch%3Amaster"">
32       <img src=""https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml/badge.svg?branch=master"" alt=""GitHub Actions Testing Status"" />
33     </a>
34     <a href=""https://codecov.io/gh/pvlib/pvlib-python"">
35     <img src=""https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg"" alt=""codecov coverage"" />
36     </a>
37   </td>
38 </tr>
39 <tr>
40   <td>Code Quality</td>
41   <td>
42     <a href=""https://lgtm.com/projects/g/pvlib/pvlib-python/context:python"">
43     <img src=""https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18"" alt=""lgtm quality grade"" />
44     </a>
45     <a href=""https://lgtm.com/projects/g/pvlib/pvlib-python/alerts"">
46     <img src=""https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18"" alt=""lgtm alters"" />
47     </a>
48   </td>
49 </tr>
50 <tr>
51   <td>Benchmarks</td>
52   <td>
53     <a href=""https://pvlib-benchmarker.github.io/pvlib-benchmarks/"">
54     <img src=""https://img.shields.io/badge/benchmarks-asv-lightgrey"" />
55     </a>
56   </td>
57 </tr>
58 <tr>
59   <td>Publications</td>
60   <td>
61     <a href=""https://doi.org/10.5281/zenodo.593284"">
62     <img src=""https://zenodo.org/badge/DOI/10.5281/zenodo.593284.svg"" alt=""zenodo reference"">
63     </a>
64     <a href=""http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1"">
65     <img src=""http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg"" alt=""JOSS reference"" />
66     </a>
67   </td>
68 </tr>
69 <tr>
70   <td>Downloads</td>
71   <td>
72     <a href=""https://pypi.org/project/pvlib/"">
73     <img src=""https://img.shields.io/pypi/dm/pvlib"" alt=""PyPI downloads"" />
74     </a>
75     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
76     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg"" alt=""conda-forge downloads"" />
77     </a>
78   </td>
79 </tr>
80 </table>
81 
82 
83 pvlib python is a community supported tool that provides a set of
84 functions and classes for simulating the performance of photovoltaic
85 energy systems. pvlib python was originally ported from the PVLIB MATLAB
86 toolbox developed at Sandia National Laboratories and it implements many
87 of the models and methods developed at the Labs. More information on
88 Sandia Labs PV performance modeling programs can be found at
89 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,
90 but operate independently of it.
91 
92 
93 Documentation
94 =============
95 
96 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/),
97 including an [FAQ](http://pvlib-python.readthedocs.io/en/stable/user_guide/faq.html) page.
98 
99 Installation
100 ============
101 
102 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.
103 Please see the [Installation page](https://pvlib-python.readthedocs.io/en/stable/user_guide/installation.html) of the documentation for complete instructions.
104 
105 
106 Contributing
107 ============
108 
109 We need your help to make pvlib-python a great tool!
110 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.
111 The long-term success of pvlib-python requires substantial community support.
112 
113 
114 Citing
115 ======
116 
117 If you use pvlib-python in a published work, please cite:
118 
119   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.
120   ""pvlib python: a python package for modeling solar energy systems.""
121   Journal of Open Source Software, 3(29), 884, (2018).
122   https://doi.org/10.21105/joss.00884
123 
124 Please also cite the DOI corresponding to the specific version of
125 pvlib-python that you used. pvlib-python DOIs are listed at
126 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)
127 
128 If you use pvlib-python in a commercial or publicly-available application, please
129 consider displaying one of the ""powered by pvlib"" logos:
130 
131 <img src=""docs/sphinx/source/_images/pvlib_powered_logo_vert.png"" width=""300""><img src=""docs/sphinx/source/_images/pvlib_powered_logo_horiz.png"" width=""300"">
132 
133 Getting support
134 ===============
135 
136 pvlib usage questions can be asked on
137 [Stack Overflow](http://stackoverflow.com) and tagged with
138 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.
139 
140 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)
141 is used for discussing various topics of interest to the pvlib-python
142 community. We also make new version announcements on the google group.
143 
144 If you suspect that you may have discovered a bug or if you'd like to
145 change something about pvlib, then please make an issue on our
146 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).
147 
148 
149 
150 License
151 =======
152 
153 BSD 3-clause.
154 
155 
156 NumFOCUS
157 ========
158 
159 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)
160 
161 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)
162 
[end of README.md]
[start of pvlib/tools.py]
1 """"""
2 Collection of functions used in pvlib_python
3 """"""
4 
5 import datetime as dt
6 import numpy as np
7 import pandas as pd
8 import pytz
9 import warnings
10 
11 
12 def cosd(angle):
13     """"""
14     Cosine with angle input in degrees
15 
16     Parameters
17     ----------
18     angle : float or array-like
19         Angle in degrees
20 
21     Returns
22     -------
23     result : float or array-like
24         Cosine of the angle
25     """"""
26 
27     res = np.cos(np.radians(angle))
28     return res
29 
30 
31 def sind(angle):
32     """"""
33     Sine with angle input in degrees
34 
35     Parameters
36     ----------
37     angle : float
38         Angle in degrees
39 
40     Returns
41     -------
42     result : float
43         Sin of the angle
44     """"""
45 
46     res = np.sin(np.radians(angle))
47     return res
48 
49 
50 def tand(angle):
51     """"""
52     Tan with angle input in degrees
53 
54     Parameters
55     ----------
56     angle : float
57         Angle in degrees
58 
59     Returns
60     -------
61     result : float
62         Tan of the angle
63     """"""
64 
65     res = np.tan(np.radians(angle))
66     return res
67 
68 
69 def asind(number):
70     """"""
71     Inverse Sine returning an angle in degrees
72 
73     Parameters
74     ----------
75     number : float
76         Input number
77 
78     Returns
79     -------
80     result : float
81         arcsin result
82     """"""
83 
84     res = np.degrees(np.arcsin(number))
85     return res
86 
87 
88 def acosd(number):
89     """"""
90     Inverse Cosine returning an angle in degrees
91 
92     Parameters
93     ----------
94     number : float
95         Input number
96 
97     Returns
98     -------
99     result : float
100         arccos result
101     """"""
102 
103     res = np.degrees(np.arccos(number))
104     return res
105 
106 
107 def localize_to_utc(time, location):
108     """"""
109     Converts or localizes a time series to UTC.
110 
111     Parameters
112     ----------
113     time : datetime.datetime, pandas.DatetimeIndex,
114            or pandas.Series/DataFrame with a DatetimeIndex.
115     location : pvlib.Location object
116 
117     Returns
118     -------
119     pandas object localized to UTC.
120     """"""
121     if isinstance(time, dt.datetime):
122         if time.tzinfo is None:
123             time = pytz.timezone(location.tz).localize(time)
124         time_utc = time.astimezone(pytz.utc)
125     else:
126         try:
127             time_utc = time.tz_convert('UTC')
128         except TypeError:
129             time_utc = time.tz_localize(location.tz).tz_convert('UTC')
130 
131     return time_utc
132 
133 
134 def datetime_to_djd(time):
135     """"""
136     Converts a datetime to the Dublin Julian Day
137 
138     Parameters
139     ----------
140     time : datetime.datetime
141         time to convert
142 
143     Returns
144     -------
145     float
146         fractional days since 12/31/1899+0000
147     """"""
148 
149     if time.tzinfo is None:
150         time_utc = pytz.utc.localize(time)
151     else:
152         time_utc = time.astimezone(pytz.utc)
153 
154     djd_start = pytz.utc.localize(dt.datetime(1899, 12, 31, 12))
155     djd = (time_utc - djd_start).total_seconds() * 1.0/(60 * 60 * 24)
156 
157     return djd
158 
159 
160 def djd_to_datetime(djd, tz='UTC'):
161     """"""
162     Converts a Dublin Julian Day float to a datetime.datetime object
163 
164     Parameters
165     ----------
166     djd : float
167         fractional days since 12/31/1899+0000
168     tz : str, default 'UTC'
169         timezone to localize the result to
170 
171     Returns
172     -------
173     datetime.datetime
174        The resultant datetime localized to tz
175     """"""
176 
177     djd_start = pytz.utc.localize(dt.datetime(1899, 12, 31, 12))
178 
179     utc_time = djd_start + dt.timedelta(days=djd)
180     return utc_time.astimezone(pytz.timezone(tz))
181 
182 
183 def _pandas_to_doy(pd_object):
184     """"""
185     Finds the day of year for a pandas datetime-like object.
186 
187     Useful for delayed evaluation of the dayofyear attribute.
188 
189     Parameters
190     ----------
191     pd_object : DatetimeIndex or Timestamp
192 
193     Returns
194     -------
195     dayofyear
196     """"""
197     return pd_object.dayofyear
198 
199 
200 def _doy_to_datetimeindex(doy, epoch_year=2014):
201     """"""
202     Convert a day of year scalar or array to a pd.DatetimeIndex.
203 
204     Parameters
205     ----------
206     doy : numeric
207         Contains days of the year
208 
209     Returns
210     -------
211     pd.DatetimeIndex
212     """"""
213     doy = np.atleast_1d(doy).astype('float')
214     epoch = pd.Timestamp('{}-12-31'.format(epoch_year - 1))
215     timestamps = [epoch + dt.timedelta(days=adoy) for adoy in doy]
216     return pd.DatetimeIndex(timestamps)
217 
218 
219 def _datetimelike_scalar_to_doy(time):
220     return pd.DatetimeIndex([pd.Timestamp(time)]).dayofyear
221 
222 
223 def _datetimelike_scalar_to_datetimeindex(time):
224     return pd.DatetimeIndex([pd.Timestamp(time)])
225 
226 
227 def _scalar_out(arg):
228     if np.isscalar(arg):
229         output = arg
230     else:  #
231         # works if it's a 1 length array and
232         # will throw a ValueError otherwise
233         output = np.asarray(arg).item()
234 
235     return output
236 
237 
238 def _array_out(arg):
239     if isinstance(arg, pd.Series):
240         output = arg.values
241     else:
242         output = arg
243 
244     return output
245 
246 
247 def _build_kwargs(keys, input_dict):
248     """"""
249     Parameters
250     ----------
251     keys : iterable
252         Typically a list of strings.
253     input_dict : dict-like
254         A dictionary from which to attempt to pull each key.
255 
256     Returns
257     -------
258     kwargs : dict
259         A dictionary with only the keys that were in input_dict
260     """"""
261 
262     kwargs = {}
263     for key in keys:
264         try:
265             kwargs[key] = input_dict[key]
266         except KeyError:
267             pass
268 
269     return kwargs
270 
271 
272 def _build_args(keys, input_dict, dict_name):
273     """"""
274     Parameters
275     ----------
276     keys : iterable
277         Typically a list of strings.
278     input_dict : dict-like
279         A dictionary from which to pull each key.
280     dict_name : str
281         A variable name to include in an error message for missing keys
282 
283     Returns
284     -------
285     kwargs : list
286         A list with values corresponding to keys
287     """"""
288     try:
289         args = [input_dict[key] for key in keys]
290     except KeyError as e:
291         missing_key = e.args[0]
292         msg = (f""Missing required parameter '{missing_key}'. Found ""
293                f""{input_dict} in {dict_name}."")
294         raise KeyError(msg)
295     return args
296 
297 
298 # Created April,2014
299 # Author: Rob Andrews, Calama Consulting
300 # Modified: November, 2020 by C. W. Hansen, to add atol and change exit
301 # criteria
302 def _golden_sect_DataFrame(params, lower, upper, func, atol=1e-8):
303     """"""
304     Vectorized golden section search for finding maximum of a function of a
305     single variable.
306 
307     Parameters
308     ----------
309     params : dict of numeric
310         Parameters to be passed to `func`. Each entry must be of the same
311         length.
312 
313     lower: numeric
314         Lower bound for the optimization. Must be the same length as each
315         entry of params.
316 
317     upper: numeric
318         Upper bound for the optimization. Must be the same length as each
319         entry of params.
320 
321     func: function
322         Function to be optimized. Must be in the form
323         result = f(dict or DataFrame, str), where result is a dict or DataFrame
324         that also contains the function output, and str is the key
325         corresponding to the function's input variable.
326 
327     Returns
328     -------
329     numeric
330         function evaluated at the optimal points
331 
332     numeric
333         optimal points
334 
335     Notes
336     -----
337     This function will find the points where the function is maximized.
338     Returns nan where lower or upper is nan, or where func evaluates to nan.
339 
340     See also
341     --------
342     pvlib.singlediode._pwr_optfcn
343     """"""
344 
345     phim1 = (np.sqrt(5) - 1) / 2
346 
347     df = params
348     df['VH'] = upper
349     df['VL'] = lower
350 
351     converged = False
352     iterations = 0
353 
354     # handle all NaN case gracefully
355     with warnings.catch_warnings():
356         warnings.filterwarnings(action='ignore',
357                                 message='All-NaN slice encountered')
358         iterlimit = 1 + np.nanmax(
359             np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))
360 
361     while not converged and (iterations <= iterlimit):
362 
363         phi = phim1 * (df['VH'] - df['VL'])
364         df['V1'] = df['VL'] + phi
365         df['V2'] = df['VH'] - phi
366 
367         df['f1'] = func(df, 'V1')
368         df['f2'] = func(df, 'V2')
369         df['SW_Flag'] = df['f1'] > df['f2']
370 
371         df['VL'] = df['V2']*df['SW_Flag'] + df['VL']*(~df['SW_Flag'])
372         df['VH'] = df['V1']*~df['SW_Flag'] + df['VH']*(df['SW_Flag'])
373 
374         err = abs(df['V2'] - df['V1'])
375 
376         # works with single value because err is np.float64
377         converged = (err[~np.isnan(err)] < atol).all()
378         # err will be less than atol before iterations hit the limit
379         # but just to be safe
380         iterations += 1
381 
382     if iterations > iterlimit:
383         raise Exception(""Iterations exceeded maximum. Check that func"",
384                         "" is not NaN in (lower, upper)"")  # pragma: no cover
385 
386     try:
387         func_result = func(df, 'V1')
388         x = np.where(np.isnan(func_result), np.nan, df['V1'])
389     except KeyError:
390         func_result = np.full_like(upper, np.nan)
391         x = func_result.copy()
392 
393     return func_result, x
394 
395 
396 def _get_sample_intervals(times, win_length):
397     """""" Calculates time interval and samples per window for Reno-style clear
398     sky detection functions
399     """"""
400     deltas = np.diff(times.values) / np.timedelta64(1, '60s')
401 
402     # determine if we can proceed
403     if times.inferred_freq and len(np.unique(deltas)) == 1:
404         sample_interval = times[1] - times[0]
405         sample_interval = sample_interval.seconds / 60  # in minutes
406         samples_per_window = int(win_length / sample_interval)
407         return sample_interval, samples_per_window
408     else:
409         message = (
410             'algorithm does not yet support unequal time intervals. consider '
411             'resampling your data and checking for gaps from missing '
412             'periods, leap days, etc.'
413         )
414         raise NotImplementedError(message)
415 
416 
417 def _degrees_to_index(degrees, coordinate):
418     """"""Transform input degrees to an output index integer.
419     Specify a degree value and either 'latitude' or 'longitude' to get
420     the appropriate index number for these two index numbers.
421     Parameters
422     ----------
423     degrees : float or int
424         Degrees of either latitude or longitude.
425     coordinate : string
426         Specify whether degrees arg is latitude or longitude. Must be set to
427         either 'latitude' or 'longitude' or an error will be raised.
428     Returns
429     -------
430     index : np.int16
431         The latitude or longitude index number to use when looking up values
432         in the Linke turbidity lookup table.
433     """"""
434     # Assign inputmin, inputmax, and outputmax based on degree type.
435     if coordinate == 'latitude':
436         inputmin = 90
437         inputmax = -90
438         outputmax = 2160
439     elif coordinate == 'longitude':
440         inputmin = -180
441         inputmax = 180
442         outputmax = 4320
443     else:
444         raise IndexError(""coordinate must be 'latitude' or 'longitude'."")
445 
446     inputrange = inputmax - inputmin
447     scale = outputmax/inputrange  # number of indices per degree
448     center = inputmin + 1 / scale / 2  # shift to center of index
449     outputmax -= 1  # shift index to zero indexing
450     index = (degrees - center) * scale
451     err = IndexError('Input, %g, is out of range (%g, %g).' %
452                      (degrees, inputmin, inputmax))
453 
454     # If the index is still out of bounds after rounding, raise an error.
455     # 0.500001 is used in comparisons instead of 0.5 to allow for a small
456     # margin of error which can occur when dealing with floating point numbers.
457     if index > outputmax:
458         if index - outputmax <= 0.500001:
459             index = outputmax
460         else:
461             raise err
462     elif index < 0:
463         if -index <= 0.500001:
464             index = 0
465         else:
466             raise err
467     # If the index wasn't set to outputmax or 0, round it and cast it as an
468     # integer so it can be used in integer-based indexing.
469     else:
470         index = int(np.around(index))
471 
472     return index
473 
[end of pvlib/tools.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pvlib/pvlib-python,c78b50f4337ecbe536a961336ca91a1176efc0e8,"golden-section search fails when upper and lower bounds are equal
**Describe the bug**
I was using pvlib for sometime now and until now I was always passing a big dataframe containing readings of a long period. Because of some changes in our software architecture, I need to pass the weather readings as a single reading (a dataframe with only one row) and I noticed that for readings that GHI-DHI are zero pvlib fails to calculate the output and returns below error while the same code executes correctly with weather information that has non-zero GHI-DHI:
```python
import os
import pathlib
import time
import json
from datetime import datetime
from time import mktime, gmtime

import pandas as pd

from pvlib import pvsystem
from pvlib import location as pvlocation
from pvlib import modelchain
from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS as PARAMS # not used -- to remove
from pvlib.bifacial.pvfactors import pvfactors_timeseries
from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS

class PV:
    def pv_transform_time(self, val):
        # tt = gmtime(val / 1000)
        tt = gmtime(val)
        dd = datetime.fromtimestamp(mktime(tt))
        timestamp = pd.Timestamp(dd)
        return timestamp

    def __init__(self, model: str, inverter: str, latitude: float, longitude: float, **kwargs):
        # super().__init__(**kwargs)

        temperature_model_parameters = TEMPERATURE_MODEL_PARAMETERS[""sapm""][
            ""open_rack_glass_glass""
        ]
        # Load the database of CEC module model parameters
        modules = pvsystem.retrieve_sam(""cecmod"")
        # Load the database of CEC inverter model parameters
        inverters = pvsystem.retrieve_sam(""cecinverter"")


        # A bare bone PV simulator

        # Load the database of CEC module model parameters
        modules = pvsystem.retrieve_sam('cecmod')
        inverters = pvsystem.retrieve_sam('cecinverter')
        module_parameters = modules[model]
        inverter_parameters = inverters[inverter]

        location = pvlocation.Location(latitude=latitude, longitude=longitude)
        system = pvsystem.PVSystem(module_parameters=module_parameters, inverter_parameters=inverter_parameters, temperature_model_parameters=temperature_model_parameters)
        self.modelchain = modelchain.ModelChain(system, location, aoi_model='no_loss', spectral_model=""no_loss"")

    def process(self, data):
        weather = pd.read_json(data)
        # print(f""raw_weather: {weather}"")
        weather.drop('time.1', axis=1, inplace=True)
        weather['time'] = pd.to_datetime(weather['time']).map(datetime.timestamp) # --> this works for the new process_weather code and also the old weather file
        weather[""time""] = weather[""time""].apply(self.pv_transform_time)
        weather.index = weather[""time""]
        # print(f""weather: {weather}"")
        # print(weather.dtypes)
        # print(weather['ghi'][0])
        # print(type(weather['ghi'][0]))

        # simulate
        self.modelchain.run_model(weather)
        # print(self.modelchain.results.ac.to_frame().to_json())
        print(self.modelchain.results.ac)


# good data
good_data = ""{\""time\"":{\""12\"":\""2010-01-01 13:30:00+00:00\""},\""ghi\"":{\""12\"":36},\""dhi\"":{\""12\"":36},\""dni\"":{\""12\"":0},\""Tamb\"":{\""12\"":8.0},\""WindVel\"":{\""12\"":5.0},\""WindDir\"":{\""12\"":270},\""time.1\"":{\""12\"":\""2010-01-01 13:30:00+00:00\""}}""

# data that causes error
data = ""{\""time\"":{\""4\"":\""2010-01-01 05:30:00+00:00\""},\""ghi\"":{\""4\"":0},\""dhi\"":{\""4\"":0},\""dni\"":{\""4\"":0},\""Tamb\"":{\""4\"":8.0},\""WindVel\"":{\""4\"":4.0},\""WindDir\"":{\""4\"":240},\""time.1\"":{\""4\"":\""2010-01-01 05:30:00+00:00\""}}""
p1 = PV(model=""Trina_Solar_TSM_300DEG5C_07_II_"", inverter=""ABB__MICRO_0_25_I_OUTD_US_208__208V_"", latitude=51.204483, longitude=5.265472)
p1.process(good_data)
print(""====="")
p1.process(data)
```
Error:
```log
$ python3 ./tmp-pv.py 
time
2010-01-01 13:30:00    7.825527
dtype: float64
=====
/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py:340: RuntimeWarning: divide by zero encountered in divide
  np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))
Traceback (most recent call last):
  File ""/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py"", line 88, in <module>
    p1.process(data)
  File ""/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py"", line 75, in process
    self.modelchain.run_model(weather)
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py"", line 1770, in run_model
    self._run_from_effective_irrad(weather)
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py"", line 1858, in _run_from_effective_irrad
    self.dc_model()
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py"", line 790, in cec
    return self._singlediode(self.system.calcparams_cec)
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py"", line 772, in _singlediode
    self.results.dc = tuple(itertools.starmap(
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py"", line 931, in singlediode
    return singlediode(photocurrent, saturation_current,
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py"", line 2826, in singlediode
    out = _singlediode._lambertw(
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/singlediode.py"", line 651, in _lambertw
    p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14,
  File ""/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py"", line 364, in _golden_sect_DataFrame
    raise Exception(""Iterations exceeded maximum. Check that func"",
Exception: ('Iterations exceeded maximum. Check that func', ' is not NaN in (lower, upper)')
```

I have to mention that for now the workaround that I am using is to pass the weather data as a dataframe with two rows, the first row is a good weather data that pvlib can process and the second row is the incoming weather reading (I can also post that code if you want).

**Expected behavior**
PVlib should have consistent behavior and regardless of GHI-DHI readings.

**Versions:**
```python
>>> import pvlib
>>> import pandas
>>> pvlib.__version__
'0.9.1'
>>> pandas.__version__
'1.4.3'
``` 
 - python: 3.10.6
- OS: Ubuntu 22.04.1 LTS
","Confirmed. This appears to be an oversight in `pvlib.tools._golden_section_DataFrame` involving error messaging, likely introduced with #1089 .

In this code when processing the content of `data`, photocurrent is 0., hence the shunt resistance is infinite and v_oc is 0. That sets the range for the golden section search to be [0., 0.]. [iterlimit](https://github.com/pvlib/pvlib-python/blob/582b956c63c463e5178fbb7a88fa545fa5b1c257/pvlib/tools.py#L358) is then -infinity, which skips the loop (`iterations <= iterlimit`) but since `iterations > iterlimit` raises the ""Iterations exceeded..."" exception.
",2022-12-07T21:12:08Z,"<patch>
diff --git a/pvlib/tools.py b/pvlib/tools.py
--- a/pvlib/tools.py
+++ b/pvlib/tools.py
@@ -341,6 +341,8 @@ def _golden_sect_DataFrame(params, lower, upper, func, atol=1e-8):
     --------
     pvlib.singlediode._pwr_optfcn
     """"""
+    if np.any(upper - lower < 0.):
+        raise ValueError('upper >= lower is required')
 
     phim1 = (np.sqrt(5) - 1) / 2
 
@@ -349,16 +351,8 @@ def _golden_sect_DataFrame(params, lower, upper, func, atol=1e-8):
     df['VL'] = lower
 
     converged = False
-    iterations = 0
 
-    # handle all NaN case gracefully
-    with warnings.catch_warnings():
-        warnings.filterwarnings(action='ignore',
-                                message='All-NaN slice encountered')
-        iterlimit = 1 + np.nanmax(
-            np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))
-
-    while not converged and (iterations <= iterlimit):
+    while not converged:
 
         phi = phim1 * (df['VH'] - df['VL'])
         df['V1'] = df['VL'] + phi
@@ -373,22 +367,16 @@ def _golden_sect_DataFrame(params, lower, upper, func, atol=1e-8):
 
         err = abs(df['V2'] - df['V1'])
 
-        # works with single value because err is np.float64
-        converged = (err[~np.isnan(err)] < atol).all()
-        # err will be less than atol before iterations hit the limit
-        # but just to be safe
-        iterations += 1
-
-    if iterations > iterlimit:
-        raise Exception(""Iterations exceeded maximum. Check that func"",
-                        "" is not NaN in (lower, upper)"")  # pragma: no cover
+        # handle all NaN case gracefully
+        with warnings.catch_warnings():
+            warnings.filterwarnings(action='ignore',
+                                    message='All-NaN slice encountered')
+            converged = np.all(err[~np.isnan(err)] < atol)
 
-    try:
-        func_result = func(df, 'V1')
-        x = np.where(np.isnan(func_result), np.nan, df['V1'])
-    except KeyError:
-        func_result = np.full_like(upper, np.nan)
-        x = func_result.copy()
+    # best estimate of location of maximum
+    df['max'] = 0.5 * (df['V1'] + df['V2'])
+    func_result = func(df, 'max')
+    x = np.where(np.isnan(func_result), np.nan, df['max'])
 
     return func_result, x
 

</patch>","diff --git a/pvlib/tests/test_tools.py b/pvlib/tests/test_tools.py
--- a/pvlib/tests/test_tools.py
+++ b/pvlib/tests/test_tools.py
@@ -45,6 +45,22 @@ def test__golden_sect_DataFrame_vector():
     v, x = tools._golden_sect_DataFrame(params, lower, upper,
                                         _obj_test_golden_sect)
     assert np.allclose(x, expected, atol=1e-8)
+    # some upper and lower bounds equal
+    params = {'c': np.array([1., 2., 1.]), 'n': np.array([1., 1., 1.])}
+    lower = np.array([0., 0.001, 1.])
+    upper = np.array([1., 1.2, 1.])
+    expected = np.array([0.5, 0.25, 1.0])  # x values for maxima
+    v, x = tools._golden_sect_DataFrame(params, lower, upper,
+                                        _obj_test_golden_sect)
+    assert np.allclose(x, expected, atol=1e-8)
+    # all upper and lower bounds equal, arrays of length 1
+    params = {'c': np.array([1.]), 'n': np.array([1.])}
+    lower = np.array([1.])
+    upper = np.array([1.])
+    expected = np.array([1.])  # x values for maxima
+    v, x = tools._golden_sect_DataFrame(params, lower, upper,
+                                        _obj_test_golden_sect)
+    assert np.allclose(x, expected, atol=1e-8)
 
 
 def test__golden_sect_DataFrame_nans():
",0.8,"[""pvlib/tests/test_tools.py::test__golden_sect_DataFrame_vector""]","[""pvlib/tests/test_tools.py::test_build_kwargs[keys0-input_dict0-expected0]"", ""pvlib/tests/test_tools.py::test_build_kwargs[keys1-input_dict1-expected1]"", ""pvlib/tests/test_tools.py::test_build_kwargs[keys2-input_dict2-expected2]"", ""pvlib/tests/test_tools.py::test_build_kwargs[keys3-input_dict3-expected3]"", ""pvlib/tests/test_tools.py::test__golden_sect_DataFrame[params0-0.0-1.0-0.5-_obj_test_golden_sect]"", ""pvlib/tests/test_tools.py::test__golden_sect_DataFrame[params1-0.0-1.0-0.07230200263994839-_obj_test_golden_sect]"", ""pvlib/tests/test_tools.py::test__golden_sect_DataFrame[params2-0.0-100.0-89.14332727531685-_obj_test_golden_sect]"", ""pvlib/tests/test_tools.py::test__golden_sect_DataFrame_atol"", ""pvlib/tests/test_tools.py::test__golden_sect_DataFrame_nans"", ""pvlib/tests/test_tools.py::test_degrees_to_index_1""]",ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91
pvlib__pvlib-python-1854,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
PVSystem with single Array generates an error
**Is your feature request related to a problem? Please describe.**

When a PVSystem has a single Array, you can't assign just the Array instance when constructing the PVSystem.

```
mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)
array = pvlib.pvsystem.Array(mount=mount)
pv = pvlib.pvsystem.PVSystem(arrays=array)

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-13-f5424e3db16a> in <module>
      3 mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)
      4 array = pvlib.pvsystem.Array(mount=mount)
----> 5 pv = pvlib.pvsystem.PVSystem(arrays=array)

~\anaconda3\lib\site-packages\pvlib\pvsystem.py in __init__(self, arrays, surface_tilt, surface_azimuth, albedo, surface_type, module, module_type, module_parameters, temperature_model_parameters, modules_per_string, strings_per_inverter, inverter, inverter_parameters, racking_model, losses_parameters, name)
    251                 array_losses_parameters,
    252             ),)
--> 253         elif len(arrays) == 0:
    254             raise ValueError(""PVSystem must have at least one Array. ""
    255                              ""If you want to create a PVSystem instance ""

TypeError: object of type 'Array' has no len()

```

Not a bug per se, since the PVSystem docstring requests that `arrays` be iterable. Still, a bit inconvenient to have to do this

```
mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)
array = pvlib.pvsystem.Array(mount=mount)
pv = pvlib.pvsystem.PVSystem(arrays=[array])
```

**Describe the solution you'd like**
Handle `arrays=array` where `array` is an instance of `Array`

**Describe alternatives you've considered**
Status quo - either make the single Array into a list, or use the PVSystem kwargs.


</issue>
<code>
[start of README.md]
1 <img src=""docs/sphinx/source/_images/pvlib_logo_horiz.png"" width=""600"">
2 
3 <table>
4 <tr>
5   <td>Latest Release</td>
6   <td>
7     <a href=""https://pypi.org/project/pvlib/"">
8     <img src=""https://img.shields.io/pypi/v/pvlib.svg"" alt=""latest release"" />
9     </a>
10     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
11     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/version.svg"" />
12     </a>
13     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
14     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg"" />
15     </a>
16 </tr>
17 <tr>
18   <td>License</td>
19   <td>
20     <a href=""https://github.com/pvlib/pvlib-python/blob/main/LICENSE"">
21     <img src=""https://img.shields.io/pypi/l/pvlib.svg"" alt=""license"" />
22     </a>
23 </td>
24 </tr>
25 <tr>
26   <td>Build Status</td>
27   <td>
28     <a href=""http://pvlib-python.readthedocs.org/en/stable/"">
29     <img src=""https://readthedocs.org/projects/pvlib-python/badge/?version=stable"" alt=""documentation build status"" />
30     </a>
31     <a href=""https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml?query=branch%3Amain"">
32       <img src=""https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml/badge.svg?branch=main"" alt=""GitHub Actions Testing Status"" />
33     </a>
34     <a href=""https://codecov.io/gh/pvlib/pvlib-python"">
35     <img src=""https://codecov.io/gh/pvlib/pvlib-python/branch/main/graph/badge.svg"" alt=""codecov coverage"" />
36     </a>
37   </td>
38 </tr>
39 <tr>
40   <td>Benchmarks</td>
41   <td>
42     <a href=""https://pvlib.github.io/pvlib-benchmarks/"">
43     <img src=""https://img.shields.io/badge/benchmarks-asv-lightgrey"" />
44     </a>
45   </td>
46 </tr>
47 <tr>
48   <td>Publications</td>
49   <td>
50     <a href=""https://doi.org/10.5281/zenodo.593284"">
51     <img src=""https://zenodo.org/badge/DOI/10.5281/zenodo.593284.svg"" alt=""zenodo reference"">
52     </a>
53     <a href=""http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1"">
54     <img src=""http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg"" alt=""JOSS reference"" />
55     </a>
56   </td>
57 </tr>
58 <tr>
59   <td>Downloads</td>
60   <td>
61     <a href=""https://pypi.org/project/pvlib/"">
62     <img src=""https://img.shields.io/pypi/dm/pvlib"" alt=""PyPI downloads"" />
63     </a>
64     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
65     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg"" alt=""conda-forge downloads"" />
66     </a>
67   </td>
68 </tr>
69 </table>
70 
71 
72 pvlib python is a community supported tool that provides a set of
73 functions and classes for simulating the performance of photovoltaic
74 energy systems. pvlib python was originally ported from the PVLIB MATLAB
75 toolbox developed at Sandia National Laboratories and it implements many
76 of the models and methods developed at the Labs. More information on
77 Sandia Labs PV performance modeling programs can be found at
78 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,
79 but operate independently of it.
80 
81 
82 Documentation
83 =============
84 
85 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/),
86 including an [FAQ](http://pvlib-python.readthedocs.io/en/stable/user_guide/faq.html) page.
87 
88 Installation
89 ============
90 
91 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.
92 Please see the [Installation page](https://pvlib-python.readthedocs.io/en/stable/user_guide/installation.html) of the documentation for complete instructions.
93 
94 
95 Contributing
96 ============
97 
98 We need your help to make pvlib-python a great tool!
99 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.
100 The long-term success of pvlib-python requires substantial community support.
101 
102 
103 Citing
104 ======
105 
106 If you use pvlib-python in a published work, please cite:
107 
108   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.
109   ""pvlib python: a python package for modeling solar energy systems.""
110   Journal of Open Source Software, 3(29), 884, (2018).
111   https://doi.org/10.21105/joss.00884
112 
113 Please also cite the DOI corresponding to the specific version of
114 pvlib-python that you used. pvlib-python DOIs are listed at
115 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)
116 
117 If you use pvlib-python in a commercial or publicly-available application, please
118 consider displaying one of the ""powered by pvlib"" logos:
119 
120 <img src=""docs/sphinx/source/_images/pvlib_powered_logo_vert.png"" width=""300""><img src=""docs/sphinx/source/_images/pvlib_powered_logo_horiz.png"" width=""300"">
121 
122 Getting support
123 ===============
124 
125 pvlib usage questions can be asked on
126 [Stack Overflow](http://stackoverflow.com) and tagged with
127 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.
128 
129 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)
130 is used for discussing various topics of interest to the pvlib-python
131 community. We also make new version announcements on the google group.
132 
133 If you suspect that you may have discovered a bug or if you'd like to
134 change something about pvlib, then please make an issue on our
135 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).
136 
137 
138 
139 License
140 =======
141 
142 BSD 3-clause.
143 
144 
145 NumFOCUS
146 ========
147 
148 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)
149 
150 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)
151 
[end of README.md]
[start of pvlib/pvsystem.py]
1 """"""
2 The ``pvsystem`` module contains functions for modeling the output and
3 performance of PV modules and inverters.
4 """"""
5 
6 from collections import OrderedDict
7 import functools
8 import io
9 import itertools
10 import os
11 import inspect
12 from urllib.request import urlopen
13 import numpy as np
14 from scipy import constants
15 import pandas as pd
16 from dataclasses import dataclass
17 from abc import ABC, abstractmethod
18 from typing import Optional
19 
20 from pvlib._deprecation import deprecated, warn_deprecated
21 
22 from pvlib import (atmosphere, iam, inverter, irradiance,
23                    singlediode as _singlediode, spectrum, temperature)
24 from pvlib.tools import _build_kwargs, _build_args
25 import pvlib.tools as tools
26 
27 
28 # a dict of required parameter names for each DC power model
29 _DC_MODEL_PARAMS = {
30     'sapm': {
31         'A0', 'A1', 'A2', 'A3', 'A4', 'B0', 'B1', 'B2', 'B3',
32         'B4', 'B5', 'C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6',
33         'C7', 'Isco', 'Impo', 'Voco', 'Vmpo', 'Aisc', 'Aimp', 'Bvoco',
34         'Mbvoc', 'Bvmpo', 'Mbvmp', 'N', 'Cells_in_Series',
35         'IXO', 'IXXO', 'FD'},
36     'desoto': {
37         'alpha_sc', 'a_ref', 'I_L_ref', 'I_o_ref',
38         'R_sh_ref', 'R_s'},
39     'cec': {
40         'alpha_sc', 'a_ref', 'I_L_ref', 'I_o_ref',
41         'R_sh_ref', 'R_s', 'Adjust'},
42     'pvsyst': {
43         'gamma_ref', 'mu_gamma', 'I_L_ref', 'I_o_ref',
44         'R_sh_ref', 'R_sh_0', 'R_s', 'alpha_sc', 'EgRef',
45         'cells_in_series'},
46     'singlediode': {
47         'alpha_sc', 'a_ref', 'I_L_ref', 'I_o_ref',
48         'R_sh_ref', 'R_s'},
49     'pvwatts': {'pdc0', 'gamma_pdc'}
50 }
51 
52 
53 def _unwrap_single_value(func):
54     """"""Decorator for functions that return iterables.
55 
56     If the length of the iterable returned by `func` is 1, then
57     the single member of the iterable is returned. If the length is
58     greater than 1, then entire iterable is returned.
59 
60     Adds 'unwrap' as a keyword argument that can be set to False
61     to force the return value to be a tuple, regardless of its length.
62     """"""
63     @functools.wraps(func)
64     def f(*args, **kwargs):
65         unwrap = kwargs.pop('unwrap', True)
66         x = func(*args, **kwargs)
67         if unwrap and len(x) == 1:
68             return x[0]
69         return x
70     return f
71 
72 
73 # not sure if this belongs in the pvsystem module.
74 # maybe something more like core.py? It may eventually grow to
75 # import a lot more functionality from other modules.
76 class PVSystem:
77     """"""
78     The PVSystem class defines a standard set of PV system attributes
79     and modeling functions. This class describes the collection and
80     interactions of PV system components rather than an installed system
81     on the ground. It is typically used in combination with
82     :py:class:`~pvlib.location.Location` and
83     :py:class:`~pvlib.modelchain.ModelChain`
84     objects.
85 
86     The class supports basic system topologies consisting of:
87 
88         * `N` total modules arranged in series
89           (`modules_per_string=N`, `strings_per_inverter=1`).
90         * `M` total modules arranged in parallel
91           (`modules_per_string=1`, `strings_per_inverter=M`).
92         * `NxM` total modules arranged in `M` strings of `N` modules each
93           (`modules_per_string=N`, `strings_per_inverter=M`).
94 
95     The class is complementary to the module-level functions.
96 
97     The attributes should generally be things that don't change about
98     the system, such the type of module and the inverter. The instance
99     methods accept arguments for things that do change, such as
100     irradiance and temperature.
101 
102     Parameters
103     ----------
104     arrays : iterable of Array, optional
105         List of arrays that are part of the system. If not specified
106         a single array is created from the other parameters (e.g.
107         `surface_tilt`, `surface_azimuth`). Must contain at least one Array,
108         if length of arrays is 0 a ValueError is raised. If `arrays` is
109         specified the following PVSystem parameters are ignored:
110 
111         - `surface_tilt`
112         - `surface_azimuth`
113         - `albedo`
114         - `surface_type`
115         - `module`
116         - `module_type`
117         - `module_parameters`
118         - `temperature_model_parameters`
119         - `modules_per_string`
120         - `strings_per_inverter`
121 
122     surface_tilt: float or array-like, default 0
123         Surface tilt angles in decimal degrees.
124         The tilt angle is defined as degrees from horizontal
125         (e.g. surface facing up = 0, surface facing horizon = 90)
126 
127     surface_azimuth: float or array-like, default 180
128         Azimuth angle of the module surface.
129         North=0, East=90, South=180, West=270.
130 
131     albedo : None or float, default None
132         Ground surface albedo. If ``None``, then ``surface_type`` is used
133         to look up a value in ``irradiance.SURFACE_ALBEDOS``.
134         If ``surface_type`` is also None then a ground surface albedo
135         of 0.25 is used.
136 
137     surface_type : None or string, default None
138         The ground surface type. See ``irradiance.SURFACE_ALBEDOS`` for
139         valid values.
140 
141     module : None or string, default None
142         The model name of the modules.
143         May be used to look up the module_parameters dictionary
144         via some other method.
145 
146     module_type : None or string, default 'glass_polymer'
147          Describes the module's construction. Valid strings are 'glass_polymer'
148          and 'glass_glass'. Used for cell and module temperature calculations.
149 
150     module_parameters : None, dict or Series, default None
151         Module parameters as defined by the SAPM, CEC, or other.
152 
153     temperature_model_parameters : None, dict or Series, default None.
154         Temperature model parameters as required by one of the models in
155         pvlib.temperature (excluding poa_global, temp_air and wind_speed).
156 
157     modules_per_string: int or float, default 1
158         See system topology discussion above.
159 
160     strings_per_inverter: int or float, default 1
161         See system topology discussion above.
162 
163     inverter : None or string, default None
164         The model name of the inverters.
165         May be used to look up the inverter_parameters dictionary
166         via some other method.
167 
168     inverter_parameters : None, dict or Series, default None
169         Inverter parameters as defined by the SAPM, CEC, or other.
170 
171     racking_model : None or string, default 'open_rack'
172         Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.
173         Used to identify a parameter set for the SAPM cell temperature model.
174 
175     losses_parameters : None, dict or Series, default None
176         Losses parameters as defined by PVWatts or other.
177 
178     name : None or string, default None
179 
180     **kwargs
181         Arbitrary keyword arguments.
182         Included for compatibility, but not used.
183 
184     Raises
185     ------
186     ValueError
187         If `arrays` is not None and has length 0.
188 
189     See also
190     --------
191     pvlib.location.Location
192     """"""
193 
194     def __init__(self,
195                  arrays=None,
196                  surface_tilt=0, surface_azimuth=180,
197                  albedo=None, surface_type=None,
198                  module=None, module_type=None,
199                  module_parameters=None,
200                  temperature_model_parameters=None,
201                  modules_per_string=1, strings_per_inverter=1,
202                  inverter=None, inverter_parameters=None,
203                  racking_model=None, losses_parameters=None, name=None):
204 
205         if arrays is None:
206             if losses_parameters is None:
207                 array_losses_parameters = {}
208             else:
209                 array_losses_parameters = _build_kwargs(['dc_ohmic_percent'],
210                                                         losses_parameters)
211             self.arrays = (Array(
212                 FixedMount(surface_tilt, surface_azimuth, racking_model),
213                 albedo,
214                 surface_type,
215                 module,
216                 module_type,
217                 module_parameters,
218                 temperature_model_parameters,
219                 modules_per_string,
220                 strings_per_inverter,
221                 array_losses_parameters,
222             ),)
223         elif len(arrays) == 0:
224             raise ValueError(""PVSystem must have at least one Array. ""
225                              ""If you want to create a PVSystem instance ""
226                              ""with a single Array pass `arrays=None` and pass ""
227                              ""values directly to PVSystem attributes, e.g., ""
228                              ""`surface_tilt=30`"")
229         else:
230             self.arrays = tuple(arrays)
231 
232         self.inverter = inverter
233         if inverter_parameters is None:
234             self.inverter_parameters = {}
235         else:
236             self.inverter_parameters = inverter_parameters
237 
238         if losses_parameters is None:
239             self.losses_parameters = {}
240         else:
241             self.losses_parameters = losses_parameters
242 
243         self.name = name
244 
245     def __repr__(self):
246         repr = f'PVSystem:\n  name: {self.name}\n  '
247         for array in self.arrays:
248             repr += '\n  '.join(array.__repr__().split('\n'))
249             repr += '\n  '
250         repr += f'inverter: {self.inverter}'
251         return repr
252 
253     def _validate_per_array(self, values, system_wide=False):
254         """"""Check that `values` is a tuple of the same length as
255         `self.arrays`.
256 
257         If `values` is not a tuple it is packed in to a length-1 tuple before
258         the check. If the lengths are not the same a ValueError is raised,
259         otherwise the tuple `values` is returned.
260 
261         When `system_wide` is True and `values` is not a tuple, `values`
262         is replicated to a tuple of the same length as `self.arrays` and that
263         tuple is returned.
264         """"""
265         if system_wide and not isinstance(values, tuple):
266             return (values,) * self.num_arrays
267         if not isinstance(values, tuple):
268             values = (values,)
269         if len(values) != len(self.arrays):
270             raise ValueError(""Length mismatch for per-array parameter"")
271         return values
272 
273     @_unwrap_single_value
274     def _infer_cell_type(self):
275         """"""
276         Examines module_parameters and maps the Technology key for the CEC
277         database and the Material key for the Sandia database to a common
278         list of strings for cell type.
279 
280         Returns
281         -------
282         cell_type: str
283         """"""
284         return tuple(array._infer_cell_type() for array in self.arrays)
285 
286     @_unwrap_single_value
287     def get_aoi(self, solar_zenith, solar_azimuth):
288         """"""Get the angle of incidence on the Array(s) in the system.
289 
290         Parameters
291         ----------
292         solar_zenith : float or Series.
293             Solar zenith angle.
294         solar_azimuth : float or Series.
295             Solar azimuth angle.
296 
297         Returns
298         -------
299         aoi : Series or tuple of Series
300             The angle of incidence
301         """"""
302 
303         return tuple(array.get_aoi(solar_zenith, solar_azimuth)
304                      for array in self.arrays)
305 
306     @_unwrap_single_value
307     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,
308                        dni_extra=None, airmass=None, albedo=None,
309                        model='haydavies', **kwargs):
310         """"""
311         Uses the :py:func:`irradiance.get_total_irradiance` function to
312         calculate the plane of array irradiance components on the tilted
313         surfaces defined by each array's ``surface_tilt`` and
314         ``surface_azimuth``.
315 
316         Parameters
317         ----------
318         solar_zenith : float or Series
319             Solar zenith angle.
320         solar_azimuth : float or Series
321             Solar azimuth angle.
322         dni : float or Series or tuple of float or Series
323             Direct Normal Irradiance. [W/m2]
324         ghi : float or Series or tuple of float or Series
325             Global horizontal irradiance. [W/m2]
326         dhi : float or Series or tuple of float or Series
327             Diffuse horizontal irradiance. [W/m2]
328         dni_extra : None, float, Series or tuple of float or Series,\
329             default None
330             Extraterrestrial direct normal irradiance. [W/m2]
331         airmass : None, float or Series, default None
332             Airmass. [unitless]
333         albedo : None, float or Series, default None
334             Ground surface albedo. [unitless]
335         model : String, default 'haydavies'
336             Irradiance model.
337 
338         kwargs
339             Extra parameters passed to :func:`irradiance.get_total_irradiance`.
340 
341         Notes
342         -----
343         Each of `dni`, `ghi`, and `dni` parameters may be passed as a tuple
344         to provide different irradiance for each array in the system. If not
345         passed as a tuple then the same value is used for input to each Array.
346         If passed as a tuple the length must be the same as the number of
347         Arrays.
348 
349         Returns
350         -------
351         poa_irradiance : DataFrame or tuple of DataFrame
352             Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',
353             'poa_sky_diffuse', 'poa_ground_diffuse'``.
354 
355         See also
356         --------
357         pvlib.irradiance.get_total_irradiance
358         """"""
359         dni = self._validate_per_array(dni, system_wide=True)
360         ghi = self._validate_per_array(ghi, system_wide=True)
361         dhi = self._validate_per_array(dhi, system_wide=True)
362 
363         albedo = self._validate_per_array(albedo, system_wide=True)
364 
365         return tuple(
366             array.get_irradiance(solar_zenith, solar_azimuth,
367                                  dni, ghi, dhi,
368                                  dni_extra=dni_extra, airmass=airmass,
369                                  albedo=albedo, model=model, **kwargs)
370             for array, dni, ghi, dhi, albedo in zip(
371                 self.arrays, dni, ghi, dhi, albedo
372             )
373         )
374 
375     @_unwrap_single_value
376     def get_iam(self, aoi, iam_model='physical'):
377         """"""
378         Determine the incidence angle modifier using the method specified by
379         ``iam_model``.
380 
381         Parameters for the selected IAM model are expected to be in
382         ``PVSystem.module_parameters``. Default parameters are available for
383         the 'physical', 'ashrae' and 'martin_ruiz' models.
384 
385         Parameters
386         ----------
387         aoi : numeric or tuple of numeric
388             The angle of incidence in degrees.
389 
390         aoi_model : string, default 'physical'
391             The IAM model to be used. Valid strings are 'physical', 'ashrae',
392             'martin_ruiz', 'sapm' and 'interp'.
393         Returns
394         -------
395         iam : numeric or tuple of numeric
396             The AOI modifier.
397 
398         Raises
399         ------
400         ValueError
401             if `iam_model` is not a valid model name.
402         """"""
403         aoi = self._validate_per_array(aoi)
404         return tuple(array.get_iam(aoi, iam_model)
405                      for array, aoi in zip(self.arrays, aoi))
406 
407     @_unwrap_single_value
408     def get_cell_temperature(self, poa_global, temp_air, wind_speed, model,
409                              effective_irradiance=None):
410         """"""
411         Determine cell temperature using the method specified by ``model``.
412 
413         Parameters
414         ----------
415         poa_global : numeric or tuple of numeric
416             Total incident irradiance in W/m^2.
417 
418         temp_air : numeric or tuple of numeric
419             Ambient dry bulb temperature in degrees C.
420 
421         wind_speed : numeric or tuple of numeric
422             Wind speed in m/s.
423 
424         model : str
425             Supported models include ``'sapm'``, ``'pvsyst'``,
426             ``'faiman'``, ``'fuentes'``, and ``'noct_sam'``
427 
428         effective_irradiance : numeric or tuple of numeric, optional
429             The irradiance that is converted to photocurrent in W/m^2.
430             Only used for some models.
431 
432         Returns
433         -------
434         numeric or tuple of numeric
435             Values in degrees C.
436 
437         See Also
438         --------
439         Array.get_cell_temperature
440 
441         Notes
442         -----
443         The `temp_air` and `wind_speed` parameters may be passed as tuples
444         to provide different values for each Array in the system. If passed as
445         a tuple the length must be the same as the number of Arrays. If not
446         passed as a tuple then the same value is used for each Array.
447         """"""
448         poa_global = self._validate_per_array(poa_global)
449         temp_air = self._validate_per_array(temp_air, system_wide=True)
450         wind_speed = self._validate_per_array(wind_speed, system_wide=True)
451         # Not used for all models, but Array.get_cell_temperature handles it
452         effective_irradiance = self._validate_per_array(effective_irradiance,
453                                                         system_wide=True)
454 
455         return tuple(
456             array.get_cell_temperature(poa_global, temp_air, wind_speed,
457                                        model, effective_irradiance)
458             for array, poa_global, temp_air, wind_speed, effective_irradiance
459             in zip(
460                 self.arrays, poa_global, temp_air, wind_speed,
461                 effective_irradiance
462             )
463         )
464 
465     @_unwrap_single_value
466     def calcparams_desoto(self, effective_irradiance, temp_cell):
467         """"""
468         Use the :py:func:`calcparams_desoto` function, the input
469         parameters and ``self.module_parameters`` to calculate the
470         module currents and resistances.
471 
472         Parameters
473         ----------
474         effective_irradiance : numeric or tuple of numeric
475             The irradiance (W/m2) that is converted to photocurrent.
476 
477         temp_cell : float or Series or tuple of float or Series
478             The average cell temperature of cells within a module in C.
479 
480         Returns
481         -------
482         See pvsystem.calcparams_desoto for details
483         """"""
484         effective_irradiance = self._validate_per_array(effective_irradiance)
485         temp_cell = self._validate_per_array(temp_cell)
486 
487         build_kwargs = functools.partial(
488             _build_kwargs,
489             ['a_ref', 'I_L_ref', 'I_o_ref', 'R_sh_ref',
490              'R_s', 'alpha_sc', 'EgRef', 'dEgdT',
491              'irrad_ref', 'temp_ref']
492         )
493 
494         return tuple(
495             calcparams_desoto(
496                 effective_irradiance, temp_cell,
497                 **build_kwargs(array.module_parameters)
498             )
499             for array, effective_irradiance, temp_cell
500             in zip(self.arrays, effective_irradiance, temp_cell)
501         )
502 
503     @_unwrap_single_value
504     def calcparams_cec(self, effective_irradiance, temp_cell):
505         """"""
506         Use the :py:func:`calcparams_cec` function, the input
507         parameters and ``self.module_parameters`` to calculate the
508         module currents and resistances.
509 
510         Parameters
511         ----------
512         effective_irradiance : numeric or tuple of numeric
513             The irradiance (W/m2) that is converted to photocurrent.
514 
515         temp_cell : float or Series or tuple of float or Series
516             The average cell temperature of cells within a module in C.
517 
518         Returns
519         -------
520         See pvsystem.calcparams_cec for details
521         """"""
522         effective_irradiance = self._validate_per_array(effective_irradiance)
523         temp_cell = self._validate_per_array(temp_cell)
524 
525         build_kwargs = functools.partial(
526             _build_kwargs,
527             ['a_ref', 'I_L_ref', 'I_o_ref', 'R_sh_ref',
528              'R_s', 'alpha_sc', 'Adjust', 'EgRef', 'dEgdT',
529              'irrad_ref', 'temp_ref']
530         )
531 
532         return tuple(
533             calcparams_cec(
534                 effective_irradiance, temp_cell,
535                 **build_kwargs(array.module_parameters)
536             )
537             for array, effective_irradiance, temp_cell
538             in zip(self.arrays, effective_irradiance, temp_cell)
539         )
540 
541     @_unwrap_single_value
542     def calcparams_pvsyst(self, effective_irradiance, temp_cell):
543         """"""
544         Use the :py:func:`calcparams_pvsyst` function, the input
545         parameters and ``self.module_parameters`` to calculate the
546         module currents and resistances.
547 
548         Parameters
549         ----------
550         effective_irradiance : numeric or tuple of numeric
551             The irradiance (W/m2) that is converted to photocurrent.
552 
553         temp_cell : float or Series or tuple of float or Series
554             The average cell temperature of cells within a module in C.
555 
556         Returns
557         -------
558         See pvsystem.calcparams_pvsyst for details
559         """"""
560         effective_irradiance = self._validate_per_array(effective_irradiance)
561         temp_cell = self._validate_per_array(temp_cell)
562 
563         build_kwargs = functools.partial(
564             _build_kwargs,
565             ['gamma_ref', 'mu_gamma', 'I_L_ref', 'I_o_ref',
566              'R_sh_ref', 'R_sh_0', 'R_sh_exp',
567              'R_s', 'alpha_sc', 'EgRef',
568              'irrad_ref', 'temp_ref',
569              'cells_in_series']
570         )
571 
572         return tuple(
573             calcparams_pvsyst(
574                 effective_irradiance, temp_cell,
575                 **build_kwargs(array.module_parameters)
576             )
577             for array, effective_irradiance, temp_cell
578             in zip(self.arrays, effective_irradiance, temp_cell)
579         )
580 
581     @_unwrap_single_value
582     def sapm(self, effective_irradiance, temp_cell):
583         """"""
584         Use the :py:func:`sapm` function, the input parameters,
585         and ``self.module_parameters`` to calculate
586         Voc, Isc, Ix, Ixx, Vmp, and Imp.
587 
588         Parameters
589         ----------
590         effective_irradiance : numeric or tuple of numeric
591             The irradiance (W/m2) that is converted to photocurrent.
592 
593         temp_cell : float or Series or tuple of float or Series
594             The average cell temperature of cells within a module in C.
595 
596         Returns
597         -------
598         See pvsystem.sapm for details
599         """"""
600         effective_irradiance = self._validate_per_array(effective_irradiance)
601         temp_cell = self._validate_per_array(temp_cell)
602 
603         return tuple(
604             sapm(effective_irradiance, temp_cell, array.module_parameters)
605             for array, effective_irradiance, temp_cell
606             in zip(self.arrays, effective_irradiance, temp_cell)
607         )
608 
609     @_unwrap_single_value
610     def sapm_spectral_loss(self, airmass_absolute):
611         """"""
612         Use the :py:func:`pvlib.spectrum.spectral_factor_sapm` function,
613         the input parameters, and ``self.module_parameters`` to calculate F1.
614 
615         Parameters
616         ----------
617         airmass_absolute : numeric
618             Absolute airmass.
619 
620         Returns
621         -------
622         F1 : numeric or tuple of numeric
623             The SAPM spectral loss coefficient.
624         """"""
625         return tuple(
626             spectrum.spectral_factor_sapm(airmass_absolute,
627                                           array.module_parameters)
628             for array in self.arrays
629         )
630 
631     @_unwrap_single_value
632     def sapm_effective_irradiance(self, poa_direct, poa_diffuse,
633                                   airmass_absolute, aoi,
634                                   reference_irradiance=1000):
635         """"""
636         Use the :py:func:`sapm_effective_irradiance` function, the input
637         parameters, and ``self.module_parameters`` to calculate
638         effective irradiance.
639 
640         Parameters
641         ----------
642         poa_direct : numeric or tuple of numeric
643             The direct irradiance incident upon the module.  [W/m2]
644 
645         poa_diffuse : numeric or tuple of numeric
646             The diffuse irradiance incident on module.  [W/m2]
647 
648         airmass_absolute : numeric
649             Absolute airmass. [unitless]
650 
651         aoi : numeric or tuple of numeric
652             Angle of incidence. [degrees]
653 
654         Returns
655         -------
656         effective_irradiance : numeric or tuple of numeric
657             The SAPM effective irradiance. [W/m2]
658         """"""
659         poa_direct = self._validate_per_array(poa_direct)
660         poa_diffuse = self._validate_per_array(poa_diffuse)
661         aoi = self._validate_per_array(aoi)
662         return tuple(
663             sapm_effective_irradiance(
664                 poa_direct, poa_diffuse, airmass_absolute, aoi,
665                 array.module_parameters)
666             for array, poa_direct, poa_diffuse, aoi
667             in zip(self.arrays, poa_direct, poa_diffuse, aoi)
668         )
669 
670     @_unwrap_single_value
671     def first_solar_spectral_loss(self, pw, airmass_absolute):
672         """"""
673         Use :py:func:`pvlib.spectrum.spectral_factor_firstsolar` to
674         calculate the spectral loss modifier. The model coefficients are
675         specific to the module's cell type, and are determined by searching
676         for one of the following keys in self.module_parameters (in order):
677 
678         - 'first_solar_spectral_coefficients' (user-supplied coefficients)
679         - 'Technology' - a string describing the cell type, can be read from
680           the CEC module parameter database
681         - 'Material' - a string describing the cell type, can be read from
682           the Sandia module database.
683 
684         Parameters
685         ----------
686         pw : array-like
687             atmospheric precipitable water (cm).
688 
689         airmass_absolute : array-like
690             absolute (pressure corrected) airmass.
691 
692         Returns
693         -------
694         modifier: array-like or tuple of array-like
695             spectral mismatch factor (unitless) which can be multiplied
696             with broadband irradiance reaching a module's cells to estimate
697             effective irradiance, i.e., the irradiance that is converted to
698             electrical current.
699         """"""
700         pw = self._validate_per_array(pw, system_wide=True)
701 
702         def _spectral_correction(array, pw):
703             if 'first_solar_spectral_coefficients' in \
704                     array.module_parameters.keys():
705                 coefficients = \
706                     array.module_parameters[
707                         'first_solar_spectral_coefficients'
708                     ]
709                 module_type = None
710             else:
711                 module_type = array._infer_cell_type()
712                 coefficients = None
713 
714             return spectrum.spectral_factor_firstsolar(
715                 pw, airmass_absolute, module_type, coefficients
716             )
717         return tuple(
718             itertools.starmap(_spectral_correction, zip(self.arrays, pw))
719         )
720 
721     def singlediode(self, photocurrent, saturation_current,
722                     resistance_series, resistance_shunt, nNsVth,
723                     ivcurve_pnts=None):
724         """"""Wrapper around the :py:func:`pvlib.pvsystem.singlediode` function.
725 
726         See :py:func:`pvsystem.singlediode` for details
727         """"""
728         return singlediode(photocurrent, saturation_current,
729                            resistance_series, resistance_shunt, nNsVth,
730                            ivcurve_pnts=ivcurve_pnts)
731 
732     def i_from_v(self, voltage, photocurrent, saturation_current,
733                  resistance_series, resistance_shunt, nNsVth):
734         """"""Wrapper around the :py:func:`pvlib.pvsystem.i_from_v` function.
735 
736         See :py:func:`pvlib.pvsystem.i_from_v` for details.
737 
738         .. versionchanged:: 0.10.0
739            The function's arguments have been reordered.
740         """"""
741         return i_from_v(voltage, photocurrent, saturation_current,
742                         resistance_series, resistance_shunt, nNsVth)
743 
744     def get_ac(self, model, p_dc, v_dc=None):
745         r""""""Calculates AC power from p_dc using the inverter model indicated
746         by model and self.inverter_parameters.
747 
748         Parameters
749         ----------
750         model : str
751             Must be one of 'sandia', 'adr', or 'pvwatts'.
752         p_dc : numeric, or tuple, list or array of numeric
753             DC power on each MPPT input of the inverter. Use tuple, list or
754             array for inverters with multiple MPPT inputs. If type is array,
755             p_dc must be 2d with axis 0 being the MPPT inputs. [W]
756         v_dc : numeric, or tuple, list or array of numeric
757             DC voltage on each MPPT input of the inverter. Required when
758             model='sandia' or model='adr'. Use tuple, list or
759             array for inverters with multiple MPPT inputs. If type is array,
760             v_dc must be 2d with axis 0 being the MPPT inputs. [V]
761 
762         Returns
763         -------
764         power_ac : numeric
765             AC power output for the inverter. [W]
766 
767         Raises
768         ------
769         ValueError
770             If model is not one of 'sandia', 'adr' or 'pvwatts'.
771         ValueError
772             If model='adr' and the PVSystem has more than one array.
773 
774         See also
775         --------
776         pvlib.inverter.sandia
777         pvlib.inverter.sandia_multi
778         pvlib.inverter.adr
779         pvlib.inverter.pvwatts
780         pvlib.inverter.pvwatts_multi
781         """"""
782         model = model.lower()
783         multiple_arrays = self.num_arrays > 1
784         if model == 'sandia':
785             p_dc = self._validate_per_array(p_dc)
786             v_dc = self._validate_per_array(v_dc)
787             if multiple_arrays:
788                 return inverter.sandia_multi(
789                     v_dc, p_dc, self.inverter_parameters)
790             return inverter.sandia(v_dc[0], p_dc[0], self.inverter_parameters)
791         elif model == 'pvwatts':
792             kwargs = _build_kwargs(['eta_inv_nom', 'eta_inv_ref'],
793                                    self.inverter_parameters)
794             p_dc = self._validate_per_array(p_dc)
795             if multiple_arrays:
796                 return inverter.pvwatts_multi(
797                     p_dc, self.inverter_parameters['pdc0'], **kwargs)
798             return inverter.pvwatts(
799                 p_dc[0], self.inverter_parameters['pdc0'], **kwargs)
800         elif model == 'adr':
801             if multiple_arrays:
802                 raise ValueError(
803                     'The adr inverter function cannot be used for an inverter',
804                     ' with multiple MPPT inputs')
805             # While this is only used for single-array systems, calling
806             # _validate_per_arry lets us pass in singleton tuples.
807             p_dc = self._validate_per_array(p_dc)
808             v_dc = self._validate_per_array(v_dc)
809             return inverter.adr(v_dc[0], p_dc[0], self.inverter_parameters)
810         else:
811             raise ValueError(
812                 model + ' is not a valid AC power model.',
813                 ' model must be one of ""sandia"", ""adr"" or ""pvwatts""')
814 
815     @_unwrap_single_value
816     def scale_voltage_current_power(self, data):
817         """"""
818         Scales the voltage, current, and power of the `data` DataFrame
819         by `self.modules_per_string` and `self.strings_per_inverter`.
820 
821         Parameters
822         ----------
823         data: DataFrame or tuple of DataFrame
824             May contain columns `'v_mp', 'v_oc', 'i_mp' ,'i_x', 'i_xx',
825             'i_sc', 'p_mp'`.
826 
827         Returns
828         -------
829         scaled_data: DataFrame or tuple of DataFrame
830             A scaled copy of the input data.
831         """"""
832         data = self._validate_per_array(data)
833         return tuple(
834             scale_voltage_current_power(data,
835                                         voltage=array.modules_per_string,
836                                         current=array.strings)
837             for array, data in zip(self.arrays, data)
838         )
839 
840     @_unwrap_single_value
841     def pvwatts_dc(self, g_poa_effective, temp_cell):
842         """"""
843         Calcuates DC power according to the PVWatts model using
844         :py:func:`pvlib.pvsystem.pvwatts_dc`, `self.module_parameters['pdc0']`,
845         and `self.module_parameters['gamma_pdc']`.
846 
847         See :py:func:`pvlib.pvsystem.pvwatts_dc` for details.
848         """"""
849         g_poa_effective = self._validate_per_array(g_poa_effective)
850         temp_cell = self._validate_per_array(temp_cell)
851         return tuple(
852             pvwatts_dc(g_poa_effective, temp_cell,
853                        array.module_parameters['pdc0'],
854                        array.module_parameters['gamma_pdc'],
855                        **_build_kwargs(['temp_ref'], array.module_parameters))
856             for array, g_poa_effective, temp_cell
857             in zip(self.arrays, g_poa_effective, temp_cell)
858         )
859 
860     def pvwatts_losses(self):
861         """"""
862         Calculates DC power losses according the PVwatts model using
863         :py:func:`pvlib.pvsystem.pvwatts_losses` and
864         ``self.losses_parameters``.
865 
866         See :py:func:`pvlib.pvsystem.pvwatts_losses` for details.
867         """"""
868         kwargs = _build_kwargs(['soiling', 'shading', 'snow', 'mismatch',
869                                 'wiring', 'connections', 'lid',
870                                 'nameplate_rating', 'age', 'availability'],
871                                self.losses_parameters)
872         return pvwatts_losses(**kwargs)
873 
874     @_unwrap_single_value
875     def dc_ohms_from_percent(self):
876         """"""
877         Calculates the equivalent resistance of the wires for each array using
878         :py:func:`pvlib.pvsystem.dc_ohms_from_percent`
879 
880         See :py:func:`pvlib.pvsystem.dc_ohms_from_percent` for details.
881         """"""
882 
883         return tuple(array.dc_ohms_from_percent() for array in self.arrays)
884 
885     @property
886     def num_arrays(self):
887         """"""The number of Arrays in the system.""""""
888         return len(self.arrays)
889 
890 
891 class Array:
892     """"""
893     An Array is a set of of modules at the same orientation.
894 
895     Specifically, an array is defined by its mount, the
896     module parameters, the number of parallel strings of modules
897     and the number of modules on each string.
898 
899     Parameters
900     ----------
901     mount: FixedMount, SingleAxisTrackerMount, or other
902         Mounting for the array, either on fixed-tilt racking or horizontal
903         single axis tracker. Mounting is used to determine module orientation.
904         If not provided, a FixedMount with zero tilt is used.
905 
906     albedo : None or float, default None
907         Ground surface albedo. If ``None``, then ``surface_type`` is used
908         to look up a value in ``irradiance.SURFACE_ALBEDOS``.
909         If ``surface_type`` is also None then a ground surface albedo
910         of 0.25 is used.
911 
912     surface_type : None or string, default None
913         The ground surface type. See ``irradiance.SURFACE_ALBEDOS`` for valid
914         values.
915 
916     module : None or string, default None
917         The model name of the modules.
918         May be used to look up the module_parameters dictionary
919         via some other method.
920 
921     module_type : None or string, default None
922          Describes the module's construction. Valid strings are 'glass_polymer'
923          and 'glass_glass'. Used for cell and module temperature calculations.
924 
925     module_parameters : None, dict or Series, default None
926         Parameters for the module model, e.g., SAPM, CEC, or other.
927 
928     temperature_model_parameters : None, dict or Series, default None.
929         Parameters for the module temperature model, e.g., SAPM, Pvsyst, or
930         other.
931 
932     modules_per_string: int, default 1
933         Number of modules per string in the array.
934 
935     strings: int, default 1
936         Number of parallel strings in the array.
937 
938     array_losses_parameters: None, dict or Series, default None.
939         Supported keys are 'dc_ohmic_percent'.
940 
941     name: None or str, default None
942         Name of Array instance.
943     """"""
944 
945     def __init__(self, mount,
946                  albedo=None, surface_type=None,
947                  module=None, module_type=None,
948                  module_parameters=None,
949                  temperature_model_parameters=None,
950                  modules_per_string=1, strings=1,
951                  array_losses_parameters=None,
952                  name=None):
953         self.mount = mount
954 
955         self.surface_type = surface_type
956         if albedo is None:
957             self.albedo = irradiance.SURFACE_ALBEDOS.get(surface_type, 0.25)
958         else:
959             self.albedo = albedo
960 
961         self.module = module
962         if module_parameters is None:
963             self.module_parameters = {}
964         else:
965             self.module_parameters = module_parameters
966 
967         self.module_type = module_type
968 
969         self.strings = strings
970         self.modules_per_string = modules_per_string
971 
972         if temperature_model_parameters is None:
973             self.temperature_model_parameters = \
974                 self._infer_temperature_model_params()
975         else:
976             self.temperature_model_parameters = temperature_model_parameters
977 
978         if array_losses_parameters is None:
979             self.array_losses_parameters = {}
980         else:
981             self.array_losses_parameters = array_losses_parameters
982 
983         self.name = name
984 
985     def __repr__(self):
986         attrs = ['name', 'mount', 'module',
987                  'albedo', 'module_type',
988                  'temperature_model_parameters',
989                  'strings', 'modules_per_string']
990 
991         return 'Array:\n  ' + '\n  '.join(
992             f'{attr}: {getattr(self, attr)}' for attr in attrs
993         )
994 
995     def _infer_temperature_model_params(self):
996         # try to infer temperature model parameters from from racking_model
997         # and module_type
998         param_set = f'{self.mount.racking_model}_{self.module_type}'
999         if param_set in temperature.TEMPERATURE_MODEL_PARAMETERS['sapm']:
1000             return temperature._temperature_model_params('sapm', param_set)
1001         elif 'freestanding' in param_set:
1002             return temperature._temperature_model_params('pvsyst',
1003                                                          'freestanding')
1004         elif 'insulated' in param_set:  # after SAPM to avoid confusing keys
1005             return temperature._temperature_model_params('pvsyst',
1006                                                          'insulated')
1007         else:
1008             return {}
1009 
1010     def _infer_cell_type(self):
1011         """"""
1012         Examines module_parameters and maps the Technology key for the CEC
1013         database and the Material key for the Sandia database to a common
1014         list of strings for cell type.
1015 
1016         Returns
1017         -------
1018         cell_type: str
1019 
1020         """"""
1021 
1022         _cell_type_dict = {'Multi-c-Si': 'multisi',
1023                            'Mono-c-Si': 'monosi',
1024                            'Thin Film': 'cigs',
1025                            'a-Si/nc': 'asi',
1026                            'CIS': 'cigs',
1027                            'CIGS': 'cigs',
1028                            '1-a-Si': 'asi',
1029                            'CdTe': 'cdte',
1030                            'a-Si': 'asi',
1031                            '2-a-Si': None,
1032                            '3-a-Si': None,
1033                            'HIT-Si': 'monosi',
1034                            'mc-Si': 'multisi',
1035                            'c-Si': 'multisi',
1036                            'Si-Film': 'asi',
1037                            'EFG mc-Si': 'multisi',
1038                            'GaAs': None,
1039                            'a-Si / mono-Si': 'monosi'}
1040 
1041         if 'Technology' in self.module_parameters.keys():
1042             # CEC module parameter set
1043             cell_type = _cell_type_dict[self.module_parameters['Technology']]
1044         elif 'Material' in self.module_parameters.keys():
1045             # Sandia module parameter set
1046             cell_type = _cell_type_dict[self.module_parameters['Material']]
1047         else:
1048             cell_type = None
1049 
1050         return cell_type
1051 
1052     def get_aoi(self, solar_zenith, solar_azimuth):
1053         """"""
1054         Get the angle of incidence on the array.
1055 
1056         Parameters
1057         ----------
1058         solar_zenith : float or Series
1059             Solar zenith angle.
1060         solar_azimuth : float or Series
1061             Solar azimuth angle
1062 
1063         Returns
1064         -------
1065         aoi : Series
1066             Then angle of incidence.
1067         """"""
1068         orientation = self.mount.get_orientation(solar_zenith, solar_azimuth)
1069         return irradiance.aoi(orientation['surface_tilt'],
1070                               orientation['surface_azimuth'],
1071                               solar_zenith, solar_azimuth)
1072 
1073     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,
1074                        dni_extra=None, airmass=None, albedo=None,
1075                        model='haydavies', **kwargs):
1076         """"""
1077         Get plane of array irradiance components.
1078 
1079         Uses the :py:func:`pvlib.irradiance.get_total_irradiance` function to
1080         calculate the plane of array irradiance components for a surface
1081         defined by ``self.surface_tilt`` and ``self.surface_azimuth``.
1082 
1083         Parameters
1084         ----------
1085         solar_zenith : float or Series.
1086             Solar zenith angle.
1087         solar_azimuth : float or Series.
1088             Solar azimuth angle.
1089         dni : float or Series
1090             Direct normal irradiance. [W/m2]
1091         ghi : float or Series. [W/m2]
1092             Global horizontal irradiance
1093         dhi : float or Series
1094             Diffuse horizontal irradiance. [W/m2]
1095         dni_extra : None, float or Series, default None
1096             Extraterrestrial direct normal irradiance. [W/m2]
1097         airmass : None, float or Series, default None
1098             Airmass. [unitless]
1099         albedo : None, float or Series, default None
1100             Ground surface albedo. [unitless]
1101         model : String, default 'haydavies'
1102             Irradiance model.
1103 
1104         kwargs
1105             Extra parameters passed to
1106             :py:func:`pvlib.irradiance.get_total_irradiance`.
1107 
1108         Returns
1109         -------
1110         poa_irradiance : DataFrame
1111             Column names are: ``'poa_global', 'poa_direct', 'poa_diffuse',
1112             'poa_sky_diffuse', 'poa_ground_diffuse'``.
1113 
1114         See also
1115         --------
1116         :py:func:`pvlib.irradiance.get_total_irradiance`
1117         """"""
1118         if albedo is None:
1119             albedo = self.albedo
1120 
1121         # not needed for all models, but this is easier
1122         if dni_extra is None:
1123             dni_extra = irradiance.get_extra_radiation(solar_zenith.index)
1124 
1125         if airmass is None:
1126             airmass = atmosphere.get_relative_airmass(solar_zenith)
1127 
1128         orientation = self.mount.get_orientation(solar_zenith, solar_azimuth)
1129         return irradiance.get_total_irradiance(orientation['surface_tilt'],
1130                                                orientation['surface_azimuth'],
1131                                                solar_zenith, solar_azimuth,
1132                                                dni, ghi, dhi,
1133                                                dni_extra=dni_extra,
1134                                                airmass=airmass,
1135                                                albedo=albedo,
1136                                                model=model,
1137                                                **kwargs)
1138 
1139     def get_iam(self, aoi, iam_model='physical'):
1140         """"""
1141         Determine the incidence angle modifier using the method specified by
1142         ``iam_model``.
1143 
1144         Parameters for the selected IAM model are expected to be in
1145         ``Array.module_parameters``. Default parameters are available for
1146         the 'physical', 'ashrae' and 'martin_ruiz' models.
1147 
1148         Parameters
1149         ----------
1150         aoi : numeric
1151             The angle of incidence in degrees.
1152 
1153         aoi_model : string, default 'physical'
1154             The IAM model to be used. Valid strings are 'physical', 'ashrae',
1155             'martin_ruiz', 'sapm' and 'interp'.
1156 
1157         Returns
1158         -------
1159         iam : numeric
1160             The AOI modifier.
1161 
1162         Raises
1163         ------
1164         ValueError
1165             if `iam_model` is not a valid model name.
1166         """"""
1167         model = iam_model.lower()
1168         if model in ['ashrae', 'physical', 'martin_ruiz', 'interp']:
1169             func = getattr(iam, model)  # get function at pvlib.iam
1170             # get all parameters from function signature to retrieve them from
1171             # module_parameters if present
1172             params = set(inspect.signature(func).parameters.keys())
1173             params.discard('aoi')  # exclude aoi so it can't be repeated
1174             kwargs = _build_kwargs(params, self.module_parameters)
1175             return func(aoi, **kwargs)
1176         elif model == 'sapm':
1177             return iam.sapm(aoi, self.module_parameters)
1178         else:
1179             raise ValueError(model + ' is not a valid IAM model')
1180 
1181     def get_cell_temperature(self, poa_global, temp_air, wind_speed, model,
1182                              effective_irradiance=None):
1183         """"""
1184         Determine cell temperature using the method specified by ``model``.
1185 
1186         Parameters
1187         ----------
1188         poa_global : numeric
1189             Total incident irradiance [W/m^2]
1190 
1191         temp_air : numeric
1192             Ambient dry bulb temperature [C]
1193 
1194         wind_speed : numeric
1195             Wind speed [m/s]
1196 
1197         model : str
1198             Supported models include ``'sapm'``, ``'pvsyst'``,
1199             ``'faiman'``, ``'fuentes'``, and ``'noct_sam'``
1200 
1201         effective_irradiance : numeric, optional
1202             The irradiance that is converted to photocurrent in W/m^2.
1203             Only used for some models.
1204 
1205         Returns
1206         -------
1207         numeric
1208             Values in degrees C.
1209 
1210         See Also
1211         --------
1212         pvlib.temperature.sapm_cell, pvlib.temperature.pvsyst_cell,
1213         pvlib.temperature.faiman, pvlib.temperature.fuentes,
1214         pvlib.temperature.noct_sam
1215 
1216         Notes
1217         -----
1218         Some temperature models have requirements for the input types;
1219         see the documentation of the underlying model function for details.
1220         """"""
1221         # convenience wrapper to avoid passing args 2 and 3 every call
1222         _build_tcell_args = functools.partial(
1223             _build_args, input_dict=self.temperature_model_parameters,
1224             dict_name='temperature_model_parameters')
1225 
1226         if model == 'sapm':
1227             func = temperature.sapm_cell
1228             required = _build_tcell_args(['a', 'b', 'deltaT'])
1229             optional = _build_kwargs(['irrad_ref'],
1230                                      self.temperature_model_parameters)
1231         elif model == 'pvsyst':
1232             func = temperature.pvsyst_cell
1233             required = tuple()
1234             optional = {
1235                 **_build_kwargs(['module_efficiency', 'alpha_absorption'],
1236                                 self.module_parameters),
1237                 **_build_kwargs(['u_c', 'u_v'],
1238                                 self.temperature_model_parameters)
1239             }
1240         elif model == 'faiman':
1241             func = temperature.faiman
1242             required = tuple()
1243             optional = _build_kwargs(['u0', 'u1'],
1244                                      self.temperature_model_parameters)
1245         elif model == 'fuentes':
1246             func = temperature.fuentes
1247             required = _build_tcell_args(['noct_installed'])
1248             optional = _build_kwargs([
1249                 'wind_height', 'emissivity', 'absorption',
1250                 'surface_tilt', 'module_width', 'module_length'],
1251                 self.temperature_model_parameters)
1252             if self.mount.module_height is not None:
1253                 optional['module_height'] = self.mount.module_height
1254         elif model == 'noct_sam':
1255             func = functools.partial(temperature.noct_sam,
1256                                      effective_irradiance=effective_irradiance)
1257             required = _build_tcell_args(['noct', 'module_efficiency'])
1258             optional = _build_kwargs(['transmittance_absorptance',
1259                                       'array_height', 'mount_standoff'],
1260                                      self.temperature_model_parameters)
1261         else:
1262             raise ValueError(f'{model} is not a valid cell temperature model')
1263 
1264         temperature_cell = func(poa_global, temp_air, wind_speed,
1265                                 *required, **optional)
1266         return temperature_cell
1267 
1268     def dc_ohms_from_percent(self):
1269         """"""
1270         Calculates the equivalent resistance of the wires using
1271         :py:func:`pvlib.pvsystem.dc_ohms_from_percent`
1272 
1273         Makes use of array module parameters according to the
1274         following DC models:
1275 
1276         CEC:
1277 
1278             * `self.module_parameters[""V_mp_ref""]`
1279             * `self.module_parameters[""I_mp_ref""]`
1280 
1281         SAPM:
1282 
1283             * `self.module_parameters[""Vmpo""]`
1284             * `self.module_parameters[""Impo""]`
1285 
1286         PVsyst-like or other:
1287 
1288             * `self.module_parameters[""Vmpp""]`
1289             * `self.module_parameters[""Impp""]`
1290 
1291         Other array parameters that are used are:
1292         `self.losses_parameters[""dc_ohmic_percent""]`,
1293         `self.modules_per_string`, and
1294         `self.strings`.
1295 
1296         See :py:func:`pvlib.pvsystem.dc_ohms_from_percent` for more details.
1297         """"""
1298 
1299         # get relevent Vmp and Imp parameters from CEC parameters
1300         if all(elem in self.module_parameters
1301                for elem in ['V_mp_ref', 'I_mp_ref']):
1302             vmp_ref = self.module_parameters['V_mp_ref']
1303             imp_ref = self.module_parameters['I_mp_ref']
1304 
1305         # get relevant Vmp and Imp parameters from SAPM parameters
1306         elif all(elem in self.module_parameters for elem in ['Vmpo', 'Impo']):
1307             vmp_ref = self.module_parameters['Vmpo']
1308             imp_ref = self.module_parameters['Impo']
1309 
1310         # get relevant Vmp and Imp parameters if they are PVsyst-like
1311         elif all(elem in self.module_parameters for elem in ['Vmpp', 'Impp']):
1312             vmp_ref = self.module_parameters['Vmpp']
1313             imp_ref = self.module_parameters['Impp']
1314 
1315         # raise error if relevant Vmp and Imp parameters are not found
1316         else:
1317             raise ValueError('Parameters for Vmp and Imp could not be found '
1318                              'in the array module parameters. Module '
1319                              'parameters must include one set of '
1320                              '{""V_mp_ref"", ""I_mp_Ref""}, '
1321                              '{""Vmpo"", ""Impo""}, or '
1322                              '{""Vmpp"", ""Impp""}.'
1323                              )
1324 
1325         return dc_ohms_from_percent(
1326             vmp_ref,
1327             imp_ref,
1328             self.array_losses_parameters['dc_ohmic_percent'],
1329             self.modules_per_string,
1330             self.strings)
1331 
1332 
1333 @dataclass
1334 class AbstractMount(ABC):
1335     """"""
1336     A base class for Mount classes to extend. It is not intended to be
1337     instantiated directly.
1338     """"""
1339 
1340     @abstractmethod
1341     def get_orientation(self, solar_zenith, solar_azimuth):
1342         """"""
1343         Determine module orientation.
1344 
1345         Parameters
1346         ----------
1347         solar_zenith : numeric
1348             Solar apparent zenith angle [degrees]
1349         solar_azimuth : numeric
1350             Solar azimuth angle [degrees]
1351 
1352         Returns
1353         -------
1354         orientation : dict-like
1355             A dict-like object with keys `'surface_tilt', 'surface_azimuth'`
1356             (typically a dict or pandas.DataFrame)
1357         """"""
1358 
1359 
1360 @dataclass
1361 class FixedMount(AbstractMount):
1362     """"""
1363     Racking at fixed (static) orientation.
1364 
1365     Parameters
1366     ----------
1367     surface_tilt : float, default 0
1368         Surface tilt angle. The tilt angle is defined as angle from horizontal
1369         (e.g. surface facing up = 0, surface facing horizon = 90) [degrees]
1370 
1371     surface_azimuth : float, default 180
1372         Azimuth angle of the module surface. North=0, East=90, South=180,
1373         West=270. [degrees]
1374 
1375     racking_model : str, optional
1376         Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.
1377         Used to identify a parameter set for the SAPM cell temperature model.
1378 
1379     module_height : float, optional
1380        The height above ground of the center of the module [m]. Used for
1381        the Fuentes cell temperature model.
1382     """"""
1383 
1384     surface_tilt: float = 0.0
1385     surface_azimuth: float = 180.0
1386     racking_model: Optional[str] = None
1387     module_height: Optional[float] = None
1388 
1389     def get_orientation(self, solar_zenith, solar_azimuth):
1390         # note -- docstring is automatically inherited from AbstractMount
1391         return {
1392             'surface_tilt': self.surface_tilt,
1393             'surface_azimuth': self.surface_azimuth,
1394         }
1395 
1396 
1397 @dataclass
1398 class SingleAxisTrackerMount(AbstractMount):
1399     """"""
1400     Single-axis tracker racking for dynamic solar tracking.
1401 
1402     Parameters
1403     ----------
1404     axis_tilt : float, default 0
1405         The tilt of the axis of rotation (i.e, the y-axis defined by
1406         axis_azimuth) with respect to horizontal. [degrees]
1407 
1408     axis_azimuth : float, default 180
1409         A value denoting the compass direction along which the axis of
1410         rotation lies, measured east of north. [degrees]
1411 
1412     max_angle : float, default 90
1413         A value denoting the maximum rotation angle
1414         of the one-axis tracker from its horizontal position (horizontal
1415         if axis_tilt = 0). A max_angle of 90 degrees allows the tracker
1416         to rotate to a vertical position to point the panel towards a
1417         horizon. max_angle of 180 degrees allows for full rotation. [degrees]
1418 
1419     backtrack : bool, default True
1420         Controls whether the tracker has the capability to ""backtrack""
1421         to avoid row-to-row shading. False denotes no backtrack
1422         capability. True denotes backtrack capability.
1423 
1424     gcr : float, default 2.0/7.0
1425         A value denoting the ground coverage ratio of a tracker system
1426         which utilizes backtracking; i.e. the ratio between the PV array
1427         surface area to total ground area. A tracker system with modules
1428         2 meters wide, centered on the tracking axis, with 6 meters
1429         between the tracking axes has a gcr of 2/6=0.333. If gcr is not
1430         provided, a gcr of 2/7 is default. gcr must be <=1. [unitless]
1431 
1432     cross_axis_tilt : float, default 0.0
1433         The angle, relative to horizontal, of the line formed by the
1434         intersection between the slope containing the tracker axes and a plane
1435         perpendicular to the tracker axes. Cross-axis tilt should be specified
1436         using a right-handed convention. For example, trackers with axis
1437         azimuth of 180 degrees (heading south) will have a negative cross-axis
1438         tilt if the tracker axes plane slopes down to the east and positive
1439         cross-axis tilt if the tracker axes plane slopes up to the east. Use
1440         :func:`~pvlib.tracking.calc_cross_axis_tilt` to calculate
1441         `cross_axis_tilt`. [degrees]
1442 
1443     racking_model : str, optional
1444         Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.
1445         Used to identify a parameter set for the SAPM cell temperature model.
1446 
1447     module_height : float, optional
1448        The height above ground of the center of the module [m]. Used for
1449        the Fuentes cell temperature model.
1450     """"""
1451     axis_tilt: float = 0.0
1452     axis_azimuth: float = 0.0
1453     max_angle: float = 90.0
1454     backtrack: bool = True
1455     gcr: float = 2.0/7.0
1456     cross_axis_tilt: float = 0.0
1457     racking_model: Optional[str] = None
1458     module_height: Optional[float] = None
1459 
1460     def get_orientation(self, solar_zenith, solar_azimuth):
1461         # note -- docstring is automatically inherited from AbstractMount
1462         from pvlib import tracking  # avoid circular import issue
1463         tracking_data = tracking.singleaxis(
1464             solar_zenith, solar_azimuth,
1465             self.axis_tilt, self.axis_azimuth,
1466             self.max_angle, self.backtrack,
1467             self.gcr, self.cross_axis_tilt
1468         )
1469         return tracking_data
1470 
1471 
1472 def calcparams_desoto(effective_irradiance, temp_cell,
1473                       alpha_sc, a_ref, I_L_ref, I_o_ref, R_sh_ref, R_s,
1474                       EgRef=1.121, dEgdT=-0.0002677,
1475                       irrad_ref=1000, temp_ref=25):
1476     '''
1477     Calculates five parameter values for the single diode equation at
1478     effective irradiance and cell temperature using the De Soto et al.
1479     model described in [1]_. The five values returned by calcparams_desoto
1480     can be used by singlediode to calculate an IV curve.
1481 
1482     Parameters
1483     ----------
1484     effective_irradiance : numeric
1485         The irradiance (W/m2) that is converted to photocurrent.
1486 
1487     temp_cell : numeric
1488         The average cell temperature of cells within a module in C.
1489 
1490     alpha_sc : float
1491         The short-circuit current temperature coefficient of the
1492         module in units of A/C.
1493 
1494     a_ref : float
1495         The product of the usual diode ideality factor (n, unitless),
1496         number of cells in series (Ns), and cell thermal voltage at reference
1497         conditions, in units of V.
1498 
1499     I_L_ref : float
1500         The light-generated current (or photocurrent) at reference conditions,
1501         in amperes.
1502 
1503     I_o_ref : float
1504         The dark or diode reverse saturation current at reference conditions,
1505         in amperes.
1506 
1507     R_sh_ref : float
1508         The shunt resistance at reference conditions, in ohms.
1509 
1510     R_s : float
1511         The series resistance at reference conditions, in ohms.
1512 
1513     EgRef : float
1514         The energy bandgap at reference temperature in units of eV.
1515         1.121 eV for crystalline silicon. EgRef must be >0.  For parameters
1516         from the SAM CEC module database, EgRef=1.121 is implicit for all
1517         cell types in the parameter estimation algorithm used by NREL.
1518 
1519     dEgdT : float
1520         The temperature dependence of the energy bandgap at reference
1521         conditions in units of 1/K. May be either a scalar value
1522         (e.g. -0.0002677 as in [1]_) or a DataFrame (this may be useful if
1523         dEgdT is a modeled as a function of temperature). For parameters from
1524         the SAM CEC module database, dEgdT=-0.0002677 is implicit for all cell
1525         types in the parameter estimation algorithm used by NREL.
1526 
1527     irrad_ref : float (optional, default=1000)
1528         Reference irradiance in W/m^2.
1529 
1530     temp_ref : float (optional, default=25)
1531         Reference cell temperature in C.
1532 
1533     Returns
1534     -------
1535     Tuple of the following results:
1536 
1537     photocurrent : numeric
1538         Light-generated current in amperes
1539 
1540     saturation_current : numeric
1541         Diode saturation curent in amperes
1542 
1543     resistance_series : numeric
1544         Series resistance in ohms
1545 
1546     resistance_shunt : numeric
1547         Shunt resistance in ohms
1548 
1549     nNsVth : numeric
1550         The product of the usual diode ideality factor (n, unitless),
1551         number of cells in series (Ns), and cell thermal voltage at
1552         specified effective irradiance and cell temperature.
1553 
1554     References
1555     ----------
1556     .. [1] W. De Soto et al., ""Improvement and validation of a model for
1557        photovoltaic array performance"", Solar Energy, vol 80, pp. 78-88,
1558        2006.
1559 
1560     .. [2] System Advisor Model web page. https://sam.nrel.gov.
1561 
1562     .. [3] A. Dobos, ""An Improved Coefficient Calculator for the California
1563        Energy Commission 6 Parameter Photovoltaic Module Model"", Journal of
1564        Solar Energy Engineering, vol 134, 2012.
1565 
1566     .. [4] O. Madelung, ""Semiconductors: Data Handbook, 3rd ed."" ISBN
1567        3-540-40488-0
1568 
1569     See Also
1570     --------
1571     singlediode
1572     retrieve_sam
1573 
1574     Notes
1575     -----
1576     If the reference parameters in the ModuleParameters struct are read
1577     from a database or library of parameters (e.g. System Advisor
1578     Model), it is important to use the same EgRef and dEgdT values that
1579     were used to generate the reference parameters, regardless of the
1580     actual bandgap characteristics of the semiconductor. For example, in
1581     the case of the System Advisor Model library, created as described
1582     in [3], EgRef and dEgdT for all modules were 1.121 and -0.0002677,
1583     respectively.
1584 
1585     This table of reference bandgap energies (EgRef), bandgap energy
1586     temperature dependence (dEgdT), and ""typical"" airmass response (M)
1587     is provided purely as reference to those who may generate their own
1588     reference module parameters (a_ref, IL_ref, I0_ref, etc.) based upon
1589     the various PV semiconductors. Again, we stress the importance of
1590     using identical EgRef and dEgdT when generation reference parameters
1591     and modifying the reference parameters (for irradiance, temperature,
1592     and airmass) per DeSoto's equations.
1593 
1594      Crystalline Silicon (Si):
1595          * EgRef = 1.121
1596          * dEgdT = -0.0002677
1597 
1598          >>> M = np.polyval([-1.26E-4, 2.816E-3, -0.024459, 0.086257, 0.9181],
1599          ...                AMa) # doctest: +SKIP
1600 
1601          Source: [1]
1602 
1603      Cadmium Telluride (CdTe):
1604          * EgRef = 1.475
1605          * dEgdT = -0.0003
1606 
1607          >>> M = np.polyval([-2.46E-5, 9.607E-4, -0.0134, 0.0716, 0.9196],
1608          ...                AMa) # doctest: +SKIP
1609 
1610          Source: [4]
1611 
1612      Copper Indium diSelenide (CIS):
1613          * EgRef = 1.010
1614          * dEgdT = -0.00011
1615 
1616          >>> M = np.polyval([-3.74E-5, 0.00125, -0.01462, 0.0718, 0.9210],
1617          ...                AMa) # doctest: +SKIP
1618 
1619          Source: [4]
1620 
1621      Copper Indium Gallium diSelenide (CIGS):
1622          * EgRef = 1.15
1623          * dEgdT = ????
1624 
1625          >>> M = np.polyval([-9.07E-5, 0.0022, -0.0202, 0.0652, 0.9417],
1626          ...                AMa) # doctest: +SKIP
1627 
1628          Source: Wikipedia
1629 
1630      Gallium Arsenide (GaAs):
1631          * EgRef = 1.424
1632          * dEgdT = -0.000433
1633          * M = unknown
1634 
1635          Source: [4]
1636     '''
1637 
1638     # Boltzmann constant in eV/K, 8.617332478e-05
1639     k = constants.value('Boltzmann constant in eV/K')
1640 
1641     # reference temperature
1642     Tref_K = temp_ref + 273.15
1643     Tcell_K = temp_cell + 273.15
1644 
1645     E_g = EgRef * (1 + dEgdT*(Tcell_K - Tref_K))
1646 
1647     nNsVth = a_ref * (Tcell_K / Tref_K)
1648 
1649     # In the equation for IL, the single factor effective_irradiance is
1650     # used, in place of the product S*M in [1]. effective_irradiance is
1651     # equivalent to the product of S (irradiance reaching a module's cells) *
1652     # M (spectral adjustment factor) as described in [1].
1653     IL = effective_irradiance / irrad_ref * \
1654         (I_L_ref + alpha_sc * (Tcell_K - Tref_K))
1655     I0 = (I_o_ref * ((Tcell_K / Tref_K) ** 3) *
1656           (np.exp(EgRef / (k*(Tref_K)) - (E_g / (k*(Tcell_K))))))
1657     # Note that the equation for Rsh differs from [1]. In [1] Rsh is given as
1658     # Rsh = Rsh_ref * (S_ref / S) where S is broadband irradiance reaching
1659     # the module's cells. If desired this model behavior can be duplicated
1660     # by applying reflection and soiling losses to broadband plane of array
1661     # irradiance and not applying a spectral loss modifier, i.e.,
1662     # spectral_modifier = 1.0.
1663     # use errstate to silence divide by warning
1664     with np.errstate(divide='ignore'):
1665         Rsh = R_sh_ref * (irrad_ref / effective_irradiance)
1666 
1667     Rs = R_s
1668 
1669     numeric_args = (effective_irradiance, temp_cell)
1670     out = (IL, I0, Rs, Rsh, nNsVth)
1671 
1672     if all(map(np.isscalar, numeric_args)):
1673         return out
1674 
1675     index = tools.get_pandas_index(*numeric_args)
1676 
1677     if index is None:
1678         return np.broadcast_arrays(*out)
1679 
1680     return tuple(pd.Series(a, index=index).rename(None) for a in out)
1681 
1682 
1683 def calcparams_cec(effective_irradiance, temp_cell,
1684                    alpha_sc, a_ref, I_L_ref, I_o_ref, R_sh_ref, R_s,
1685                    Adjust, EgRef=1.121, dEgdT=-0.0002677,
1686                    irrad_ref=1000, temp_ref=25):
1687     '''
1688     Calculates five parameter values for the single diode equation at
1689     effective irradiance and cell temperature using the CEC
1690     model. The CEC model [1]_ differs from the De soto et al.
1691     model [3]_ by the parameter Adjust. The five values returned by
1692     calcparams_cec can be used by singlediode to calculate an IV curve.
1693 
1694     Parameters
1695     ----------
1696     effective_irradiance : numeric
1697         The irradiance (W/m2) that is converted to photocurrent.
1698 
1699     temp_cell : numeric
1700         The average cell temperature of cells within a module in C.
1701 
1702     alpha_sc : float
1703         The short-circuit current temperature coefficient of the
1704         module in units of A/C.
1705 
1706     a_ref : float
1707         The product of the usual diode ideality factor (n, unitless),
1708         number of cells in series (Ns), and cell thermal voltage at reference
1709         conditions, in units of V.
1710 
1711     I_L_ref : float
1712         The light-generated current (or photocurrent) at reference conditions,
1713         in amperes.
1714 
1715     I_o_ref : float
1716         The dark or diode reverse saturation current at reference conditions,
1717         in amperes.
1718 
1719     R_sh_ref : float
1720         The shunt resistance at reference conditions, in ohms.
1721 
1722     R_s : float
1723         The series resistance at reference conditions, in ohms.
1724 
1725     Adjust : float
1726         The adjustment to the temperature coefficient for short circuit
1727         current, in percent
1728 
1729     EgRef : float
1730         The energy bandgap at reference temperature in units of eV.
1731         1.121 eV for crystalline silicon. EgRef must be >0.  For parameters
1732         from the SAM CEC module database, EgRef=1.121 is implicit for all
1733         cell types in the parameter estimation algorithm used by NREL.
1734 
1735     dEgdT : float
1736         The temperature dependence of the energy bandgap at reference
1737         conditions in units of 1/K. May be either a scalar value
1738         (e.g. -0.0002677 as in [3]) or a DataFrame (this may be useful if
1739         dEgdT is a modeled as a function of temperature). For parameters from
1740         the SAM CEC module database, dEgdT=-0.0002677 is implicit for all cell
1741         types in the parameter estimation algorithm used by NREL.
1742 
1743     irrad_ref : float (optional, default=1000)
1744         Reference irradiance in W/m^2.
1745 
1746     temp_ref : float (optional, default=25)
1747         Reference cell temperature in C.
1748 
1749     Returns
1750     -------
1751     Tuple of the following results:
1752 
1753     photocurrent : numeric
1754         Light-generated current in amperes
1755 
1756     saturation_current : numeric
1757         Diode saturation curent in amperes
1758 
1759     resistance_series : numeric
1760         Series resistance in ohms
1761 
1762     resistance_shunt : numeric
1763         Shunt resistance in ohms
1764 
1765     nNsVth : numeric
1766         The product of the usual diode ideality factor (n, unitless),
1767         number of cells in series (Ns), and cell thermal voltage at
1768         specified effective irradiance and cell temperature.
1769 
1770     References
1771     ----------
1772     .. [1] A. Dobos, ""An Improved Coefficient Calculator for the California
1773        Energy Commission 6 Parameter Photovoltaic Module Model"", Journal of
1774        Solar Energy Engineering, vol 134, 2012.
1775 
1776     .. [2] System Advisor Model web page. https://sam.nrel.gov.
1777 
1778     .. [3] W. De Soto et al., ""Improvement and validation of a model for
1779        photovoltaic array performance"", Solar Energy, vol 80, pp. 78-88,
1780        2006.
1781 
1782     See Also
1783     --------
1784     calcparams_desoto
1785     singlediode
1786     retrieve_sam
1787 
1788     '''
1789 
1790     # pass adjusted temperature coefficient to desoto
1791     return calcparams_desoto(effective_irradiance, temp_cell,
1792                              alpha_sc*(1.0 - Adjust/100),
1793                              a_ref, I_L_ref, I_o_ref,
1794                              R_sh_ref, R_s,
1795                              EgRef=EgRef, dEgdT=dEgdT,
1796                              irrad_ref=irrad_ref, temp_ref=temp_ref)
1797 
1798 
1799 def calcparams_pvsyst(effective_irradiance, temp_cell,
1800                       alpha_sc, gamma_ref, mu_gamma,
1801                       I_L_ref, I_o_ref,
1802                       R_sh_ref, R_sh_0, R_s,
1803                       cells_in_series,
1804                       R_sh_exp=5.5,
1805                       EgRef=1.121,
1806                       irrad_ref=1000, temp_ref=25):
1807     '''
1808     Calculates five parameter values for the single diode equation at
1809     effective irradiance and cell temperature using the PVsyst v6
1810     model.  The PVsyst v6 model is described in [1]_, [2]_, [3]_.
1811     The five values returned by calcparams_pvsyst can be used by singlediode
1812     to calculate an IV curve.
1813 
1814     Parameters
1815     ----------
1816     effective_irradiance : numeric
1817         The irradiance (W/m2) that is converted to photocurrent.
1818 
1819     temp_cell : numeric
1820         The average cell temperature of cells within a module in C.
1821 
1822     alpha_sc : float
1823         The short-circuit current temperature coefficient of the
1824         module in units of A/C.
1825 
1826     gamma_ref : float
1827         The diode ideality factor
1828 
1829     mu_gamma : float
1830         The temperature coefficient for the diode ideality factor, 1/K
1831 
1832     I_L_ref : float
1833         The light-generated current (or photocurrent) at reference conditions,
1834         in amperes.
1835 
1836     I_o_ref : float
1837         The dark or diode reverse saturation current at reference conditions,
1838         in amperes.
1839 
1840     R_sh_ref : float
1841         The shunt resistance at reference conditions, in ohms.
1842 
1843     R_sh_0 : float
1844         The shunt resistance at zero irradiance conditions, in ohms.
1845 
1846     R_s : float
1847         The series resistance at reference conditions, in ohms.
1848 
1849     cells_in_series : integer
1850         The number of cells connected in series.
1851 
1852     R_sh_exp : float
1853         The exponent in the equation for shunt resistance, unitless. Defaults
1854         to 5.5.
1855 
1856     EgRef : float
1857         The energy bandgap at reference temperature in units of eV.
1858         1.121 eV for crystalline silicon. EgRef must be >0.
1859 
1860     irrad_ref : float (optional, default=1000)
1861         Reference irradiance in W/m^2.
1862 
1863     temp_ref : float (optional, default=25)
1864         Reference cell temperature in C.
1865 
1866     Returns
1867     -------
1868     Tuple of the following results:
1869 
1870     photocurrent : numeric
1871         Light-generated current in amperes
1872 
1873     saturation_current : numeric
1874         Diode saturation current in amperes
1875 
1876     resistance_series : numeric
1877         Series resistance in ohms
1878 
1879     resistance_shunt : numeric
1880         Shunt resistance in ohms
1881 
1882     nNsVth : numeric
1883         The product of the usual diode ideality factor (n, unitless),
1884         number of cells in series (Ns), and cell thermal voltage at
1885         specified effective irradiance and cell temperature.
1886 
1887     References
1888     ----------
1889     .. [1] K. Sauer, T. Roessler, C. W. Hansen, Modeling the Irradiance and
1890        Temperature Dependence of Photovoltaic Modules in PVsyst,
1891        IEEE Journal of Photovoltaics v5(1), January 2015.
1892 
1893     .. [2] A. Mermoud, PV modules modelling, Presentation at the 2nd PV
1894        Performance Modeling Workshop, Santa Clara, CA, May 2013
1895 
1896     .. [3] A. Mermoud, T. Lejeune, Performance Assessment of a Simulation Model
1897        for PV modules of any available technology, 25th European Photovoltaic
1898        Solar Energy Conference, Valencia, Spain, Sept. 2010
1899 
1900     See Also
1901     --------
1902     calcparams_desoto
1903     singlediode
1904 
1905     '''
1906 
1907     # Boltzmann constant in J/K
1908     k = constants.k
1909 
1910     # elementary charge in coulomb
1911     q = constants.e
1912 
1913     # reference temperature
1914     Tref_K = temp_ref + 273.15
1915     Tcell_K = temp_cell + 273.15
1916 
1917     gamma = gamma_ref + mu_gamma * (Tcell_K - Tref_K)
1918     nNsVth = gamma * k / q * cells_in_series * Tcell_K
1919 
1920     IL = effective_irradiance / irrad_ref * \
1921         (I_L_ref + alpha_sc * (Tcell_K - Tref_K))
1922 
1923     I0 = I_o_ref * ((Tcell_K / Tref_K) ** 3) * \
1924         (np.exp((q * EgRef) / (k * gamma) * (1 / Tref_K - 1 / Tcell_K)))
1925 
1926     Rsh_tmp = \
1927         (R_sh_ref - R_sh_0 * np.exp(-R_sh_exp)) / (1.0 - np.exp(-R_sh_exp))
1928     Rsh_base = np.maximum(0.0, Rsh_tmp)
1929 
1930     Rsh = Rsh_base + (R_sh_0 - Rsh_base) * \
1931         np.exp(-R_sh_exp * effective_irradiance / irrad_ref)
1932 
1933     Rs = R_s
1934 
1935     numeric_args = (effective_irradiance, temp_cell)
1936     out = (IL, I0, Rs, Rsh, nNsVth)
1937 
1938     if all(map(np.isscalar, numeric_args)):
1939         return out
1940 
1941     index = tools.get_pandas_index(*numeric_args)
1942 
1943     if index is None:
1944         return np.broadcast_arrays(*out)
1945 
1946     return tuple(pd.Series(a, index=index).rename(None) for a in out)
1947 
1948 
1949 def retrieve_sam(name=None, path=None):
1950     '''
1951     Retrieve latest module and inverter info from a local file or the
1952     SAM website.
1953 
1954     This function will retrieve either:
1955 
1956         * CEC module database
1957         * Sandia Module database
1958         * CEC Inverter database
1959         * Anton Driesse Inverter database
1960 
1961     and return it as a pandas DataFrame.
1962 
1963     Parameters
1964     ----------
1965     name : None or string, default None
1966         Name can be one of:
1967 
1968         * 'CECMod' - returns the CEC module database
1969         * 'CECInverter' - returns the CEC Inverter database
1970         * 'SandiaInverter' - returns the CEC Inverter database
1971           (CEC is only current inverter db available; tag kept for
1972           backwards compatibility)
1973         * 'SandiaMod' - returns the Sandia Module database
1974         * 'ADRInverter' - returns the ADR Inverter database
1975 
1976     path : None or string, default None
1977         Path to the SAM file. May also be a URL.
1978 
1979     Returns
1980     -------
1981     samfile : DataFrame
1982         A DataFrame containing all the elements of the desired database.
1983         Each column represents a module or inverter, and a specific
1984         dataset can be retrieved by the command
1985 
1986     Raises
1987     ------
1988     ValueError
1989         If no name or path is provided.
1990 
1991     Notes
1992     -----
1993     Files available at
1994         https://github.com/NREL/SAM/tree/develop/deploy/libraries
1995     Documentation for module and inverter data sets:
1996         https://sam.nrel.gov/photovoltaic/pv-sub-page-2.html
1997 
1998     Examples
1999     --------
2000 
2001     >>> from pvlib import pvsystem
2002     >>> invdb = pvsystem.retrieve_sam('CECInverter')
2003     >>> inverter = invdb.AE_Solar_Energy__AE6_0__277V_
2004     >>> inverter
2005     Vac                          277
2006     Pso                    36.197575
2007     Paco                      6000.0
2008     Pdco                 6158.746094
2009     Vdco                       360.0
2010     C0                     -0.000002
2011     C1                     -0.000026
2012     C2                     -0.001253
2013     C3                       0.00021
2014     Pnt                          1.8
2015     Vdcmax                     450.0
2016     Idcmax                 17.107628
2017     Mppt_low                   100.0
2018     Mppt_high                  450.0
2019     CEC_Date                     NaN
2020     CEC_Type     Utility Interactive
2021     Name: AE_Solar_Energy__AE6_0__277V_, dtype: object
2022     '''
2023 
2024     if name is not None:
2025         name = name.lower()
2026         data_path = os.path.join(
2027             os.path.dirname(os.path.abspath(__file__)), 'data')
2028         if name == 'cecmod':
2029             csvdata = os.path.join(
2030                 data_path, 'sam-library-cec-modules-2019-03-05.csv')
2031         elif name == 'sandiamod':
2032             csvdata = os.path.join(
2033                 data_path, 'sam-library-sandia-modules-2015-6-30.csv')
2034         elif name == 'adrinverter':
2035             csvdata = os.path.join(
2036                 data_path, 'adr-library-cec-inverters-2019-03-05.csv')
2037         elif name in ['cecinverter', 'sandiainverter']:
2038             # Allowing either, to provide for old code,
2039             # while aligning with current expectations
2040             csvdata = os.path.join(
2041                 data_path, 'sam-library-cec-inverters-2019-03-05.csv')
2042         else:
2043             raise ValueError(f'invalid name {name}')
2044     elif path is not None:
2045         if path.startswith('http'):
2046             response = urlopen(path)
2047             csvdata = io.StringIO(response.read().decode(errors='ignore'))
2048         else:
2049             csvdata = path
2050     elif name is None and path is None:
2051         raise ValueError(""A name or path must be provided!"")
2052 
2053     return _parse_raw_sam_df(csvdata)
2054 
2055 
2056 def _normalize_sam_product_names(names):
2057     '''
2058     Replace special characters within the product names to make them more
2059     suitable for use as Dataframe column names.
2060     '''
2061     # Contributed by Anton Driesse (@adriesse), PV Performance Labs. July, 2019
2062 
2063     import warnings
2064 
2065     BAD_CHARS = ' -.()[]:+/"",'
2066     GOOD_CHARS = '____________'
2067 
2068     mapping = str.maketrans(BAD_CHARS, GOOD_CHARS)
2069     names = pd.Series(data=names)
2070     norm_names = names.str.translate(mapping)
2071 
2072     n_duplicates = names.duplicated().sum()
2073     if n_duplicates > 0:
2074         warnings.warn('Original names contain %d duplicate(s).' % n_duplicates)
2075 
2076     n_duplicates = norm_names.duplicated().sum()
2077     if n_duplicates > 0:
2078         warnings.warn(
2079             'Normalized names contain %d duplicate(s).' % n_duplicates)
2080 
2081     return norm_names.values
2082 
2083 
2084 def _parse_raw_sam_df(csvdata):
2085 
2086     df = pd.read_csv(csvdata, index_col=0, skiprows=[1, 2])
2087 
2088     df.columns = df.columns.str.replace(' ', '_')
2089     df.index = _normalize_sam_product_names(df.index)
2090     df = df.transpose()
2091 
2092     if 'ADRCoefficients' in df.index:
2093         ad_ce = 'ADRCoefficients'
2094         # for each inverter, parses a string of coefficients like
2095         # ' 1.33, 2.11, 3.12' into a list containing floats:
2096         # [1.33, 2.11, 3.12]
2097         df.loc[ad_ce] = df.loc[ad_ce].map(lambda x: list(
2098             map(float, x.strip(' []').split())))
2099 
2100     return df
2101 
2102 
2103 def sapm(effective_irradiance, temp_cell, module):
2104     '''
2105     The Sandia PV Array Performance Model (SAPM) generates 5 points on a
2106     PV module's I-V curve (Voc, Isc, Ix, Ixx, Vmp/Imp) according to
2107     SAND2004-3535. Assumes a reference cell temperature of 25 C.
2108 
2109     Parameters
2110     ----------
2111     effective_irradiance : numeric
2112         Irradiance reaching the module's cells, after reflections and
2113         adjustment for spectrum. [W/m2]
2114 
2115     temp_cell : numeric
2116         Cell temperature [C].
2117 
2118     module : dict-like
2119         A dict or Series defining the SAPM parameters. See the notes section
2120         for more details.
2121 
2122     Returns
2123     -------
2124     A DataFrame with the columns:
2125 
2126         * i_sc : Short-circuit current (A)
2127         * i_mp : Current at the maximum-power point (A)
2128         * v_oc : Open-circuit voltage (V)
2129         * v_mp : Voltage at maximum-power point (V)
2130         * p_mp : Power at maximum-power point (W)
2131         * i_x : Current at module V = 0.5Voc, defines 4th point on I-V
2132           curve for modeling curve shape
2133         * i_xx : Current at module V = 0.5(Voc+Vmp), defines 5th point on
2134           I-V curve for modeling curve shape
2135 
2136     Notes
2137     -----
2138     The SAPM parameters which are required in ``module`` are
2139     listed in the following table.
2140 
2141     The Sandia module database contains parameter values for a limited set
2142     of modules. The CEC module database does not contain these parameters.
2143     Both databases can be accessed using :py:func:`retrieve_sam`.
2144 
2145     ================   ========================================================
2146     Key                Description
2147     ================   ========================================================
2148     A0-A4              The airmass coefficients used in calculating
2149                        effective irradiance
2150     B0-B5              The angle of incidence coefficients used in calculating
2151                        effective irradiance
2152     C0-C7              The empirically determined coefficients relating
2153                        Imp, Vmp, Ix, and Ixx to effective irradiance
2154     Isco               Short circuit current at reference condition (amps)
2155     Impo               Maximum power current at reference condition (amps)
2156     Voco               Open circuit voltage at reference condition (amps)
2157     Vmpo               Maximum power voltage at reference condition (amps)
2158     Aisc               Short circuit current temperature coefficient at
2159                        reference condition (1/C)
2160     Aimp               Maximum power current temperature coefficient at
2161                        reference condition (1/C)
2162     Bvoco              Open circuit voltage temperature coefficient at
2163                        reference condition (V/C)
2164     Mbvoc              Coefficient providing the irradiance dependence for the
2165                        BetaVoc temperature coefficient at reference irradiance
2166                        (V/C)
2167     Bvmpo              Maximum power voltage temperature coefficient at
2168                        reference condition
2169     Mbvmp              Coefficient providing the irradiance dependence for the
2170                        BetaVmp temperature coefficient at reference irradiance
2171                        (V/C)
2172     N                  Empirically determined ""diode factor"" (dimensionless)
2173     Cells_in_Series    Number of cells in series in a module's cell string(s)
2174     IXO                Ix at reference conditions
2175     IXXO               Ixx at reference conditions
2176     FD                 Fraction of diffuse irradiance used by module
2177     ================   ========================================================
2178 
2179     References
2180     ----------
2181     .. [1] King, D. et al, 2004, ""Sandia Photovoltaic Array Performance
2182        Model"", SAND Report 3535, Sandia National Laboratories, Albuquerque,
2183        NM.
2184 
2185     See Also
2186     --------
2187     retrieve_sam
2188     pvlib.temperature.sapm_cell
2189     pvlib.temperature.sapm_module
2190     '''
2191 
2192     # TODO: someday, change temp_ref and irrad_ref to reference_temperature and
2193     # reference_irradiance and expose
2194     temp_ref = 25
2195     irrad_ref = 1000
2196 
2197     q = constants.e  # Elementary charge in units of coulombs
2198     kb = constants.k  # Boltzmann's constant in units of J/K
2199 
2200     # avoid problem with integer input
2201     Ee = np.array(effective_irradiance, dtype='float64') / irrad_ref
2202 
2203     # set up masking for 0, positive, and nan inputs
2204     Ee_gt_0 = np.full_like(Ee, False, dtype='bool')
2205     Ee_eq_0 = np.full_like(Ee, False, dtype='bool')
2206     notnan = ~np.isnan(Ee)
2207     np.greater(Ee, 0, where=notnan, out=Ee_gt_0)
2208     np.equal(Ee, 0, where=notnan, out=Ee_eq_0)
2209 
2210     Bvmpo = module['Bvmpo'] + module['Mbvmp']*(1 - Ee)
2211     Bvoco = module['Bvoco'] + module['Mbvoc']*(1 - Ee)
2212     delta = module['N'] * kb * (temp_cell + 273.15) / q
2213 
2214     # avoid repeated computation
2215     logEe = np.full_like(Ee, np.nan)
2216     np.log(Ee, where=Ee_gt_0, out=logEe)
2217     logEe = np.where(Ee_eq_0, -np.inf, logEe)
2218     # avoid repeated __getitem__
2219     cells_in_series = module['Cells_in_Series']
2220 
2221     out = OrderedDict()
2222 
2223     out['i_sc'] = (
2224         module['Isco'] * Ee * (1 + module['Aisc']*(temp_cell - temp_ref)))
2225 
2226     out['i_mp'] = (
2227         module['Impo'] * (module['C0']*Ee + module['C1']*(Ee**2)) *
2228         (1 + module['Aimp']*(temp_cell - temp_ref)))
2229 
2230     out['v_oc'] = np.maximum(0, (
2231         module['Voco'] + cells_in_series * delta * logEe +
2232         Bvoco*(temp_cell - temp_ref)))
2233 
2234     out['v_mp'] = np.maximum(0, (
2235         module['Vmpo'] +
2236         module['C2'] * cells_in_series * delta * logEe +
2237         module['C3'] * cells_in_series * ((delta * logEe) ** 2) +
2238         Bvmpo*(temp_cell - temp_ref)))
2239 
2240     out['p_mp'] = out['i_mp'] * out['v_mp']
2241 
2242     out['i_x'] = (
2243         module['IXO'] * (module['C4']*Ee + module['C5']*(Ee**2)) *
2244         (1 + module['Aisc']*(temp_cell - temp_ref)))
2245 
2246     # the Ixx calculation in King 2004 has a typo (mixes up Aisc and Aimp)
2247     out['i_xx'] = (
2248         module['IXXO'] * (module['C6']*Ee + module['C7']*(Ee**2)) *
2249         (1 + module['Aisc']*(temp_cell - temp_ref)))
2250 
2251     if isinstance(out['i_sc'], pd.Series):
2252         out = pd.DataFrame(out)
2253 
2254     return out
2255 
2256 
2257 sapm_spectral_loss = deprecated(
2258     since='0.10.0',
2259     alternative='pvlib.spectrum.spectral_factor_sapm'
2260 )(spectrum.spectral_factor_sapm)
2261 
2262 
2263 def sapm_effective_irradiance(poa_direct, poa_diffuse, airmass_absolute, aoi,
2264                               module):
2265     r""""""
2266     Calculates the SAPM effective irradiance using the SAPM spectral
2267     loss and SAPM angle of incidence loss functions.
2268 
2269     Parameters
2270     ----------
2271     poa_direct : numeric
2272         The direct irradiance incident upon the module. [W/m2]
2273 
2274     poa_diffuse : numeric
2275         The diffuse irradiance incident on module.  [W/m2]
2276 
2277     airmass_absolute : numeric
2278         Absolute airmass. [unitless]
2279 
2280     aoi : numeric
2281         Angle of incidence. [degrees]
2282 
2283     module : dict-like
2284         A dict, Series, or DataFrame defining the SAPM performance
2285         parameters. See the :py:func:`sapm` notes section for more
2286         details.
2287 
2288     Returns
2289     -------
2290     effective_irradiance : numeric
2291         Effective irradiance accounting for reflections and spectral content.
2292         [W/m2]
2293 
2294     Notes
2295     -----
2296     The SAPM model for effective irradiance [1]_ translates broadband direct
2297     and diffuse irradiance on the plane of array to the irradiance absorbed by
2298     a module's cells.
2299 
2300     The model is
2301     .. math::
2302 
2303         `Ee = f_1(AM_a) (E_b f_2(AOI) + f_d E_d)`
2304 
2305     where :math:`Ee` is effective irradiance (W/m2), :math:`f_1` is a fourth
2306     degree polynomial in air mass :math:`AM_a`, :math:`E_b` is beam (direct)
2307     irradiance on the plane of array, :math:`E_d` is diffuse irradiance on the
2308     plane of array, :math:`f_2` is a fifth degree polynomial in the angle of
2309     incidence :math:`AOI`, and :math:`f_d` is the fraction of diffuse
2310     irradiance on the plane of array that is not reflected away.
2311 
2312     References
2313     ----------
2314     .. [1] D. King et al, ""Sandia Photovoltaic Array Performance Model"",
2315        SAND2004-3535, Sandia National Laboratories, Albuquerque, NM
2316 
2317     See also
2318     --------
2319     pvlib.iam.sapm
2320     pvlib.spectrum.spectral_factor_sapm
2321     pvlib.pvsystem.sapm
2322     """"""
2323 
2324     F1 = spectrum.spectral_factor_sapm(airmass_absolute, module)
2325     F2 = iam.sapm(aoi, module)
2326 
2327     Ee = F1 * (poa_direct * F2 + module['FD'] * poa_diffuse)
2328 
2329     return Ee
2330 
2331 
2332 def singlediode(photocurrent, saturation_current, resistance_series,
2333                 resistance_shunt, nNsVth, ivcurve_pnts=None,
2334                 method='lambertw'):
2335     r""""""
2336     Solve the single diode equation to obtain a photovoltaic IV curve.
2337 
2338     Solves the single diode equation [1]_
2339 
2340     .. math::
2341 
2342         I = I_L -
2343             I_0 \left[
2344                 \exp \left(\frac{V+I R_s}{n N_s V_{th}} \right)-1
2345             \right] -
2346             \frac{V + I R_s}{R_{sh}}
2347 
2348     for :math:`I` and :math:`V` when given :math:`I_L, I_0, R_s, R_{sh},` and
2349     :math:`n N_s V_{th}` which are described later. The five points on the I-V
2350     curve specified in [3]_ are returned. If :math:`I_L, I_0, R_s, R_{sh},` and
2351     :math:`n N_s V_{th}` are all scalars, a single curve is returned. If any
2352     are array-like (of the same length), multiple IV curves are calculated.
2353 
2354     The input parameters can be calculated from meteorological data using a
2355     function for a single diode model, e.g.,
2356     :py:func:`~pvlib.pvsystem.calcparams_desoto`.
2357 
2358     Parameters
2359     ----------
2360     photocurrent : numeric
2361         Light-generated current :math:`I_L` (photocurrent)
2362         ``0 <= photocurrent``. [A]
2363 
2364     saturation_current : numeric
2365         Diode saturation :math:`I_0` current under desired IV curve
2366         conditions. ``0 < saturation_current``. [A]
2367 
2368     resistance_series : numeric
2369         Series resistance :math:`R_s` under desired IV curve conditions.
2370         ``0 <= resistance_series < numpy.inf``.  [ohm]
2371 
2372     resistance_shunt : numeric
2373         Shunt resistance :math:`R_{sh}` under desired IV curve conditions.
2374         ``0 < resistance_shunt <= numpy.inf``.  [ohm]
2375 
2376     nNsVth : numeric
2377         The product of three components: 1) the usual diode ideality factor
2378         :math:`n`, 2) the number of cells in series :math:`N_s`, and 3)
2379         the cell thermal voltage
2380         :math:`V_{th}`. The thermal voltage of the cell (in volts) may be
2381         calculated as :math:`k_B T_c / q`, where :math:`k_B` is
2382         Boltzmann's constant (J/K), :math:`T_c` is the temperature of the p-n
2383         junction in Kelvin, and :math:`q` is the charge of an electron
2384         (coulombs). ``0 < nNsVth``.  [V]
2385 
2386     ivcurve_pnts : None or int, default None
2387         Number of points in the desired IV curve. If None or 0, no points on
2388         the IV curves will be produced.
2389 
2390         .. deprecated:: 0.10.0
2391            Use :py:func:`pvlib.pvsystem.v_from_i` and
2392            :py:func:`pvlib.pvsystem.i_from_v` instead.
2393 
2394     method : str, default 'lambertw'
2395         Determines the method used to calculate points on the IV curve. The
2396         options are ``'lambertw'``, ``'newton'``, or ``'brentq'``.
2397 
2398     Returns
2399     -------
2400     dict or pandas.DataFrame
2401         The returned dict-like object always contains the keys/columns:
2402 
2403             * i_sc - short circuit current in amperes.
2404             * v_oc - open circuit voltage in volts.
2405             * i_mp - current at maximum power point in amperes.
2406             * v_mp - voltage at maximum power point in volts.
2407             * p_mp - power at maximum power point in watts.
2408             * i_x - current, in amperes, at ``v = 0.5*v_oc``.
2409             * i_xx - current, in amperes, at ``v = 0.5*(v_oc+v_mp)``.
2410 
2411         A dict is returned when the input parameters are scalars or
2412         ``ivcurve_pnts > 0``. If ``ivcurve_pnts > 0``, the output dictionary
2413         will also include the keys:
2414 
2415             * i - IV curve current in amperes.
2416             * v - IV curve voltage in volts.
2417 
2418     See also
2419     --------
2420     calcparams_desoto
2421     calcparams_cec
2422     calcparams_pvsyst
2423     sapm
2424     pvlib.singlediode.bishop88
2425 
2426     Notes
2427     -----
2428     If the method is ``'lambertw'`` then the solution employed to solve the
2429     implicit diode equation utilizes the Lambert W function to obtain an
2430     explicit function of :math:`V=f(I)` and :math:`I=f(V)` as shown in [2]_.
2431 
2432     If the method is ``'newton'`` then the root-finding Newton-Raphson method
2433     is used. It should be safe for well behaved IV-curves, but the ``'brentq'``
2434     method is recommended for reliability.
2435 
2436     If the method is ``'brentq'`` then Brent's bisection search method is used
2437     that guarantees convergence by bounding the voltage between zero and
2438     open-circuit.
2439 
2440     If the method is either ``'newton'`` or ``'brentq'`` and ``ivcurve_pnts``
2441     are indicated, then :func:`pvlib.singlediode.bishop88` [4]_ is used to
2442     calculate the points on the IV curve points at diode voltages from zero to
2443     open-circuit voltage with a log spacing that gets closer as voltage
2444     increases. If the method is ``'lambertw'`` then the calculated points on
2445     the IV curve are linearly spaced.
2446 
2447     References
2448     ----------
2449     .. [1] S.R. Wenham, M.A. Green, M.E. Watt, ""Applied Photovoltaics"" ISBN
2450        0 86758 909 4
2451 
2452     .. [2] A. Jain, A. Kapoor, ""Exact analytical solutions of the
2453        parameters of real solar cells using Lambert W-function"", Solar
2454        Energy Materials and Solar Cells, 81 (2004) 269-277.
2455 
2456     .. [3] D. King et al, ""Sandia Photovoltaic Array Performance Model"",
2457        SAND2004-3535, Sandia National Laboratories, Albuquerque, NM
2458 
2459     .. [4] ""Computer simulation of the effects of electrical mismatches in
2460        photovoltaic cell interconnection circuits"" JW Bishop, Solar Cell (1988)
2461        https://doi.org/10.1016/0379-6787(88)90059-2
2462     """"""
2463     if ivcurve_pnts:
2464         warn_deprecated('0.10.0', name='pvlib.pvsystem.singlediode',
2465                         alternative=('pvlib.pvsystem.v_from_i and '
2466                                      'pvlib.pvsystem.i_from_v'),
2467                         obj_type='parameter ivcurve_pnts',
2468                         removal='0.11.0')
2469     args = (photocurrent, saturation_current, resistance_series,
2470             resistance_shunt, nNsVth)  # collect args
2471     # Calculate points on the IV curve using the LambertW solution to the
2472     # single diode equation
2473     if method.lower() == 'lambertw':
2474         out = _singlediode._lambertw(*args, ivcurve_pnts)
2475         points = out[:7]
2476         if ivcurve_pnts:
2477             ivcurve_i, ivcurve_v = out[7:]
2478     else:
2479         # Calculate points on the IV curve using either 'newton' or 'brentq'
2480         # methods. Voltages are determined by first solving the single diode
2481         # equation for the diode voltage V_d then backing out voltage
2482         v_oc = _singlediode.bishop88_v_from_i(
2483             0.0, *args, method=method.lower()
2484         )
2485         i_mp, v_mp, p_mp = _singlediode.bishop88_mpp(
2486             *args, method=method.lower()
2487         )
2488         i_sc = _singlediode.bishop88_i_from_v(
2489             0.0, *args, method=method.lower()
2490         )
2491         i_x = _singlediode.bishop88_i_from_v(
2492             v_oc / 2.0, *args, method=method.lower()
2493         )
2494         i_xx = _singlediode.bishop88_i_from_v(
2495             (v_oc + v_mp) / 2.0, *args, method=method.lower()
2496         )
2497         points = i_sc, v_oc, i_mp, v_mp, p_mp, i_x, i_xx
2498 
2499         # calculate the IV curve if requested using bishop88
2500         if ivcurve_pnts:
2501             vd = v_oc * (
2502                 (11.0 - np.logspace(np.log10(11.0), 0.0, ivcurve_pnts)) / 10.0
2503             )
2504             ivcurve_i, ivcurve_v, _ = _singlediode.bishop88(vd, *args)
2505 
2506     columns = ('i_sc', 'v_oc', 'i_mp', 'v_mp', 'p_mp', 'i_x', 'i_xx')
2507 
2508     if all(map(np.isscalar, args)) or ivcurve_pnts:
2509         out = {c: p for c, p in zip(columns, points)}
2510 
2511         if ivcurve_pnts:
2512             out.update(i=ivcurve_i, v=ivcurve_v)
2513 
2514         return out
2515 
2516     points = np.atleast_1d(*points)  # convert scalars to 1d-arrays
2517     points = np.vstack(points).T  # collect rows into DataFrame columns
2518 
2519     # save the first available pd.Series index, otherwise set to None
2520     index = next((a.index for a in args if isinstance(a, pd.Series)), None)
2521 
2522     out = pd.DataFrame(points, columns=columns, index=index)
2523 
2524     return out
2525 
2526 
2527 def max_power_point(photocurrent, saturation_current, resistance_series,
2528                     resistance_shunt, nNsVth, d2mutau=0, NsVbi=np.Inf,
2529                     method='brentq'):
2530     """"""
2531     Given the single diode equation coefficients, calculates the maximum power
2532     point (MPP).
2533 
2534     Parameters
2535     ----------
2536     photocurrent : numeric
2537         photo-generated current [A]
2538     saturation_current : numeric
2539         diode reverse saturation current [A]
2540     resistance_series : numeric
2541         series resitance [ohms]
2542     resistance_shunt : numeric
2543         shunt resitance [ohms]
2544     nNsVth : numeric
2545         product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,
2546         and number of serices cells ``Ns``
2547     d2mutau : numeric, default 0
2548         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon
2549         (a-Si) modules that accounts for recombination current in the
2550         intrinsic layer. The value is the ratio of intrinsic layer thickness
2551         squared :math:`d^2` to the diffusion length of charge carriers
2552         :math:`\\mu \\tau`. [V]
2553     NsVbi : numeric, default np.inf
2554         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon
2555         (a-Si) modules that is the product of the PV module number of series
2556         cells ``Ns`` and the builtin voltage ``Vbi`` of the intrinsic layer.
2557         [V].
2558     method : str
2559         either ``'newton'`` or ``'brentq'``
2560 
2561     Returns
2562     -------
2563     OrderedDict or pandas.DataFrame
2564         ``(i_mp, v_mp, p_mp)``
2565 
2566     Notes
2567     -----
2568     Use this function when you only want to find the maximum power point. Use
2569     :func:`singlediode` when you need to find additional points on the IV
2570     curve. This function uses Brent's method by default because it is
2571     guaranteed to converge.
2572     """"""
2573     i_mp, v_mp, p_mp = _singlediode.bishop88_mpp(
2574         photocurrent, saturation_current, resistance_series,
2575         resistance_shunt, nNsVth, d2mutau, NsVbi, method=method.lower()
2576     )
2577     if isinstance(photocurrent, pd.Series):
2578         ivp = {'i_mp': i_mp, 'v_mp': v_mp, 'p_mp': p_mp}
2579         out = pd.DataFrame(ivp, index=photocurrent.index)
2580     else:
2581         out = OrderedDict()
2582         out['i_mp'] = i_mp
2583         out['v_mp'] = v_mp
2584         out['p_mp'] = p_mp
2585     return out
2586 
2587 
2588 def v_from_i(current, photocurrent, saturation_current, resistance_series,
2589              resistance_shunt, nNsVth, method='lambertw'):
2590     '''
2591     Device voltage at the given device current for the single diode model.
2592 
2593     Uses the single diode model (SDM) as described in, e.g.,
2594     Jain and Kapoor 2004 [1]_.
2595     The solution is per Eq 3 of [1]_ except when resistance_shunt=numpy.inf,
2596     in which case the explict solution for voltage is used.
2597     Ideal device parameters are specified by resistance_shunt=np.inf and
2598     resistance_series=0.
2599     Inputs to this function can include scalars and pandas.Series, but it is
2600     the caller's responsibility to ensure that the arguments are all float64
2601     and within the proper ranges.
2602 
2603     .. versionchanged:: 0.10.0
2604        The function's arguments have been reordered.
2605 
2606     Parameters
2607     ----------
2608     current : numeric
2609         The current in amperes under desired IV curve conditions.
2610 
2611     photocurrent : numeric
2612         Light-generated current (photocurrent) in amperes under desired
2613         IV curve conditions. Often abbreviated ``I_L``.
2614         0 <= photocurrent
2615 
2616     saturation_current : numeric
2617         Diode saturation current in amperes under desired IV curve
2618         conditions. Often abbreviated ``I_0``.
2619         0 < saturation_current
2620 
2621     resistance_series : numeric
2622         Series resistance in ohms under desired IV curve conditions.
2623         Often abbreviated ``Rs``.
2624         0 <= resistance_series < numpy.inf
2625 
2626     resistance_shunt : numeric
2627         Shunt resistance in ohms under desired IV curve conditions.
2628         Often abbreviated ``Rsh``.
2629         0 < resistance_shunt <= numpy.inf
2630 
2631     nNsVth : numeric
2632         The product of three components. 1) The usual diode ideal factor
2633         (n), 2) the number of cells in series (Ns), and 3) the cell
2634         thermal voltage under the desired IV curve conditions (Vth). The
2635         thermal voltage of the cell (in volts) may be calculated as
2636         ``k*temp_cell/q``, where k is Boltzmann's constant (J/K),
2637         temp_cell is the temperature of the p-n junction in Kelvin, and
2638         q is the charge of an electron (coulombs).
2639         0 < nNsVth
2640 
2641     method : str
2642         Method to use: ``'lambertw'``, ``'newton'``, or ``'brentq'``. *Note*:
2643         ``'brentq'`` is limited to 1st quadrant only.
2644 
2645     Returns
2646     -------
2647     current : np.ndarray or scalar
2648 
2649     References
2650     ----------
2651     .. [1] A. Jain, A. Kapoor, ""Exact analytical solutions of the
2652        parameters of real solar cells using Lambert W-function"", Solar
2653        Energy Materials and Solar Cells, 81 (2004) 269-277.
2654     '''
2655     args = (current, photocurrent, saturation_current,
2656             resistance_series, resistance_shunt, nNsVth)
2657     if method.lower() == 'lambertw':
2658         return _singlediode._lambertw_v_from_i(*args)
2659     else:
2660         # Calculate points on the IV curve using either 'newton' or 'brentq'
2661         # methods. Voltages are determined by first solving the single diode
2662         # equation for the diode voltage V_d then backing out voltage
2663         V = _singlediode.bishop88_v_from_i(*args, method=method.lower())
2664         if all(map(np.isscalar, args)):
2665             return V
2666         shape = _singlediode._shape_of_max_size(*args)
2667         return np.broadcast_to(V, shape)
2668 
2669 
2670 def i_from_v(voltage, photocurrent, saturation_current, resistance_series,
2671              resistance_shunt, nNsVth, method='lambertw'):
2672     '''
2673     Device current at the given device voltage for the single diode model.
2674 
2675     Uses the single diode model (SDM) as described in, e.g.,
2676     Jain and Kapoor 2004 [1]_.
2677     The solution is per Eq 2 of [1] except when resistance_series=0,
2678     in which case the explict solution for current is used.
2679     Ideal device parameters are specified by resistance_shunt=np.inf and
2680     resistance_series=0.
2681     Inputs to this function can include scalars and pandas.Series, but it is
2682     the caller's responsibility to ensure that the arguments are all float64
2683     and within the proper ranges.
2684 
2685     .. versionchanged:: 0.10.0
2686        The function's arguments have been reordered.
2687 
2688     Parameters
2689     ----------
2690     voltage : numeric
2691         The voltage in Volts under desired IV curve conditions.
2692 
2693     photocurrent : numeric
2694         Light-generated current (photocurrent) in amperes under desired
2695         IV curve conditions. Often abbreviated ``I_L``.
2696         0 <= photocurrent
2697 
2698     saturation_current : numeric
2699         Diode saturation current in amperes under desired IV curve
2700         conditions. Often abbreviated ``I_0``.
2701         0 < saturation_current
2702 
2703     resistance_series : numeric
2704         Series resistance in ohms under desired IV curve conditions.
2705         Often abbreviated ``Rs``.
2706         0 <= resistance_series < numpy.inf
2707 
2708     resistance_shunt : numeric
2709         Shunt resistance in ohms under desired IV curve conditions.
2710         Often abbreviated ``Rsh``.
2711         0 < resistance_shunt <= numpy.inf
2712 
2713     nNsVth : numeric
2714         The product of three components. 1) The usual diode ideal factor
2715         (n), 2) the number of cells in series (Ns), and 3) the cell
2716         thermal voltage under the desired IV curve conditions (Vth). The
2717         thermal voltage of the cell (in volts) may be calculated as
2718         ``k*temp_cell/q``, where k is Boltzmann's constant (J/K),
2719         temp_cell is the temperature of the p-n junction in Kelvin, and
2720         q is the charge of an electron (coulombs).
2721         0 < nNsVth
2722 
2723     method : str
2724         Method to use: ``'lambertw'``, ``'newton'``, or ``'brentq'``. *Note*:
2725         ``'brentq'`` is limited to 1st quadrant only.
2726 
2727     Returns
2728     -------
2729     current : np.ndarray or scalar
2730 
2731     References
2732     ----------
2733     .. [1] A. Jain, A. Kapoor, ""Exact analytical solutions of the
2734        parameters of real solar cells using Lambert W-function"", Solar
2735        Energy Materials and Solar Cells, 81 (2004) 269-277.
2736     '''
2737     args = (voltage, photocurrent, saturation_current,
2738             resistance_series, resistance_shunt, nNsVth)
2739     if method.lower() == 'lambertw':
2740         return _singlediode._lambertw_i_from_v(*args)
2741     else:
2742         # Calculate points on the IV curve using either 'newton' or 'brentq'
2743         # methods. Voltages are determined by first solving the single diode
2744         # equation for the diode voltage V_d then backing out voltage
2745         current = _singlediode.bishop88_i_from_v(*args, method=method.lower())
2746         if all(map(np.isscalar, args)):
2747             return current
2748         shape = _singlediode._shape_of_max_size(*args)
2749         return np.broadcast_to(current, shape)
2750 
2751 
2752 def scale_voltage_current_power(data, voltage=1, current=1):
2753     """"""
2754     Scales the voltage, current, and power in data by the voltage
2755     and current factors.
2756 
2757     Parameters
2758     ----------
2759     data: DataFrame
2760         May contain columns `'v_mp', 'v_oc', 'i_mp' ,'i_x', 'i_xx',
2761         'i_sc', 'p_mp'`.
2762     voltage: numeric, default 1
2763         The amount by which to multiply the voltages.
2764     current: numeric, default 1
2765         The amount by which to multiply the currents.
2766 
2767     Returns
2768     -------
2769     scaled_data: DataFrame
2770         A scaled copy of the input data.
2771         `'p_mp'` is scaled by `voltage * current`.
2772     """"""
2773 
2774     # as written, only works with a DataFrame
2775     # could make it work with a dict, but it would be more verbose
2776     voltage_keys = ['v_mp', 'v_oc']
2777     current_keys = ['i_mp', 'i_x', 'i_xx', 'i_sc']
2778     power_keys = ['p_mp']
2779     voltage_df = data.filter(voltage_keys, axis=1) * voltage
2780     current_df = data.filter(current_keys, axis=1) * current
2781     power_df = data.filter(power_keys, axis=1) * voltage * current
2782     df = pd.concat([voltage_df, current_df, power_df], axis=1)
2783     df_sorted = df[data.columns]  # retain original column order
2784     return df_sorted
2785 
2786 
2787 def pvwatts_dc(g_poa_effective, temp_cell, pdc0, gamma_pdc, temp_ref=25.):
2788     r""""""
2789     Implements NREL's PVWatts DC power model. The PVWatts DC model [1]_ is:
2790 
2791     .. math::
2792 
2793         P_{dc} = \frac{G_{poa eff}}{1000} P_{dc0} ( 1 + \gamma_{pdc} (T_{cell} - T_{ref}))
2794 
2795     Note that ``pdc0`` is also used as a symbol in
2796     :py:func:`pvlib.inverter.pvwatts`. ``pdc0`` in this function refers to the DC
2797     power of the modules at reference conditions. ``pdc0`` in
2798     :py:func:`pvlib.inverter.pvwatts` refers to the DC power input limit of
2799     the inverter.
2800 
2801     Parameters
2802     ----------
2803     g_poa_effective: numeric
2804         Irradiance transmitted to the PV cells. To be
2805         fully consistent with PVWatts, the user must have already
2806         applied angle of incidence losses, but not soiling, spectral,
2807         etc. [W/m^2]
2808     temp_cell: numeric
2809         Cell temperature [C].
2810     pdc0: numeric
2811         Power of the modules at 1000 W/m^2 and cell reference temperature. [W]
2812     gamma_pdc: numeric
2813         The temperature coefficient of power. Typically -0.002 to
2814         -0.005 per degree C. [1/C]
2815     temp_ref: numeric, default 25.0
2816         Cell reference temperature. PVWatts defines it to be 25 C and
2817         is included here for flexibility. [C]
2818 
2819     Returns
2820     -------
2821     pdc: numeric
2822         DC power. [W]
2823 
2824     References
2825     ----------
2826     .. [1] A. P. Dobos, ""PVWatts Version 5 Manual""
2827            http://pvwatts.nrel.gov/downloads/pvwattsv5.pdf
2828            (2014).
2829     """"""  # noqa: E501
2830 
2831     pdc = (g_poa_effective * 0.001 * pdc0 *
2832            (1 + gamma_pdc * (temp_cell - temp_ref)))
2833 
2834     return pdc
2835 
2836 
2837 def pvwatts_losses(soiling=2, shading=3, snow=0, mismatch=2, wiring=2,
2838                    connections=0.5, lid=1.5, nameplate_rating=1, age=0,
2839                    availability=3):
2840     r""""""
2841     Implements NREL's PVWatts system loss model.
2842     The PVWatts loss model [1]_ is:
2843 
2844     .. math::
2845 
2846         L_{total}(\%) = 100 [ 1 - \Pi_i ( 1 - \frac{L_i}{100} ) ]
2847 
2848     All parameters must be in units of %. Parameters may be
2849     array-like, though all array sizes must match.
2850 
2851     Parameters
2852     ----------
2853     soiling: numeric, default 2
2854     shading: numeric, default 3
2855     snow: numeric, default 0
2856     mismatch: numeric, default 2
2857     wiring: numeric, default 2
2858     connections: numeric, default 0.5
2859     lid: numeric, default 1.5
2860         Light induced degradation
2861     nameplate_rating: numeric, default 1
2862     age: numeric, default 0
2863     availability: numeric, default 3
2864 
2865     Returns
2866     -------
2867     losses: numeric
2868         System losses in units of %.
2869 
2870     References
2871     ----------
2872     .. [1] A. P. Dobos, ""PVWatts Version 5 Manual""
2873            http://pvwatts.nrel.gov/downloads/pvwattsv5.pdf
2874            (2014).
2875     """"""
2876 
2877     params = [soiling, shading, snow, mismatch, wiring, connections, lid,
2878               nameplate_rating, age, availability]
2879 
2880     # manually looping over params allows for numpy/pandas to handle any
2881     # array-like broadcasting that might be necessary.
2882     perf = 1
2883     for param in params:
2884         perf *= 1 - param/100
2885 
2886     losses = (1 - perf) * 100.
2887 
2888     return losses
2889 
2890 
2891 def dc_ohms_from_percent(vmp_ref, imp_ref, dc_ohmic_percent,
2892                          modules_per_string=1,
2893                          strings=1):
2894     """"""
2895     Calculates the equivalent resistance of the wires from a percent
2896     ohmic loss at STC.
2897 
2898     Equivalent resistance is calculated with the function:
2899 
2900     .. math::
2901         Rw = (L_{stc} / 100) * (Varray / Iarray)
2902 
2903     :math:`Rw` is the equivalent resistance in ohms
2904     :math:`Varray` is the Vmp of the modules times modules per string
2905     :math:`Iarray` is the Imp of the modules times strings per array
2906     :math:`L_{stc}` is the input dc loss percent
2907 
2908     Parameters
2909     ----------
2910     vmp_ref: numeric
2911         Voltage at maximum power in reference conditions [V]
2912     imp_ref: numeric
2913         Current at maximum power in reference conditions [V]
2914     dc_ohmic_percent: numeric, default 0
2915         input dc loss as a percent, e.g. 1.5% loss is input as 1.5
2916     modules_per_string: int, default 1
2917         Number of modules per string in the array.
2918     strings: int, default 1
2919         Number of parallel strings in the array.
2920 
2921     Returns
2922     ----------
2923     Rw: numeric
2924         Equivalent resistance [ohm]
2925 
2926     See Also
2927     --------
2928     pvlib.pvsystem.dc_ohmic_losses
2929 
2930     References
2931     ----------
2932     .. [1] PVsyst 7 Help. ""Array ohmic wiring loss"".
2933        https://www.pvsyst.com/help/ohmic_loss.htm
2934     """"""
2935     vmp = modules_per_string * vmp_ref
2936 
2937     imp = strings * imp_ref
2938 
2939     Rw = (dc_ohmic_percent / 100) * (vmp / imp)
2940 
2941     return Rw
2942 
2943 
2944 def dc_ohmic_losses(resistance, current):
2945     """"""
2946     Returns ohmic losses in units of power from the equivalent
2947     resistance of the wires and the operating current.
2948 
2949     Parameters
2950     ----------
2951     resistance: numeric
2952         Equivalent resistance of wires [ohm]
2953     current: numeric, float or array-like
2954         Operating current [A]
2955 
2956     Returns
2957     ----------
2958     loss: numeric
2959         Power Loss [W]
2960 
2961     See Also
2962     --------
2963     pvlib.pvsystem.dc_ohms_from_percent
2964 
2965     References
2966     ----------
2967     .. [1] PVsyst 7 Help. ""Array ohmic wiring loss"".
2968        https://www.pvsyst.com/help/ohmic_loss.htm
2969     """"""
2970     return resistance * current * current
2971 
2972 
2973 def combine_loss_factors(index, *losses, fill_method='ffill'):
2974     r""""""
2975     Combines Series loss fractions while setting a common index.
2976 
2977     The separate losses are compounded using the following equation:
2978 
2979     .. math::
2980 
2981         L_{total} = 1 - [ 1 - \Pi_i ( 1 - L_i ) ]
2982 
2983     :math:`L_{total}` is the total loss returned
2984     :math:`L_i` is each individual loss factor input
2985 
2986     Note the losses must each be a series with a DatetimeIndex.
2987     All losses will be resampled to match the index parameter using
2988     the fill method specified (defaults to ""fill forward"").
2989 
2990     Parameters
2991     ----------
2992     index : DatetimeIndex
2993         The index of the returned loss factors
2994 
2995     *losses : Series
2996         One or more Series of fractions to be compounded
2997 
2998     fill_method : {'ffill', 'bfill', 'nearest'}, default 'ffill'
2999         Method to use for filling holes in reindexed DataFrame
3000 
3001     Returns
3002     -------
3003     Series
3004         Fractions resulting from the combination of each loss factor
3005     """"""
3006     combined_factor = 1
3007 
3008     for loss in losses:
3009         loss = loss.reindex(index, method=fill_method)
3010         combined_factor *= (1 - loss)
3011 
3012     return 1 - combined_factor
3013 
[end of pvlib/pvsystem.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pvlib/pvlib-python,27a3a07ebc84b11014d3753e4923902adf9a38c0,"PVSystem with single Array generates an error
**Is your feature request related to a problem? Please describe.**

When a PVSystem has a single Array, you can't assign just the Array instance when constructing the PVSystem.

```
mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)
array = pvlib.pvsystem.Array(mount=mount)
pv = pvlib.pvsystem.PVSystem(arrays=array)

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-13-f5424e3db16a> in <module>
      3 mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)
      4 array = pvlib.pvsystem.Array(mount=mount)
----> 5 pv = pvlib.pvsystem.PVSystem(arrays=array)

~\anaconda3\lib\site-packages\pvlib\pvsystem.py in __init__(self, arrays, surface_tilt, surface_azimuth, albedo, surface_type, module, module_type, module_parameters, temperature_model_parameters, modules_per_string, strings_per_inverter, inverter, inverter_parameters, racking_model, losses_parameters, name)
    251                 array_losses_parameters,
    252             ),)
--> 253         elif len(arrays) == 0:
    254             raise ValueError(""PVSystem must have at least one Array. ""
    255                              ""If you want to create a PVSystem instance ""

TypeError: object of type 'Array' has no len()

```

Not a bug per se, since the PVSystem docstring requests that `arrays` be iterable. Still, a bit inconvenient to have to do this

```
mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)
array = pvlib.pvsystem.Array(mount=mount)
pv = pvlib.pvsystem.PVSystem(arrays=[array])
```

**Describe the solution you'd like**
Handle `arrays=array` where `array` is an instance of `Array`

**Describe alternatives you've considered**
Status quo - either make the single Array into a list, or use the PVSystem kwargs.

",,2023-09-13T17:25:47Z,"<patch>
diff --git a/pvlib/pvsystem.py b/pvlib/pvsystem.py
--- a/pvlib/pvsystem.py
+++ b/pvlib/pvsystem.py
@@ -101,10 +101,11 @@ class PVSystem:
 
     Parameters
     ----------
-    arrays : iterable of Array, optional
-        List of arrays that are part of the system. If not specified
-        a single array is created from the other parameters (e.g.
-        `surface_tilt`, `surface_azimuth`). Must contain at least one Array,
+    arrays : Array or iterable of Array, optional
+        An Array or list of arrays that are part of the system. If not
+        specified a single array is created from the other parameters (e.g.
+        `surface_tilt`, `surface_azimuth`). If specified as a list, the list
+        must contain at least one Array;
         if length of arrays is 0 a ValueError is raised. If `arrays` is
         specified the following PVSystem parameters are ignored:
 
@@ -220,6 +221,8 @@ def __init__(self,
                 strings_per_inverter,
                 array_losses_parameters,
             ),)
+        elif isinstance(arrays, Array):
+            self.arrays = (arrays,)
         elif len(arrays) == 0:
             raise ValueError(""PVSystem must have at least one Array. ""
                              ""If you want to create a PVSystem instance ""

</patch>","diff --git a/pvlib/tests/test_pvsystem.py b/pvlib/tests/test_pvsystem.py
--- a/pvlib/tests/test_pvsystem.py
+++ b/pvlib/tests/test_pvsystem.py
@@ -1887,8 +1887,6 @@ def test_PVSystem_multiple_array_creation():
     assert pv_system.arrays[0].module_parameters == {}
     assert pv_system.arrays[1].module_parameters == {'pdc0': 1}
     assert pv_system.arrays == (array_one, array_two)
-    with pytest.raises(TypeError):
-        pvsystem.PVSystem(arrays=array_one)
 
 
 def test_PVSystem_get_aoi():
@@ -2362,6 +2360,14 @@ def test_PVSystem_at_least_one_array():
         pvsystem.PVSystem(arrays=[])
 
 
+def test_PVSystem_single_array():
+    # GH 1831
+    single_array = pvsystem.Array(pvsystem.FixedMount())
+    system = pvsystem.PVSystem(arrays=single_array)
+    assert isinstance(system.arrays, tuple)
+    assert system.arrays[0] is single_array
+
+
 def test_combine_loss_factors():
     test_index = pd.date_range(start='1990/01/01T12:00', periods=365, freq='D')
     loss_1 = pd.Series(.10, index=test_index)
",0.9,"[""pvlib/tests/test_pvsystem.py::test_PVSystem_single_array""]","[""pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam[ashrae-model_params0]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam[physical-model_params1]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam[martin_ruiz-model_params2]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_get_iam"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam_sapm"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam_interp"", ""pvlib/tests/test_pvsystem.py::test__normalize_sam_product_names"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_iam_invalid"", ""pvlib/tests/test_pvsystem.py::test_retrieve_sam_raise_no_parameters"", ""pvlib/tests/test_pvsystem.py::test_retrieve_sam_cecmod"", ""pvlib/tests/test_pvsystem.py::test_retrieve_sam_cecinverter"", ""pvlib/tests/test_pvsystem.py::test_sapm"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_sapm"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm"", ""pvlib/tests/test_pvsystem.py::test_sapm_spectral_loss_deprecated"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_spectral_loss"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm_spectral_loss"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_first_solar_spectral_loss[module_parameters0-multisi-None]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_first_solar_spectral_loss[module_parameters1-multisi-None]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_first_solar_spectral_loss[module_parameters2-None-coefficients2]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_first_solar_spectral_loss"", ""pvlib/tests/test_pvsystem.py::test_sapm_effective_irradiance[test_input0-1140.0510967821876]"", ""pvlib/tests/test_pvsystem.py::test_sapm_effective_irradiance[test_input1-expected1]"", ""pvlib/tests/test_pvsystem.py::test_sapm_effective_irradiance[test_input2-expected2]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_effective_irradiance"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm_effective_irradiance"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_effective_irradiance_value_error[20-poa_diffuse0-aoi0]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_effective_irradiance_value_error[poa_direct1-poa_diffuse1-aoi1]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_effective_irradiance_value_error[poa_direct2-poa_diffuse2-20]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_celltemp"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_sapm_celltemp_kwargs"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_sapm_celltemp_different_arrays"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_pvsyst_celltemp"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_faiman_celltemp"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_noct_celltemp"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_noct_celltemp_error"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_functions[faiman]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_functions[pvsyst]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_functions[sapm]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_functions[fuentes]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_functions[noct_sam]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_multi_temp[faiman]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_multi_temp[pvsyst]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_multi_temp[sapm]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_multi_temp[fuentes]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_multi_temp[noct_sam]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_multi_wind[faiman]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_multi_wind[pvsyst]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_multi_wind[sapm]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_multi_wind[fuentes]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_multi_wind[noct_sam]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_cell_temperature_invalid"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_temp_too_short[faiman]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_temp_too_short[pvsyst]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_temp_too_short[sapm]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_temp_too_short[fuentes]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_temp_too_short[noct_sam]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_temp_too_long[faiman]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_temp_too_long[pvsyst]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_temp_too_long[sapm]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_temp_too_long[fuentes]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_temp_too_long[noct_sam]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_wind_too_short[faiman]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_wind_too_short[pvsyst]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_wind_too_short[sapm]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_wind_too_short[fuentes]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_wind_too_short[noct_sam]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_wind_too_long[faiman]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_wind_too_long[pvsyst]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_wind_too_long[sapm]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_wind_too_long[fuentes]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_wind_too_long[noct_sam]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_poa_length_mismatch[faiman]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_poa_length_mismatch[pvsyst]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_poa_length_mismatch[sapm]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_poa_length_mismatch[fuentes]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_celltemp_poa_length_mismatch[noct_sam]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_fuentes_celltemp"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_fuentes_module_height"", ""pvlib/tests/test_pvsystem.py::test_Array__infer_temperature_model_params"", ""pvlib/tests/test_pvsystem.py::test_Array__infer_cell_type"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs0]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs1]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs2]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs3]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs4]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs5]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs6]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs7]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs8]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs9]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs10]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs11]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs12]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs13]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs14]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_returns_correct_Python_type[numeric_type_funcs15]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs0]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs1]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs2]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs3]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs4]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs5]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs6]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs7]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs8]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs9]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs10]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs11]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs12]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs13]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs14]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_returns_correct_Python_type[numeric_type_funcs15]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs0]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs1]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs2]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs3]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs4]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs5]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs6]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs7]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs8]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs9]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs10]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs11]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs12]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs13]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs14]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_returns_correct_Python_type[numeric_type_funcs15]"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto_all_scalars"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_all_scalars"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst_all_scalars"", ""pvlib/tests/test_pvsystem.py::test_calcparams_desoto"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec"", ""pvlib/tests/test_pvsystem.py::test_calcparams_cec_extra_params_propagation"", ""pvlib/tests/test_pvsystem.py::test_calcparams_pvsyst"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_calcparams_desoto"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_calcparams_pvsyst"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_calcparams[calcparams_pvsyst]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_calcparams[calcparams_desoto]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_calcparams[calcparams_cec]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_calcparams_value_error[calcparams_desoto-1-celltemp0]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_calcparams_value_error[calcparams_desoto-irrad1-1]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_calcparams_value_error[calcparams_cec-1-celltemp2]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_calcparams_value_error[calcparams_cec-irrad3-1]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_calcparams_value_error[calcparams_pvsyst-1-celltemp4]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_calcparams_value_error[calcparams_pvsyst-irrad5-1]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i0-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i0-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i0-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i1-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i1-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i1-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i2-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i2-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i2-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i3-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i3-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i3-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i4-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i4-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i4-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i5-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i5-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i5-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i6-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i6-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i6-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i7-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i7-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i7-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i8-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i8-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i8-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i9-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i9-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i9-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i10-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i10-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_v_from_i[fixture_v_from_i10-newton-1e-08]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i0]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i1]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i2]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i3]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i4]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i5]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i6]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i7]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i8]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i9]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_from_i[fixture_v_from_i10]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v0-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v0-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v0-newton-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v1-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v1-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v1-newton-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v2-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v2-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v2-newton-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v3-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v3-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v3-newton-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v4-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v4-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v4-newton-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v5-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v5-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v5-newton-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v6-lambertw-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v6-brentq-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_i_from_v[fixture_i_from_v6-newton-1e-11]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_i_from_v"", ""pvlib/tests/test_pvsystem.py::test_i_from_v_size"", ""pvlib/tests/test_pvsystem.py::test_v_from_i_size"", ""pvlib/tests/test_pvsystem.py::test_mpp_floats"", ""pvlib/tests/test_pvsystem.py::test_mpp_recombination"", ""pvlib/tests/test_pvsystem.py::test_mpp_array"", ""pvlib/tests/test_pvsystem.py::test_mpp_series"", ""pvlib/tests/test_pvsystem.py::test_singlediode_series"", ""pvlib/tests/test_pvsystem.py::test_singlediode_array"", ""pvlib/tests/test_pvsystem.py::test_singlediode_floats"", ""pvlib/tests/test_pvsystem.py::test_singlediode_floats_ivcurve"", ""pvlib/tests/test_pvsystem.py::test_singlediode_series_ivcurve"", ""pvlib/tests/test_pvsystem.py::test_singlediode_ivcurvepnts_deprecation_warning[lambertw]"", ""pvlib/tests/test_pvsystem.py::test_singlediode_ivcurvepnts_deprecation_warning[brentq]"", ""pvlib/tests/test_pvsystem.py::test_singlediode_ivcurvepnts_deprecation_warning[newton]"", ""pvlib/tests/test_pvsystem.py::test_scale_voltage_current_power"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_scale_voltage_current_power"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_scale_voltage_current_power"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_sandia"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_sandia_multi"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_pvwatts"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_pvwatts_kwargs"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_pvwatts_multi"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_single_array_tuple_input[sandia]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_single_array_tuple_input[adr]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_single_array_tuple_input[pvwatts]"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_adr"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_adr_multi"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_ac_invalid"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_creation"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_creation"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_aoi"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_get_aoi"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_irradiance"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_irradiance_albedo"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_get_irradiance_model"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_get_irradiance"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array_get_irradiance_multi_irrad"", ""pvlib/tests/test_pvsystem.py::test_Array_get_irradiance"", ""pvlib/tests/test_pvsystem.py::test_PVSystem___repr__"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multi_array___repr__"", ""pvlib/tests/test_pvsystem.py::test_Array___repr__"", ""pvlib/tests/test_pvsystem.py::test_pvwatts_dc_scalars"", ""pvlib/tests/test_pvsystem.py::test_pvwatts_dc_arrays"", ""pvlib/tests/test_pvsystem.py::test_pvwatts_dc_series"", ""pvlib/tests/test_pvsystem.py::test_pvwatts_losses_default"", ""pvlib/tests/test_pvsystem.py::test_pvwatts_losses_arrays"", ""pvlib/tests/test_pvsystem.py::test_pvwatts_losses_series"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_pvwatts_dc"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_pvwatts_dc_kwargs"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_pvwatts_dc"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_multiple_array_pvwatts_dc_value_error"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_pvwatts_losses"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_num_arrays"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_at_least_one_array"", ""pvlib/tests/test_pvsystem.py::test_combine_loss_factors"", ""pvlib/tests/test_pvsystem.py::test_no_extra_kwargs"", ""pvlib/tests/test_pvsystem.py::test_AbstractMount_constructor"", ""pvlib/tests/test_pvsystem.py::test_FixedMount_constructor"", ""pvlib/tests/test_pvsystem.py::test_FixedMount_get_orientation"", ""pvlib/tests/test_pvsystem.py::test_SingleAxisTrackerMount_constructor"", ""pvlib/tests/test_pvsystem.py::test_SingleAxisTrackerMount_get_orientation"", ""pvlib/tests/test_pvsystem.py::test_dc_ohms_from_percent"", ""pvlib/tests/test_pvsystem.py::test_PVSystem_dc_ohms_from_percent"", ""pvlib/tests/test_pvsystem.py::test_dc_ohmic_losses"", ""pvlib/tests/test_pvsystem.py::test_Array_dc_ohms_from_percent"", ""pvlib/tests/test_pvsystem.py::test_Array_temperature_missing_parameters[sapm-keys0]"", ""pvlib/tests/test_pvsystem.py::test_Array_temperature_missing_parameters[fuentes-keys1]"", ""pvlib/tests/test_pvsystem.py::test_Array_temperature_missing_parameters[noct_sam-keys2]""]",6072e0982c3c0236f532ddfa48fbf461180d834e
pvlib__pvlib-python-1154,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
pvlib.irradiance.reindl() model generates NaNs when GHI = 0
**Describe the bug**
The reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to ""term3"" having a quotient that divides by GHI.  

**Expected behavior**
The reindl function should result in zero sky diffuse when GHI is zero.


pvlib.irradiance.reindl() model generates NaNs when GHI = 0
**Describe the bug**
The reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to ""term3"" having a quotient that divides by GHI.  

**Expected behavior**
The reindl function should result in zero sky diffuse when GHI is zero.



</issue>
<code>
[start of README.md]
1 <img src=""docs/sphinx/source/_images/pvlib_logo_horiz.png"" width=""600"">
2 
3 <table>
4 <tr>
5   <td>Latest Release</td>
6   <td>
7     <a href=""https://pypi.org/project/pvlib/"">
8     <img src=""https://img.shields.io/pypi/v/pvlib.svg"" alt=""latest release"" />
9     </a>
10     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
11     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/version.svg"" />
12     </a>
13     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
14     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg"" />
15     </a>
16 </tr>
17 <tr>
18   <td>License</td>
19   <td>
20     <a href=""https://github.com/pvlib/pvlib-python/blob/master/LICENSE"">
21     <img src=""https://img.shields.io/pypi/l/pvlib.svg"" alt=""license"" />
22     </a>
23 </td>
24 </tr>
25 <tr>
26   <td>Build Status</td>
27   <td>
28     <a href=""http://pvlib-python.readthedocs.org/en/stable/"">
29     <img src=""https://readthedocs.org/projects/pvlib-python/badge/?version=stable"" alt=""documentation build status"" />
30     </a>
31     <a href=""https://dev.azure.com/solararbiter/pvlib%20python/_build/latest?definitionId=4&branchName=master"">
32       <img src=""https://dev.azure.com/solararbiter/pvlib%20python/_apis/build/status/pvlib.pvlib-python?branchName=master"" alt=""Azure Pipelines build status"" />
33     </a>
34   </td>
35 </tr>
36 <tr>
37   <td>Code Quality</td>
38  Â <td>
39     <a href=""https://lgtm.com/projects/g/pvlib/pvlib-python/context:python"">
40     <img src=""https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18"" alt=""lgtm quality grade"" />
41     </a>
42     <a href=""https://lgtm.com/projects/g/pvlib/pvlib-python/alerts"">
43     <img src=""https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18"" alt=""lgtm alters"" />
44     </a>
45   </td>
46 </tr>
47 <tr>
48   <td>Coverage</td>
49  Â <td>
50     <a href=""https://coveralls.io/r/pvlib/pvlib-python"">
51     <img src=""https://img.shields.io/coveralls/pvlib/pvlib-python.svg"" alt=""coveralls coverage"" />
52     </a>
53     <a href=""https://codecov.io/gh/pvlib/pvlib-python"">
54     <img src=""https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg"" alt=""codecov coverage"" />
55     </a>
56   </td>
57 </tr>
58 <tr>
59   <td>Publications</td>
60   <td>
61     <a href=""https://doi.org/10.5281/zenodo.3762635"">
62     <img src=""https://zenodo.org/badge/DOI/10.5281/zenodo.3762635.svg"" alt=""zenodo reference"">
63     </a>
64     <a href=""http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1"">
65     <img src=""http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg"" alt=""JOSS reference"" />
66     </a>
67   </td>
68 </tr>
69 <tr>
70   <td>Downloads</td>
71   <td>
72     <a href=""https://pypi.org/project/pvlib/"">
73     <img src=""https://img.shields.io/pypi/dm/pvlib"" alt=""PyPI downloads"" />
74     </a>
75     <a href=""https://anaconda.org/conda-forge/pvlib-python"">
76     <img src=""https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg"" alt=""conda-forge downloads"" />
77     </a>
78   </td>
79 </tr>
80 </table>
81 
82 
83 pvlib python is a community supported tool that provides a set of
84 functions and classes for simulating the performance of photovoltaic
85 energy systems. pvlib python was originally ported from the PVLIB MATLAB
86 toolbox developed at Sandia National Laboratories and it implements many
87 of the models and methods developed at the Labs. More information on
88 Sandia Labs PV performance modeling programs can be found at
89 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,
90 but operate independently of it.
91 
92 
93 Documentation
94 =============
95 
96 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/).
97 
98 
99 Installation
100 ============
101 
102 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.
103 Please see the [Installation page](http://pvlib-python.readthedocs.io/en/stable/installation.html) of the documentation for complete instructions.
104 
105 
106 Contributing
107 ============
108 
109 We need your help to make pvlib-python a great tool!
110 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.
111 The long-term success of pvlib-python requires substantial community support.
112 
113 
114 License
115 =======
116 
117 BSD 3-clause
118 
119 
120 Getting support
121 ===============
122 
123 pvlib usage questions can be asked on
124 [Stack Overflow](http://stackoverflow.com) and tagged with
125 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.
126 
127 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)
128 is used for discussing various topics of interest to the pvlib-python
129 community. We also make new version announcements on the google group.
130 
131 If you suspect that you may have discovered a bug or if you'd like to
132 change something about pvlib, then please make an issue on our
133 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).
134 
135 
136 Citing
137 ======
138 
139 If you use pvlib-python in a published work, please cite:
140 
141   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.
142   ""pvlib python: a python package for modeling solar energy systems.""
143   Journal of Open Source Software, 3(29), 884, (2018).
144   https://doi.org/10.21105/joss.00884
145 
146 Please also cite the DOI corresponding to the specific version of
147 pvlib-python that you used. pvlib-python DOIs are listed at
148 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)
149 
150 NumFOCUS
151 ========
152 
153 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)
154 
155 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)
156 
[end of README.md]
[start of pvlib/irradiance.py]
1 """"""
2 The ``irradiance`` module contains functions for modeling global
3 horizontal irradiance, direct normal irradiance, diffuse horizontal
4 irradiance, and total irradiance under various conditions.
5 """"""
6 
7 import datetime
8 from collections import OrderedDict
9 from functools import partial
10 
11 import numpy as np
12 import pandas as pd
13 
14 from pvlib import atmosphere, solarposition, tools
15 
16 
17 # see References section of grounddiffuse function
18 SURFACE_ALBEDOS = {'urban': 0.18,
19                    'grass': 0.20,
20                    'fresh grass': 0.26,
21                    'soil': 0.17,
22                    'sand': 0.40,
23                    'snow': 0.65,
24                    'fresh snow': 0.75,
25                    'asphalt': 0.12,
26                    'concrete': 0.30,
27                    'aluminum': 0.85,
28                    'copper': 0.74,
29                    'fresh steel': 0.35,
30                    'dirty steel': 0.08,
31                    'sea': 0.06}
32 
33 
34 def get_extra_radiation(datetime_or_doy, solar_constant=1366.1,
35                         method='spencer', epoch_year=2014, **kwargs):
36     """"""
37     Determine extraterrestrial radiation from day of year.
38 
39     Parameters
40     ----------
41     datetime_or_doy : numeric, array, date, datetime, Timestamp, DatetimeIndex
42         Day of year, array of days of year, or datetime-like object
43 
44     solar_constant : float, default 1366.1
45         The solar constant.
46 
47     method : string, default 'spencer'
48         The method by which the ET radiation should be calculated.
49         Options include ``'pyephem', 'spencer', 'asce', 'nrel'``.
50 
51     epoch_year : int, default 2014
52         The year in which a day of year input will be calculated. Only
53         applies to day of year input used with the pyephem or nrel
54         methods.
55 
56     kwargs :
57         Passed to solarposition.nrel_earthsun_distance
58 
59     Returns
60     -------
61     dni_extra : float, array, or Series
62         The extraterrestrial radiation present in watts per square meter
63         on a surface which is normal to the sun. Pandas Timestamp and
64         DatetimeIndex inputs will yield a Pandas TimeSeries. All other
65         inputs will yield a float or an array of floats.
66 
67     References
68     ----------
69     .. [1] M. Reno, C. Hansen, and J. Stein, ""Global Horizontal Irradiance
70        Clear Sky Models: Implementation and Analysis"", Sandia National
71        Laboratories, SAND2012-2389, 2012.
72 
73     .. [2] <http://solardat.uoregon.edu/SolarRadiationBasics.html>, Eqs.
74        SR1 and SR2
75 
76     .. [3] Partridge, G. W. and Platt, C. M. R. 1976. Radiative Processes
77        in Meteorology and Climatology.
78 
79     .. [4] Duffie, J. A. and Beckman, W. A. 1991. Solar Engineering of
80        Thermal Processes, 2nd edn. J. Wiley and Sons, New York.
81 
82     .. [5] ASCE, 2005. The ASCE Standardized Reference Evapotranspiration
83        Equation, Environmental and Water Resources Institute of the American
84        Civil Engineers, Ed. R. G. Allen et al.
85     """"""
86 
87     to_doy, to_datetimeindex, to_output = \
88         _handle_extra_radiation_types(datetime_or_doy, epoch_year)
89 
90     # consider putting asce and spencer methods in their own functions
91     method = method.lower()
92     if method == 'asce':
93         B = solarposition._calculate_simple_day_angle(to_doy(datetime_or_doy),
94                                                       offset=0)
95         RoverR0sqrd = 1 + 0.033 * np.cos(B)
96     elif method == 'spencer':
97         B = solarposition._calculate_simple_day_angle(to_doy(datetime_or_doy))
98         RoverR0sqrd = (1.00011 + 0.034221 * np.cos(B) + 0.00128 * np.sin(B) +
99                        0.000719 * np.cos(2 * B) + 7.7e-05 * np.sin(2 * B))
100     elif method == 'pyephem':
101         times = to_datetimeindex(datetime_or_doy)
102         RoverR0sqrd = solarposition.pyephem_earthsun_distance(times) ** (-2)
103     elif method == 'nrel':
104         times = to_datetimeindex(datetime_or_doy)
105         RoverR0sqrd = \
106             solarposition.nrel_earthsun_distance(times, **kwargs) ** (-2)
107     else:
108         raise ValueError('Invalid method: %s', method)
109 
110     Ea = solar_constant * RoverR0sqrd
111 
112     Ea = to_output(Ea)
113 
114     return Ea
115 
116 
117 def _handle_extra_radiation_types(datetime_or_doy, epoch_year):
118     # This block will set the functions that can be used to convert the
119     # inputs to either day of year or pandas DatetimeIndex, and the
120     # functions that will yield the appropriate output type. It's
121     # complicated because there are many day-of-year-like input types,
122     # and the different algorithms need different types. Maybe you have
123     # a better way to do it.
124     if isinstance(datetime_or_doy, pd.DatetimeIndex):
125         to_doy = tools._pandas_to_doy  # won't be evaluated unless necessary
126         def to_datetimeindex(x): return x                       # noqa: E306
127         to_output = partial(pd.Series, index=datetime_or_doy)
128     elif isinstance(datetime_or_doy, pd.Timestamp):
129         to_doy = tools._pandas_to_doy
130         to_datetimeindex = \
131             tools._datetimelike_scalar_to_datetimeindex
132         to_output = tools._scalar_out
133     elif isinstance(datetime_or_doy,
134                     (datetime.date, datetime.datetime, np.datetime64)):
135         to_doy = tools._datetimelike_scalar_to_doy
136         to_datetimeindex = \
137             tools._datetimelike_scalar_to_datetimeindex
138         to_output = tools._scalar_out
139     elif np.isscalar(datetime_or_doy):  # ints and floats of various types
140         def to_doy(x): return x                                 # noqa: E306
141         to_datetimeindex = partial(tools._doy_to_datetimeindex,
142                                    epoch_year=epoch_year)
143         to_output = tools._scalar_out
144     else:  # assume that we have an array-like object of doy
145         def to_doy(x): return x                                 # noqa: E306
146         to_datetimeindex = partial(tools._doy_to_datetimeindex,
147                                    epoch_year=epoch_year)
148         to_output = tools._array_out
149 
150     return to_doy, to_datetimeindex, to_output
151 
152 
153 def aoi_projection(surface_tilt, surface_azimuth, solar_zenith, solar_azimuth):
154     """"""
155     Calculates the dot product of the sun position unit vector and the surface
156     normal unit vector; in other words, the cosine of the angle of incidence.
157 
158     Usage note: When the sun is behind the surface the value returned is
159     negative.  For many uses negative values must be set to zero.
160 
161     Input all angles in degrees.
162 
163     Parameters
164     ----------
165     surface_tilt : numeric
166         Panel tilt from horizontal.
167     surface_azimuth : numeric
168         Panel azimuth from north.
169     solar_zenith : numeric
170         Solar zenith angle.
171     solar_azimuth : numeric
172         Solar azimuth angle.
173 
174     Returns
175     -------
176     projection : numeric
177         Dot product of panel normal and solar angle.
178     """"""
179 
180     projection = (
181         tools.cosd(surface_tilt) * tools.cosd(solar_zenith) +
182         tools.sind(surface_tilt) * tools.sind(solar_zenith) *
183         tools.cosd(solar_azimuth - surface_azimuth))
184 
185     try:
186         projection.name = 'aoi_projection'
187     except AttributeError:
188         pass
189 
190     return projection
191 
192 
193 def aoi(surface_tilt, surface_azimuth, solar_zenith, solar_azimuth):
194     """"""
195     Calculates the angle of incidence of the solar vector on a surface.
196     This is the angle between the solar vector and the surface normal.
197 
198     Input all angles in degrees.
199 
200     Parameters
201     ----------
202     surface_tilt : numeric
203         Panel tilt from horizontal.
204     surface_azimuth : numeric
205         Panel azimuth from north.
206     solar_zenith : numeric
207         Solar zenith angle.
208     solar_azimuth : numeric
209         Solar azimuth angle.
210 
211     Returns
212     -------
213     aoi : numeric
214         Angle of incidence in degrees.
215     """"""
216 
217     projection = aoi_projection(surface_tilt, surface_azimuth,
218                                 solar_zenith, solar_azimuth)
219     aoi_value = np.rad2deg(np.arccos(projection))
220 
221     try:
222         aoi_value.name = 'aoi'
223     except AttributeError:
224         pass
225 
226     return aoi_value
227 
228 
229 def poa_horizontal_ratio(surface_tilt, surface_azimuth,
230                          solar_zenith, solar_azimuth):
231     """"""
232     Calculates the ratio of the beam components of the plane of array
233     irradiance and the horizontal irradiance.
234 
235     Input all angles in degrees.
236 
237     Parameters
238     ----------
239     surface_tilt : numeric
240         Panel tilt from horizontal.
241     surface_azimuth : numeric
242         Panel azimuth from north.
243     solar_zenith : numeric
244         Solar zenith angle.
245     solar_azimuth : numeric
246         Solar azimuth angle.
247 
248     Returns
249     -------
250     ratio : numeric
251         Ratio of the plane of array irradiance to the horizontal plane
252         irradiance
253     """"""
254 
255     cos_poa_zen = aoi_projection(surface_tilt, surface_azimuth,
256                                  solar_zenith, solar_azimuth)
257 
258     cos_solar_zenith = tools.cosd(solar_zenith)
259 
260     # ratio of tilted and horizontal beam irradiance
261     ratio = cos_poa_zen / cos_solar_zenith
262 
263     try:
264         ratio.name = 'poa_ratio'
265     except AttributeError:
266         pass
267 
268     return ratio
269 
270 
271 def beam_component(surface_tilt, surface_azimuth, solar_zenith, solar_azimuth,
272                    dni):
273     """"""
274     Calculates the beam component of the plane of array irradiance.
275 
276     Parameters
277     ----------
278     surface_tilt : numeric
279         Panel tilt from horizontal.
280     surface_azimuth : numeric
281         Panel azimuth from north.
282     solar_zenith : numeric
283         Solar zenith angle.
284     solar_azimuth : numeric
285         Solar azimuth angle.
286     dni : numeric
287         Direct Normal Irradiance
288 
289     Returns
290     -------
291     beam : numeric
292         Beam component
293     """"""
294     beam = dni * aoi_projection(surface_tilt, surface_azimuth,
295                                 solar_zenith, solar_azimuth)
296     beam = np.maximum(beam, 0)
297 
298     return beam
299 
300 
301 def get_total_irradiance(surface_tilt, surface_azimuth,
302                          solar_zenith, solar_azimuth,
303                          dni, ghi, dhi, dni_extra=None, airmass=None,
304                          albedo=.25, surface_type=None,
305                          model='isotropic',
306                          model_perez='allsitescomposite1990', **kwargs):
307     r""""""
308     Determine total in-plane irradiance and its beam, sky diffuse and ground
309     reflected components, using the specified sky diffuse irradiance model.
310 
311     .. math::
312 
313        I_{tot} = I_{beam} + I_{sky diffuse} + I_{ground}
314 
315     Sky diffuse models include:
316         * isotropic (default)
317         * klucher
318         * haydavies
319         * reindl
320         * king
321         * perez
322 
323     Parameters
324     ----------
325     surface_tilt : numeric
326         Panel tilt from horizontal.
327     surface_azimuth : numeric
328         Panel azimuth from north.
329     solar_zenith : numeric
330         Solar zenith angle.
331     solar_azimuth : numeric
332         Solar azimuth angle.
333     dni : numeric
334         Direct Normal Irradiance
335     ghi : numeric
336         Global horizontal irradiance
337     dhi : numeric
338         Diffuse horizontal irradiance
339     dni_extra : None or numeric, default None
340         Extraterrestrial direct normal irradiance
341     airmass : None or numeric, default None
342         Airmass
343     albedo : numeric, default 0.25
344         Surface albedo
345     surface_type : None or String, default None
346         Surface type. See grounddiffuse.
347     model : String, default 'isotropic'
348         Irradiance model.
349     model_perez : String, default 'allsitescomposite1990'
350         Used only if model='perez'. See :py:func:`perez`.
351 
352     Returns
353     -------
354     total_irrad : OrderedDict or DataFrame
355         Contains keys/columns ``'poa_global', 'poa_direct', 'poa_diffuse',
356         'poa_sky_diffuse', 'poa_ground_diffuse'``.
357     """"""
358     poa_sky_diffuse = get_sky_diffuse(
359         surface_tilt, surface_azimuth, solar_zenith, solar_azimuth,
360         dni, ghi, dhi, dni_extra=dni_extra, airmass=airmass, model=model,
361         model_perez=model_perez)
362 
363     poa_ground_diffuse = get_ground_diffuse(surface_tilt, ghi, albedo,
364                                             surface_type)
365     aoi_ = aoi(surface_tilt, surface_azimuth, solar_zenith, solar_azimuth)
366     irrads = poa_components(aoi_, dni, poa_sky_diffuse, poa_ground_diffuse)
367     return irrads
368 
369 
370 def get_sky_diffuse(surface_tilt, surface_azimuth,
371                     solar_zenith, solar_azimuth,
372                     dni, ghi, dhi, dni_extra=None, airmass=None,
373                     model='isotropic',
374                     model_perez='allsitescomposite1990'):
375     r""""""
376     Determine in-plane sky diffuse irradiance component
377     using the specified sky diffuse irradiance model.
378 
379     Sky diffuse models include:
380         * isotropic (default)
381         * klucher
382         * haydavies
383         * reindl
384         * king
385         * perez
386 
387     Parameters
388     ----------
389     surface_tilt : numeric
390         Panel tilt from horizontal.
391     surface_azimuth : numeric
392         Panel azimuth from north.
393     solar_zenith : numeric
394         Solar zenith angle.
395     solar_azimuth : numeric
396         Solar azimuth angle.
397     dni : numeric
398         Direct Normal Irradiance
399     ghi : numeric
400         Global horizontal irradiance
401     dhi : numeric
402         Diffuse horizontal irradiance
403     dni_extra : None or numeric, default None
404         Extraterrestrial direct normal irradiance
405     airmass : None or numeric, default None
406         Airmass
407     model : String, default 'isotropic'
408         Irradiance model.
409     model_perez : String, default 'allsitescomposite1990'
410         See perez.
411 
412     Returns
413     -------
414     poa_sky_diffuse : numeric
415     """"""
416 
417     model = model.lower()
418     if model == 'isotropic':
419         sky = isotropic(surface_tilt, dhi)
420     elif model == 'klucher':
421         sky = klucher(surface_tilt, surface_azimuth, dhi, ghi,
422                       solar_zenith, solar_azimuth)
423     elif model == 'haydavies':
424         sky = haydavies(surface_tilt, surface_azimuth, dhi, dni, dni_extra,
425                         solar_zenith, solar_azimuth)
426     elif model == 'reindl':
427         sky = reindl(surface_tilt, surface_azimuth, dhi, dni, ghi, dni_extra,
428                      solar_zenith, solar_azimuth)
429     elif model == 'king':
430         sky = king(surface_tilt, dhi, ghi, solar_zenith)
431     elif model == 'perez':
432         sky = perez(surface_tilt, surface_azimuth, dhi, dni, dni_extra,
433                     solar_zenith, solar_azimuth, airmass,
434                     model=model_perez)
435     else:
436         raise ValueError(f'invalid model selection {model}')
437 
438     return sky
439 
440 
441 def poa_components(aoi, dni, poa_sky_diffuse, poa_ground_diffuse):
442     r'''
443     Determine in-plane irradiance components.
444 
445     Combines DNI with sky diffuse and ground-reflected irradiance to calculate
446     total, direct and diffuse irradiance components in the plane of array.
447 
448     Parameters
449     ----------
450     aoi : numeric
451         Angle of incidence of solar rays with respect to the module
452         surface, from :func:`aoi`.
453 
454     dni : numeric
455         Direct normal irradiance (W/m^2), as measured from a TMY file or
456         calculated with a clearsky model.
457 
458     poa_sky_diffuse : numeric
459         Diffuse irradiance (W/m^2) in the plane of the modules, as
460         calculated by a diffuse irradiance translation function
461 
462     poa_ground_diffuse : numeric
463         Ground reflected irradiance (W/m^2) in the plane of the modules,
464         as calculated by an albedo model (eg. :func:`grounddiffuse`)
465 
466     Returns
467     -------
468     irrads : OrderedDict or DataFrame
469         Contains the following keys:
470 
471         * ``poa_global`` : Total in-plane irradiance (W/m^2)
472         * ``poa_direct`` : Total in-plane beam irradiance (W/m^2)
473         * ``poa_diffuse`` : Total in-plane diffuse irradiance (W/m^2)
474         * ``poa_sky_diffuse`` : In-plane diffuse irradiance from sky (W/m^2)
475         * ``poa_ground_diffuse`` : In-plane diffuse irradiance from ground
476           (W/m^2)
477 
478     Notes
479     ------
480     Negative beam irradiation due to aoi :math:`> 90^{\circ}` or AOI
481     :math:`< 0^{\circ}` is set to zero.
482     '''
483 
484     poa_direct = np.maximum(dni * np.cos(np.radians(aoi)), 0)
485     poa_diffuse = poa_sky_diffuse + poa_ground_diffuse
486     poa_global = poa_direct + poa_diffuse
487 
488     irrads = OrderedDict()
489     irrads['poa_global'] = poa_global
490     irrads['poa_direct'] = poa_direct
491     irrads['poa_diffuse'] = poa_diffuse
492     irrads['poa_sky_diffuse'] = poa_sky_diffuse
493     irrads['poa_ground_diffuse'] = poa_ground_diffuse
494 
495     if isinstance(poa_direct, pd.Series):
496         irrads = pd.DataFrame(irrads)
497 
498     return irrads
499 
500 
501 def get_ground_diffuse(surface_tilt, ghi, albedo=.25, surface_type=None):
502     '''
503     Estimate diffuse irradiance from ground reflections given
504     irradiance, albedo, and surface tilt
505 
506     Function to determine the portion of irradiance on a tilted surface
507     due to ground reflections. Any of the inputs may be DataFrames or
508     scalars.
509 
510     Parameters
511     ----------
512     surface_tilt : numeric
513         Surface tilt angles in decimal degrees. Tilt must be >=0 and
514         <=180. The tilt angle is defined as degrees from horizontal
515         (e.g. surface facing up = 0, surface facing horizon = 90).
516 
517     ghi : numeric
518         Global horizontal irradiance in W/m^2.
519 
520     albedo : numeric, default 0.25
521         Ground reflectance, typically 0.1-0.4 for surfaces on Earth
522         (land), may increase over snow, ice, etc. May also be known as
523         the reflection coefficient. Must be >=0 and <=1. Will be
524         overridden if surface_type is supplied.
525 
526     surface_type: None or string, default None
527         If not None, overrides albedo. String can be one of 'urban',
528         'grass', 'fresh grass', 'snow', 'fresh snow', 'asphalt', 'concrete',
529         'aluminum', 'copper', 'fresh steel', 'dirty steel', 'sea'.
530 
531     Returns
532     -------
533     grounddiffuse : numeric
534         Ground reflected irradiances in W/m^2.
535 
536 
537     References
538     ----------
539     .. [1] Loutzenhiser P.G. et. al. ""Empirical validation of models to compute
540        solar irradiance on inclined surfaces for building energy simulation""
541        2007, Solar Energy vol. 81. pp. 254-267.
542 
543     The calculation is the last term of equations 3, 4, 7, 8, 10, 11, and 12.
544 
545     .. [2] albedos from:
546        http://files.pvsyst.com/help/albedo.htm
547        and
548        http://en.wikipedia.org/wiki/Albedo
549        and
550        https://doi.org/10.1175/1520-0469(1972)029<0959:AOTSS>2.0.CO;2
551     '''
552 
553     if surface_type is not None:
554         albedo = SURFACE_ALBEDOS[surface_type]
555 
556     diffuse_irrad = ghi * albedo * (1 - np.cos(np.radians(surface_tilt))) * 0.5
557 
558     try:
559         diffuse_irrad.name = 'diffuse_ground'
560     except AttributeError:
561         pass
562 
563     return diffuse_irrad
564 
565 
566 def isotropic(surface_tilt, dhi):
567     r'''
568     Determine diffuse irradiance from the sky on a tilted surface using
569     the isotropic sky model.
570 
571     .. math::
572 
573        I_{d} = DHI \frac{1 + \cos\beta}{2}
574 
575     Hottel and Woertz's model treats the sky as a uniform source of
576     diffuse irradiance. Thus the diffuse irradiance from the sky (ground
577     reflected irradiance is not included in this algorithm) on a tilted
578     surface can be found from the diffuse horizontal irradiance and the
579     tilt angle of the surface.
580 
581     Parameters
582     ----------
583     surface_tilt : numeric
584         Surface tilt angle in decimal degrees. Tilt must be >=0 and
585         <=180. The tilt angle is defined as degrees from horizontal
586         (e.g. surface facing up = 0, surface facing horizon = 90)
587 
588     dhi : numeric
589         Diffuse horizontal irradiance in W/m^2. DHI must be >=0.
590 
591     Returns
592     -------
593     diffuse : numeric
594         The sky diffuse component of the solar radiation.
595 
596     References
597     ----------
598     .. [1] Loutzenhiser P.G. et. al. ""Empirical validation of models to
599        compute solar irradiance on inclined surfaces for building energy
600        simulation"" 2007, Solar Energy vol. 81. pp. 254-267
601 
602     .. [2] Hottel, H.C., Woertz, B.B., 1942. Evaluation of flat-plate solar
603        heat collector. Trans. ASME 64, 91.
604     '''
605 
606     sky_diffuse = dhi * (1 + tools.cosd(surface_tilt)) * 0.5
607 
608     return sky_diffuse
609 
610 
611 def klucher(surface_tilt, surface_azimuth, dhi, ghi, solar_zenith,
612             solar_azimuth):
613     r'''
614     Determine diffuse irradiance from the sky on a tilted surface
615     using Klucher's 1979 model
616 
617     .. math::
618 
619        I_{d} = DHI \frac{1 + \cos\beta}{2} (1 + F' \sin^3(\beta/2))
620        (1 + F' \cos^2\theta\sin^3\theta_z)
621 
622     where
623 
624     .. math::
625 
626        F' = 1 - (I_{d0} / GHI)^2
627 
628     Klucher's 1979 model determines the diffuse irradiance from the sky
629     (ground reflected irradiance is not included in this algorithm) on a
630     tilted surface using the surface tilt angle, surface azimuth angle,
631     diffuse horizontal irradiance, direct normal irradiance, global
632     horizontal irradiance, extraterrestrial irradiance, sun zenith
633     angle, and sun azimuth angle.
634 
635     Parameters
636     ----------
637     surface_tilt : numeric
638         Surface tilt angles in decimal degrees. surface_tilt must be >=0
639         and <=180. The tilt angle is defined as degrees from horizontal
640         (e.g. surface facing up = 0, surface facing horizon = 90)
641 
642     surface_azimuth : numeric
643         Surface azimuth angles in decimal degrees. surface_azimuth must
644         be >=0 and <=360. The Azimuth convention is defined as degrees
645         east of north (e.g. North = 0, South=180 East = 90, West = 270).
646 
647     dhi : numeric
648         Diffuse horizontal irradiance in W/m^2. DHI must be >=0.
649 
650     ghi : numeric
651         Global irradiance in W/m^2. DNI must be >=0.
652 
653     solar_zenith : numeric
654         Apparent (refraction-corrected) zenith angles in decimal
655         degrees. solar_zenith must be >=0 and <=180.
656 
657     solar_azimuth : numeric
658         Sun azimuth angles in decimal degrees. solar_azimuth must be >=0
659         and <=360. The Azimuth convention is defined as degrees east of
660         north (e.g. North = 0, East = 90, West = 270).
661 
662     Returns
663     -------
664     diffuse : numeric
665         The sky diffuse component of the solar radiation.
666 
667     References
668     ----------
669     .. [1] Loutzenhiser P.G. et. al. ""Empirical validation of models to compute
670        solar irradiance on inclined surfaces for building energy simulation""
671        2007, Solar Energy vol. 81. pp. 254-267
672 
673     .. [2] Klucher, T.M., 1979. Evaluation of models to predict insolation on
674        tilted surfaces. Solar Energy 23 (2), 111-114.
675     '''
676 
677     # zenith angle with respect to panel normal.
678     cos_tt = aoi_projection(surface_tilt, surface_azimuth,
679                             solar_zenith, solar_azimuth)
680     cos_tt = np.maximum(cos_tt, 0)  # GH 526
681 
682     # silence warning from 0 / 0
683     with np.errstate(invalid='ignore'):
684         F = 1 - ((dhi / ghi) ** 2)
685 
686     try:
687         # fails with single point input
688         F.fillna(0, inplace=True)
689     except AttributeError:
690         F = np.where(np.isnan(F), 0, F)
691 
692     term1 = 0.5 * (1 + tools.cosd(surface_tilt))
693     term2 = 1 + F * (tools.sind(0.5 * surface_tilt) ** 3)
694     term3 = 1 + F * (cos_tt ** 2) * (tools.sind(solar_zenith) ** 3)
695 
696     sky_diffuse = dhi * term1 * term2 * term3
697 
698     return sky_diffuse
699 
700 
701 def haydavies(surface_tilt, surface_azimuth, dhi, dni, dni_extra,
702               solar_zenith=None, solar_azimuth=None, projection_ratio=None):
703     r'''
704     Determine diffuse irradiance from the sky on a tilted surface using
705     Hay & Davies' 1980 model
706 
707     .. math::
708         I_{d} = DHI ( A R_b + (1 - A) (\frac{1 + \cos\beta}{2}) )
709 
710     Hay and Davies' 1980 model determines the diffuse irradiance from
711     the sky (ground reflected irradiance is not included in this
712     algorithm) on a tilted surface using the surface tilt angle, surface
713     azimuth angle, diffuse horizontal irradiance, direct normal
714     irradiance, extraterrestrial irradiance, sun zenith angle, and sun
715     azimuth angle.
716 
717     Parameters
718     ----------
719     surface_tilt : numeric
720         Surface tilt angles in decimal degrees. The tilt angle is
721         defined as degrees from horizontal (e.g. surface facing up = 0,
722         surface facing horizon = 90)
723 
724     surface_azimuth : numeric
725         Surface azimuth angles in decimal degrees. The azimuth
726         convention is defined as degrees east of north (e.g. North=0,
727         South=180, East=90, West=270).
728 
729     dhi : numeric
730         Diffuse horizontal irradiance in W/m^2.
731 
732     dni : numeric
733         Direct normal irradiance in W/m^2.
734 
735     dni_extra : numeric
736         Extraterrestrial normal irradiance in W/m^2.
737 
738     solar_zenith : None or numeric, default None
739         Solar apparent (refraction-corrected) zenith angles in decimal
740         degrees. Must supply ``solar_zenith`` and ``solar_azimuth`` or
741         supply ``projection_ratio``.
742 
743     solar_azimuth : None or numeric, default None
744         Solar azimuth angles in decimal degrees. Must supply
745         ``solar_zenith`` and ``solar_azimuth`` or supply
746         ``projection_ratio``.
747 
748     projection_ratio : None or numeric, default None
749         Ratio of angle of incidence projection to solar zenith angle
750         projection. Must supply ``solar_zenith`` and ``solar_azimuth``
751         or supply ``projection_ratio``.
752 
753     Returns
754     --------
755     sky_diffuse : numeric
756         The sky diffuse component of the solar radiation.
757 
758     References
759     -----------
760     .. [1] Loutzenhiser P.G. et. al. ""Empirical validation of models to
761        compute solar irradiance on inclined surfaces for building energy
762        simulation"" 2007, Solar Energy vol. 81. pp. 254-267
763 
764     .. [2] Hay, J.E., Davies, J.A., 1980. Calculations of the solar
765        radiation incident on an inclined surface. In: Hay, J.E., Won, T.K.
766        (Eds.), Proc. of First Canadian Solar Radiation Data Workshop, 59.
767        Ministry of Supply and Services, Canada.
768     '''
769 
770     # if necessary, calculate ratio of titled and horizontal beam irradiance
771     if projection_ratio is None:
772         cos_tt = aoi_projection(surface_tilt, surface_azimuth,
773                                 solar_zenith, solar_azimuth)
774         cos_tt = np.maximum(cos_tt, 0)  # GH 526
775         cos_solar_zenith = tools.cosd(solar_zenith)
776         Rb = cos_tt / np.maximum(cos_solar_zenith, 0.01745)  # GH 432
777     else:
778         Rb = projection_ratio
779 
780     # Anisotropy Index
781     AI = dni / dni_extra
782 
783     # these are the () and [] sub-terms of the second term of eqn 7
784     term1 = 1 - AI
785     term2 = 0.5 * (1 + tools.cosd(surface_tilt))
786 
787     sky_diffuse = dhi * (AI * Rb + term1 * term2)
788     sky_diffuse = np.maximum(sky_diffuse, 0)
789 
790     return sky_diffuse
791 
792 
793 def reindl(surface_tilt, surface_azimuth, dhi, dni, ghi, dni_extra,
794            solar_zenith, solar_azimuth):
795     r'''
796     Determine diffuse irradiance from the sky on a tilted surface using
797     Reindl's 1990 model
798 
799     .. math::
800 
801        I_{d} = DHI (A R_b + (1 - A) (\frac{1 + \cos\beta}{2})
802        (1 + \sqrt{\frac{I_{hb}}{I_h}} \sin^3(\beta/2)) )
803 
804     Reindl's 1990 model determines the diffuse irradiance from the sky
805     (ground reflected irradiance is not included in this algorithm) on a
806     tilted surface using the surface tilt angle, surface azimuth angle,
807     diffuse horizontal irradiance, direct normal irradiance, global
808     horizontal irradiance, extraterrestrial irradiance, sun zenith
809     angle, and sun azimuth angle.
810 
811     Parameters
812     ----------
813     surface_tilt : numeric
814         Surface tilt angles in decimal degrees. The tilt angle is
815         defined as degrees from horizontal (e.g. surface facing up = 0,
816         surface facing horizon = 90)
817 
818     surface_azimuth : numeric
819         Surface azimuth angles in decimal degrees. The azimuth
820         convention is defined as degrees east of north (e.g. North = 0,
821         South=180 East = 90, West = 270).
822 
823     dhi : numeric
824         diffuse horizontal irradiance in W/m^2.
825 
826     dni : numeric
827         direct normal irradiance in W/m^2.
828 
829     ghi: numeric
830         Global irradiance in W/m^2.
831 
832     dni_extra : numeric
833         Extraterrestrial normal irradiance in W/m^2.
834 
835     solar_zenith : numeric
836         Apparent (refraction-corrected) zenith angles in decimal degrees.
837 
838     solar_azimuth : numeric
839         Sun azimuth angles in decimal degrees. The azimuth convention is
840         defined as degrees east of north (e.g. North = 0, East = 90,
841         West = 270).
842 
843     Returns
844     -------
845     poa_sky_diffuse : numeric
846         The sky diffuse component of the solar radiation.
847 
848     Notes
849     -----
850     The poa_sky_diffuse calculation is generated from the Loutzenhiser et al.
851     (2007) paper, equation 8. Note that I have removed the beam and ground
852     reflectance portion of the equation and this generates ONLY the diffuse
853     radiation from the sky and circumsolar, so the form of the equation
854     varies slightly from equation 8.
855 
856     References
857     ----------
858     .. [1] Loutzenhiser P.G. et. al. ""Empirical validation of models to
859        compute solar irradiance on inclined surfaces for building energy
860        simulation"" 2007, Solar Energy vol. 81. pp. 254-267
861 
862     .. [2] Reindl, D.T., Beckmann, W.A., Duffie, J.A., 1990a. Diffuse
863        fraction correlations. Solar Energy 45(1), 1-7.
864 
865     .. [3] Reindl, D.T., Beckmann, W.A., Duffie, J.A., 1990b. Evaluation of
866        hourly tilted surface radiation models. Solar Energy 45(1), 9-17.
867     '''
868 
869     cos_tt = aoi_projection(surface_tilt, surface_azimuth,
870                             solar_zenith, solar_azimuth)
871     cos_tt = np.maximum(cos_tt, 0)  # GH 526
872 
873     # do not apply cos(zen) limit here (needed for HB below)
874     cos_solar_zenith = tools.cosd(solar_zenith)
875 
876     # ratio of titled and horizontal beam irradiance
877     Rb = cos_tt / np.maximum(cos_solar_zenith, 0.01745)  # GH 432
878 
879     # Anisotropy Index
880     AI = dni / dni_extra
881 
882     # DNI projected onto horizontal
883     HB = dni * cos_solar_zenith
884     HB = np.maximum(HB, 0)
885 
886     # these are the () and [] sub-terms of the second term of eqn 8
887     term1 = 1 - AI
888     term2 = 0.5 * (1 + tools.cosd(surface_tilt))
889     term3 = 1 + np.sqrt(HB / ghi) * (tools.sind(0.5 * surface_tilt) ** 3)
890 
891     sky_diffuse = dhi * (AI * Rb + term1 * term2 * term3)
892     sky_diffuse = np.maximum(sky_diffuse, 0)
893 
894     return sky_diffuse
895 
896 
897 def king(surface_tilt, dhi, ghi, solar_zenith):
898     '''
899     Determine diffuse irradiance from the sky on a tilted surface using
900     the King model.
901 
902     King's model determines the diffuse irradiance from the sky (ground
903     reflected irradiance is not included in this algorithm) on a tilted
904     surface using the surface tilt angle, diffuse horizontal irradiance,
905     global horizontal irradiance, and sun zenith angle. Note that this
906     model is not well documented and has not been published in any
907     fashion (as of January 2012).
908 
909     Parameters
910     ----------
911     surface_tilt : numeric
912         Surface tilt angles in decimal degrees. The tilt angle is
913         defined as degrees from horizontal (e.g. surface facing up = 0,
914         surface facing horizon = 90)
915 
916     dhi : numeric
917         Diffuse horizontal irradiance in W/m^2.
918 
919     ghi : numeric
920         Global horizontal irradiance in W/m^2.
921 
922     solar_zenith : numeric
923         Apparent (refraction-corrected) zenith angles in decimal degrees.
924 
925     Returns
926     --------
927     poa_sky_diffuse : numeric
928         The diffuse component of the solar radiation.
929     '''
930 
931     sky_diffuse = (dhi * (1 + tools.cosd(surface_tilt)) / 2 + ghi *
932                    (0.012 * solar_zenith - 0.04) *
933                    (1 - tools.cosd(surface_tilt)) / 2)
934     sky_diffuse = np.maximum(sky_diffuse, 0)
935 
936     return sky_diffuse
937 
938 
939 def perez(surface_tilt, surface_azimuth, dhi, dni, dni_extra,
940           solar_zenith, solar_azimuth, airmass,
941           model='allsitescomposite1990', return_components=False):
942     '''
943     Determine diffuse irradiance from the sky on a tilted surface using
944     one of the Perez models.
945 
946     Perez models determine the diffuse irradiance from the sky (ground
947     reflected irradiance is not included in this algorithm) on a tilted
948     surface using the surface tilt angle, surface azimuth angle, diffuse
949     horizontal irradiance, direct normal irradiance, extraterrestrial
950     irradiance, sun zenith angle, sun azimuth angle, and relative (not
951     pressure-corrected) airmass. Optionally a selector may be used to
952     use any of Perez's model coefficient sets.
953 
954     Parameters
955     ----------
956     surface_tilt : numeric
957         Surface tilt angles in decimal degrees. surface_tilt must be >=0
958         and <=180. The tilt angle is defined as degrees from horizontal
959         (e.g. surface facing up = 0, surface facing horizon = 90)
960 
961     surface_azimuth : numeric
962         Surface azimuth angles in decimal degrees. surface_azimuth must
963         be >=0 and <=360. The azimuth convention is defined as degrees
964         east of north (e.g. North = 0, South=180 East = 90, West = 270).
965 
966     dhi : numeric
967         Diffuse horizontal irradiance in W/m^2. DHI must be >=0.
968 
969     dni : numeric
970         Direct normal irradiance in W/m^2. DNI must be >=0.
971 
972     dni_extra : numeric
973         Extraterrestrial normal irradiance in W/m^2.
974 
975     solar_zenith : numeric
976         apparent (refraction-corrected) zenith angles in decimal
977         degrees. solar_zenith must be >=0 and <=180.
978 
979     solar_azimuth : numeric
980         Sun azimuth angles in decimal degrees. solar_azimuth must be >=0
981         and <=360. The azimuth convention is defined as degrees east of
982         north (e.g. North = 0, East = 90, West = 270).
983 
984     airmass : numeric
985         Relative (not pressure-corrected) airmass values. If AM is a
986         DataFrame it must be of the same size as all other DataFrame
987         inputs. AM must be >=0 (careful using the 1/sec(z) model of AM
988         generation)
989 
990     model : string (optional, default='allsitescomposite1990')
991         A string which selects the desired set of Perez coefficients. If
992         model is not provided as an input, the default, '1990' will be
993         used. All possible model selections are:
994 
995         * '1990'
996         * 'allsitescomposite1990' (same as '1990')
997         * 'allsitescomposite1988'
998         * 'sandiacomposite1988'
999         * 'usacomposite1988'
1000         * 'france1988'
1001         * 'phoenix1988'
1002         * 'elmonte1988'
1003         * 'osage1988'
1004         * 'albuquerque1988'
1005         * 'capecanaveral1988'
1006         * 'albany1988'
1007 
1008     return_components: bool (optional, default=False)
1009         Flag used to decide whether to return the calculated diffuse components
1010         or not.
1011 
1012     Returns
1013     --------
1014     numeric, OrderedDict, or DataFrame
1015         Return type controlled by `return_components` argument.
1016         If ``return_components=False``, `sky_diffuse` is returned.
1017         If ``return_components=True``, `diffuse_components` is returned.
1018 
1019     sky_diffuse : numeric
1020         The sky diffuse component of the solar radiation on a tilted
1021         surface.
1022 
1023     diffuse_components : OrderedDict (array input) or DataFrame (Series input)
1024         Keys/columns are:
1025             * sky_diffuse: Total sky diffuse
1026             * isotropic
1027             * circumsolar
1028             * horizon
1029 
1030 
1031     References
1032     ----------
1033     .. [1] Loutzenhiser P.G. et. al. ""Empirical validation of models to
1034        compute solar irradiance on inclined surfaces for building energy
1035        simulation"" 2007, Solar Energy vol. 81. pp. 254-267
1036 
1037     .. [2] Perez, R., Seals, R., Ineichen, P., Stewart, R., Menicucci, D.,
1038        1987. A new simplified version of the Perez diffuse irradiance model
1039        for tilted surfaces. Solar Energy 39(3), 221-232.
1040 
1041     .. [3] Perez, R., Ineichen, P., Seals, R., Michalsky, J., Stewart, R.,
1042        1990. Modeling daylight availability and irradiance components from
1043        direct and global irradiance. Solar Energy 44 (5), 271-289.
1044 
1045     .. [4] Perez, R. et. al 1988. ""The Development and Verification of the
1046        Perez Diffuse Radiation Model"". SAND88-7030
1047     '''
1048 
1049     kappa = 1.041  # for solar_zenith in radians
1050     z = np.radians(solar_zenith)  # convert to radians
1051 
1052     # delta is the sky's ""brightness""
1053     delta = dhi * airmass / dni_extra
1054 
1055     # epsilon is the sky's ""clearness""
1056     with np.errstate(invalid='ignore'):
1057         eps = ((dhi + dni) / dhi + kappa * (z ** 3)) / (1 + kappa * (z ** 3))
1058 
1059     # numpy indexing below will not work with a Series
1060     if isinstance(eps, pd.Series):
1061         eps = eps.values
1062 
1063     # Perez et al define clearness bins according to the following
1064     # rules. 1 = overcast ... 8 = clear (these names really only make
1065     # sense for small zenith angles, but...) these values will
1066     # eventually be used as indicies for coeffecient look ups
1067     ebin = np.digitize(eps, (0., 1.065, 1.23, 1.5, 1.95, 2.8, 4.5, 6.2))
1068     ebin = np.array(ebin)  # GH 642
1069     ebin[np.isnan(eps)] = 0
1070 
1071     # correct for 0 indexing in coeffecient lookup
1072     # later, ebin = -1 will yield nan coefficients
1073     ebin -= 1
1074 
1075     # The various possible sets of Perez coefficients are contained
1076     # in a subfunction to clean up the code.
1077     F1c, F2c = _get_perez_coefficients(model)
1078 
1079     # results in invalid eps (ebin = -1) being mapped to nans
1080     nans = np.array([np.nan, np.nan, np.nan])
1081     F1c = np.vstack((F1c, nans))
1082     F2c = np.vstack((F2c, nans))
1083 
1084     F1 = (F1c[ebin, 0] + F1c[ebin, 1] * delta + F1c[ebin, 2] * z)
1085     F1 = np.maximum(F1, 0)
1086 
1087     F2 = (F2c[ebin, 0] + F2c[ebin, 1] * delta + F2c[ebin, 2] * z)
1088     F2 = np.maximum(F2, 0)
1089 
1090     A = aoi_projection(surface_tilt, surface_azimuth,
1091                        solar_zenith, solar_azimuth)
1092     A = np.maximum(A, 0)
1093 
1094     B = tools.cosd(solar_zenith)
1095     B = np.maximum(B, tools.cosd(85))
1096 
1097     # Calculate Diffuse POA from sky dome
1098     term1 = 0.5 * (1 - F1) * (1 + tools.cosd(surface_tilt))
1099     term2 = F1 * A / B
1100     term3 = F2 * tools.sind(surface_tilt)
1101 
1102     sky_diffuse = np.maximum(dhi * (term1 + term2 + term3), 0)
1103 
1104     # we've preserved the input type until now, so don't ruin it!
1105     if isinstance(sky_diffuse, pd.Series):
1106         sky_diffuse[np.isnan(airmass)] = 0
1107     else:
1108         sky_diffuse = np.where(np.isnan(airmass), 0, sky_diffuse)
1109 
1110     if return_components:
1111         diffuse_components = OrderedDict()
1112         diffuse_components['sky_diffuse'] = sky_diffuse
1113 
1114         # Calculate the different components
1115         diffuse_components['isotropic'] = dhi * term1
1116         diffuse_components['circumsolar'] = dhi * term2
1117         diffuse_components['horizon'] = dhi * term3
1118 
1119         # Set values of components to 0 when sky_diffuse is 0
1120         mask = sky_diffuse == 0
1121         if isinstance(sky_diffuse, pd.Series):
1122             diffuse_components = pd.DataFrame(diffuse_components)
1123             diffuse_components.loc[mask] = 0
1124         else:
1125             diffuse_components = {k: np.where(mask, 0, v) for k, v in
1126                                   diffuse_components.items()}
1127         return diffuse_components
1128     else:
1129         return sky_diffuse
1130 
1131 
1132 def clearsky_index(ghi, clearsky_ghi, max_clearsky_index=2.0):
1133     """"""
1134     Calculate the clearsky index.
1135 
1136     The clearsky index is the ratio of global to clearsky global irradiance.
1137     Negative and non-finite clearsky index values will be truncated to zero.
1138 
1139     Parameters
1140     ----------
1141     ghi : numeric
1142         Global horizontal irradiance in W/m^2.
1143 
1144     clearsky_ghi : numeric
1145         Modeled clearsky GHI
1146 
1147     max_clearsky_index : numeric, default 2.0
1148         Maximum value of the clearsky index. The default, 2.0, allows
1149         for over-irradiance events typically seen in sub-hourly data.
1150 
1151     Returns
1152     -------
1153     clearsky_index : numeric
1154         Clearsky index
1155     """"""
1156     clearsky_index = ghi / clearsky_ghi
1157     # set +inf, -inf, and nans to zero
1158     clearsky_index = np.where(~np.isfinite(clearsky_index), 0,
1159                               clearsky_index)
1160     # but preserve nans in the input arrays
1161     input_is_nan = ~np.isfinite(ghi) | ~np.isfinite(clearsky_ghi)
1162     clearsky_index = np.where(input_is_nan, np.nan, clearsky_index)
1163 
1164     clearsky_index = np.maximum(clearsky_index, 0)
1165     clearsky_index = np.minimum(clearsky_index, max_clearsky_index)
1166 
1167     # preserve input type
1168     if isinstance(ghi, pd.Series):
1169         clearsky_index = pd.Series(clearsky_index, index=ghi.index)
1170 
1171     return clearsky_index
1172 
1173 
1174 def clearness_index(ghi, solar_zenith, extra_radiation, min_cos_zenith=0.065,
1175                     max_clearness_index=2.0):
1176     """"""
1177     Calculate the clearness index.
1178 
1179     The clearness index is the ratio of global to extraterrestrial
1180     irradiance on a horizontal plane [1]_.
1181 
1182     Parameters
1183     ----------
1184     ghi : numeric
1185         Global horizontal irradiance in W/m^2.
1186 
1187     solar_zenith : numeric
1188         True (not refraction-corrected) solar zenith angle in decimal
1189         degrees.
1190 
1191     extra_radiation : numeric
1192         Irradiance incident at the top of the atmosphere
1193 
1194     min_cos_zenith : numeric, default 0.065
1195         Minimum value of cos(zenith) to allow when calculating global
1196         clearness index `kt`. Equivalent to zenith = 86.273 degrees.
1197 
1198     max_clearness_index : numeric, default 2.0
1199         Maximum value of the clearness index. The default, 2.0, allows
1200         for over-irradiance events typically seen in sub-hourly data.
1201         NREL's SRRL Fortran code used 0.82 for hourly data.
1202 
1203     Returns
1204     -------
1205     kt : numeric
1206         Clearness index
1207 
1208     References
1209     ----------
1210     .. [1] Maxwell, E. L., ""A Quasi-Physical Model for Converting Hourly
1211            Global Horizontal to Direct Normal Insolation"", Technical
1212            Report No. SERI/TR-215-3087, Golden, CO: Solar Energy Research
1213            Institute, 1987.
1214     """"""
1215     cos_zenith = tools.cosd(solar_zenith)
1216     I0h = extra_radiation * np.maximum(cos_zenith, min_cos_zenith)
1217     # consider adding
1218     # with np.errstate(invalid='ignore', divide='ignore'):
1219     # to kt calculation, but perhaps it's good to allow these
1220     # warnings to the users that override min_cos_zenith
1221     kt = ghi / I0h
1222     kt = np.maximum(kt, 0)
1223     kt = np.minimum(kt, max_clearness_index)
1224     return kt
1225 
1226 
1227 def clearness_index_zenith_independent(clearness_index, airmass,
1228                                        max_clearness_index=2.0):
1229     """"""
1230     Calculate the zenith angle independent clearness index.
1231 
1232     See [1]_ for details.
1233 
1234     Parameters
1235     ----------
1236     clearness_index : numeric
1237         Ratio of global to extraterrestrial irradiance on a horizontal
1238         plane
1239 
1240     airmass : numeric
1241         Airmass
1242 
1243     max_clearness_index : numeric, default 2.0
1244         Maximum value of the clearness index. The default, 2.0, allows
1245         for over-irradiance events typically seen in sub-hourly data.
1246         NREL's SRRL Fortran code used 0.82 for hourly data.
1247 
1248     Returns
1249     -------
1250     kt_prime : numeric
1251         Zenith independent clearness index
1252 
1253     References
1254     ----------
1255     .. [1] Perez, R., P. Ineichen, E. Maxwell, R. Seals and A. Zelenka,
1256            (1992). ""Dynamic Global-to-Direct Irradiance Conversion Models"".
1257            ASHRAE Transactions-Research Series, pp. 354-369
1258     """"""
1259     # Perez eqn 1
1260     kt_prime = clearness_index / _kt_kt_prime_factor(airmass)
1261     kt_prime = np.maximum(kt_prime, 0)
1262     kt_prime = np.minimum(kt_prime, max_clearness_index)
1263     return kt_prime
1264 
1265 
1266 def _kt_kt_prime_factor(airmass):
1267     """"""
1268     Calculate the conversion factor between kt and kt prime.
1269     Function is useful because DIRINT and GTI-DIRINT both use this.
1270     """"""
1271     # consider adding
1272     # airmass = np.maximum(airmass, 12)  # GH 450
1273     return 1.031 * np.exp(-1.4 / (0.9 + 9.4 / airmass)) + 0.1
1274 
1275 
1276 def disc(ghi, solar_zenith, datetime_or_doy, pressure=101325,
1277          min_cos_zenith=0.065, max_zenith=87, max_airmass=12):
1278     """"""
1279     Estimate Direct Normal Irradiance from Global Horizontal Irradiance
1280     using the DISC model.
1281 
1282     The DISC algorithm converts global horizontal irradiance to direct
1283     normal irradiance through empirical relationships between the global
1284     and direct clearness indices.
1285 
1286     The pvlib implementation limits the clearness index to 1.
1287 
1288     The original report describing the DISC model [1]_ uses the
1289     relative airmass rather than the absolute (pressure-corrected)
1290     airmass. However, the NREL implementation of the DISC model [2]_
1291     uses absolute airmass. PVLib Matlab also uses the absolute airmass.
1292     pvlib python defaults to absolute airmass, but the relative airmass
1293     can be used by supplying `pressure=None`.
1294 
1295     Parameters
1296     ----------
1297     ghi : numeric
1298         Global horizontal irradiance in W/m^2.
1299 
1300     solar_zenith : numeric
1301         True (not refraction-corrected) solar zenith angles in decimal
1302         degrees.
1303 
1304     datetime_or_doy : int, float, array, pd.DatetimeIndex
1305         Day of year or array of days of year e.g.
1306         pd.DatetimeIndex.dayofyear, or pd.DatetimeIndex.
1307 
1308     pressure : None or numeric, default 101325
1309         Site pressure in Pascal. If None, relative airmass is used
1310         instead of absolute (pressure-corrected) airmass.
1311 
1312     min_cos_zenith : numeric, default 0.065
1313         Minimum value of cos(zenith) to allow when calculating global
1314         clearness index `kt`. Equivalent to zenith = 86.273 degrees.
1315 
1316     max_zenith : numeric, default 87
1317         Maximum value of zenith to allow in DNI calculation. DNI will be
1318         set to 0 for times with zenith values greater than `max_zenith`.
1319 
1320     max_airmass : numeric, default 12
1321         Maximum value of the airmass to allow in Kn calculation.
1322         Default value (12) comes from range over which Kn was fit
1323         to airmass in the original paper.
1324 
1325     Returns
1326     -------
1327     output : OrderedDict or DataFrame
1328         Contains the following keys:
1329 
1330         * ``dni``: The modeled direct normal irradiance
1331           in W/m^2 provided by the
1332           Direct Insolation Simulation Code (DISC) model.
1333         * ``kt``: Ratio of global to extraterrestrial
1334           irradiance on a horizontal plane.
1335         * ``airmass``: Airmass
1336 
1337     References
1338     ----------
1339     .. [1] Maxwell, E. L., ""A Quasi-Physical Model for Converting Hourly
1340        Global Horizontal to Direct Normal Insolation"", Technical
1341        Report No. SERI/TR-215-3087, Golden, CO: Solar Energy Research
1342        Institute, 1987.
1343 
1344     .. [2] Maxwell, E. ""DISC Model"", Excel Worksheet.
1345        https://www.nrel.gov/grid/solar-resource/disc.html
1346 
1347     See Also
1348     --------
1349     dirint
1350     """"""
1351 
1352     # this is the I0 calculation from the reference
1353     # SSC uses solar constant = 1367.0 (checked 2018 08 15)
1354     I0 = get_extra_radiation(datetime_or_doy, 1370., 'spencer')
1355 
1356     kt = clearness_index(ghi, solar_zenith, I0, min_cos_zenith=min_cos_zenith,
1357                          max_clearness_index=1)
1358 
1359     am = atmosphere.get_relative_airmass(solar_zenith, model='kasten1966')
1360     if pressure is not None:
1361         am = atmosphere.get_absolute_airmass(am, pressure)
1362 
1363     Kn, am = _disc_kn(kt, am, max_airmass=max_airmass)
1364     dni = Kn * I0
1365 
1366     bad_values = (solar_zenith > max_zenith) | (ghi < 0) | (dni < 0)
1367     dni = np.where(bad_values, 0, dni)
1368 
1369     output = OrderedDict()
1370     output['dni'] = dni
1371     output['kt'] = kt
1372     output['airmass'] = am
1373 
1374     if isinstance(datetime_or_doy, pd.DatetimeIndex):
1375         output = pd.DataFrame(output, index=datetime_or_doy)
1376 
1377     return output
1378 
1379 
1380 def _disc_kn(clearness_index, airmass, max_airmass=12):
1381     """"""
1382     Calculate Kn for `disc`
1383 
1384     Parameters
1385     ----------
1386     clearness_index : numeric
1387     airmass : numeric
1388     max_airmass : float
1389         airmass > max_airmass is set to max_airmass before being used
1390         in calculating Kn.
1391 
1392     Returns
1393     -------
1394     Kn : numeric
1395     am : numeric
1396         airmass used in the calculation of Kn. am <= max_airmass.
1397     """"""
1398     # short names for equations
1399     kt = clearness_index
1400     am = airmass
1401 
1402     am = np.minimum(am, max_airmass)  # GH 450
1403 
1404     # powers of kt will be used repeatedly, so compute only once
1405     kt2 = kt * kt  # about the same as kt ** 2
1406     kt3 = kt2 * kt  # 5-10x faster than kt ** 3
1407 
1408     bools = (kt <= 0.6)
1409     a = np.where(bools,
1410                  0.512 - 1.56*kt + 2.286*kt2 - 2.222*kt3,
1411                  -5.743 + 21.77*kt - 27.49*kt2 + 11.56*kt3)
1412     b = np.where(bools,
1413                  0.37 + 0.962*kt,
1414                  41.4 - 118.5*kt + 66.05*kt2 + 31.9*kt3)
1415     c = np.where(bools,
1416                  -0.28 + 0.932*kt - 2.048*kt2,
1417                  -47.01 + 184.2*kt - 222.0*kt2 + 73.81*kt3)
1418 
1419     delta_kn = a + b * np.exp(c*am)
1420 
1421     Knc = 0.866 - 0.122*am + 0.0121*am**2 - 0.000653*am**3 + 1.4e-05*am**4
1422     Kn = Knc - delta_kn
1423     return Kn, am
1424 
1425 
1426 def dirint(ghi, solar_zenith, times, pressure=101325., use_delta_kt_prime=True,
1427            temp_dew=None, min_cos_zenith=0.065, max_zenith=87):
1428     """"""
1429     Determine DNI from GHI using the DIRINT modification of the DISC
1430     model.
1431 
1432     Implements the modified DISC model known as ""DIRINT"" introduced in
1433     [1]. DIRINT predicts direct normal irradiance (DNI) from measured
1434     global horizontal irradiance (GHI). DIRINT improves upon the DISC
1435     model by using time-series GHI data and dew point temperature
1436     information. The effectiveness of the DIRINT model improves with
1437     each piece of information provided.
1438 
1439     The pvlib implementation limits the clearness index to 1.
1440 
1441     Parameters
1442     ----------
1443     ghi : array-like
1444         Global horizontal irradiance in W/m^2.
1445 
1446     solar_zenith : array-like
1447         True (not refraction-corrected) solar_zenith angles in decimal
1448         degrees.
1449 
1450     times : DatetimeIndex
1451 
1452     pressure : float or array-like, default 101325.0
1453         The site pressure in Pascal. Pressure may be measured or an
1454         average pressure may be calculated from site altitude.
1455 
1456     use_delta_kt_prime : bool, default True
1457         If True, indicates that the stability index delta_kt_prime is
1458         included in the model. The stability index adjusts the estimated
1459         DNI in response to dynamics in the time series of GHI. It is
1460         recommended that delta_kt_prime is not used if the time between
1461         GHI points is 1.5 hours or greater. If use_delta_kt_prime=True,
1462         input data must be Series.
1463 
1464     temp_dew : None, float, or array-like, default None
1465         Surface dew point temperatures, in degrees C. Values of temp_dew
1466         may be numeric or NaN. Any single time period point with a
1467         temp_dew=NaN does not have dew point improvements applied. If
1468         temp_dew is not provided, then dew point improvements are not
1469         applied.
1470 
1471     min_cos_zenith : numeric, default 0.065
1472         Minimum value of cos(zenith) to allow when calculating global
1473         clearness index `kt`. Equivalent to zenith = 86.273 degrees.
1474 
1475     max_zenith : numeric, default 87
1476         Maximum value of zenith to allow in DNI calculation. DNI will be
1477         set to 0 for times with zenith values greater than `max_zenith`.
1478 
1479     Returns
1480     -------
1481     dni : array-like
1482         The modeled direct normal irradiance in W/m^2 provided by the
1483         DIRINT model.
1484 
1485     Notes
1486     -----
1487     DIRINT model requires time series data (ie. one of the inputs must
1488     be a vector of length > 2).
1489 
1490     References
1491     ----------
1492     .. [1] Perez, R., P. Ineichen, E. Maxwell, R. Seals and A. Zelenka,
1493        (1992). ""Dynamic Global-to-Direct Irradiance Conversion Models"".
1494        ASHRAE Transactions-Research Series, pp. 354-369
1495 
1496     .. [2] Maxwell, E. L., ""A Quasi-Physical Model for Converting Hourly
1497        Global Horizontal to Direct Normal Insolation"", Technical Report No.
1498        SERI/TR-215-3087, Golden, CO: Solar Energy Research Institute, 1987.
1499     """"""
1500 
1501     disc_out = disc(ghi, solar_zenith, times, pressure=pressure,
1502                     min_cos_zenith=min_cos_zenith, max_zenith=max_zenith)
1503     airmass = disc_out['airmass']
1504     kt = disc_out['kt']
1505 
1506     kt_prime = clearness_index_zenith_independent(
1507         kt, airmass, max_clearness_index=1)
1508     delta_kt_prime = _delta_kt_prime_dirint(kt_prime, use_delta_kt_prime,
1509                                             times)
1510     w = _temp_dew_dirint(temp_dew, times)
1511 
1512     dirint_coeffs = _dirint_coeffs(times, kt_prime, solar_zenith, w,
1513                                    delta_kt_prime)
1514 
1515     # Perez eqn 5
1516     dni = disc_out['dni'] * dirint_coeffs
1517 
1518     return dni
1519 
1520 
1521 def _dirint_from_dni_ktprime(dni, kt_prime, solar_zenith, use_delta_kt_prime,
1522                              temp_dew):
1523     """"""
1524     Calculate DIRINT DNI from supplied DISC DNI and Kt'.
1525 
1526     Supports :py:func:`gti_dirint`
1527     """"""
1528     times = dni.index
1529     delta_kt_prime = _delta_kt_prime_dirint(kt_prime, use_delta_kt_prime,
1530                                             times)
1531     w = _temp_dew_dirint(temp_dew, times)
1532     dirint_coeffs = _dirint_coeffs(times, kt_prime, solar_zenith, w,
1533                                    delta_kt_prime)
1534     dni_dirint = dni * dirint_coeffs
1535     return dni_dirint
1536 
1537 
1538 def _delta_kt_prime_dirint(kt_prime, use_delta_kt_prime, times):
1539     """"""
1540     Calculate delta_kt_prime (Perez eqn 2 and eqn 3), or return a default value
1541     for use with :py:func:`_dirint_bins`.
1542     """"""
1543     if use_delta_kt_prime:
1544         # Perez eqn 2
1545         kt_next = kt_prime.shift(-1)
1546         kt_previous = kt_prime.shift(1)
1547         # replace nan with values that implement Perez Eq 3 for first and last
1548         # positions. Use kt_previous and kt_next to handle series of length 1
1549         kt_next.iloc[-1] = kt_previous.iloc[-1]
1550         kt_previous.iloc[0] = kt_next.iloc[0]
1551         delta_kt_prime = 0.5 * ((kt_prime - kt_next).abs().add(
1552                                 (kt_prime - kt_previous).abs(),
1553                                 fill_value=0))
1554     else:
1555         # do not change unless also modifying _dirint_bins
1556         delta_kt_prime = pd.Series(-1, index=times)
1557     return delta_kt_prime
1558 
1559 
1560 def _temp_dew_dirint(temp_dew, times):
1561     """"""
1562     Calculate precipitable water from surface dew point temp (Perez eqn 4),
1563     or return a default value for use with :py:func:`_dirint_bins`.
1564     """"""
1565     if temp_dew is not None:
1566         # Perez eqn 4
1567         w = pd.Series(np.exp(0.07 * temp_dew - 0.075), index=times)
1568     else:
1569         # do not change unless also modifying _dirint_bins
1570         w = pd.Series(-1, index=times)
1571     return w
1572 
1573 
1574 def _dirint_coeffs(times, kt_prime, solar_zenith, w, delta_kt_prime):
1575     """"""
1576     Determine the DISC to DIRINT multiplier `dirint_coeffs`.
1577 
1578     dni = disc_out['dni'] * dirint_coeffs
1579 
1580     Parameters
1581     ----------
1582     times : pd.DatetimeIndex
1583     kt_prime : Zenith-independent clearness index
1584     solar_zenith : Solar zenith angle
1585     w : precipitable water estimated from surface dew-point temperature
1586     delta_kt_prime : stability index
1587 
1588     Returns
1589     -------
1590     dirint_coeffs : array-like
1591     """"""
1592     kt_prime_bin, zenith_bin, w_bin, delta_kt_prime_bin = \
1593         _dirint_bins(times, kt_prime, solar_zenith, w, delta_kt_prime)
1594 
1595     # get the coefficients
1596     coeffs = _get_dirint_coeffs()
1597 
1598     # subtract 1 to account for difference between MATLAB-style bin
1599     # assignment and Python-style array lookup.
1600     dirint_coeffs = coeffs[kt_prime_bin-1, zenith_bin-1,
1601                            delta_kt_prime_bin-1, w_bin-1]
1602 
1603     # convert unassigned bins to nan
1604     dirint_coeffs = np.where((kt_prime_bin == 0) | (zenith_bin == 0) |
1605                              (w_bin == 0) | (delta_kt_prime_bin == 0),
1606                              np.nan, dirint_coeffs)
1607     return dirint_coeffs
1608 
1609 
1610 def _dirint_bins(times, kt_prime, zenith, w, delta_kt_prime):
1611     """"""
1612     Determine the bins for the DIRINT coefficients.
1613 
1614     Parameters
1615     ----------
1616     times : pd.DatetimeIndex
1617     kt_prime : Zenith-independent clearness index
1618     zenith : Solar zenith angle
1619     w : precipitable water estimated from surface dew-point temperature
1620     delta_kt_prime : stability index
1621 
1622     Returns
1623     -------
1624     tuple of kt_prime_bin, zenith_bin, w_bin, delta_kt_prime_bin
1625     """"""
1626     # @wholmgren: the following bin assignments use MATLAB's 1-indexing.
1627     # Later, we'll subtract 1 to conform to Python's 0-indexing.
1628 
1629     # Create kt_prime bins
1630     kt_prime_bin = pd.Series(0, index=times, dtype=np.int64)
1631     kt_prime_bin[(kt_prime >= 0) & (kt_prime < 0.24)] = 1
1632     kt_prime_bin[(kt_prime >= 0.24) & (kt_prime < 0.4)] = 2
1633     kt_prime_bin[(kt_prime >= 0.4) & (kt_prime < 0.56)] = 3
1634     kt_prime_bin[(kt_prime >= 0.56) & (kt_prime < 0.7)] = 4
1635     kt_prime_bin[(kt_prime >= 0.7) & (kt_prime < 0.8)] = 5
1636     kt_prime_bin[(kt_prime >= 0.8) & (kt_prime <= 1)] = 6
1637 
1638     # Create zenith angle bins
1639     zenith_bin = pd.Series(0, index=times, dtype=np.int64)
1640     zenith_bin[(zenith >= 0) & (zenith < 25)] = 1
1641     zenith_bin[(zenith >= 25) & (zenith < 40)] = 2
1642     zenith_bin[(zenith >= 40) & (zenith < 55)] = 3
1643     zenith_bin[(zenith >= 55) & (zenith < 70)] = 4
1644     zenith_bin[(zenith >= 70) & (zenith < 80)] = 5
1645     zenith_bin[(zenith >= 80)] = 6
1646 
1647     # Create the bins for w based on dew point temperature
1648     w_bin = pd.Series(0, index=times, dtype=np.int64)
1649     w_bin[(w >= 0) & (w < 1)] = 1
1650     w_bin[(w >= 1) & (w < 2)] = 2
1651     w_bin[(w >= 2) & (w < 3)] = 3
1652     w_bin[(w >= 3)] = 4
1653     w_bin[(w == -1)] = 5
1654 
1655     # Create delta_kt_prime binning.
1656     delta_kt_prime_bin = pd.Series(0, index=times, dtype=np.int64)
1657     delta_kt_prime_bin[(delta_kt_prime >= 0) & (delta_kt_prime < 0.015)] = 1
1658     delta_kt_prime_bin[(delta_kt_prime >= 0.015) &
1659                        (delta_kt_prime < 0.035)] = 2
1660     delta_kt_prime_bin[(delta_kt_prime >= 0.035) & (delta_kt_prime < 0.07)] = 3
1661     delta_kt_prime_bin[(delta_kt_prime >= 0.07) & (delta_kt_prime < 0.15)] = 4
1662     delta_kt_prime_bin[(delta_kt_prime >= 0.15) & (delta_kt_prime < 0.3)] = 5
1663     delta_kt_prime_bin[(delta_kt_prime >= 0.3) & (delta_kt_prime <= 1)] = 6
1664     delta_kt_prime_bin[delta_kt_prime == -1] = 7
1665 
1666     return kt_prime_bin, zenith_bin, w_bin, delta_kt_prime_bin
1667 
1668 
1669 def dirindex(ghi, ghi_clearsky, dni_clearsky, zenith, times, pressure=101325.,
1670              use_delta_kt_prime=True, temp_dew=None, min_cos_zenith=0.065,
1671              max_zenith=87):
1672     """"""
1673     Determine DNI from GHI using the DIRINDEX model.
1674 
1675     The DIRINDEX model [1] modifies the DIRINT model implemented in
1676     :py:func:``pvlib.irradiance.dirint`` by taking into account information
1677     from a clear sky model. It is recommended that ``ghi_clearsky`` be
1678     calculated using the Ineichen clear sky model
1679     :py:func:``pvlib.clearsky.ineichen`` with ``perez_enhancement=True``.
1680 
1681     The pvlib implementation limits the clearness index to 1.
1682 
1683     Parameters
1684     ----------
1685     ghi : array-like
1686         Global horizontal irradiance in W/m^2.
1687 
1688     ghi_clearsky : array-like
1689         Global horizontal irradiance from clear sky model, in W/m^2.
1690 
1691     dni_clearsky : array-like
1692         Direct normal irradiance from clear sky model, in W/m^2.
1693 
1694     zenith : array-like
1695         True (not refraction-corrected) zenith angles in decimal
1696         degrees. If Z is a vector it must be of the same size as all
1697         other vector inputs. Z must be >=0 and <=180.
1698 
1699     times : DatetimeIndex
1700 
1701     pressure : float or array-like, default 101325.0
1702         The site pressure in Pascal. Pressure may be measured or an
1703         average pressure may be calculated from site altitude.
1704 
1705     use_delta_kt_prime : bool, default True
1706         If True, indicates that the stability index delta_kt_prime is
1707         included in the model. The stability index adjusts the estimated
1708         DNI in response to dynamics in the time series of GHI. It is
1709         recommended that delta_kt_prime is not used if the time between
1710         GHI points is 1.5 hours or greater. If use_delta_kt_prime=True,
1711         input data must be Series.
1712 
1713     temp_dew : None, float, or array-like, default None
1714         Surface dew point temperatures, in degrees C. Values of temp_dew
1715         may be numeric or NaN. Any single time period point with a
1716         temp_dew=NaN does not have dew point improvements applied. If
1717         temp_dew is not provided, then dew point improvements are not
1718         applied.
1719 
1720     min_cos_zenith : numeric, default 0.065
1721         Minimum value of cos(zenith) to allow when calculating global
1722         clearness index `kt`. Equivalent to zenith = 86.273 degrees.
1723 
1724     max_zenith : numeric, default 87
1725         Maximum value of zenith to allow in DNI calculation. DNI will be
1726         set to 0 for times with zenith values greater than `max_zenith`.
1727 
1728     Returns
1729     -------
1730     dni : array-like
1731         The modeled direct normal irradiance in W/m^2.
1732 
1733     Notes
1734     -----
1735     DIRINDEX model requires time series data (ie. one of the inputs must
1736     be a vector of length > 2).
1737 
1738     References
1739     ----------
1740     .. [1] Perez, R., Ineichen, P., Moore, K., Kmiecik, M., Chain, C., George,
1741        R., & Vignola, F. (2002). A new operational model for satellite-derived
1742        irradiances: description and validation. Solar Energy, 73(5), 307-317.
1743     """"""
1744 
1745     dni_dirint = dirint(ghi, zenith, times, pressure=pressure,
1746                         use_delta_kt_prime=use_delta_kt_prime,
1747                         temp_dew=temp_dew, min_cos_zenith=min_cos_zenith,
1748                         max_zenith=max_zenith)
1749 
1750     dni_dirint_clearsky = dirint(ghi_clearsky, zenith, times,
1751                                  pressure=pressure,
1752                                  use_delta_kt_prime=use_delta_kt_prime,
1753                                  temp_dew=temp_dew,
1754                                  min_cos_zenith=min_cos_zenith,
1755                                  max_zenith=max_zenith)
1756 
1757     dni_dirindex = dni_clearsky * dni_dirint / dni_dirint_clearsky
1758 
1759     dni_dirindex[dni_dirindex < 0] = 0.
1760 
1761     return dni_dirindex
1762 
1763 
1764 def gti_dirint(poa_global, aoi, solar_zenith, solar_azimuth, times,
1765                surface_tilt, surface_azimuth, pressure=101325.,
1766                use_delta_kt_prime=True, temp_dew=None, albedo=.25,
1767                model='perez', model_perez='allsitescomposite1990',
1768                calculate_gt_90=True, max_iterations=30):
1769     """"""
1770     Determine GHI, DNI, DHI from POA global using the GTI DIRINT model.
1771 
1772     The GTI DIRINT model is described in [1]_.
1773 
1774     .. warning::
1775 
1776         Model performance is poor for AOI greater than approximately
1777         80 degrees `and` plane of array irradiance greater than
1778         approximately 200 W/m^2.
1779 
1780     Parameters
1781     ----------
1782     poa_global : array-like
1783         Plane of array global irradiance in W/m^2.
1784 
1785     aoi : array-like
1786         Angle of incidence of solar rays with respect to the module
1787         surface normal.
1788 
1789     solar_zenith : array-like
1790         True (not refraction-corrected) solar zenith angles in decimal
1791         degrees.
1792 
1793     solar_azimuth : array-like
1794         Solar azimuth angles in decimal degrees.
1795 
1796     times : DatetimeIndex
1797         Time indices for the input array-like data.
1798 
1799     surface_tilt : numeric
1800         Surface tilt angles in decimal degrees. Tilt must be >=0 and
1801         <=180. The tilt angle is defined as degrees from horizontal
1802         (e.g. surface facing up = 0, surface facing horizon = 90).
1803 
1804     surface_azimuth : numeric
1805         Surface azimuth angles in decimal degrees. surface_azimuth must
1806         be >=0 and <=360. The Azimuth convention is defined as degrees
1807         east of north (e.g. North = 0, South=180 East = 90, West = 270).
1808 
1809     pressure : numeric, default 101325.0
1810         The site pressure in Pascal. Pressure may be measured or an
1811         average pressure may be calculated from site altitude.
1812 
1813     use_delta_kt_prime : bool, default True
1814         If True, indicates that the stability index delta_kt_prime is
1815         included in the model. The stability index adjusts the estimated
1816         DNI in response to dynamics in the time series of GHI. It is
1817         recommended that delta_kt_prime is not used if the time between
1818         GHI points is 1.5 hours or greater. If use_delta_kt_prime=True,
1819         input data must be Series.
1820 
1821     temp_dew : None, float, or array-like, default None
1822         Surface dew point temperatures, in degrees C. Values of temp_dew
1823         may be numeric or NaN. Any single time period point with a
1824         temp_dew=NaN does not have dew point improvements applied. If
1825         temp_dew is not provided, then dew point improvements are not
1826         applied.
1827 
1828     albedo : numeric, default 0.25
1829         Surface albedo
1830 
1831     model : String, default 'isotropic'
1832         Irradiance model.
1833 
1834     model_perez : String, default 'allsitescomposite1990'
1835         Used only if model='perez'. See :py:func:`perez`.
1836 
1837     calculate_gt_90 : bool, default True
1838         Controls if the algorithm evaluates inputs with AOI >= 90 degrees.
1839         If False, returns nan for AOI >= 90 degrees. Significant speed ups
1840         can be achieved by setting this parameter to False.
1841 
1842     max_iterations : int, default 30
1843         Maximum number of iterations for the aoi < 90 deg algorithm.
1844 
1845     Returns
1846     -------
1847     data : OrderedDict or DataFrame
1848         Contains the following keys/columns:
1849 
1850             * ``ghi``: the modeled global horizontal irradiance in W/m^2.
1851             * ``dni``: the modeled direct normal irradiance in W/m^2.
1852             * ``dhi``: the modeled diffuse horizontal irradiance in
1853               W/m^2.
1854 
1855     References
1856     ----------
1857     .. [1] B. Marion, A model for deriving the direct normal and
1858            diffuse horizontal irradiance from the global tilted
1859            irradiance, Solar Energy 122, 1037-1046.
1860            :doi:`10.1016/j.solener.2015.10.024`
1861     """"""
1862 
1863     aoi_lt_90 = aoi < 90
1864 
1865     # for AOI less than 90 degrees
1866     ghi, dni, dhi, kt_prime = _gti_dirint_lt_90(
1867         poa_global, aoi, aoi_lt_90, solar_zenith, solar_azimuth, times,
1868         surface_tilt, surface_azimuth, pressure=pressure,
1869         use_delta_kt_prime=use_delta_kt_prime, temp_dew=temp_dew,
1870         albedo=albedo, model=model, model_perez=model_perez,
1871         max_iterations=max_iterations)
1872 
1873     # for AOI greater than or equal to 90 degrees
1874     if calculate_gt_90:
1875         ghi_gte_90, dni_gte_90, dhi_gte_90 = _gti_dirint_gte_90(
1876             poa_global, aoi, solar_zenith, solar_azimuth,
1877             surface_tilt, times, kt_prime,
1878             pressure=pressure, temp_dew=temp_dew, albedo=albedo)
1879     else:
1880         ghi_gte_90, dni_gte_90, dhi_gte_90 = np.nan, np.nan, np.nan
1881 
1882     # put the AOI < 90 and AOI >= 90 conditions together
1883     output = OrderedDict()
1884     output['ghi'] = ghi.where(aoi_lt_90, ghi_gte_90)
1885     output['dni'] = dni.where(aoi_lt_90, dni_gte_90)
1886     output['dhi'] = dhi.where(aoi_lt_90, dhi_gte_90)
1887 
1888     output = pd.DataFrame(output, index=times)
1889 
1890     return output
1891 
1892 
1893 def _gti_dirint_lt_90(poa_global, aoi, aoi_lt_90, solar_zenith, solar_azimuth,
1894                       times, surface_tilt, surface_azimuth, pressure=101325.,
1895                       use_delta_kt_prime=True, temp_dew=None, albedo=.25,
1896                       model='perez', model_perez='allsitescomposite1990',
1897                       max_iterations=30):
1898     """"""
1899     GTI-DIRINT model for AOI < 90 degrees. See Marion 2015 Section 2.1.
1900 
1901     See gti_dirint signature for parameter details.
1902     """"""
1903     I0 = get_extra_radiation(times, 1370, 'spencer')
1904     cos_zenith = tools.cosd(solar_zenith)
1905     # I0h as in Marion 2015 eqns 1, 3
1906     I0h = I0 * np.maximum(0.065, cos_zenith)
1907 
1908     airmass = atmosphere.get_relative_airmass(solar_zenith, model='kasten1966')
1909     airmass = atmosphere.get_absolute_airmass(airmass, pressure)
1910 
1911     # these coeffs and diff variables and the loop below
1912     # implement figure 1 of Marion 2015
1913 
1914     # make coeffs that is at least 30 elements long so that all
1915     # coeffs can be assigned as specified in Marion 2015.
1916     # slice below will limit iterations if necessary
1917     coeffs = np.empty(max(30, max_iterations))
1918     coeffs[0:3] = 1
1919     coeffs[3:10] = 0.5
1920     coeffs[10:20] = 0.25
1921     coeffs[20:] = 0.125
1922     coeffs = coeffs[:max_iterations]  # covers case where max_iterations < 30
1923 
1924     # initialize diff
1925     diff = pd.Series(9999, index=times)
1926     best_diff = diff
1927 
1928     # initialize poa_global_i
1929     poa_global_i = poa_global
1930 
1931     for iteration, coeff in enumerate(coeffs):
1932 
1933         # test if difference between modeled GTI and
1934         # measured GTI (poa_global) is less than 1 W/m^2
1935         # only test for aoi less than 90 deg
1936         best_diff_lte_1 = best_diff <= 1
1937         best_diff_lte_1_lt_90 = best_diff_lte_1[aoi_lt_90]
1938         if best_diff_lte_1_lt_90.all():
1939             # all aoi < 90 points have a difference <= 1, so break loop
1940             break
1941 
1942         # calculate kt and DNI from GTI
1943         kt = clearness_index(poa_global_i, aoi, I0)  # kt from Marion eqn 2
1944         disc_dni = np.maximum(_disc_kn(kt, airmass)[0] * I0, 0)
1945         kt_prime = clearness_index_zenith_independent(kt, airmass)
1946         # dirint DNI in Marion eqn 3
1947         dni = _dirint_from_dni_ktprime(disc_dni, kt_prime, solar_zenith,
1948                                        use_delta_kt_prime, temp_dew)
1949 
1950         # calculate DHI using Marion eqn 3 (identify 1st term on RHS as GHI)
1951         # I0h has a minimum zenith projection, but multiplier of DNI does not
1952         ghi = kt * I0h                  # Kt * I0 * max(0.065, cos(zen))
1953         dhi = ghi - dni * cos_zenith    # no cos(zen) restriction here
1954 
1955         # following SSC code
1956         dni = np.maximum(dni, 0)
1957         ghi = np.maximum(ghi, 0)
1958         dhi = np.maximum(dhi, 0)
1959 
1960         # use DNI and DHI to model GTI
1961         # GTI-DIRINT uses perez transposition model, but we allow for
1962         # any model here
1963         all_irrad = get_total_irradiance(
1964             surface_tilt, surface_azimuth, solar_zenith, solar_azimuth,
1965             dni, ghi, dhi, dni_extra=I0, airmass=airmass,
1966             albedo=albedo, model=model, model_perez=model_perez)
1967 
1968         gti_model = all_irrad['poa_global']
1969 
1970         # calculate new diff
1971         diff = gti_model - poa_global
1972 
1973         # determine if the new diff is smaller in magnitude
1974         # than the old diff
1975         diff_abs = diff.abs()
1976         smallest_diff = diff_abs < best_diff
1977 
1978         # save the best differences
1979         best_diff = diff_abs.where(smallest_diff, best_diff)
1980 
1981         # on first iteration, the best values are the only values
1982         if iteration == 0:
1983             best_ghi = ghi
1984             best_dni = dni
1985             best_dhi = dhi
1986             best_kt_prime = kt_prime
1987         else:
1988             # save new DNI, DHI, DHI if they provide the best consistency
1989             # otherwise use the older values.
1990             best_ghi = ghi.where(smallest_diff, best_ghi)
1991             best_dni = dni.where(smallest_diff, best_dni)
1992             best_dhi = dhi.where(smallest_diff, best_dhi)
1993             best_kt_prime = kt_prime.where(smallest_diff, best_kt_prime)
1994 
1995         # calculate adjusted inputs for next iteration. Marion eqn 4
1996         poa_global_i = np.maximum(1.0, poa_global_i - coeff * diff)
1997     else:
1998         # we are here because we ran out of coeffs to loop over and
1999         # therefore we have exceeded max_iterations
2000         import warnings
2001         failed_points = best_diff[aoi_lt_90][~best_diff_lte_1_lt_90]
2002         warnings.warn(
2003             ('%s points failed to converge after %s iterations. best_diff:\n%s'
2004              % (len(failed_points), max_iterations, failed_points)),
2005             RuntimeWarning)
2006 
2007     # return the best data, whether or not the solution converged
2008     return best_ghi, best_dni, best_dhi, best_kt_prime
2009 
2010 
2011 def _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,
2012                        surface_tilt, times, kt_prime,
2013                        pressure=101325., temp_dew=None, albedo=.25):
2014     """"""
2015     GTI-DIRINT model for AOI >= 90 degrees. See Marion 2015 Section 2.2.
2016 
2017     See gti_dirint signature for parameter details.
2018     """"""
2019     kt_prime_gte_90 = _gti_dirint_gte_90_kt_prime(aoi, solar_zenith,
2020                                                   solar_azimuth, times,
2021                                                   kt_prime)
2022 
2023     I0 = get_extra_radiation(times, 1370, 'spencer')
2024     airmass = atmosphere.get_relative_airmass(solar_zenith, model='kasten1966')
2025     airmass = atmosphere.get_absolute_airmass(airmass, pressure)
2026     kt = kt_prime_gte_90 * _kt_kt_prime_factor(airmass)
2027     disc_dni = np.maximum(_disc_kn(kt, airmass)[0] * I0, 0)
2028 
2029     dni_gte_90 = _dirint_from_dni_ktprime(disc_dni, kt_prime, solar_zenith,
2030                                           False, temp_dew)
2031 
2032     dni_gte_90_proj = dni_gte_90 * tools.cosd(solar_zenith)
2033     cos_surface_tilt = tools.cosd(surface_tilt)
2034 
2035     # isotropic sky plus ground diffuse
2036     dhi_gte_90 = (
2037         (2 * poa_global - dni_gte_90_proj * albedo * (1 - cos_surface_tilt)) /
2038         (1 + cos_surface_tilt + albedo * (1 - cos_surface_tilt)))
2039 
2040     ghi_gte_90 = dni_gte_90_proj + dhi_gte_90
2041 
2042     return ghi_gte_90, dni_gte_90, dhi_gte_90
2043 
2044 
2045 def _gti_dirint_gte_90_kt_prime(aoi, solar_zenith, solar_azimuth, times,
2046                                 kt_prime):
2047     """"""
2048     Determine kt' values to be used in GTI-DIRINT AOI >= 90 deg case.
2049     See Marion 2015 Section 2.2.
2050 
2051     For AOI >= 90 deg: average of the kt_prime values for 65 < AOI < 80
2052     in each day's morning and afternoon. Morning and afternoon are treated
2053     separately.
2054 
2055     For AOI < 90 deg: NaN.
2056 
2057     See gti_dirint signature for parameter details.
2058 
2059     Returns
2060     -------
2061     kt_prime_gte_90 : Series
2062         Index is `times`.
2063     """"""
2064     # kt_prime values from DIRINT calculation for AOI < 90 case
2065     # set the kt_prime from sunrise to AOI=90 to be equal to
2066     # the kt_prime for 65 < AOI < 80 during the morning.
2067     # similar for the afternoon. repeat for every day.
2068     aoi_gte_90 = aoi >= 90
2069     aoi_65_80 = (aoi > 65) & (aoi < 80)
2070     zenith_lt_90 = solar_zenith < 90
2071     morning = solar_azimuth < 180
2072     afternoon = solar_azimuth > 180
2073     aoi_65_80_morning = aoi_65_80 & morning
2074     aoi_65_80_afternoon = aoi_65_80 & afternoon
2075     zenith_lt_90_aoi_gte_90_morning = zenith_lt_90 & aoi_gte_90 & morning
2076     zenith_lt_90_aoi_gte_90_afternoon = zenith_lt_90 & aoi_gte_90 & afternoon
2077 
2078     kt_prime_gte_90 = []
2079     for date, data in kt_prime.groupby(times.date):
2080         kt_prime_am_avg = data[aoi_65_80_morning].mean()
2081         kt_prime_pm_avg = data[aoi_65_80_afternoon].mean()
2082 
2083         kt_prime_by_date = pd.Series(np.nan, index=data.index)
2084         kt_prime_by_date[zenith_lt_90_aoi_gte_90_morning] = kt_prime_am_avg
2085         kt_prime_by_date[zenith_lt_90_aoi_gte_90_afternoon] = kt_prime_pm_avg
2086         kt_prime_gte_90.append(kt_prime_by_date)
2087     kt_prime_gte_90 = pd.concat(kt_prime_gte_90)
2088 
2089     return kt_prime_gte_90
2090 
2091 
2092 def erbs(ghi, zenith, datetime_or_doy, min_cos_zenith=0.065, max_zenith=87):
2093     r""""""
2094     Estimate DNI and DHI from GHI using the Erbs model.
2095 
2096     The Erbs model [1]_ estimates the diffuse fraction DF from global
2097     horizontal irradiance through an empirical relationship between DF
2098     and the ratio of GHI to extraterrestrial irradiance, Kt. The
2099     function uses the diffuse fraction to compute DHI as
2100 
2101     .. math::
2102 
2103         DHI = DF \times GHI
2104 
2105     DNI is then estimated as
2106 
2107     .. math::
2108 
2109         DNI = (GHI - DHI)/\cos(Z)
2110 
2111     where Z is the zenith angle.
2112 
2113     Parameters
2114     ----------
2115     ghi: numeric
2116         Global horizontal irradiance in W/m^2.
2117     zenith: numeric
2118         True (not refraction-corrected) zenith angles in decimal degrees.
2119     datetime_or_doy : int, float, array, pd.DatetimeIndex
2120         Day of year or array of days of year e.g.
2121         pd.DatetimeIndex.dayofyear, or pd.DatetimeIndex.
2122     min_cos_zenith : numeric, default 0.065
2123         Minimum value of cos(zenith) to allow when calculating global
2124         clearness index `kt`. Equivalent to zenith = 86.273 degrees.
2125     max_zenith : numeric, default 87
2126         Maximum value of zenith to allow in DNI calculation. DNI will be
2127         set to 0 for times with zenith values greater than `max_zenith`.
2128 
2129     Returns
2130     -------
2131     data : OrderedDict or DataFrame
2132         Contains the following keys/columns:
2133 
2134             * ``dni``: the modeled direct normal irradiance in W/m^2.
2135             * ``dhi``: the modeled diffuse horizontal irradiance in
2136               W/m^2.
2137             * ``kt``: Ratio of global to extraterrestrial irradiance
2138               on a horizontal plane.
2139 
2140     References
2141     ----------
2142     .. [1] D. G. Erbs, S. A. Klein and J. A. Duffie, Estimation of the
2143        diffuse radiation fraction for hourly, daily and monthly-average
2144        global radiation, Solar Energy 28(4), pp 293-302, 1982. Eq. 1
2145 
2146     See also
2147     --------
2148     dirint
2149     disc
2150     """"""
2151 
2152     dni_extra = get_extra_radiation(datetime_or_doy)
2153 
2154     kt = clearness_index(ghi, zenith, dni_extra, min_cos_zenith=min_cos_zenith,
2155                          max_clearness_index=1)
2156 
2157     # For Kt <= 0.22, set the diffuse fraction
2158     df = 1 - 0.09*kt
2159 
2160     # For Kt > 0.22 and Kt <= 0.8, set the diffuse fraction
2161     df = np.where((kt > 0.22) & (kt <= 0.8),
2162                   0.9511 - 0.1604*kt + 4.388*kt**2 -
2163                   16.638*kt**3 + 12.336*kt**4,
2164                   df)
2165 
2166     # For Kt > 0.8, set the diffuse fraction
2167     df = np.where(kt > 0.8, 0.165, df)
2168 
2169     dhi = df * ghi
2170 
2171     dni = (ghi - dhi) / tools.cosd(zenith)
2172     bad_values = (zenith > max_zenith) | (ghi < 0) | (dni < 0)
2173     dni = np.where(bad_values, 0, dni)
2174     # ensure that closure relationship remains valid
2175     dhi = np.where(bad_values, ghi, dhi)
2176 
2177     data = OrderedDict()
2178     data['dni'] = dni
2179     data['dhi'] = dhi
2180     data['kt'] = kt
2181 
2182     if isinstance(datetime_or_doy, pd.DatetimeIndex):
2183         data = pd.DataFrame(data, index=datetime_or_doy)
2184 
2185     return data
2186 
2187 
2188 def campbell_norman(zenith, transmittance, pressure=101325.0,
2189                     dni_extra=1367.0):
2190     '''
2191     Determine DNI, DHI, GHI from extraterrestrial flux, transmittance,
2192     and atmospheric pressure.
2193 
2194     Parameters
2195     ----------
2196     zenith: pd.Series
2197         True (not refraction-corrected) zenith angles in decimal
2198         degrees. If Z is a vector it must be of the same size as all
2199         other vector inputs. Z must be >=0 and <=180.
2200 
2201     transmittance: float
2202         Atmospheric transmittance between 0 and 1.
2203 
2204     pressure: float, default 101325.0
2205         Air pressure
2206 
2207     dni_extra: float, default 1367.0
2208         Direct irradiance incident at the top of the atmosphere.
2209 
2210     Returns
2211     -------
2212     irradiance: DataFrame
2213         Modeled direct normal irradiance, direct horizontal irradiance,
2214         and global horizontal irradiance in W/m^2
2215 
2216     References
2217     ----------
2218     .. [1] Campbell, G. S., J. M. Norman (1998) An Introduction to
2219        Environmental Biophysics. 2nd Ed. New York: Springer.
2220     '''
2221 
2222     tau = transmittance
2223 
2224     airmass = atmosphere.get_relative_airmass(zenith, model='simple')
2225     airmass = atmosphere.get_absolute_airmass(airmass, pressure=pressure)
2226     dni = dni_extra*tau**airmass
2227     cos_zen = tools.cosd(zenith)
2228     dhi = 0.3 * (1.0 - tau**airmass) * dni_extra * cos_zen
2229     ghi = dhi + dni * cos_zen
2230 
2231     irrads = OrderedDict()
2232     irrads['ghi'] = ghi
2233     irrads['dni'] = dni
2234     irrads['dhi'] = dhi
2235 
2236     if isinstance(ghi, pd.Series):
2237         irrads = pd.DataFrame(irrads)
2238 
2239     return irrads
2240 
2241 
2242 def _liujordan(zenith, transmittance, airmass, dni_extra=1367.0):
2243     '''
2244     Determine DNI, DHI, GHI from extraterrestrial flux, transmittance,
2245     and optical air mass number.
2246 
2247     Liu and Jordan, 1960, developed a simplified direct radiation model.
2248     DHI is from an empirical equation for diffuse radiation from Liu and
2249     Jordan, 1960.
2250 
2251     Parameters
2252     ----------
2253     zenith: pd.Series
2254         True (not refraction-corrected) zenith angles in decimal
2255         degrees. If Z is a vector it must be of the same size as all
2256         other vector inputs. Z must be >=0 and <=180.
2257 
2258     transmittance: float
2259         Atmospheric transmittance between 0 and 1.
2260 
2261     pressure: float, default 101325.0
2262         Air pressure
2263 
2264     dni_extra: float, default 1367.0
2265         Direct irradiance incident at the top of the atmosphere.
2266 
2267     Returns
2268     -------
2269     irradiance: DataFrame
2270         Modeled direct normal irradiance, direct horizontal irradiance,
2271         and global horizontal irradiance in W/m^2
2272 
2273     References
2274     ----------
2275     .. [1] Campbell, G. S., J. M. Norman (1998) An Introduction to
2276        Environmental Biophysics. 2nd Ed. New York: Springer.
2277 
2278     .. [2] Liu, B. Y., R. C. Jordan, (1960). ""The interrelationship and
2279        characteristic distribution of direct, diffuse, and total solar
2280        radiation"".  Solar Energy 4:1-19
2281     '''
2282 
2283     tau = transmittance
2284 
2285     dni = dni_extra*tau**airmass
2286     dhi = 0.3 * (1.0 - tau**airmass) * dni_extra * np.cos(np.radians(zenith))
2287     ghi = dhi + dni * np.cos(np.radians(zenith))
2288 
2289     irrads = OrderedDict()
2290     irrads['ghi'] = ghi
2291     irrads['dni'] = dni
2292     irrads['dhi'] = dhi
2293 
2294     if isinstance(ghi, pd.Series):
2295         irrads = pd.DataFrame(irrads)
2296 
2297     return irrads
2298 
2299 
2300 def _get_perez_coefficients(perezmodel):
2301     '''
2302     Find coefficients for the Perez model
2303 
2304     Parameters
2305     ----------
2306 
2307     perezmodel : string (optional, default='allsitescomposite1990')
2308 
2309           a character string which selects the desired set of Perez
2310           coefficients. If model is not provided as an input, the default,
2311           '1990' will be used.
2312 
2313     All possible model selections are:
2314 
2315           * '1990'
2316           * 'allsitescomposite1990' (same as '1990')
2317           * 'allsitescomposite1988'
2318           * 'sandiacomposite1988'
2319           * 'usacomposite1988'
2320           * 'france1988'
2321           * 'phoenix1988'
2322           * 'elmonte1988'
2323           * 'osage1988'
2324           * 'albuquerque1988'
2325           * 'capecanaveral1988'
2326           * 'albany1988'
2327 
2328     Returns
2329     --------
2330     F1coeffs, F2coeffs : (array, array)
2331           F1 and F2 coefficients for the Perez model
2332 
2333     References
2334     ----------
2335     .. [1] Loutzenhiser P.G. et. al. ""Empirical validation of models to
2336        compute solar irradiance on inclined surfaces for building energy
2337        simulation"" 2007, Solar Energy vol. 81. pp. 254-267
2338 
2339     .. [2] Perez, R., Seals, R., Ineichen, P., Stewart, R., Menicucci, D.,
2340        1987. A new simplified version of the Perez diffuse irradiance model
2341        for tilted surfaces. Solar Energy 39(3), 221-232.
2342 
2343     .. [3] Perez, R., Ineichen, P., Seals, R., Michalsky, J., Stewart, R.,
2344        1990. Modeling daylight availability and irradiance components from
2345        direct and global irradiance. Solar Energy 44 (5), 271-289.
2346 
2347     .. [4] Perez, R. et. al 1988. ""The Development and Verification of the
2348        Perez Diffuse Radiation Model"". SAND88-7030
2349 
2350     '''
2351     coeffdict = {
2352         'allsitescomposite1990': [
2353             [-0.0080,    0.5880,   -0.0620,   -0.0600,    0.0720,   -0.0220],
2354             [0.1300,    0.6830,   -0.1510,   -0.0190,    0.0660,   -0.0290],
2355             [0.3300,    0.4870,   -0.2210,    0.0550,   -0.0640,   -0.0260],
2356             [0.5680,    0.1870,   -0.2950,    0.1090,   -0.1520,   -0.0140],
2357             [0.8730,   -0.3920,   -0.3620,    0.2260,   -0.4620,    0.0010],
2358             [1.1320,   -1.2370,   -0.4120,    0.2880,   -0.8230,    0.0560],
2359             [1.0600,   -1.6000,   -0.3590,    0.2640,   -1.1270,    0.1310],
2360             [0.6780,   -0.3270,   -0.2500,    0.1560,   -1.3770,    0.2510]],
2361         'allsitescomposite1988': [
2362             [-0.0180,    0.7050,   -0.071,   -0.0580,    0.1020,   -0.0260],
2363             [0.1910,    0.6450,   -0.1710,    0.0120,    0.0090,   -0.0270],
2364             [0.4400,    0.3780,   -0.2560,    0.0870,   -0.1040,   -0.0250],
2365             [0.7560,   -0.1210,   -0.3460,    0.1790,   -0.3210,   -0.0080],
2366             [0.9960,   -0.6450,   -0.4050,    0.2600,   -0.5900,    0.0170],
2367             [1.0980,   -1.2900,   -0.3930,    0.2690,   -0.8320,    0.0750],
2368             [0.9730,   -1.1350,   -0.3780,    0.1240,   -0.2580,    0.1490],
2369             [0.6890,   -0.4120,   -0.2730,    0.1990,   -1.6750,    0.2370]],
2370         'sandiacomposite1988': [
2371             [-0.1960,    1.0840,   -0.0060,   -0.1140,    0.1800,   -0.0190],
2372             [0.2360,    0.5190,   -0.1800,   -0.0110,    0.0200,   -0.0380],
2373             [0.4540,    0.3210,   -0.2550,    0.0720,   -0.0980,   -0.0460],
2374             [0.8660,   -0.3810,   -0.3750,    0.2030,   -0.4030,   -0.0490],
2375             [1.0260,   -0.7110,   -0.4260,    0.2730,   -0.6020,   -0.0610],
2376             [0.9780,   -0.9860,   -0.3500,    0.2800,   -0.9150,   -0.0240],
2377             [0.7480,   -0.9130,   -0.2360,    0.1730,   -1.0450,    0.0650],
2378             [0.3180,   -0.7570,    0.1030,    0.0620,   -1.6980,    0.2360]],
2379         'usacomposite1988': [
2380             [-0.0340,    0.6710,   -0.0590,   -0.0590,    0.0860,   -0.0280],
2381             [0.2550,    0.4740,   -0.1910,    0.0180,   -0.0140,   -0.0330],
2382             [0.4270,    0.3490,   -0.2450,    0.0930,   -0.1210,   -0.0390],
2383             [0.7560,   -0.2130,   -0.3280,    0.1750,   -0.3040,   -0.0270],
2384             [1.0200,   -0.8570,   -0.3850,    0.2800,   -0.6380,   -0.0190],
2385             [1.0500,   -1.3440,   -0.3480,    0.2800,   -0.8930,    0.0370],
2386             [0.9740,   -1.5070,   -0.3700,    0.1540,   -0.5680,    0.1090],
2387             [0.7440,   -1.8170,   -0.2560,    0.2460,   -2.6180,    0.2300]],
2388         'france1988': [
2389             [0.0130,    0.7640,   -0.1000,   -0.0580,    0.1270,   -0.0230],
2390             [0.0950,    0.9200,   -0.1520,         0,    0.0510,   -0.0200],
2391             [0.4640,    0.4210,   -0.2800,    0.0640,   -0.0510,   -0.0020],
2392             [0.7590,   -0.0090,   -0.3730,    0.2010,   -0.3820,    0.0100],
2393             [0.9760,   -0.4000,   -0.4360,    0.2710,   -0.6380,    0.0510],
2394             [1.1760,   -1.2540,   -0.4620,    0.2950,   -0.9750,    0.1290],
2395             [1.1060,   -1.5630,   -0.3980,    0.3010,   -1.4420,    0.2120],
2396             [0.9340,   -1.5010,   -0.2710,    0.4200,   -2.9170,    0.2490]],
2397         'phoenix1988': [
2398             [-0.0030,    0.7280,   -0.0970,   -0.0750,    0.1420,   -0.0430],
2399             [0.2790,    0.3540,   -0.1760,    0.0300,   -0.0550,   -0.0540],
2400             [0.4690,    0.1680,   -0.2460,    0.0480,   -0.0420,   -0.0570],
2401             [0.8560,   -0.5190,   -0.3400,    0.1760,   -0.3800,   -0.0310],
2402             [0.9410,   -0.6250,   -0.3910,    0.1880,   -0.3600,   -0.0490],
2403             [1.0560,   -1.1340,   -0.4100,    0.2810,   -0.7940,   -0.0650],
2404             [0.9010,   -2.1390,   -0.2690,    0.1180,   -0.6650,    0.0460],
2405             [0.1070,    0.4810,    0.1430,   -0.1110,   -0.1370,    0.2340]],
2406         'elmonte1988': [
2407             [0.0270,    0.7010,   -0.1190,   -0.0580,    0.1070,  -0.0600],
2408             [0.1810,    0.6710,   -0.1780,   -0.0790,    0.1940,  -0.0350],
2409             [0.4760,    0.4070,   -0.2880,    0.0540,   -0.0320,  -0.0550],
2410             [0.8750,   -0.2180,   -0.4030,    0.1870,   -0.3090,  -0.0610],
2411             [1.1660,   -1.0140,   -0.4540,    0.2110,   -0.4100,  -0.0440],
2412             [1.1430,   -2.0640,   -0.2910,    0.0970,   -0.3190,   0.0530],
2413             [1.0940,   -2.6320,   -0.2590,    0.0290,   -0.4220,   0.1470],
2414             [0.1550,    1.7230,    0.1630,   -0.1310,   -0.0190,   0.2770]],
2415         'osage1988': [
2416             [-0.3530,    1.4740,   0.0570,   -0.1750,    0.3120,   0.0090],
2417             [0.3630,    0.2180,  -0.2120,    0.0190,   -0.0340,  -0.0590],
2418             [-0.0310,    1.2620,  -0.0840,   -0.0820,    0.2310,  -0.0170],
2419             [0.6910,    0.0390,  -0.2950,    0.0910,   -0.1310,  -0.0350],
2420             [1.1820,   -1.3500,  -0.3210,    0.4080,   -0.9850,  -0.0880],
2421             [0.7640,    0.0190,  -0.2030,    0.2170,   -0.2940,  -0.1030],
2422             [0.2190,    1.4120,   0.2440,    0.4710,   -2.9880,   0.0340],
2423             [3.5780,   22.2310, -10.7450,    2.4260,    4.8920,  -5.6870]],
2424         'albuquerque1988': [
2425             [0.0340,    0.5010,  -0.0940,   -0.0630,    0.1060,  -0.0440],
2426             [0.2290,    0.4670,  -0.1560,   -0.0050,   -0.0190,  -0.0230],
2427             [0.4860,    0.2410,  -0.2530,    0.0530,   -0.0640,  -0.0220],
2428             [0.8740,   -0.3930,  -0.3970,    0.1810,   -0.3270,  -0.0370],
2429             [1.1930,   -1.2960,  -0.5010,    0.2810,   -0.6560,  -0.0450],
2430             [1.0560,   -1.7580,  -0.3740,    0.2260,   -0.7590,   0.0340],
2431             [0.9010,   -4.7830,  -0.1090,    0.0630,   -0.9700,   0.1960],
2432             [0.8510,   -7.0550,  -0.0530,    0.0600,   -2.8330,   0.3300]],
2433         'capecanaveral1988': [
2434             [0.0750,    0.5330,   -0.1240,  -0.0670,   0.0420,  -0.0200],
2435             [0.2950,    0.4970,   -0.2180,  -0.0080,   0.0030,  -0.0290],
2436             [0.5140,    0.0810,   -0.2610,   0.0750,  -0.1600,  -0.0290],
2437             [0.7470,   -0.3290,   -0.3250,   0.1810,  -0.4160,  -0.0300],
2438             [0.9010,   -0.8830,   -0.2970,   0.1780,  -0.4890,   0.0080],
2439             [0.5910,   -0.0440,   -0.1160,   0.2350,  -0.9990,   0.0980],
2440             [0.5370,   -2.4020,    0.3200,   0.1690,  -1.9710,   0.3100],
2441             [-0.8050,    4.5460,    1.0720,  -0.2580,  -0.9500,    0.7530]],
2442         'albany1988': [
2443             [0.0120,    0.5540,   -0.0760, -0.0520,   0.0840,  -0.0290],
2444             [0.2670,    0.4370,   -0.1940,  0.0160,   0.0220,  -0.0360],
2445             [0.4200,    0.3360,   -0.2370,  0.0740,  -0.0520,  -0.0320],
2446             [0.6380,   -0.0010,   -0.2810,  0.1380,  -0.1890,  -0.0120],
2447             [1.0190,   -1.0270,   -0.3420,  0.2710,  -0.6280,   0.0140],
2448             [1.1490,   -1.9400,   -0.3310,  0.3220,  -1.0970,   0.0800],
2449             [1.4340,   -3.9940,   -0.4920,  0.4530,  -2.3760,   0.1170],
2450             [1.0070,   -2.2920,   -0.4820,  0.3900,  -3.3680,   0.2290]], }
2451 
2452     array = np.array(coeffdict[perezmodel])
2453 
2454     F1coeffs = array[:, 0:3]
2455     F2coeffs = array[:, 3:7]
2456 
2457     return F1coeffs, F2coeffs
2458 
2459 
2460 def _get_dirint_coeffs():
2461     """"""
2462     A place to stash the dirint coefficients.
2463 
2464     Returns
2465     -------
2466     np.array with shape ``(6, 6, 7, 5)``.
2467     Ordering is ``[kt_prime_bin, zenith_bin, delta_kt_prime_bin, w_bin]``
2468     """"""
2469 
2470     # To allow for maximum copy/paste from the MATLAB 1-indexed code,
2471     # we create and assign values to an oversized array.
2472     # Then, we return the [1:, 1:, :, :] slice.
2473 
2474     coeffs = np.zeros((7, 7, 7, 5))
2475 
2476     coeffs[1, 1, :, :] = [
2477         [0.385230, 0.385230, 0.385230, 0.462880, 0.317440],
2478         [0.338390, 0.338390, 0.221270, 0.316730, 0.503650],
2479         [0.235680, 0.235680, 0.241280, 0.157830, 0.269440],
2480         [0.830130, 0.830130, 0.171970, 0.841070, 0.457370],
2481         [0.548010, 0.548010, 0.478000, 0.966880, 1.036370],
2482         [0.548010, 0.548010, 1.000000, 3.012370, 1.976540],
2483         [0.582690, 0.582690, 0.229720, 0.892710, 0.569950]]
2484 
2485     coeffs[1, 2, :, :] = [
2486         [0.131280, 0.131280, 0.385460, 0.511070, 0.127940],
2487         [0.223710, 0.223710, 0.193560, 0.304560, 0.193940],
2488         [0.229970, 0.229970, 0.275020, 0.312730, 0.244610],
2489         [0.090100, 0.184580, 0.260500, 0.687480, 0.579440],
2490         [0.131530, 0.131530, 0.370190, 1.380350, 1.052270],
2491         [1.116250, 1.116250, 0.928030, 3.525490, 2.316920],
2492         [0.090100, 0.237000, 0.300040, 0.812470, 0.664970]]
2493 
2494     coeffs[1, 3, :, :] = [
2495         [0.587510, 0.130000, 0.400000, 0.537210, 0.832490],
2496         [0.306210, 0.129830, 0.204460, 0.500000, 0.681640],
2497         [0.224020, 0.260620, 0.334080, 0.501040, 0.350470],
2498         [0.421540, 0.753970, 0.750660, 3.706840, 0.983790],
2499         [0.706680, 0.373530, 1.245670, 0.864860, 1.992630],
2500         [4.864400, 0.117390, 0.265180, 0.359180, 3.310820],
2501         [0.392080, 0.493290, 0.651560, 1.932780, 0.898730]]
2502 
2503     coeffs[1, 4, :, :] = [
2504         [0.126970, 0.126970, 0.126970, 0.126970, 0.126970],
2505         [0.810820, 0.810820, 0.810820, 0.810820, 0.810820],
2506         [3.241680, 2.500000, 2.291440, 2.291440, 2.291440],
2507         [4.000000, 3.000000, 2.000000, 0.975430, 1.965570],
2508         [12.494170, 12.494170, 8.000000, 5.083520, 8.792390],
2509         [21.744240, 21.744240, 21.744240, 21.744240, 21.744240],
2510         [3.241680, 12.494170, 1.620760, 1.375250, 2.331620]]
2511 
2512     coeffs[1, 5, :, :] = [
2513         [0.126970, 0.126970, 0.126970, 0.126970, 0.126970],
2514         [0.810820, 0.810820, 0.810820, 0.810820, 0.810820],
2515         [3.241680, 2.500000, 2.291440, 2.291440, 2.291440],
2516         [4.000000, 3.000000, 2.000000, 0.975430, 1.965570],
2517         [12.494170, 12.494170, 8.000000, 5.083520, 8.792390],
2518         [21.744240, 21.744240, 21.744240, 21.744240, 21.744240],
2519         [3.241680, 12.494170, 1.620760, 1.375250, 2.331620]]
2520 
2521     coeffs[1, 6, :, :] = [
2522         [0.126970, 0.126970, 0.126970, 0.126970, 0.126970],
2523         [0.810820, 0.810820, 0.810820, 0.810820, 0.810820],
2524         [3.241680, 2.500000, 2.291440, 2.291440, 2.291440],
2525         [4.000000, 3.000000, 2.000000, 0.975430, 1.965570],
2526         [12.494170, 12.494170, 8.000000, 5.083520, 8.792390],
2527         [21.744240, 21.744240, 21.744240, 21.744240, 21.744240],
2528         [3.241680, 12.494170, 1.620760, 1.375250, 2.331620]]
2529 
2530     coeffs[2, 1, :, :] = [
2531         [0.337440, 0.337440, 0.969110, 1.097190, 1.116080],
2532         [0.337440, 0.337440, 0.969110, 1.116030, 0.623900],
2533         [0.337440, 0.337440, 1.530590, 1.024420, 0.908480],
2534         [0.584040, 0.584040, 0.847250, 0.914940, 1.289300],
2535         [0.337440, 0.337440, 0.310240, 1.435020, 1.852830],
2536         [0.337440, 0.337440, 1.015010, 1.097190, 2.117230],
2537         [0.337440, 0.337440, 0.969110, 1.145730, 1.476400]]
2538 
2539     coeffs[2, 2, :, :] = [
2540         [0.300000, 0.300000, 0.700000, 1.100000, 0.796940],
2541         [0.219870, 0.219870, 0.526530, 0.809610, 0.649300],
2542         [0.386650, 0.386650, 0.119320, 0.576120, 0.685460],
2543         [0.746730, 0.399830, 0.470970, 0.986530, 0.785370],
2544         [0.575420, 0.936700, 1.649200, 1.495840, 1.335590],
2545         [1.319670, 4.002570, 1.276390, 2.644550, 2.518670],
2546         [0.665190, 0.678910, 1.012360, 1.199940, 0.986580]]
2547 
2548     coeffs[2, 3, :, :] = [
2549         [0.378870, 0.974060, 0.500000, 0.491880, 0.665290],
2550         [0.105210, 0.263470, 0.407040, 0.553460, 0.582590],
2551         [0.312900, 0.345240, 1.144180, 0.854790, 0.612280],
2552         [0.119070, 0.365120, 0.560520, 0.793720, 0.802600],
2553         [0.781610, 0.837390, 1.270420, 1.537980, 1.292950],
2554         [1.152290, 1.152290, 1.492080, 1.245370, 2.177100],
2555         [0.424660, 0.529550, 0.966910, 1.033460, 0.958730]]
2556 
2557     coeffs[2, 4, :, :] = [
2558         [0.310590, 0.714410, 0.252450, 0.500000, 0.607600],
2559         [0.975190, 0.363420, 0.500000, 0.400000, 0.502800],
2560         [0.175580, 0.196250, 0.476360, 1.072470, 0.490510],
2561         [0.719280, 0.698620, 0.657770, 1.190840, 0.681110],
2562         [0.426240, 1.464840, 0.678550, 1.157730, 0.978430],
2563         [2.501120, 1.789130, 1.387090, 2.394180, 2.394180],
2564         [0.491640, 0.677610, 0.685610, 1.082400, 0.735410]]
2565 
2566     coeffs[2, 5, :, :] = [
2567         [0.597000, 0.500000, 0.300000, 0.310050, 0.413510],
2568         [0.314790, 0.336310, 0.400000, 0.400000, 0.442460],
2569         [0.166510, 0.460440, 0.552570, 1.000000, 0.461610],
2570         [0.401020, 0.559110, 0.403630, 1.016710, 0.671490],
2571         [0.400360, 0.750830, 0.842640, 1.802600, 1.023830],
2572         [3.315300, 1.510380, 2.443650, 1.638820, 2.133990],
2573         [0.530790, 0.745850, 0.693050, 1.458040, 0.804500]]
2574 
2575     coeffs[2, 6, :, :] = [
2576         [0.597000, 0.500000, 0.300000, 0.310050, 0.800920],
2577         [0.314790, 0.336310, 0.400000, 0.400000, 0.237040],
2578         [0.166510, 0.460440, 0.552570, 1.000000, 0.581990],
2579         [0.401020, 0.559110, 0.403630, 1.016710, 0.898570],
2580         [0.400360, 0.750830, 0.842640, 1.802600, 3.400390],
2581         [3.315300, 1.510380, 2.443650, 1.638820, 2.508780],
2582         [0.204340, 1.157740, 2.003080, 2.622080, 1.409380]]
2583 
2584     coeffs[3, 1, :, :] = [
2585         [1.242210, 1.242210, 1.242210, 1.242210, 1.242210],
2586         [0.056980, 0.056980, 0.656990, 0.656990, 0.925160],
2587         [0.089090, 0.089090, 1.040430, 1.232480, 1.205300],
2588         [1.053850, 1.053850, 1.399690, 1.084640, 1.233340],
2589         [1.151540, 1.151540, 1.118290, 1.531640, 1.411840],
2590         [1.494980, 1.494980, 1.700000, 1.800810, 1.671600],
2591         [1.018450, 1.018450, 1.153600, 1.321890, 1.294670]]
2592 
2593     coeffs[3, 2, :, :] = [
2594         [0.700000, 0.700000, 1.023460, 0.700000, 0.945830],
2595         [0.886300, 0.886300, 1.333620, 0.800000, 1.066620],
2596         [0.902180, 0.902180, 0.954330, 1.126690, 1.097310],
2597         [1.095300, 1.075060, 1.176490, 1.139470, 1.096110],
2598         [1.201660, 1.201660, 1.438200, 1.256280, 1.198060],
2599         [1.525850, 1.525850, 1.869160, 1.985410, 1.911590],
2600         [1.288220, 1.082810, 1.286370, 1.166170, 1.119330]]
2601 
2602     coeffs[3, 3, :, :] = [
2603         [0.600000, 1.029910, 0.859890, 0.550000, 0.813600],
2604         [0.604450, 1.029910, 0.859890, 0.656700, 0.928840],
2605         [0.455850, 0.750580, 0.804930, 0.823000, 0.911000],
2606         [0.526580, 0.932310, 0.908620, 0.983520, 0.988090],
2607         [1.036110, 1.100690, 0.848380, 1.035270, 1.042380],
2608         [1.048440, 1.652720, 0.900000, 2.350410, 1.082950],
2609         [0.817410, 0.976160, 0.861300, 0.974780, 1.004580]]
2610 
2611     coeffs[3, 4, :, :] = [
2612         [0.782110, 0.564280, 0.600000, 0.600000, 0.665740],
2613         [0.894480, 0.680730, 0.541990, 0.800000, 0.669140],
2614         [0.487460, 0.818950, 0.841830, 0.872540, 0.709040],
2615         [0.709310, 0.872780, 0.908480, 0.953290, 0.844350],
2616         [0.863920, 0.947770, 0.876220, 1.078750, 0.936910],
2617         [1.280350, 0.866720, 0.769790, 1.078750, 0.975130],
2618         [0.725420, 0.869970, 0.868810, 0.951190, 0.829220]]
2619 
2620     coeffs[3, 5, :, :] = [
2621         [0.791750, 0.654040, 0.483170, 0.409000, 0.597180],
2622         [0.566140, 0.948990, 0.971820, 0.653570, 0.718550],
2623         [0.648710, 0.637730, 0.870510, 0.860600, 0.694300],
2624         [0.637630, 0.767610, 0.925670, 0.990310, 0.847670],
2625         [0.736380, 0.946060, 1.117590, 1.029340, 0.947020],
2626         [1.180970, 0.850000, 1.050000, 0.950000, 0.888580],
2627         [0.700560, 0.801440, 0.961970, 0.906140, 0.823880]]
2628 
2629     coeffs[3, 6, :, :] = [
2630         [0.500000, 0.500000, 0.586770, 0.470550, 0.629790],
2631         [0.500000, 0.500000, 1.056220, 1.260140, 0.658140],
2632         [0.500000, 0.500000, 0.631830, 0.842620, 0.582780],
2633         [0.554710, 0.734730, 0.985820, 0.915640, 0.898260],
2634         [0.712510, 1.205990, 0.909510, 1.078260, 0.885610],
2635         [1.899260, 1.559710, 1.000000, 1.150000, 1.120390],
2636         [0.653880, 0.793120, 0.903320, 0.944070, 0.796130]]
2637 
2638     coeffs[4, 1, :, :] = [
2639         [1.000000, 1.000000, 1.050000, 1.170380, 1.178090],
2640         [0.960580, 0.960580, 1.059530, 1.179030, 1.131690],
2641         [0.871470, 0.871470, 0.995860, 1.141910, 1.114600],
2642         [1.201590, 1.201590, 0.993610, 1.109380, 1.126320],
2643         [1.065010, 1.065010, 0.828660, 0.939970, 1.017930],
2644         [1.065010, 1.065010, 0.623690, 1.119620, 1.132260],
2645         [1.071570, 1.071570, 0.958070, 1.114130, 1.127110]]
2646 
2647     coeffs[4, 2, :, :] = [
2648         [0.950000, 0.973390, 0.852520, 1.092200, 1.096590],
2649         [0.804120, 0.913870, 0.980990, 1.094580, 1.042420],
2650         [0.737540, 0.935970, 0.999940, 1.056490, 1.050060],
2651         [1.032980, 1.034540, 0.968460, 1.032080, 1.015780],
2652         [0.900000, 0.977210, 0.945960, 1.008840, 0.969960],
2653         [0.600000, 0.750000, 0.750000, 0.844710, 0.899100],
2654         [0.926800, 0.965030, 0.968520, 1.044910, 1.032310]]
2655 
2656     coeffs[4, 3, :, :] = [
2657         [0.850000, 1.029710, 0.961100, 1.055670, 1.009700],
2658         [0.818530, 0.960010, 0.996450, 1.081970, 1.036470],
2659         [0.765380, 0.953500, 0.948260, 1.052110, 1.000140],
2660         [0.775610, 0.909610, 0.927800, 0.987800, 0.952100],
2661         [1.000990, 0.881880, 0.875950, 0.949100, 0.893690],
2662         [0.902370, 0.875960, 0.807990, 0.942410, 0.917920],
2663         [0.856580, 0.928270, 0.946820, 1.032260, 0.972990]]
2664 
2665     coeffs[4, 4, :, :] = [
2666         [0.750000, 0.857930, 0.983800, 1.056540, 0.980240],
2667         [0.750000, 0.987010, 1.013730, 1.133780, 1.038250],
2668         [0.800000, 0.947380, 1.012380, 1.091270, 0.999840],
2669         [0.800000, 0.914550, 0.908570, 0.999190, 0.915230],
2670         [0.778540, 0.800590, 0.799070, 0.902180, 0.851560],
2671         [0.680190, 0.317410, 0.507680, 0.388910, 0.646710],
2672         [0.794920, 0.912780, 0.960830, 1.057110, 0.947950]]
2673 
2674     coeffs[4, 5, :, :] = [
2675         [0.750000, 0.833890, 0.867530, 1.059890, 0.932840],
2676         [0.979700, 0.971470, 0.995510, 1.068490, 1.030150],
2677         [0.858850, 0.987920, 1.043220, 1.108700, 1.044900],
2678         [0.802400, 0.955110, 0.911660, 1.045070, 0.944470],
2679         [0.884890, 0.766210, 0.885390, 0.859070, 0.818190],
2680         [0.615680, 0.700000, 0.850000, 0.624620, 0.669300],
2681         [0.835570, 0.946150, 0.977090, 1.049350, 0.979970]]
2682 
2683     coeffs[4, 6, :, :] = [
2684         [0.689220, 0.809600, 0.900000, 0.789500, 0.853990],
2685         [0.854660, 0.852840, 0.938200, 0.923110, 0.955010],
2686         [0.938600, 0.932980, 1.010390, 1.043950, 1.041640],
2687         [0.843620, 0.981300, 0.951590, 0.946100, 0.966330],
2688         [0.694740, 0.814690, 0.572650, 0.400000, 0.726830],
2689         [0.211370, 0.671780, 0.416340, 0.297290, 0.498050],
2690         [0.843540, 0.882330, 0.911760, 0.898420, 0.960210]]
2691 
2692     coeffs[5, 1, :, :] = [
2693         [1.054880, 1.075210, 1.068460, 1.153370, 1.069220],
2694         [1.000000, 1.062220, 1.013470, 1.088170, 1.046200],
2695         [0.885090, 0.993530, 0.942590, 1.054990, 1.012740],
2696         [0.920000, 0.950000, 0.978720, 1.020280, 0.984440],
2697         [0.850000, 0.908500, 0.839940, 0.985570, 0.962180],
2698         [0.800000, 0.800000, 0.810080, 0.950000, 0.961550],
2699         [1.038590, 1.063200, 1.034440, 1.112780, 1.037800]]
2700 
2701     coeffs[5, 2, :, :] = [
2702         [1.017610, 1.028360, 1.058960, 1.133180, 1.045620],
2703         [0.920000, 0.998970, 1.033590, 1.089030, 1.022060],
2704         [0.912370, 0.949930, 0.979770, 1.020420, 0.981770],
2705         [0.847160, 0.935300, 0.930540, 0.955050, 0.946560],
2706         [0.880260, 0.867110, 0.874130, 0.972650, 0.883420],
2707         [0.627150, 0.627150, 0.700000, 0.774070, 0.845130],
2708         [0.973700, 1.006240, 1.026190, 1.071960, 1.017240]]
2709 
2710     coeffs[5, 3, :, :] = [
2711         [1.028710, 1.017570, 1.025900, 1.081790, 1.024240],
2712         [0.924980, 0.985500, 1.014100, 1.092210, 0.999610],
2713         [0.828570, 0.934920, 0.994950, 1.024590, 0.949710],
2714         [0.900810, 0.901330, 0.928830, 0.979570, 0.913100],
2715         [0.761030, 0.845150, 0.805360, 0.936790, 0.853460],
2716         [0.626400, 0.546750, 0.730500, 0.850000, 0.689050],
2717         [0.957630, 0.985480, 0.991790, 1.050220, 0.987900]]
2718 
2719     coeffs[5, 4, :, :] = [
2720         [0.992730, 0.993880, 1.017150, 1.059120, 1.017450],
2721         [0.975610, 0.987160, 1.026820, 1.075440, 1.007250],
2722         [0.871090, 0.933190, 0.974690, 0.979840, 0.952730],
2723         [0.828750, 0.868090, 0.834920, 0.905510, 0.871530],
2724         [0.781540, 0.782470, 0.767910, 0.764140, 0.795890],
2725         [0.743460, 0.693390, 0.514870, 0.630150, 0.715660],
2726         [0.934760, 0.957870, 0.959640, 0.972510, 0.981640]]
2727 
2728     coeffs[5, 5, :, :] = [
2729         [0.965840, 0.941240, 0.987100, 1.022540, 1.011160],
2730         [0.988630, 0.994770, 0.976590, 0.950000, 1.034840],
2731         [0.958200, 1.018080, 0.974480, 0.920000, 0.989870],
2732         [0.811720, 0.869090, 0.812020, 0.850000, 0.821050],
2733         [0.682030, 0.679480, 0.632450, 0.746580, 0.738550],
2734         [0.668290, 0.445860, 0.500000, 0.678920, 0.696510],
2735         [0.926940, 0.953350, 0.959050, 0.876210, 0.991490]]
2736 
2737     coeffs[5, 6, :, :] = [
2738         [0.948940, 0.997760, 0.850000, 0.826520, 0.998470],
2739         [1.017860, 0.970000, 0.850000, 0.700000, 0.988560],
2740         [1.000000, 0.950000, 0.850000, 0.606240, 0.947260],
2741         [1.000000, 0.746140, 0.751740, 0.598390, 0.725230],
2742         [0.922210, 0.500000, 0.376800, 0.517110, 0.548630],
2743         [0.500000, 0.450000, 0.429970, 0.404490, 0.539940],
2744         [0.960430, 0.881630, 0.775640, 0.596350, 0.937680]]
2745 
2746     coeffs[6, 1, :, :] = [
2747         [1.030000, 1.040000, 1.000000, 1.000000, 1.049510],
2748         [1.050000, 0.990000, 0.990000, 0.950000, 0.996530],
2749         [1.050000, 0.990000, 0.990000, 0.820000, 0.971940],
2750         [1.050000, 0.790000, 0.880000, 0.820000, 0.951840],
2751         [1.000000, 0.530000, 0.440000, 0.710000, 0.928730],
2752         [0.540000, 0.470000, 0.500000, 0.550000, 0.773950],
2753         [1.038270, 0.920180, 0.910930, 0.821140, 1.034560]]
2754 
2755     coeffs[6, 2, :, :] = [
2756         [1.041020, 0.997520, 0.961600, 1.000000, 1.035780],
2757         [0.948030, 0.980000, 0.900000, 0.950360, 0.977460],
2758         [0.950000, 0.977250, 0.869270, 0.800000, 0.951680],
2759         [0.951870, 0.850000, 0.748770, 0.700000, 0.883850],
2760         [0.900000, 0.823190, 0.727450, 0.600000, 0.839870],
2761         [0.850000, 0.805020, 0.692310, 0.500000, 0.788410],
2762         [1.010090, 0.895270, 0.773030, 0.816280, 1.011680]]
2763 
2764     coeffs[6, 3, :, :] = [
2765         [1.022450, 1.004600, 0.983650, 1.000000, 1.032940],
2766         [0.943960, 0.999240, 0.983920, 0.905990, 0.978150],
2767         [0.936240, 0.946480, 0.850000, 0.850000, 0.930320],
2768         [0.816420, 0.885000, 0.644950, 0.817650, 0.865310],
2769         [0.742960, 0.765690, 0.561520, 0.700000, 0.827140],
2770         [0.643870, 0.596710, 0.474460, 0.600000, 0.651200],
2771         [0.971740, 0.940560, 0.714880, 0.864380, 1.001650]]
2772 
2773     coeffs[6, 4, :, :] = [
2774         [0.995260, 0.977010, 1.000000, 1.000000, 1.035250],
2775         [0.939810, 0.975250, 0.939980, 0.950000, 0.982550],
2776         [0.876870, 0.879440, 0.850000, 0.900000, 0.917810],
2777         [0.873480, 0.873450, 0.751470, 0.850000, 0.863040],
2778         [0.761470, 0.702360, 0.638770, 0.750000, 0.783120],
2779         [0.734080, 0.650000, 0.600000, 0.650000, 0.715660],
2780         [0.942160, 0.919100, 0.770340, 0.731170, 0.995180]]
2781 
2782     coeffs[6, 5, :, :] = [
2783         [0.952560, 0.916780, 0.920000, 0.900000, 1.005880],
2784         [0.928620, 0.994420, 0.900000, 0.900000, 0.983720],
2785         [0.913070, 0.850000, 0.850000, 0.800000, 0.924280],
2786         [0.868090, 0.807170, 0.823550, 0.600000, 0.844520],
2787         [0.769570, 0.719870, 0.650000, 0.550000, 0.733500],
2788         [0.580250, 0.650000, 0.600000, 0.500000, 0.628850],
2789         [0.904770, 0.852650, 0.708370, 0.493730, 0.949030]]
2790 
2791     coeffs[6, 6, :, :] = [
2792         [0.911970, 0.800000, 0.800000, 0.800000, 0.956320],
2793         [0.912620, 0.682610, 0.750000, 0.700000, 0.950110],
2794         [0.653450, 0.659330, 0.700000, 0.600000, 0.856110],
2795         [0.648440, 0.600000, 0.641120, 0.500000, 0.695780],
2796         [0.570000, 0.550000, 0.598800, 0.400000, 0.560150],
2797         [0.475230, 0.500000, 0.518640, 0.339970, 0.520230],
2798         [0.743440, 0.592190, 0.603060, 0.316930, 0.794390]]
2799 
2800     return coeffs[1:, 1:, :, :]
2801 
2802 
2803 def dni(ghi, dhi, zenith, clearsky_dni=None, clearsky_tolerance=1.1,
2804         zenith_threshold_for_zero_dni=88.0,
2805         zenith_threshold_for_clearsky_limit=80.0):
2806     """"""
2807     Determine DNI from GHI and DHI.
2808 
2809     When calculating the DNI from GHI and DHI the calculated DNI may be
2810     unreasonably high or negative for zenith angles close to 90 degrees
2811     (sunrise/sunset transitions). This function identifies unreasonable DNI
2812     values and sets them to NaN. If the clearsky DNI is given unreasonably high
2813     values are cut off.
2814 
2815     Parameters
2816     ----------
2817     ghi : Series
2818         Global horizontal irradiance.
2819 
2820     dhi : Series
2821         Diffuse horizontal irradiance.
2822 
2823     zenith : Series
2824         True (not refraction-corrected) zenith angles in decimal
2825         degrees. Angles must be >=0 and <=180.
2826 
2827     clearsky_dni : None or Series, default None
2828         Clearsky direct normal irradiance.
2829 
2830     clearsky_tolerance : float, default 1.1
2831         If 'clearsky_dni' is given this parameter can be used to allow a
2832         tolerance by how much the calculated DNI value can be greater than
2833         the clearsky value before it is identified as an unreasonable value.
2834 
2835     zenith_threshold_for_zero_dni : float, default 88.0
2836         Non-zero DNI values for zenith angles greater than or equal to
2837         'zenith_threshold_for_zero_dni' will be set to NaN.
2838 
2839     zenith_threshold_for_clearsky_limit : float, default 80.0
2840         DNI values for zenith angles greater than or equal to
2841         'zenith_threshold_for_clearsky_limit' and smaller the
2842         'zenith_threshold_for_zero_dni' that are greater than the clearsky DNI
2843         (times allowed tolerance) will be corrected. Only applies if
2844         'clearsky_dni' is not None.
2845 
2846     Returns
2847     -------
2848     dni : Series
2849         The modeled direct normal irradiance.
2850     """"""
2851 
2852     # calculate DNI
2853     dni = (ghi - dhi) / tools.cosd(zenith)
2854 
2855     # cutoff negative values
2856     dni[dni < 0] = float('nan')
2857 
2858     # set non-zero DNI values for zenith angles >=
2859     # zenith_threshold_for_zero_dni to NaN
2860     dni[(zenith >= zenith_threshold_for_zero_dni) & (dni != 0)] = float('nan')
2861 
2862     # correct DNI values for zenith angles greater or equal to the
2863     # zenith_threshold_for_clearsky_limit and smaller than the
2864     # upper_cutoff_zenith that are greater than the clearsky DNI (times
2865     # clearsky_tolerance)
2866     if clearsky_dni is not None:
2867         max_dni = clearsky_dni * clearsky_tolerance
2868         dni[(zenith >= zenith_threshold_for_clearsky_limit) &
2869             (zenith < zenith_threshold_for_zero_dni) &
2870             (dni > max_dni)] = max_dni
2871     return dni
2872 
[end of pvlib/irradiance.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pvlib/pvlib-python,0b8f24c265d76320067a5ee908a57d475cd1bb24,"pvlib.irradiance.reindl() model generates NaNs when GHI = 0
**Describe the bug**
The reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to ""term3"" having a quotient that divides by GHI.  

**Expected behavior**
The reindl function should result in zero sky diffuse when GHI is zero.


pvlib.irradiance.reindl() model generates NaNs when GHI = 0
**Describe the bug**
The reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to ""term3"" having a quotient that divides by GHI.  

**Expected behavior**
The reindl function should result in zero sky diffuse when GHI is zero.


","Verified. Looks like an easy fix.
Verified. Looks like an easy fix.",2021-01-29T20:53:24Z,"<patch>
diff --git a/pvlib/irradiance.py b/pvlib/irradiance.py
--- a/pvlib/irradiance.py
+++ b/pvlib/irradiance.py
@@ -886,8 +886,9 @@ def reindl(surface_tilt, surface_azimuth, dhi, dni, ghi, dni_extra,
     # these are the () and [] sub-terms of the second term of eqn 8
     term1 = 1 - AI
     term2 = 0.5 * (1 + tools.cosd(surface_tilt))
-    term3 = 1 + np.sqrt(HB / ghi) * (tools.sind(0.5 * surface_tilt) ** 3)
-
+    with np.errstate(invalid='ignore', divide='ignore'):
+        hb_to_ghi = np.where(ghi == 0, 0, np.divide(HB, ghi))
+    term3 = 1 + np.sqrt(hb_to_ghi) * (tools.sind(0.5 * surface_tilt)**3)
     sky_diffuse = dhi * (AI * Rb + term1 * term2 * term3)
     sky_diffuse = np.maximum(sky_diffuse, 0)
 

</patch>","diff --git a/pvlib/tests/test_irradiance.py b/pvlib/tests/test_irradiance.py
--- a/pvlib/tests/test_irradiance.py
+++ b/pvlib/tests/test_irradiance.py
@@ -203,7 +203,7 @@ def test_reindl(irrad_data, ephem_data, dni_et):
         40, 180, irrad_data['dhi'], irrad_data['dni'], irrad_data['ghi'],
         dni_et, ephem_data['apparent_zenith'], ephem_data['azimuth'])
     # values from matlab 1.4 code
-    assert_allclose(result, [np.nan, 27.9412, 104.1317, 34.1663], atol=1e-4)
+    assert_allclose(result, [0., 27.9412, 104.1317, 34.1663], atol=1e-4)
 
 
 def test_king(irrad_data, ephem_data):
",0.8,"[""pvlib/tests/test_irradiance.py::test_reindl""]","[""pvlib/tests/test_irradiance.py::test_get_extra_radiation[asce-300-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[asce-300.0-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[asce-testval2-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[asce-testval3-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[asce-testval4-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[asce-testval5-expected5]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[asce-testval6-expected6]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[asce-testval7-expected7]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[asce-testval8-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[spencer-300-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[spencer-300.0-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[spencer-testval2-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[spencer-testval3-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[spencer-testval4-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[spencer-testval5-expected5]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[spencer-testval6-expected6]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[spencer-testval7-expected7]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[spencer-testval8-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[nrel-300-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[nrel-300.0-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[nrel-testval2-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[nrel-testval3-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[nrel-testval4-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[nrel-testval5-expected5]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[nrel-testval6-expected6]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[nrel-testval7-expected7]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[nrel-testval8-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[pyephem-300-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[pyephem-300.0-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[pyephem-testval2-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[pyephem-testval3-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[pyephem-testval4-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[pyephem-testval5-expected5]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[pyephem-testval6-expected6]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[pyephem-testval7-expected7]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation[pyephem-testval8-1383.636203]"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation_epoch_year"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation_nrel_numba"", ""pvlib/tests/test_irradiance.py::test_get_extra_radiation_invalid"", ""pvlib/tests/test_irradiance.py::test_grounddiffuse_simple_float"", ""pvlib/tests/test_irradiance.py::test_grounddiffuse_simple_series"", ""pvlib/tests/test_irradiance.py::test_grounddiffuse_albedo_0"", ""pvlib/tests/test_irradiance.py::test_grounddiffuse_albedo_invalid_surface"", ""pvlib/tests/test_irradiance.py::test_grounddiffuse_albedo_surface"", ""pvlib/tests/test_irradiance.py::test_isotropic_float"", ""pvlib/tests/test_irradiance.py::test_isotropic_series"", ""pvlib/tests/test_irradiance.py::test_klucher_series_float"", ""pvlib/tests/test_irradiance.py::test_klucher_series"", ""pvlib/tests/test_irradiance.py::test_haydavies"", ""pvlib/tests/test_irradiance.py::test_king"", ""pvlib/tests/test_irradiance.py::test_perez"", ""pvlib/tests/test_irradiance.py::test_perez_components"", ""pvlib/tests/test_irradiance.py::test_perez_arrays"", ""pvlib/tests/test_irradiance.py::test_perez_scalar"", ""pvlib/tests/test_irradiance.py::test_sky_diffuse_zenith_close_to_90[isotropic]"", ""pvlib/tests/test_irradiance.py::test_sky_diffuse_zenith_close_to_90[klucher]"", ""pvlib/tests/test_irradiance.py::test_sky_diffuse_zenith_close_to_90[haydavies]"", ""pvlib/tests/test_irradiance.py::test_sky_diffuse_zenith_close_to_90[reindl]"", ""pvlib/tests/test_irradiance.py::test_sky_diffuse_zenith_close_to_90[king]"", ""pvlib/tests/test_irradiance.py::test_sky_diffuse_zenith_close_to_90[perez]"", ""pvlib/tests/test_irradiance.py::test_get_sky_diffuse_invalid"", ""pvlib/tests/test_irradiance.py::test_campbell_norman"", ""pvlib/tests/test_irradiance.py::test_get_total_irradiance"", ""pvlib/tests/test_irradiance.py::test_get_total_irradiance_scalars[isotropic]"", ""pvlib/tests/test_irradiance.py::test_get_total_irradiance_scalars[klucher]"", ""pvlib/tests/test_irradiance.py::test_get_total_irradiance_scalars[haydavies]"", ""pvlib/tests/test_irradiance.py::test_get_total_irradiance_scalars[reindl]"", ""pvlib/tests/test_irradiance.py::test_get_total_irradiance_scalars[king]"", ""pvlib/tests/test_irradiance.py::test_get_total_irradiance_scalars[perez]"", ""pvlib/tests/test_irradiance.py::test_poa_components"", ""pvlib/tests/test_irradiance.py::test_disc_value[93193-expected0]"", ""pvlib/tests/test_irradiance.py::test_disc_value[None-expected1]"", ""pvlib/tests/test_irradiance.py::test_disc_value[101325-expected2]"", ""pvlib/tests/test_irradiance.py::test_disc_overirradiance"", ""pvlib/tests/test_irradiance.py::test_disc_min_cos_zenith_max_zenith"", ""pvlib/tests/test_irradiance.py::test_dirint_value"", ""pvlib/tests/test_irradiance.py::test_dirint_nans"", ""pvlib/tests/test_irradiance.py::test_dirint_tdew"", ""pvlib/tests/test_irradiance.py::test_dirint_no_delta_kt"", ""pvlib/tests/test_irradiance.py::test_dirint_coeffs"", ""pvlib/tests/test_irradiance.py::test_dirint_min_cos_zenith_max_zenith"", ""pvlib/tests/test_irradiance.py::test_gti_dirint"", ""pvlib/tests/test_irradiance.py::test_erbs"", ""pvlib/tests/test_irradiance.py::test_erbs_min_cos_zenith_max_zenith"", ""pvlib/tests/test_irradiance.py::test_erbs_all_scalar"", ""pvlib/tests/test_irradiance.py::test_dirindex"", ""pvlib/tests/test_irradiance.py::test_dirindex_min_cos_zenith_max_zenith"", ""pvlib/tests/test_irradiance.py::test_dni"", ""pvlib/tests/test_irradiance.py::test_aoi_and_aoi_projection[0-0-0-0-0-1]"", ""pvlib/tests/test_irradiance.py::test_aoi_and_aoi_projection[30-180-30-180-0-1]"", ""pvlib/tests/test_irradiance.py::test_aoi_and_aoi_projection[30-180-150-0-180--1]"", ""pvlib/tests/test_irradiance.py::test_aoi_and_aoi_projection[90-0-30-60-75.5224878-0.25]"", ""pvlib/tests/test_irradiance.py::test_aoi_and_aoi_projection[90-0-30-170-119.4987042--0.4924038]"", ""pvlib/tests/test_irradiance.py::test_kt_kt_prime_factor"", ""pvlib/tests/test_irradiance.py::test_clearsky_index"", ""pvlib/tests/test_irradiance.py::test_clearness_index"", ""pvlib/tests/test_irradiance.py::test_clearness_index_zenith_independent""]",ef8ad2fee9840a77d14b0dfd17fc489dd85c9b91
pylint-dev__astroid-1978,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
Deprecation warnings from numpy
### Steps to reproduce

1. Run pylint over the following test case:

```
""""""Test case""""""

import numpy as np
value = np.random.seed(1234)
```

### Current behavior
```
/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.
  getattr(sys.modules[modname], name)
/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.
  getattr(sys.modules[modname], name)
```

### Expected behavior
There should be no future warnings.

### python -c ""from astroid import __pkginfo__; print(__pkginfo__.version)"" output
2.12.13

</issue>
<code>
[start of README.rst]
1 Astroid
2 =======
3 
4 .. image:: https://codecov.io/gh/PyCQA/astroid/branch/main/graph/badge.svg?token=Buxy4WptLb
5     :target: https://codecov.io/gh/PyCQA/astroid
6     :alt: Coverage badge from codecov
7 
8 .. image:: https://readthedocs.org/projects/astroid/badge/?version=latest
9     :target: http://astroid.readthedocs.io/en/latest/?badge=latest
10     :alt: Documentation Status
11 
12 .. image:: https://img.shields.io/badge/code%20style-black-000000.svg
13     :target: https://github.com/ambv/black
14 
15 .. image:: https://results.pre-commit.ci/badge/github/PyCQA/astroid/main.svg
16    :target: https://results.pre-commit.ci/latest/github/PyCQA/astroid/main
17    :alt: pre-commit.ci status
18 
19 .. |tidelift_logo| image:: https://raw.githubusercontent.com/PyCQA/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png
20    :width: 200
21    :alt: Tidelift
22 
23 .. list-table::
24    :widths: 10 100
25 
26    * - |tidelift_logo|
27      - Professional support for astroid is available as part of the
28        `Tidelift Subscription`_.  Tidelift gives software development teams a single source for
29        purchasing and maintaining their software, with professional grade assurances
30        from the experts who know it best, while seamlessly integrating with existing
31        tools.
32 
33 .. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme
34 
35 
36 
37 What's this?
38 ------------
39 
40 The aim of this module is to provide a common base representation of
41 python source code. It is currently the library powering pylint's capabilities.
42 
43 It provides a compatible representation which comes from the `_ast`
44 module.  It rebuilds the tree generated by the builtin _ast module by
45 recursively walking down the AST and building an extended ast. The new
46 node classes have additional methods and attributes for different
47 usages. They include some support for static inference and local name
48 scopes. Furthermore, astroid can also build partial trees by inspecting living
49 objects.
50 
51 
52 Installation
53 ------------
54 
55 Extract the tarball, jump into the created directory and run::
56 
57     pip install .
58 
59 
60 If you want to do an editable installation, you can run::
61 
62     pip install -e .
63 
64 
65 If you have any questions, please mail the code-quality@python.org
66 mailing list for support. See
67 http://mail.python.org/mailman/listinfo/code-quality for subscription
68 information and archives.
69 
70 Documentation
71 -------------
72 http://astroid.readthedocs.io/en/latest/
73 
74 
75 Python Versions
76 ---------------
77 
78 astroid 2.0 is currently available for Python 3 only. If you want Python 2
79 support, use an older version of astroid (though note that these versions
80 are no longer supported).
81 
82 Test
83 ----
84 
85 Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use
86 either `tox` or `pytest`::
87 
88     tox
89     pytest
90 
[end of README.rst]
[start of astroid/raw_building.py]
1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html
2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE
3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt
4 
5 """"""this module contains a set of functions to create astroid trees from scratch
6 (build_* functions) or from living object (object_build_* functions)
7 """"""
8 
9 from __future__ import annotations
10 
11 import builtins
12 import inspect
13 import os
14 import sys
15 import types
16 import warnings
17 from collections.abc import Iterable
18 from typing import Any, Union
19 
20 from astroid import bases, nodes
21 from astroid.const import _EMPTY_OBJECT_MARKER, IS_PYPY
22 from astroid.manager import AstroidManager
23 from astroid.nodes import node_classes
24 
25 _FunctionTypes = Union[
26     types.FunctionType,
27     types.MethodType,
28     types.BuiltinFunctionType,
29     types.WrapperDescriptorType,
30     types.MethodDescriptorType,
31     types.ClassMethodDescriptorType,
32 ]
33 
34 # the keys of CONST_CLS eg python builtin types
35 _CONSTANTS = tuple(node_classes.CONST_CLS)
36 _BUILTINS = vars(builtins)
37 TYPE_NONE = type(None)
38 TYPE_NOTIMPLEMENTED = type(NotImplemented)
39 TYPE_ELLIPSIS = type(...)
40 
41 
42 def _attach_local_node(parent, node, name: str) -> None:
43     node.name = name  # needed by add_local_node
44     parent.add_local_node(node)
45 
46 
47 def _add_dunder_class(func, member) -> None:
48     """"""Add a __class__ member to the given func node, if we can determine it.""""""
49     python_cls = member.__class__
50     cls_name = getattr(python_cls, ""__name__"", None)
51     if not cls_name:
52         return
53     cls_bases = [ancestor.__name__ for ancestor in python_cls.__bases__]
54     ast_klass = build_class(cls_name, cls_bases, python_cls.__doc__)
55     func.instance_attrs[""__class__""] = [ast_klass]
56 
57 
58 def attach_dummy_node(node, name: str, runtime_object=_EMPTY_OBJECT_MARKER) -> None:
59     """"""create a dummy node and register it in the locals of the given
60     node with the specified name
61     """"""
62     enode = nodes.EmptyNode()
63     enode.object = runtime_object
64     _attach_local_node(node, enode, name)
65 
66 
67 def attach_const_node(node, name: str, value) -> None:
68     """"""create a Const node and register it in the locals of the given
69     node with the specified name
70     """"""
71     if name not in node.special_attributes:
72         _attach_local_node(node, nodes.const_factory(value), name)
73 
74 
75 def attach_import_node(node, modname: str, membername: str) -> None:
76     """"""create a ImportFrom node and register it in the locals of the given
77     node with the specified name
78     """"""
79     from_node = nodes.ImportFrom(modname, [(membername, None)])
80     _attach_local_node(node, from_node, membername)
81 
82 
83 def build_module(name: str, doc: str | None = None) -> nodes.Module:
84     """"""create and initialize an astroid Module node""""""
85     node = nodes.Module(name, pure_python=False, package=False)
86     node.postinit(
87         body=[],
88         doc_node=nodes.Const(value=doc) if doc else None,
89     )
90     return node
91 
92 
93 def build_class(
94     name: str, basenames: Iterable[str] = (), doc: str | None = None
95 ) -> nodes.ClassDef:
96     """"""Create and initialize an astroid ClassDef node.""""""
97     node = nodes.ClassDef(name)
98     node.postinit(
99         bases=[nodes.Name(name=base, parent=node) for base in basenames],
100         body=[],
101         decorators=None,
102         doc_node=nodes.Const(value=doc) if doc else None,
103     )
104     return node
105 
106 
107 def build_function(
108     name: str,
109     args: list[str] | None = None,
110     posonlyargs: list[str] | None = None,
111     defaults: list[Any] | None = None,
112     doc: str | None = None,
113     kwonlyargs: list[str] | None = None,
114 ) -> nodes.FunctionDef:
115     """"""create and initialize an astroid FunctionDef node""""""
116     # first argument is now a list of decorators
117     func = nodes.FunctionDef(name)
118     argsnode = nodes.Arguments(parent=func)
119 
120     # If args is None we don't have any information about the signature
121     # (in contrast to when there are no arguments and args == []). We pass
122     # this to the builder to indicate this.
123     if args is not None:
124         arguments = [nodes.AssignName(name=arg, parent=argsnode) for arg in args]
125     else:
126         arguments = None
127 
128     default_nodes: list[nodes.NodeNG] | None = []
129     if defaults is not None:
130         for default in defaults:
131             default_node = nodes.const_factory(default)
132             default_node.parent = argsnode
133             default_nodes.append(default_node)
134     else:
135         default_nodes = None
136 
137     argsnode.postinit(
138         args=arguments,
139         defaults=default_nodes,
140         kwonlyargs=[
141             nodes.AssignName(name=arg, parent=argsnode) for arg in kwonlyargs or ()
142         ],
143         kw_defaults=[],
144         annotations=[],
145         posonlyargs=[
146             nodes.AssignName(name=arg, parent=argsnode) for arg in posonlyargs or ()
147         ],
148     )
149     func.postinit(
150         args=argsnode,
151         body=[],
152         doc_node=nodes.Const(value=doc) if doc else None,
153     )
154     if args:
155         register_arguments(func)
156     return func
157 
158 
159 def build_from_import(fromname: str, names: list[str]) -> nodes.ImportFrom:
160     """"""create and initialize an astroid ImportFrom import statement""""""
161     return nodes.ImportFrom(fromname, [(name, None) for name in names])
162 
163 
164 def register_arguments(func: nodes.FunctionDef, args: list | None = None) -> None:
165     """"""add given arguments to local
166 
167     args is a list that may contains nested lists
168     (i.e. def func(a, (b, c, d)): ...)
169     """"""
170     # If no args are passed in, get the args from the function.
171     if args is None:
172         if func.args.vararg:
173             func.set_local(func.args.vararg, func.args)
174         if func.args.kwarg:
175             func.set_local(func.args.kwarg, func.args)
176         args = func.args.args
177         # If the function has no args, there is nothing left to do.
178         if args is None:
179             return
180     for arg in args:
181         if isinstance(arg, nodes.AssignName):
182             func.set_local(arg.name, arg)
183         else:
184             register_arguments(func, arg.elts)
185 
186 
187 def object_build_class(
188     node: nodes.Module | nodes.ClassDef, member: type, localname: str
189 ) -> nodes.ClassDef:
190     """"""create astroid for a living class object""""""
191     basenames = [base.__name__ for base in member.__bases__]
192     return _base_class_object_build(node, member, basenames, localname=localname)
193 
194 
195 def _get_args_info_from_callable(
196     member: _FunctionTypes,
197 ) -> tuple[list[str], list[str], list[Any], list[str]]:
198     """"""Returns args, posonlyargs, defaults, kwonlyargs.
199 
200     :note: currently ignores the return annotation.
201     """"""
202     signature = inspect.signature(member)
203     args: list[str] = []
204     defaults: list[Any] = []
205     posonlyargs: list[str] = []
206     kwonlyargs: list[str] = []
207 
208     for param_name, param in signature.parameters.items():
209         if param.kind == inspect.Parameter.POSITIONAL_ONLY:
210             posonlyargs.append(param_name)
211         elif param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
212             args.append(param_name)
213         elif param.kind == inspect.Parameter.VAR_POSITIONAL:
214             args.append(param_name)
215         elif param.kind == inspect.Parameter.VAR_KEYWORD:
216             args.append(param_name)
217         elif param.kind == inspect.Parameter.KEYWORD_ONLY:
218             kwonlyargs.append(param_name)
219         if param.default is not inspect._empty:
220             defaults.append(param.default)
221 
222     return args, posonlyargs, defaults, kwonlyargs
223 
224 
225 def object_build_function(
226     node: nodes.Module | nodes.ClassDef, member: _FunctionTypes, localname: str
227 ) -> None:
228     """"""create astroid for a living function object""""""
229     args, posonlyargs, defaults, kwonlyargs = _get_args_info_from_callable(member)
230 
231     func = build_function(
232         getattr(member, ""__name__"", None) or localname,
233         args,
234         posonlyargs,
235         defaults,
236         member.__doc__,
237         kwonlyargs=kwonlyargs,
238     )
239 
240     node.add_local_node(func, localname)
241 
242 
243 def object_build_datadescriptor(
244     node: nodes.Module | nodes.ClassDef, member: type, name: str
245 ) -> nodes.ClassDef:
246     """"""create astroid for a living data descriptor object""""""
247     return _base_class_object_build(node, member, [], name)
248 
249 
250 def object_build_methoddescriptor(
251     node: nodes.Module | nodes.ClassDef,
252     member: _FunctionTypes,
253     localname: str,
254 ) -> None:
255     """"""create astroid for a living method descriptor object""""""
256     # FIXME get arguments ?
257     func = build_function(
258         getattr(member, ""__name__"", None) or localname, doc=member.__doc__
259     )
260     node.add_local_node(func, localname)
261     _add_dunder_class(func, member)
262 
263 
264 def _base_class_object_build(
265     node: nodes.Module | nodes.ClassDef,
266     member: type,
267     basenames: list[str],
268     name: str | None = None,
269     localname: str | None = None,
270 ) -> nodes.ClassDef:
271     """"""create astroid for a living class object, with a given set of base names
272     (e.g. ancestors)
273     """"""
274     class_name = name or getattr(member, ""__name__"", None) or localname
275     assert isinstance(class_name, str)
276     klass = build_class(
277         class_name,
278         basenames,
279         member.__doc__,
280     )
281     klass._newstyle = isinstance(member, type)
282     node.add_local_node(klass, localname)
283     try:
284         # limit the instantiation trick since it's too dangerous
285         # (such as infinite test execution...)
286         # this at least resolves common case such as Exception.args,
287         # OSError.errno
288         if issubclass(member, Exception):
289             instdict = member().__dict__
290         else:
291             raise TypeError
292     except TypeError:
293         pass
294     else:
295         for item_name, obj in instdict.items():
296             valnode = nodes.EmptyNode()
297             valnode.object = obj
298             valnode.parent = klass
299             valnode.lineno = 1
300             klass.instance_attrs[item_name] = [valnode]
301     return klass
302 
303 
304 def _build_from_function(
305     node: nodes.Module | nodes.ClassDef,
306     name: str,
307     member: _FunctionTypes,
308     module: types.ModuleType,
309 ) -> None:
310     # verify this is not an imported function
311     try:
312         code = member.__code__  # type: ignore[union-attr]
313     except AttributeError:
314         # Some implementations don't provide the code object,
315         # such as Jython.
316         code = None
317     filename = getattr(code, ""co_filename"", None)
318     if filename is None:
319         assert isinstance(member, object)
320         object_build_methoddescriptor(node, member, name)
321     elif filename != getattr(module, ""__file__"", None):
322         attach_dummy_node(node, name, member)
323     else:
324         object_build_function(node, member, name)
325 
326 
327 def _safe_has_attribute(obj, member: str) -> bool:
328     """"""Required because unexpected RunTimeError can be raised.
329 
330     See https://github.com/PyCQA/astroid/issues/1958
331     """"""
332     try:
333         return hasattr(obj, member)
334     except Exception:  # pylint: disable=broad-except
335         return False
336 
337 
338 class InspectBuilder:
339     """"""class for building nodes from living object
340 
341     this is actually a really minimal representation, including only Module,
342     FunctionDef and ClassDef nodes and some others as guessed.
343     """"""
344 
345     def __init__(self, manager_instance: AstroidManager | None = None) -> None:
346         self._manager = manager_instance or AstroidManager()
347         self._done: dict[types.ModuleType | type, nodes.Module | nodes.ClassDef] = {}
348         self._module: types.ModuleType
349 
350     def inspect_build(
351         self,
352         module: types.ModuleType,
353         modname: str | None = None,
354         path: str | None = None,
355     ) -> nodes.Module:
356         """"""build astroid from a living module (i.e. using inspect)
357         this is used when there is no python source code available (either
358         because it's a built-in module or because the .py is not available)
359         """"""
360         self._module = module
361         if modname is None:
362             modname = module.__name__
363         try:
364             node = build_module(modname, module.__doc__)
365         except AttributeError:
366             # in jython, java modules have no __doc__ (see #109562)
367             node = build_module(modname)
368         if path is None:
369             node.path = node.file = path
370         else:
371             node.path = [os.path.abspath(path)]
372             node.file = node.path[0]
373         node.name = modname
374         self._manager.cache_module(node)
375         node.package = hasattr(module, ""__path__"")
376         self._done = {}
377         self.object_build(node, module)
378         return node
379 
380     def object_build(
381         self, node: nodes.Module | nodes.ClassDef, obj: types.ModuleType | type
382     ) -> None:
383         """"""recursive method which create a partial ast from real objects
384         (only function, class, and method are handled)
385         """"""
386         if obj in self._done:
387             return None
388         self._done[obj] = node
389         for name in dir(obj):
390             # inspect.ismethod() and inspect.isbuiltin() in PyPy return
391             # the opposite of what they do in CPython for __class_getitem__.
392             pypy__class_getitem__ = IS_PYPY and name == ""__class_getitem__""
393             try:
394                 with warnings.catch_warnings():
395                     warnings.simplefilter(""ignore"")
396                     member = getattr(obj, name)
397             except AttributeError:
398                 # damned ExtensionClass.Base, I know you're there !
399                 attach_dummy_node(node, name)
400                 continue
401             if inspect.ismethod(member) and not pypy__class_getitem__:
402                 member = member.__func__
403             if inspect.isfunction(member):
404                 _build_from_function(node, name, member, self._module)
405             elif inspect.isbuiltin(member) or pypy__class_getitem__:
406                 if self.imported_member(node, member, name):
407                     continue
408                 object_build_methoddescriptor(node, member, name)
409             elif inspect.isclass(member):
410                 if self.imported_member(node, member, name):
411                     continue
412                 if member in self._done:
413                     class_node = self._done[member]
414                     assert isinstance(class_node, nodes.ClassDef)
415                     if class_node not in node.locals.get(name, ()):
416                         node.add_local_node(class_node, name)
417                 else:
418                     class_node = object_build_class(node, member, name)
419                     # recursion
420                     self.object_build(class_node, member)
421                 if name == ""__class__"" and class_node.parent is None:
422                     class_node.parent = self._done[self._module]
423             elif inspect.ismethoddescriptor(member):
424                 object_build_methoddescriptor(node, member, name)
425             elif inspect.isdatadescriptor(member):
426                 object_build_datadescriptor(node, member, name)
427             elif isinstance(member, _CONSTANTS):
428                 attach_const_node(node, name, member)
429             elif inspect.isroutine(member):
430                 # This should be called for Jython, where some builtin
431                 # methods aren't caught by isbuiltin branch.
432                 _build_from_function(node, name, member, self._module)
433             elif _safe_has_attribute(member, ""__all__""):
434                 module = build_module(name)
435                 _attach_local_node(node, module, name)
436                 # recursion
437                 self.object_build(module, member)
438             else:
439                 # create an empty node so that the name is actually defined
440                 attach_dummy_node(node, name, member)
441         return None
442 
443     def imported_member(self, node, member, name: str) -> bool:
444         """"""verify this is not an imported class or handle it""""""
445         # /!\ some classes like ExtensionClass doesn't have a __module__
446         # attribute ! Also, this may trigger an exception on badly built module
447         # (see http://www.logilab.org/ticket/57299 for instance)
448         try:
449             modname = getattr(member, ""__module__"", None)
450         except TypeError:
451             modname = None
452         if modname is None:
453             if name in {""__new__"", ""__subclasshook__""}:
454                 # Python 2.5.1 (r251:54863, Sep  1 2010, 22:03:14)
455                 # >>> print object.__new__.__module__
456                 # None
457                 modname = builtins.__name__
458             else:
459                 attach_dummy_node(node, name, member)
460                 return True
461 
462         # On PyPy during bootstrapping we infer _io while _module is
463         # builtins. In CPython _io names itself io, see http://bugs.python.org/issue18602
464         # Therefore, this basically checks whether we are not in PyPy.
465         if modname == ""_io"" and not self._module.__name__ == ""builtins"":
466             return False
467 
468         real_name = {""gtk"": ""gtk_gtk""}.get(modname, modname)
469 
470         if real_name != self._module.__name__:
471             # check if it sounds valid and then add an import node, else use a
472             # dummy node
473             try:
474                 getattr(sys.modules[modname], name)
475             except (KeyError, AttributeError):
476                 attach_dummy_node(node, name, member)
477             else:
478                 attach_import_node(node, modname, name)
479             return True
480         return False
481 
482 
483 # astroid bootstrapping ######################################################
484 
485 _CONST_PROXY: dict[type, nodes.ClassDef] = {}
486 
487 
488 def _set_proxied(const) -> nodes.ClassDef:
489     # TODO : find a nicer way to handle this situation;
490     return _CONST_PROXY[const.value.__class__]
491 
492 
493 def _astroid_bootstrapping() -> None:
494     """"""astroid bootstrapping the builtins module""""""
495     # this boot strapping is necessary since we need the Const nodes to
496     # inspect_build builtins, and then we can proxy Const
497     builder = InspectBuilder()
498     astroid_builtin = builder.inspect_build(builtins)
499 
500     for cls, node_cls in node_classes.CONST_CLS.items():
501         if cls is TYPE_NONE:
502             proxy = build_class(""NoneType"")
503             proxy.parent = astroid_builtin
504         elif cls is TYPE_NOTIMPLEMENTED:
505             proxy = build_class(""NotImplementedType"")
506             proxy.parent = astroid_builtin
507         elif cls is TYPE_ELLIPSIS:
508             proxy = build_class(""Ellipsis"")
509             proxy.parent = astroid_builtin
510         else:
511             proxy = astroid_builtin.getattr(cls.__name__)[0]
512             assert isinstance(proxy, nodes.ClassDef)
513         if cls in (dict, list, set, tuple):
514             node_cls._proxied = proxy
515         else:
516             _CONST_PROXY[cls] = proxy
517 
518     # Set the builtin module as parent for some builtins.
519     nodes.Const._proxied = property(_set_proxied)
520 
521     _GeneratorType = nodes.ClassDef(types.GeneratorType.__name__)
522     _GeneratorType.parent = astroid_builtin
523     generator_doc_node = (
524         nodes.Const(value=types.GeneratorType.__doc__)
525         if types.GeneratorType.__doc__
526         else None
527     )
528     _GeneratorType.postinit(
529         bases=[],
530         body=[],
531         decorators=None,
532         doc_node=generator_doc_node,
533     )
534     bases.Generator._proxied = _GeneratorType
535     builder.object_build(bases.Generator._proxied, types.GeneratorType)
536 
537     if hasattr(types, ""AsyncGeneratorType""):
538         _AsyncGeneratorType = nodes.ClassDef(types.AsyncGeneratorType.__name__)
539         _AsyncGeneratorType.parent = astroid_builtin
540         async_generator_doc_node = (
541             nodes.Const(value=types.AsyncGeneratorType.__doc__)
542             if types.AsyncGeneratorType.__doc__
543             else None
544         )
545         _AsyncGeneratorType.postinit(
546             bases=[],
547             body=[],
548             decorators=None,
549             doc_node=async_generator_doc_node,
550         )
551         bases.AsyncGenerator._proxied = _AsyncGeneratorType
552         builder.object_build(bases.AsyncGenerator._proxied, types.AsyncGeneratorType)
553     builtin_types = (
554         types.GetSetDescriptorType,
555         types.GeneratorType,
556         types.MemberDescriptorType,
557         TYPE_NONE,
558         TYPE_NOTIMPLEMENTED,
559         types.FunctionType,
560         types.MethodType,
561         types.BuiltinFunctionType,
562         types.ModuleType,
563         types.TracebackType,
564     )
565     for _type in builtin_types:
566         if _type.__name__ not in astroid_builtin:
567             klass = nodes.ClassDef(_type.__name__)
568             klass.parent = astroid_builtin
569             klass.postinit(
570                 bases=[],
571                 body=[],
572                 decorators=None,
573                 doc_node=nodes.Const(value=_type.__doc__) if _type.__doc__ else None,
574             )
575             builder.object_build(klass, _type)
576             astroid_builtin[_type.__name__] = klass
577 
578 
579 _astroid_bootstrapping()
580 
[end of astroid/raw_building.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pylint-dev/astroid,0c9ab0fe56703fa83c73e514a1020d398d23fa7f,"Deprecation warnings from numpy
### Steps to reproduce

1. Run pylint over the following test case:

```
""""""Test case""""""

import numpy as np
value = np.random.seed(1234)
```

### Current behavior
```
/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.
  getattr(sys.modules[modname], name)
/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.
  getattr(sys.modules[modname], name)
```

### Expected behavior
There should be no future warnings.

### python -c ""from astroid import __pkginfo__; print(__pkginfo__.version)"" output
2.12.13
","This seems very similar to https://github.com/PyCQA/astroid/pull/1514 that was fixed in 2.12.0.
I'm running 2.12.13 (> 2.12.0), so the fix isn't working in this case?
I don't know why #1514 did not fix this, I think we were capturing both stdout and stderr, so this will need some investigation. My guess would be that there's somewhere else to apply the same method to.
Hello, 
I see the same error with pylint on our tool [demcompare](https://github.com/CNES/demcompare). Pylint version:
```
pylint --version
pylint 2.15.9
astroid 2.12.13
Python 3.8.10 (default, Nov 14 2022, 12:59:47) 
[GCC 9.4.0]
```
I confirm the weird astroid lower warning and I don't know how to bypass it with pylint checking. 

```
pylint demcompare 
/home/duboise/work/src/demcompare/venv/lib/python3.8/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.
  getattr(sys.modules[modname], name)
... (four times)
```

Thanks in advance if there is a solution
Cordially

> Thanks in advance if there is a solution

while annoying the warning does not make pylint fail. Just ignore it. In a CI you can just check pylint return code. It will return 0 as expected
I agree, even if annoying because it feels our code as a problem somewhere, the CI with pylint doesn't fail indeed. Thanks for the answer that confirm to not bother for now. 
That might be fine in a CI environment, but for users, ultimately, ignoring warnings becomes difficult when there are too many such warnings. I would like to see this fixed.
Oh, it was not an argument in favour of not fixing it. It was just to point out that it is not a breaking problem. It is ""just"" a lot of quite annoying warnings. I am following the issue because it annoys me too. So I am in the same ""I hope they will fix it"" boat
> I don't know why https://github.com/PyCQA/astroid/pull/1514 did not fix this, I think we were capturing both stdout and stderr, so this will need some investigation. My guess would be that there's somewhere else to apply the same method to.

That PR only addressed import-time. This `FutureWarning` is emitted by numpy's package-level `__getattr__` method, not during import.",2023-01-28T06:14:39Z,"<patch>
diff --git a/astroid/raw_building.py b/astroid/raw_building.py
--- a/astroid/raw_building.py
+++ b/astroid/raw_building.py
@@ -10,11 +10,14 @@
 
 import builtins
 import inspect
+import io
+import logging
 import os
 import sys
 import types
 import warnings
 from collections.abc import Iterable
+from contextlib import redirect_stderr, redirect_stdout
 from typing import Any, Union
 
 from astroid import bases, nodes
@@ -22,6 +25,9 @@
 from astroid.manager import AstroidManager
 from astroid.nodes import node_classes
 
+logger = logging.getLogger(__name__)
+
+
 _FunctionTypes = Union[
     types.FunctionType,
     types.MethodType,
@@ -471,7 +477,26 @@ def imported_member(self, node, member, name: str) -> bool:
             # check if it sounds valid and then add an import node, else use a
             # dummy node
             try:
-                getattr(sys.modules[modname], name)
+                with redirect_stderr(io.StringIO()) as stderr, redirect_stdout(
+                    io.StringIO()
+                ) as stdout:
+                    getattr(sys.modules[modname], name)
+                    stderr_value = stderr.getvalue()
+                    if stderr_value:
+                        logger.error(
+                            ""Captured stderr while getting %s from %s:\n%s"",
+                            name,
+                            sys.modules[modname],
+                            stderr_value,
+                        )
+                    stdout_value = stdout.getvalue()
+                    if stdout_value:
+                        logger.info(
+                            ""Captured stdout while getting %s from %s:\n%s"",
+                            name,
+                            sys.modules[modname],
+                            stdout_value,
+                        )
             except (KeyError, AttributeError):
                 attach_dummy_node(node, name, member)
             else:

</patch>","diff --git a/tests/unittest_raw_building.py b/tests/unittest_raw_building.py
--- a/tests/unittest_raw_building.py
+++ b/tests/unittest_raw_building.py
@@ -8,8 +8,15 @@
 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE
 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt
 
+from __future__ import annotations
+
+import logging
+import os
+import sys
 import types
 import unittest
+from typing import Any
+from unittest import mock
 
 import _io
 import pytest
@@ -117,5 +124,45 @@ def test_module_object_with_broken_getattr(self) -> None:
         AstroidBuilder().inspect_build(fm_getattr, ""test"")
 
 
+@pytest.mark.skipif(
+    ""posix"" not in sys.builtin_module_names, reason=""Platform doesn't support posix""
+)
+def test_build_module_getattr_catch_output(
+    capsys: pytest.CaptureFixture[str],
+    caplog: pytest.LogCaptureFixture,
+) -> None:
+    """"""Catch stdout and stderr in module __getattr__ calls when building a module.
+
+    Usually raised by DeprecationWarning or FutureWarning.
+    """"""
+    caplog.set_level(logging.INFO)
+    original_sys = sys.modules
+    original_module = sys.modules[""posix""]
+    expected_out = ""INFO (TEST): Welcome to posix!""
+    expected_err = ""WARNING (TEST): Monkey-patched version of posix - module getattr""
+
+    class CustomGetattr:
+        def __getattr__(self, name: str) -> Any:
+            print(f""{expected_out}"")
+            print(expected_err, file=sys.stderr)
+            return getattr(original_module, name)
+
+    def mocked_sys_modules_getitem(name: str) -> types.ModuleType | CustomGetattr:
+        if name != ""posix"":
+            return original_sys[name]
+        return CustomGetattr()
+
+    with mock.patch(""astroid.raw_building.sys.modules"") as sys_mock:
+        sys_mock.__getitem__.side_effect = mocked_sys_modules_getitem
+        builder = AstroidBuilder()
+        builder.inspect_build(os)
+
+    out, err = capsys.readouterr()
+    assert expected_out in caplog.text
+    assert expected_err in caplog.text
+    assert not out
+    assert not err
+
+
 if __name__ == ""__main__"":
     unittest.main()
",2.14,"[""tests/unittest_raw_building.py::test_build_module_getattr_catch_output""]","[""tests/unittest_raw_building.py::RawBuildingTC::test_attach_dummy_node"", ""tests/unittest_raw_building.py::RawBuildingTC::test_build_class"", ""tests/unittest_raw_building.py::RawBuildingTC::test_build_from_import"", ""tests/unittest_raw_building.py::RawBuildingTC::test_build_function"", ""tests/unittest_raw_building.py::RawBuildingTC::test_build_function_args"", ""tests/unittest_raw_building.py::RawBuildingTC::test_build_function_deepinspect_deprecation"", ""tests/unittest_raw_building.py::RawBuildingTC::test_build_function_defaults"", ""tests/unittest_raw_building.py::RawBuildingTC::test_build_function_kwonlyargs"", ""tests/unittest_raw_building.py::RawBuildingTC::test_build_function_posonlyargs"", ""tests/unittest_raw_building.py::RawBuildingTC::test_build_module"", ""tests/unittest_raw_building.py::RawBuildingTC::test_io_is__io"", ""tests/unittest_raw_building.py::RawBuildingTC::test_module_object_with_broken_getattr""]",0c9ab0fe56703fa83c73e514a1020d398d23fa7f
pylint-dev__astroid-1333,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
astroid 2.9.1 breaks pylint with missing __init__.py: F0010: error while code parsing: Unable to load file __init__.py
### Steps to reproduce
> Steps provided are for Windows 11, but initial problem found in Ubuntu 20.04

> Update 2022-01-04: Corrected repro steps and added more environment details

1. Set up simple repo with following structure (all files can be empty):
```
root_dir/
|--src/
|----project/ # Notice the missing __init__.py
|------file.py # It can be empty, but I added `import os` at the top
|----__init__.py
```
2. Open a command prompt
3. `cd root_dir`
4. `python -m venv venv`
5. `venv/Scripts/activate`
6. `pip install pylint astroid==2.9.1` # I also repro'd on the latest, 2.9.2
7. `pylint src/project` # Updated from `pylint src`
8. Observe failure:
```
src\project\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\project\__init__.py:
```

### Current behavior
Fails with `src\project\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\project\__init__.py:`

### Expected behavior
Does not fail with error.
> If you replace step 6 with `pip install pylint astroid==2.9.0`, you get no failure with an empty output - since no files have content

### `python -c ""from astroid import __pkginfo__; print(__pkginfo__.version)""` output
2.9.1

`python 3.9.1`
`pylint 2.12.2 `



This issue has been observed with astroid `2.9.1` and `2.9.2`

</issue>
<code>
[start of README.rst]
1 Astroid
2 =======
3 
4 .. image:: https://coveralls.io/repos/github/PyCQA/astroid/badge.svg?branch=main
5     :target: https://coveralls.io/github/PyCQA/astroid?branch=main
6     :alt: Coverage badge from coveralls.io
7 
8 .. image:: https://readthedocs.org/projects/astroid/badge/?version=latest
9     :target: http://astroid.readthedocs.io/en/latest/?badge=latest
10     :alt: Documentation Status
11 
12 .. image:: https://img.shields.io/badge/code%20style-black-000000.svg
13     :target: https://github.com/ambv/black
14 
15 .. image:: https://results.pre-commit.ci/badge/github/PyCQA/astroid/main.svg
16    :target: https://results.pre-commit.ci/latest/github/PyCQA/astroid/main
17    :alt: pre-commit.ci status
18 
19 .. |tidelift_logo| image:: https://raw.githubusercontent.com/PyCQA/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png
20    :width: 75
21    :height: 60
22    :alt: Tidelift
23 
24 .. list-table::
25    :widths: 10 100
26 
27    * - |tidelift_logo|
28      - Professional support for astroid is available as part of the
29        `Tidelift Subscription`_.  Tidelift gives software development teams a single source for
30        purchasing and maintaining their software, with professional grade assurances
31        from the experts who know it best, while seamlessly integrating with existing
32        tools.
33 
34 .. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme
35 
36 
37 
38 What's this?
39 ------------
40 
41 The aim of this module is to provide a common base representation of
42 python source code. It is currently the library powering pylint's capabilities.
43 
44 It provides a compatible representation which comes from the `_ast`
45 module.  It rebuilds the tree generated by the builtin _ast module by
46 recursively walking down the AST and building an extended ast. The new
47 node classes have additional methods and attributes for different
48 usages. They include some support for static inference and local name
49 scopes. Furthermore, astroid can also build partial trees by inspecting living
50 objects.
51 
52 
53 Installation
54 ------------
55 
56 Extract the tarball, jump into the created directory and run::
57 
58     pip install .
59 
60 
61 If you want to do an editable installation, you can run::
62 
63     pip install -e .
64 
65 
66 If you have any questions, please mail the code-quality@python.org
67 mailing list for support. See
68 http://mail.python.org/mailman/listinfo/code-quality for subscription
69 information and archives.
70 
71 Documentation
72 -------------
73 http://astroid.readthedocs.io/en/latest/
74 
75 
76 Python Versions
77 ---------------
78 
79 astroid 2.0 is currently available for Python 3 only. If you want Python 2
80 support, use an older version of astroid (though note that these versions
81 are no longer supported).
82 
83 Test
84 ----
85 
86 Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use
87 either `tox` or `pytest`::
88 
89     tox
90     pytest astroid
91 
[end of README.rst]
[start of astroid/modutils.py]
1 # Copyright (c) 2014-2018, 2020 Claudiu Popa <pcmanticore@gmail.com>
2 # Copyright (c) 2014 Google, Inc.
3 # Copyright (c) 2014 Denis Laxalde <denis.laxalde@logilab.fr>
4 # Copyright (c) 2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>
5 # Copyright (c) 2014 Eevee (Alex Munroe) <amunroe@yelp.com>
6 # Copyright (c) 2015 Florian Bruhin <me@the-compiler.org>
7 # Copyright (c) 2015 RadosÅ‚aw Ganczarek <radoslaw@ganczarek.in>
8 # Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>
9 # Copyright (c) 2016 Jakub Wilk <jwilk@jwilk.net>
10 # Copyright (c) 2016 Ceridwen <ceridwenv@gmail.com>
11 # Copyright (c) 2018 Ville SkyttÃ¤ <ville.skytta@iki.fi>
12 # Copyright (c) 2018 Mario Corchero <mcorcherojim@bloomberg.net>
13 # Copyright (c) 2018 Mario Corchero <mariocj89@gmail.com>
14 # Copyright (c) 2018 Anthony Sottile <asottile@umich.edu>
15 # Copyright (c) 2019 Hugo van Kemenade <hugovk@users.noreply.github.com>
16 # Copyright (c) 2019 markmcclain <markmcclain@users.noreply.github.com>
17 # Copyright (c) 2019 BasPH <BasPH@users.noreply.github.com>
18 # Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>
19 # Copyright (c) 2020 Peter Kolbus <peter.kolbus@gmail.com>
20 # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>
21 # Copyright (c) 2021 DaniÃ«l van Noord <13665637+DanielNoord@users.noreply.github.com>
22 # Copyright (c) 2021 Keichi Takahashi <hello@keichi.dev>
23 # Copyright (c) 2021 Nick Drozd <nicholasdrozd@gmail.com>
24 # Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>
25 # Copyright (c) 2021 DudeNr33 <3929834+DudeNr33@users.noreply.github.com>
26 
27 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html
28 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE
29 
30 """"""Python modules manipulation utility functions.
31 
32 :type PY_SOURCE_EXTS: tuple(str)
33 :var PY_SOURCE_EXTS: list of possible python source file extension
34 
35 :type STD_LIB_DIRS: set of str
36 :var STD_LIB_DIRS: directories where standard modules are located
37 
38 :type BUILTIN_MODULES: dict
39 :var BUILTIN_MODULES: dictionary with builtin module names has key
40 """"""
41 
42 # We disable the import-error so pylint can work without distutils installed.
43 # pylint: disable=no-name-in-module,useless-suppression
44 
45 import importlib
46 import importlib.machinery
47 import importlib.util
48 import itertools
49 import os
50 import platform
51 import sys
52 import types
53 from distutils.errors import DistutilsPlatformError  # pylint: disable=import-error
54 from distutils.sysconfig import get_python_lib  # pylint: disable=import-error
55 from typing import Dict, Set
56 
57 from astroid.interpreter._import import spec, util
58 
59 # distutils is replaced by virtualenv with a module that does
60 # weird path manipulations in order to get to the
61 # real distutils module.
62 
63 
64 if sys.platform.startswith(""win""):
65     PY_SOURCE_EXTS = (""py"", ""pyw"")
66     PY_COMPILED_EXTS = (""dll"", ""pyd"")
67 else:
68     PY_SOURCE_EXTS = (""py"",)
69     PY_COMPILED_EXTS = (""so"",)
70 
71 
72 try:
73     # The explicit sys.prefix is to work around a patch in virtualenv that
74     # replaces the 'real' sys.prefix (i.e. the location of the binary)
75     # with the prefix from which the virtualenv was created. This throws
76     # off the detection logic for standard library modules, thus the
77     # workaround.
78     STD_LIB_DIRS = {
79         get_python_lib(standard_lib=True, prefix=sys.prefix),
80         # Take care of installations where exec_prefix != prefix.
81         get_python_lib(standard_lib=True, prefix=sys.exec_prefix),
82         get_python_lib(standard_lib=True),
83     }
84 # get_python_lib(standard_lib=1) is not available on pypy, set STD_LIB_DIR to
85 # non-valid path, see https://bugs.pypy.org/issue1164
86 except DistutilsPlatformError:
87     STD_LIB_DIRS = set()
88 
89 if os.name == ""nt"":
90     STD_LIB_DIRS.add(os.path.join(sys.prefix, ""dlls""))
91     try:
92         # real_prefix is defined when running inside virtual environments,
93         # created with the **virtualenv** library.
94         # Deprecated in virtualenv==16.7.9
95         # See: https://github.com/pypa/virtualenv/issues/1622
96         STD_LIB_DIRS.add(os.path.join(sys.real_prefix, ""dlls""))  # type: ignore[attr-defined]
97     except AttributeError:
98         # sys.base_exec_prefix is always defined, but in a virtual environment
99         # created with the stdlib **venv** module, it points to the original
100         # installation, if the virtual env is activated.
101         try:
102             STD_LIB_DIRS.add(os.path.join(sys.base_exec_prefix, ""dlls""))
103         except AttributeError:
104             pass
105 
106 if platform.python_implementation() == ""PyPy"":
107     # The get_python_lib(standard_lib=True) function does not give valid
108     # result with pypy in a virtualenv.
109     # In a virtual environment, with CPython implementation the call to this function returns a path toward
110     # the binary (its libraries) which has been used to create the virtual environment.
111     # Not with pypy implementation.
112     # The only way to retrieve such information is to use the sys.base_prefix hint.
113     # It's worth noticing that under CPython implementation the return values of
114     # get_python_lib(standard_lib=True) and get_python_lib(santdard_lib=True, prefix=sys.base_prefix)
115     # are the same.
116     # In the lines above, we could have replace the call to get_python_lib(standard=True)
117     # with the one using prefix=sys.base_prefix but we prefer modifying only what deals with pypy.
118     STD_LIB_DIRS.add(get_python_lib(standard_lib=True, prefix=sys.base_prefix))
119     _root = os.path.join(sys.prefix, ""lib_pypy"")
120     STD_LIB_DIRS.add(_root)
121     try:
122         # real_prefix is defined when running inside virtualenv.
123         STD_LIB_DIRS.add(os.path.join(sys.base_prefix, ""lib_pypy""))
124     except AttributeError:
125         pass
126     del _root
127 if os.name == ""posix"":
128     # Need the real prefix if we're in a virtualenv, otherwise
129     # the usual one will do.
130     # Deprecated in virtualenv==16.7.9
131     # See: https://github.com/pypa/virtualenv/issues/1622
132     try:
133         prefix = sys.real_prefix  # type: ignore[attr-defined]
134     except AttributeError:
135         prefix = sys.prefix
136 
137     def _posix_path(path):
138         base_python = ""python%d.%d"" % sys.version_info[:2]
139         return os.path.join(prefix, path, base_python)
140 
141     STD_LIB_DIRS.add(_posix_path(""lib""))
142     if sys.maxsize > 2 ** 32:
143         # This tries to fix a problem with /usr/lib64 builds,
144         # where systems are running both 32-bit and 64-bit code
145         # on the same machine, which reflects into the places where
146         # standard library could be found. More details can be found
147         # here http://bugs.python.org/issue1294959.
148         # An easy reproducing case would be
149         # https://github.com/PyCQA/pylint/issues/712#issuecomment-163178753
150         STD_LIB_DIRS.add(_posix_path(""lib64""))
151 
152 EXT_LIB_DIRS = {get_python_lib(), get_python_lib(True)}
153 IS_JYTHON = platform.python_implementation() == ""Jython""
154 BUILTIN_MODULES = dict.fromkeys(sys.builtin_module_names, True)
155 
156 
157 class NoSourceFile(Exception):
158     """"""exception raised when we are not able to get a python
159     source file for a precompiled file
160     """"""
161 
162 
163 def _normalize_path(path: str) -> str:
164     """"""Resolve symlinks in path and convert to absolute path.
165 
166     Note that environment variables and ~ in the path need to be expanded in
167     advance.
168 
169     This can be cached by using _cache_normalize_path.
170     """"""
171     return os.path.normcase(os.path.realpath(path))
172 
173 
174 def _path_from_filename(filename, is_jython=IS_JYTHON):
175     if not is_jython:
176         return filename
177     head, has_pyclass, _ = filename.partition(""$py.class"")
178     if has_pyclass:
179         return head + "".py""
180     return filename
181 
182 
183 def _handle_blacklist(blacklist, dirnames, filenames):
184     """"""remove files/directories in the black list
185 
186     dirnames/filenames are usually from os.walk
187     """"""
188     for norecurs in blacklist:
189         if norecurs in dirnames:
190             dirnames.remove(norecurs)
191         elif norecurs in filenames:
192             filenames.remove(norecurs)
193 
194 
195 _NORM_PATH_CACHE: Dict[str, str] = {}
196 
197 
198 def _cache_normalize_path(path: str) -> str:
199     """"""Normalize path with caching.""""""
200     # _module_file calls abspath on every path in sys.path every time it's
201     # called; on a larger codebase this easily adds up to half a second just
202     # assembling path components. This cache alleviates that.
203     try:
204         return _NORM_PATH_CACHE[path]
205     except KeyError:
206         if not path:  # don't cache result for ''
207             return _normalize_path(path)
208         result = _NORM_PATH_CACHE[path] = _normalize_path(path)
209         return result
210 
211 
212 def load_module_from_name(dotted_name: str) -> types.ModuleType:
213     """"""Load a Python module from its name.
214 
215     :type dotted_name: str
216     :param dotted_name: python name of a module or package
217 
218     :raise ImportError: if the module or package is not found
219 
220     :rtype: module
221     :return: the loaded module
222     """"""
223     try:
224         return sys.modules[dotted_name]
225     except KeyError:
226         pass
227 
228     return importlib.import_module(dotted_name)
229 
230 
231 def load_module_from_modpath(parts):
232     """"""Load a python module from its split name.
233 
234     :type parts: list(str) or tuple(str)
235     :param parts:
236       python name of a module or package split on '.'
237 
238     :raise ImportError: if the module or package is not found
239 
240     :rtype: module
241     :return: the loaded module
242     """"""
243     return load_module_from_name(""."".join(parts))
244 
245 
246 def load_module_from_file(filepath: str):
247     """"""Load a Python module from it's path.
248 
249     :type filepath: str
250     :param filepath: path to the python module or package
251 
252     :raise ImportError: if the module or package is not found
253 
254     :rtype: module
255     :return: the loaded module
256     """"""
257     modpath = modpath_from_file(filepath)
258     return load_module_from_modpath(modpath)
259 
260 
261 def check_modpath_has_init(path, mod_path):
262     """"""check there are some __init__.py all along the way""""""
263     modpath = []
264     for part in mod_path:
265         modpath.append(part)
266         path = os.path.join(path, part)
267         if not _has_init(path):
268             old_namespace = util.is_namespace(""."".join(modpath))
269             if not old_namespace:
270                 return False
271     return True
272 
273 
274 def _get_relative_base_path(filename, path_to_check):
275     """"""Extracts the relative mod path of the file to import from
276 
277     Check if a file is within the passed in path and if so, returns the
278     relative mod path from the one passed in.
279 
280     If the filename is no in path_to_check, returns None
281 
282     Note this function will look for both abs and realpath of the file,
283     this allows to find the relative base path even if the file is a
284     symlink of a file in the passed in path
285 
286     Examples:
287         _get_relative_base_path(""/a/b/c/d.py"", ""/a/b"") ->  [""c"",""d""]
288         _get_relative_base_path(""/a/b/c/d.py"", ""/dev"") ->  None
289     """"""
290     importable_path = None
291     path_to_check = os.path.normcase(path_to_check)
292     abs_filename = os.path.abspath(filename)
293     if os.path.normcase(abs_filename).startswith(path_to_check):
294         importable_path = abs_filename
295 
296     real_filename = os.path.realpath(filename)
297     if os.path.normcase(real_filename).startswith(path_to_check):
298         importable_path = real_filename
299 
300     if importable_path:
301         base_path = os.path.splitext(importable_path)[0]
302         relative_base_path = base_path[len(path_to_check) :]
303         return [pkg for pkg in relative_base_path.split(os.sep) if pkg]
304 
305     return None
306 
307 
308 def modpath_from_file_with_callback(filename, path=None, is_package_cb=None):
309     filename = os.path.expanduser(_path_from_filename(filename))
310     for pathname in itertools.chain(
311         path or [], map(_cache_normalize_path, sys.path), sys.path
312     ):
313         if not pathname:
314             continue
315         modpath = _get_relative_base_path(filename, pathname)
316         if not modpath:
317             continue
318         if is_package_cb(pathname, modpath[:-1]):
319             return modpath
320 
321     raise ImportError(
322         ""Unable to find module for {} in {}"".format(filename, "", \n"".join(sys.path))
323     )
324 
325 
326 def modpath_from_file(filename, path=None):
327     """"""Get the corresponding split module's name from a filename
328 
329     This function will return the name of a module or package split on `.`.
330 
331     :type filename: str
332     :param filename: file's path for which we want the module's name
333 
334     :type Optional[List[str]] path:
335       Optional list of path where the module or package should be
336       searched (use sys.path if nothing or None is given)
337 
338     :raise ImportError:
339       if the corresponding module's name has not been found
340 
341     :rtype: list(str)
342     :return: the corresponding split module's name
343     """"""
344     return modpath_from_file_with_callback(filename, path, check_modpath_has_init)
345 
346 
347 def file_from_modpath(modpath, path=None, context_file=None):
348     return file_info_from_modpath(modpath, path, context_file).location
349 
350 
351 def file_info_from_modpath(modpath, path=None, context_file=None):
352     """"""given a mod path (i.e. split module / package name), return the
353     corresponding file, giving priority to source file over precompiled
354     file if it exists
355 
356     :type modpath: list or tuple
357     :param modpath:
358       split module's name (i.e name of a module or package split
359       on '.')
360       (this means explicit relative imports that start with dots have
361       empty strings in this list!)
362 
363     :type path: list or None
364     :param path:
365       optional list of path where the module or package should be
366       searched (use sys.path if nothing or None is given)
367 
368     :type context_file: str or None
369     :param context_file:
370       context file to consider, necessary if the identifier has been
371       introduced using a relative import unresolvable in the actual
372       context (i.e. modutils)
373 
374     :raise ImportError: if there is no such module in the directory
375 
376     :rtype: (str or None, import type)
377     :return:
378       the path to the module's file or None if it's an integrated
379       builtin module such as 'sys'
380     """"""
381     if context_file is not None:
382         context = os.path.dirname(context_file)
383     else:
384         context = context_file
385     if modpath[0] == ""xml"":
386         # handle _xmlplus
387         try:
388             return _spec_from_modpath([""_xmlplus""] + modpath[1:], path, context)
389         except ImportError:
390             return _spec_from_modpath(modpath, path, context)
391     elif modpath == [""os"", ""path""]:
392         # FIXME: currently ignoring search_path...
393         return spec.ModuleSpec(
394             name=""os.path"",
395             location=os.path.__file__,
396             module_type=spec.ModuleType.PY_SOURCE,
397         )
398     return _spec_from_modpath(modpath, path, context)
399 
400 
401 def get_module_part(dotted_name, context_file=None):
402     """"""given a dotted name return the module part of the name :
403 
404     >>> get_module_part('astroid.as_string.dump')
405     'astroid.as_string'
406 
407     :type dotted_name: str
408     :param dotted_name: full name of the identifier we are interested in
409 
410     :type context_file: str or None
411     :param context_file:
412       context file to consider, necessary if the identifier has been
413       introduced using a relative import unresolvable in the actual
414       context (i.e. modutils)
415 
416 
417     :raise ImportError: if there is no such module in the directory
418 
419     :rtype: str or None
420     :return:
421       the module part of the name or None if we have not been able at
422       all to import the given name
423 
424     XXX: deprecated, since it doesn't handle package precedence over module
425     (see #10066)
426     """"""
427     # os.path trick
428     if dotted_name.startswith(""os.path""):
429         return ""os.path""
430     parts = dotted_name.split(""."")
431     if context_file is not None:
432         # first check for builtin module which won't be considered latter
433         # in that case (path != None)
434         if parts[0] in BUILTIN_MODULES:
435             if len(parts) > 2:
436                 raise ImportError(dotted_name)
437             return parts[0]
438         # don't use += or insert, we want a new list to be created !
439     path = None
440     starti = 0
441     if parts[0] == """":
442         assert (
443             context_file is not None
444         ), ""explicit relative import, but no context_file?""
445         path = []  # prevent resolving the import non-relatively
446         starti = 1
447     while parts[starti] == """":  # for all further dots: change context
448         starti += 1
449         context_file = os.path.dirname(context_file)
450     for i in range(starti, len(parts)):
451         try:
452             file_from_modpath(
453                 parts[starti : i + 1], path=path, context_file=context_file
454             )
455         except ImportError:
456             if i < max(1, len(parts) - 2):
457                 raise
458             return ""."".join(parts[:i])
459     return dotted_name
460 
461 
462 def get_module_files(src_directory, blacklist, list_all=False):
463     """"""given a package directory return a list of all available python
464     module's files in the package and its subpackages
465 
466     :type src_directory: str
467     :param src_directory:
468       path of the directory corresponding to the package
469 
470     :type blacklist: list or tuple
471     :param blacklist: iterable
472       list of files or directories to ignore.
473 
474     :type list_all: bool
475     :param list_all:
476         get files from all paths, including ones without __init__.py
477 
478     :rtype: list
479     :return:
480       the list of all available python module's files in the package and
481       its subpackages
482     """"""
483     files = []
484     for directory, dirnames, filenames in os.walk(src_directory):
485         if directory in blacklist:
486             continue
487         _handle_blacklist(blacklist, dirnames, filenames)
488         # check for __init__.py
489         if not list_all and ""__init__.py"" not in filenames:
490             dirnames[:] = ()
491             continue
492         for filename in filenames:
493             if _is_python_file(filename):
494                 src = os.path.join(directory, filename)
495                 files.append(src)
496     return files
497 
498 
499 def get_source_file(filename, include_no_ext=False):
500     """"""given a python module's file name return the matching source file
501     name (the filename will be returned identically if it's already an
502     absolute path to a python source file...)
503 
504     :type filename: str
505     :param filename: python module's file name
506 
507 
508     :raise NoSourceFile: if no source file exists on the file system
509 
510     :rtype: str
511     :return: the absolute path of the source file if it exists
512     """"""
513     filename = os.path.abspath(_path_from_filename(filename))
514     base, orig_ext = os.path.splitext(filename)
515     for ext in PY_SOURCE_EXTS:
516         source_path = f""{base}.{ext}""
517         if os.path.exists(source_path):
518             return source_path
519     if include_no_ext and not orig_ext and os.path.exists(base):
520         return base
521     raise NoSourceFile(filename)
522 
523 
524 def is_python_source(filename):
525     """"""
526     rtype: bool
527     return: True if the filename is a python source file
528     """"""
529     return os.path.splitext(filename)[1][1:] in PY_SOURCE_EXTS
530 
531 
532 def is_standard_module(modname, std_path=None):
533     """"""try to guess if a module is a standard python module (by default,
534     see `std_path` parameter's description)
535 
536     :type modname: str
537     :param modname: name of the module we are interested in
538 
539     :type std_path: list(str) or tuple(str)
540     :param std_path: list of path considered has standard
541 
542 
543     :rtype: bool
544     :return:
545       true if the module:
546       - is located on the path listed in one of the directory in `std_path`
547       - is a built-in module
548     """"""
549     modname = modname.split(""."")[0]
550     try:
551         filename = file_from_modpath([modname])
552     except ImportError:
553         # import failed, i'm probably not so wrong by supposing it's
554         # not standard...
555         return False
556     # modules which are not living in a file are considered standard
557     # (sys and __builtin__ for instance)
558     if filename is None:
559         # we assume there are no namespaces in stdlib
560         return not util.is_namespace(modname)
561     filename = _normalize_path(filename)
562     for path in EXT_LIB_DIRS:
563         if filename.startswith(_cache_normalize_path(path)):
564             return False
565     if std_path is None:
566         std_path = STD_LIB_DIRS
567 
568     return any(filename.startswith(_cache_normalize_path(path)) for path in std_path)
569 
570 
571 def is_relative(modname, from_file):
572     """"""return true if the given module name is relative to the given
573     file name
574 
575     :type modname: str
576     :param modname: name of the module we are interested in
577 
578     :type from_file: str
579     :param from_file:
580       path of the module from which modname has been imported
581 
582     :rtype: bool
583     :return:
584       true if the module has been imported relatively to `from_file`
585     """"""
586     if not os.path.isdir(from_file):
587         from_file = os.path.dirname(from_file)
588     if from_file in sys.path:
589         return False
590     return bool(
591         importlib.machinery.PathFinder.find_spec(
592             modname.split(""."", maxsplit=1)[0], [from_file]
593         )
594     )
595 
596 
597 # internal only functions #####################################################
598 
599 
600 def _spec_from_modpath(modpath, path=None, context=None):
601     """"""given a mod path (i.e. split module / package name), return the
602     corresponding spec
603 
604     this function is used internally, see `file_from_modpath`'s
605     documentation for more information
606     """"""
607     assert modpath
608     location = None
609     if context is not None:
610         try:
611             found_spec = spec.find_spec(modpath, [context])
612             location = found_spec.location
613         except ImportError:
614             found_spec = spec.find_spec(modpath, path)
615             location = found_spec.location
616     else:
617         found_spec = spec.find_spec(modpath, path)
618     if found_spec.type == spec.ModuleType.PY_COMPILED:
619         try:
620             location = get_source_file(found_spec.location)
621             return found_spec._replace(
622                 location=location, type=spec.ModuleType.PY_SOURCE
623             )
624         except NoSourceFile:
625             return found_spec._replace(location=location)
626     elif found_spec.type == spec.ModuleType.C_BUILTIN:
627         # integrated builtin module
628         return found_spec._replace(location=None)
629     elif found_spec.type == spec.ModuleType.PKG_DIRECTORY:
630         location = _has_init(found_spec.location)
631         return found_spec._replace(location=location, type=spec.ModuleType.PY_SOURCE)
632     return found_spec
633 
634 
635 def _is_python_file(filename):
636     """"""return true if the given filename should be considered as a python file
637 
638     .pyc and .pyo are ignored
639     """"""
640     return filename.endswith(("".py"", "".so"", "".pyd"", "".pyw""))
641 
642 
643 def _has_init(directory):
644     """"""if the given directory has a valid __init__ file, return its path,
645     else return None
646     """"""
647     mod_or_pack = os.path.join(directory, ""__init__"")
648     for ext in PY_SOURCE_EXTS + (""pyc"", ""pyo""):
649         if os.path.exists(mod_or_pack + ""."" + ext):
650             return mod_or_pack + ""."" + ext
651     return None
652 
653 
654 def is_namespace(specobj):
655     return specobj.type == spec.ModuleType.PY_NAMESPACE
656 
657 
658 def is_directory(specobj):
659     return specobj.type == spec.ModuleType.PKG_DIRECTORY
660 
661 
662 def is_module_name_part_of_extension_package_whitelist(
663     module_name: str, package_whitelist: Set[str]
664 ) -> bool:
665     """"""
666     Returns True if one part of the module name is in the package whitelist
667 
668     >>> is_module_name_part_of_extension_package_whitelist('numpy.core.umath', {'numpy'})
669     True
670     """"""
671     parts = module_name.split(""."")
672     return any(
673         ""."".join(parts[:x]) in package_whitelist for x in range(1, len(parts) + 1)
674     )
675 
[end of astroid/modutils.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pylint-dev/astroid,d2a5b3c7b1e203fec3c7ca73c30eb1785d3d4d0a,"astroid 2.9.1 breaks pylint with missing __init__.py: F0010: error while code parsing: Unable to load file __init__.py
### Steps to reproduce
> Steps provided are for Windows 11, but initial problem found in Ubuntu 20.04

> Update 2022-01-04: Corrected repro steps and added more environment details

1. Set up simple repo with following structure (all files can be empty):
```
root_dir/
|--src/
|----project/ # Notice the missing __init__.py
|------file.py # It can be empty, but I added `import os` at the top
|----__init__.py
```
2. Open a command prompt
3. `cd root_dir`
4. `python -m venv venv`
5. `venv/Scripts/activate`
6. `pip install pylint astroid==2.9.1` # I also repro'd on the latest, 2.9.2
7. `pylint src/project` # Updated from `pylint src`
8. Observe failure:
```
src\project\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\project\__init__.py:
```

### Current behavior
Fails with `src\project\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\project\__init__.py:`

### Expected behavior
Does not fail with error.
> If you replace step 6 with `pip install pylint astroid==2.9.0`, you get no failure with an empty output - since no files have content

### `python -c ""from astroid import __pkginfo__; print(__pkginfo__.version)""` output
2.9.1

`python 3.9.1`
`pylint 2.12.2 `



This issue has been observed with astroid `2.9.1` and `2.9.2`
","I can't seem to reproduce this in my `virtualenv`. This might be specific to `venv`? Needs some further investigation.
@interifter Which version of `pylint` are you using?
Right, ``pip install pylint astroid==2.9.0``, will keep the local version if you already have one, so I thought it was ``2.12.2`` but that could be false. In fact it probably isn't 2.12.2. For the record, you're not supposed to set the version of ``astroid`` yourself, pylint does, and bad thing will happen if you try to set the version of an incompatible astroid. We might want to update the issue's template to have this information next.
My apologies... I updated the repro steps with a critical missed detail: `pylint src/project`, instead of `pylint src`

But I verified that either with, or without, `venv`, the issue is reproduced.

Also, I never have specified the `astroid` version, before. 

However, this isn't the first time the issue has been observed.
Back in early 2019, a [similar issue](https://stackoverflow.com/questions/48024049/pylint-raises-error-if-directory-doesnt-contain-init-py-file) was observed with either `astroid 2.2.0` or `isort 4.3.5`, which led me to try pinning `astroid==2.9.0`, which worked.
> @interifter Which version of `pylint` are you using?

`2.12.2`

Full env info:

```
Package           Version
----------------- -------
astroid           2.9.2
colorama          0.4.4
isort             5.10.1
lazy-object-proxy 1.7.1
mccabe            0.6.1
pip               20.2.3
platformdirs      2.4.1
pylint            2.12.2
setuptools        49.2.1
toml              0.10.2
typing-extensions 4.0.1
wrapt             1.13.3
```

I confirm the bug and i'm able to reproduce it with `python 3.9.1`. 
```
$> pip freeze
astroid==2.9.2
isort==5.10.1
lazy-object-proxy==1.7.1
mccabe==0.6.1
platformdirs==2.4.1
pylint==2.12.2
toml==0.10.2
typing-extensions==4.0.1
wrapt==1.13.3
```
Bisected and this is the faulty commit:
https://github.com/PyCQA/astroid/commit/2ee20ccdf62450db611acc4a1a7e42f407ce8a14
Fix in #1333, no time to write tests yet so if somebody has any good ideas: please let me know!",2022-01-08T19:36:45Z,"<patch>
diff --git a/astroid/modutils.py b/astroid/modutils.py
--- a/astroid/modutils.py
+++ b/astroid/modutils.py
@@ -297,6 +297,9 @@ def _get_relative_base_path(filename, path_to_check):
     if os.path.normcase(real_filename).startswith(path_to_check):
         importable_path = real_filename
 
+    # if ""var"" in path_to_check:
+    #     breakpoint()
+
     if importable_path:
         base_path = os.path.splitext(importable_path)[0]
         relative_base_path = base_path[len(path_to_check) :]
@@ -307,8 +310,11 @@ def _get_relative_base_path(filename, path_to_check):
 
 def modpath_from_file_with_callback(filename, path=None, is_package_cb=None):
     filename = os.path.expanduser(_path_from_filename(filename))
+    paths_to_check = sys.path.copy()
+    if path:
+        paths_to_check += path
     for pathname in itertools.chain(
-        path or [], map(_cache_normalize_path, sys.path), sys.path
+        paths_to_check, map(_cache_normalize_path, paths_to_check)
     ):
         if not pathname:
             continue

</patch>","diff --git a/tests/unittest_modutils.py b/tests/unittest_modutils.py
--- a/tests/unittest_modutils.py
+++ b/tests/unittest_modutils.py
@@ -30,6 +30,7 @@
 import tempfile
 import unittest
 import xml
+from pathlib import Path
 from xml import etree
 from xml.etree import ElementTree
 
@@ -189,6 +190,30 @@ def test_load_from_module_symlink_on_symlinked_paths_in_syspath(self) -> None:
         # this should be equivalent to: import secret
         self.assertEqual(modutils.modpath_from_file(symlink_secret_path), [""secret""])
 
+    def test_load_packages_without_init(self) -> None:
+        """"""Test that we correctly find packages with an __init__.py file.
+
+        Regression test for issue reported in:
+        https://github.com/PyCQA/astroid/issues/1327
+        """"""
+        tmp_dir = Path(tempfile.gettempdir())
+        self.addCleanup(os.chdir, os.curdir)
+        os.chdir(tmp_dir)
+
+        self.addCleanup(shutil.rmtree, tmp_dir / ""src"")
+        os.mkdir(tmp_dir / ""src"")
+        os.mkdir(tmp_dir / ""src"" / ""package"")
+        with open(tmp_dir / ""src"" / ""__init__.py"", ""w"", encoding=""utf-8""):
+            pass
+        with open(tmp_dir / ""src"" / ""package"" / ""file.py"", ""w"", encoding=""utf-8""):
+            pass
+
+        # this should be equivalent to: import secret
+        self.assertEqual(
+            modutils.modpath_from_file(str(Path(""src"") / ""package""), ["".""]),
+            [""src"", ""package""],
+        )
+
 
 class LoadModuleFromPathTest(resources.SysPathSetup, unittest.TestCase):
     def test_do_not_load_twice(self) -> None:
",2.10,"[""tests/unittest_modutils.py::ModPathFromFileTest::test_load_packages_without_init""]","[""tests/unittest_modutils.py::ModuleFileTest::test_find_egg_module"", ""tests/unittest_modutils.py::ModuleFileTest::test_find_zipped_module"", ""tests/unittest_modutils.py::LoadModuleFromNameTest::test_known_values_load_module_from_name_1"", ""tests/unittest_modutils.py::LoadModuleFromNameTest::test_known_values_load_module_from_name_2"", ""tests/unittest_modutils.py::LoadModuleFromNameTest::test_raise_load_module_from_name_1"", ""tests/unittest_modutils.py::GetModulePartTest::test_get_module_part_exception"", ""tests/unittest_modutils.py::GetModulePartTest::test_known_values_get_builtin_module_part"", ""tests/unittest_modutils.py::GetModulePartTest::test_known_values_get_compiled_module_part"", ""tests/unittest_modutils.py::GetModulePartTest::test_known_values_get_module_part_1"", ""tests/unittest_modutils.py::GetModulePartTest::test_known_values_get_module_part_2"", ""tests/unittest_modutils.py::GetModulePartTest::test_known_values_get_module_part_3"", ""tests/unittest_modutils.py::ModPathFromFileTest::test_import_symlink_both_outside_of_path"", ""tests/unittest_modutils.py::ModPathFromFileTest::test_import_symlink_with_source_outside_of_path"", ""tests/unittest_modutils.py::ModPathFromFileTest::test_known_values_modpath_from_file_1"", ""tests/unittest_modutils.py::ModPathFromFileTest::test_load_from_module_symlink_on_symlinked_paths_in_syspath"", ""tests/unittest_modutils.py::ModPathFromFileTest::test_raise_modpath_from_file_exception"", ""tests/unittest_modutils.py::LoadModuleFromPathTest::test_do_not_load_twice"", ""tests/unittest_modutils.py::FileFromModPathTest::test_builtin"", ""tests/unittest_modutils.py::FileFromModPathTest::test_site_packages"", ""tests/unittest_modutils.py::FileFromModPathTest::test_std_lib"", ""tests/unittest_modutils.py::FileFromModPathTest::test_unexisting"", ""tests/unittest_modutils.py::FileFromModPathTest::test_unicode_in_package_init"", ""tests/unittest_modutils.py::GetSourceFileTest::test"", ""tests/unittest_modutils.py::GetSourceFileTest::test_raise"", ""tests/unittest_modutils.py::StandardLibModuleTest::test_4"", ""tests/unittest_modutils.py::StandardLibModuleTest::test_builtin"", ""tests/unittest_modutils.py::StandardLibModuleTest::test_builtins"", ""tests/unittest_modutils.py::StandardLibModuleTest::test_custom_path"", ""tests/unittest_modutils.py::StandardLibModuleTest::test_datetime"", ""tests/unittest_modutils.py::StandardLibModuleTest::test_failing_edge_cases"", ""tests/unittest_modutils.py::StandardLibModuleTest::test_nonstandard"", ""tests/unittest_modutils.py::StandardLibModuleTest::test_unknown"", ""tests/unittest_modutils.py::IsRelativeTest::test_deep_relative"", ""tests/unittest_modutils.py::IsRelativeTest::test_deep_relative2"", ""tests/unittest_modutils.py::IsRelativeTest::test_deep_relative3"", ""tests/unittest_modutils.py::IsRelativeTest::test_deep_relative4"", ""tests/unittest_modutils.py::IsRelativeTest::test_is_relative_bad_path"", ""tests/unittest_modutils.py::IsRelativeTest::test_known_values_is_relative_1"", ""tests/unittest_modutils.py::IsRelativeTest::test_known_values_is_relative_3"", ""tests/unittest_modutils.py::IsRelativeTest::test_known_values_is_relative_4"", ""tests/unittest_modutils.py::IsRelativeTest::test_known_values_is_relative_5"", ""tests/unittest_modutils.py::GetModuleFilesTest::test_get_all_files"", ""tests/unittest_modutils.py::GetModuleFilesTest::test_get_module_files_1"", ""tests/unittest_modutils.py::GetModuleFilesTest::test_load_module_set_attribute"", ""tests/unittest_modutils.py::ExtensionPackageWhitelistTest::test_is_module_name_part_of_extension_package_whitelist_success"", ""tests/unittest_modutils.py::ExtensionPackageWhitelistTest::test_is_module_name_part_of_extension_package_whitelist_true""]",da745538c7236028a22cdf0405f6829fcf6886bc
pylint-dev__astroid-1196,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
getitem does not infer the actual unpacked value
When trying to call `Dict.getitem()` on a context where we have a dict unpacking of anything beside a real dict, astroid currently raises an `AttributeError: 'getitem'`, which has 2 problems:

- The object might be a reference against something constant, this pattern is usually seen when we have different sets of dicts that extend each other, and all of their values are inferrable. 
- We can have something that is uninferable, but in that case instead of an `AttributeError` I think it makes sense to raise the usual `AstroidIndexError` which is supposed to be already handled by the downstream.


Here is a short reproducer;

```py
from astroid import parse


source = """"""
X = {
    'A': 'B'
}

Y = {
    **X
}

KEY = 'A'
""""""

tree = parse(source)

first_dict = tree.body[0].value
second_dict = tree.body[1].value
key = tree.body[2].value

print(f'{first_dict.getitem(key).value = }')
print(f'{second_dict.getitem(key).value = }')


```

The current output;

```
 $ python t1.py                                                                                                 3ms
first_dict.getitem(key).value = 'B'
Traceback (most recent call last):
  File ""/home/isidentical/projects/astroid/t1.py"", line 23, in <module>
    print(f'{second_dict.getitem(key).value = }')
  File ""/home/isidentical/projects/astroid/astroid/nodes/node_classes.py"", line 2254, in getitem
    return value.getitem(index, context)
AttributeError: 'Name' object has no attribute 'getitem'
```

Expeceted output;
```
 $ python t1.py                                                                                                 4ms
first_dict.getitem(key).value = 'B'
second_dict.getitem(key).value = 'B'

```


</issue>
<code>
[start of README.rst]
1 Astroid
2 =======
3 
4 .. image:: https://coveralls.io/repos/github/PyCQA/astroid/badge.svg?branch=main
5     :target: https://coveralls.io/github/PyCQA/astroid?branch=main
6     :alt: Coverage badge from coveralls.io
7 
8 .. image:: https://readthedocs.org/projects/astroid/badge/?version=latest
9     :target: http://astroid.readthedocs.io/en/latest/?badge=latest
10     :alt: Documentation Status
11 
12 .. image:: https://img.shields.io/badge/code%20style-black-000000.svg
13     :target: https://github.com/ambv/black
14 
15 .. image:: https://results.pre-commit.ci/badge/github/PyCQA/astroid/main.svg
16    :target: https://results.pre-commit.ci/latest/github/PyCQA/astroid/main
17    :alt: pre-commit.ci status
18 
19 .. |tidelift_logo| image:: https://raw.githubusercontent.com/PyCQA/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png
20    :width: 200
21    :alt: Tidelift
22 
23 .. list-table::
24    :widths: 10 100
25 
26    * - |tidelift_logo|
27      - Professional support for astroid is available as part of the
28        `Tidelift Subscription`_.  Tidelift gives software development teams a single source for
29        purchasing and maintaining their software, with professional grade assurances
30        from the experts who know it best, while seamlessly integrating with existing
31        tools.
32 
33 .. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme
34 
35 
36 
37 What's this?
38 ------------
39 
40 The aim of this module is to provide a common base representation of
41 python source code. It is currently the library powering pylint's capabilities.
42 
43 It provides a compatible representation which comes from the `_ast`
44 module.  It rebuilds the tree generated by the builtin _ast module by
45 recursively walking down the AST and building an extended ast. The new
46 node classes have additional methods and attributes for different
47 usages. They include some support for static inference and local name
48 scopes. Furthermore, astroid can also build partial trees by inspecting living
49 objects.
50 
51 
52 Installation
53 ------------
54 
55 Extract the tarball, jump into the created directory and run::
56 
57     pip install .
58 
59 
60 If you want to do an editable installation, you can run::
61 
62     pip install -e .
63 
64 
65 If you have any questions, please mail the code-quality@python.org
66 mailing list for support. See
67 http://mail.python.org/mailman/listinfo/code-quality for subscription
68 information and archives.
69 
70 Documentation
71 -------------
72 http://astroid.readthedocs.io/en/latest/
73 
74 
75 Python Versions
76 ---------------
77 
78 astroid 2.0 is currently available for Python 3 only. If you want Python 2
79 support, use an older version of astroid (though note that these versions
80 are no longer supported).
81 
82 Test
83 ----
84 
85 Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use
86 either `tox` or `pytest`::
87 
88     tox
89     pytest astroid
90 
[end of README.rst]
[start of astroid/nodes/node_classes.py]
1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html
2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE
3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt
4 
5 """"""Module for some node classes. More nodes in scoped_nodes.py""""""
6 
7 from __future__ import annotations
8 
9 import abc
10 import itertools
11 import sys
12 import typing
13 import warnings
14 from collections.abc import Generator, Iterator
15 from functools import lru_cache
16 from typing import TYPE_CHECKING, Any, Callable, ClassVar, Optional, TypeVar, Union
17 
18 from astroid import decorators, mixins, util
19 from astroid.bases import Instance, _infer_stmts
20 from astroid.const import Context
21 from astroid.context import InferenceContext
22 from astroid.exceptions import (
23     AstroidIndexError,
24     AstroidTypeError,
25     InferenceError,
26     NoDefault,
27     ParentMissingError,
28 )
29 from astroid.manager import AstroidManager
30 from astroid.nodes.const import OP_PRECEDENCE
31 from astroid.nodes.node_ng import NodeNG
32 
33 if sys.version_info >= (3, 8):
34     from typing import Literal
35 else:
36     from typing_extensions import Literal
37 
38 if TYPE_CHECKING:
39     from astroid import nodes
40     from astroid.nodes import LocalsDictNodeNG
41 
42 if sys.version_info >= (3, 8):
43     from functools import cached_property
44 else:
45     from astroid.decorators import cachedproperty as cached_property
46 
47 
48 def _is_const(value):
49     return isinstance(value, tuple(CONST_CLS))
50 
51 
52 _NodesT = TypeVar(""_NodesT"", bound=NodeNG)
53 
54 AssignedStmtsPossibleNode = Union[""List"", ""Tuple"", ""AssignName"", ""AssignAttr"", None]
55 AssignedStmtsCall = Callable[
56     [
57         _NodesT,
58         AssignedStmtsPossibleNode,
59         Optional[InferenceContext],
60         Optional[typing.List[int]],
61     ],
62     Any,
63 ]
64 
65 
66 @decorators.raise_if_nothing_inferred
67 def unpack_infer(stmt, context=None):
68     """"""recursively generate nodes inferred by the given statement.
69     If the inferred value is a list or a tuple, recurse on the elements
70     """"""
71     if isinstance(stmt, (List, Tuple)):
72         for elt in stmt.elts:
73             if elt is util.Uninferable:
74                 yield elt
75                 continue
76             yield from unpack_infer(elt, context)
77         return dict(node=stmt, context=context)
78     # if inferred is a final node, return it and stop
79     inferred = next(stmt.infer(context), util.Uninferable)
80     if inferred is stmt:
81         yield inferred
82         return dict(node=stmt, context=context)
83     # else, infer recursively, except Uninferable object that should be returned as is
84     for inferred in stmt.infer(context):
85         if inferred is util.Uninferable:
86             yield inferred
87         else:
88             yield from unpack_infer(inferred, context)
89 
90     return dict(node=stmt, context=context)
91 
92 
93 def are_exclusive(stmt1, stmt2, exceptions: list[str] | None = None) -> bool:
94     """"""return true if the two given statements are mutually exclusive
95 
96     `exceptions` may be a list of exception names. If specified, discard If
97     branches and check one of the statement is in an exception handler catching
98     one of the given exceptions.
99 
100     algorithm :
101      1) index stmt1's parents
102      2) climb among stmt2's parents until we find a common parent
103      3) if the common parent is a If or TryExcept statement, look if nodes are
104         in exclusive branches
105     """"""
106     # index stmt1's parents
107     stmt1_parents = {}
108     children = {}
109     previous = stmt1
110     for node in stmt1.node_ancestors():
111         stmt1_parents[node] = 1
112         children[node] = previous
113         previous = node
114     # climb among stmt2's parents until we find a common parent
115     previous = stmt2
116     for node in stmt2.node_ancestors():
117         if node in stmt1_parents:
118             # if the common parent is a If or TryExcept statement, look if
119             # nodes are in exclusive branches
120             if isinstance(node, If) and exceptions is None:
121                 if (
122                     node.locate_child(previous)[1]
123                     is not node.locate_child(children[node])[1]
124                 ):
125                     return True
126             elif isinstance(node, TryExcept):
127                 c2attr, c2node = node.locate_child(previous)
128                 c1attr, c1node = node.locate_child(children[node])
129                 if c1node is not c2node:
130                     first_in_body_caught_by_handlers = (
131                         c2attr == ""handlers""
132                         and c1attr == ""body""
133                         and previous.catch(exceptions)
134                     )
135                     second_in_body_caught_by_handlers = (
136                         c2attr == ""body""
137                         and c1attr == ""handlers""
138                         and children[node].catch(exceptions)
139                     )
140                     first_in_else_other_in_handlers = (
141                         c2attr == ""handlers"" and c1attr == ""orelse""
142                     )
143                     second_in_else_other_in_handlers = (
144                         c2attr == ""orelse"" and c1attr == ""handlers""
145                     )
146                     if any(
147                         (
148                             first_in_body_caught_by_handlers,
149                             second_in_body_caught_by_handlers,
150                             first_in_else_other_in_handlers,
151                             second_in_else_other_in_handlers,
152                         )
153                     ):
154                         return True
155                 elif c2attr == ""handlers"" and c1attr == ""handlers"":
156                     return previous is not children[node]
157             return False
158         previous = node
159     return False
160 
161 
162 # getitem() helpers.
163 
164 _SLICE_SENTINEL = object()
165 
166 
167 def _slice_value(index, context=None):
168     """"""Get the value of the given slice index.""""""
169 
170     if isinstance(index, Const):
171         if isinstance(index.value, (int, type(None))):
172             return index.value
173     elif index is None:
174         return None
175     else:
176         # Try to infer what the index actually is.
177         # Since we can't return all the possible values,
178         # we'll stop at the first possible value.
179         try:
180             inferred = next(index.infer(context=context))
181         except (InferenceError, StopIteration):
182             pass
183         else:
184             if isinstance(inferred, Const):
185                 if isinstance(inferred.value, (int, type(None))):
186                     return inferred.value
187 
188     # Use a sentinel, because None can be a valid
189     # value that this function can return,
190     # as it is the case for unspecified bounds.
191     return _SLICE_SENTINEL
192 
193 
194 def _infer_slice(node, context=None):
195     lower = _slice_value(node.lower, context)
196     upper = _slice_value(node.upper, context)
197     step = _slice_value(node.step, context)
198     if all(elem is not _SLICE_SENTINEL for elem in (lower, upper, step)):
199         return slice(lower, upper, step)
200 
201     raise AstroidTypeError(
202         message=""Could not infer slice used in subscript"",
203         node=node,
204         index=node.parent,
205         context=context,
206     )
207 
208 
209 def _container_getitem(instance, elts, index, context=None):
210     """"""Get a slice or an item, using the given *index*, for the given sequence.""""""
211     try:
212         if isinstance(index, Slice):
213             index_slice = _infer_slice(index, context=context)
214             new_cls = instance.__class__()
215             new_cls.elts = elts[index_slice]
216             new_cls.parent = instance.parent
217             return new_cls
218         if isinstance(index, Const):
219             return elts[index.value]
220     except IndexError as exc:
221         raise AstroidIndexError(
222             message=""Index {index!s} out of range"",
223             node=instance,
224             index=index,
225             context=context,
226         ) from exc
227     except TypeError as exc:
228         raise AstroidTypeError(
229             message=""Type error {error!r}"", node=instance, index=index, context=context
230         ) from exc
231 
232     raise AstroidTypeError(f""Could not use {index} as subscript index"")
233 
234 
235 class Statement(NodeNG):
236     """"""Statement node adding a few attributes""""""
237 
238     is_statement = True
239     """"""Whether this node indicates a statement.""""""
240 
241     def next_sibling(self):
242         """"""The next sibling statement node.
243 
244         :returns: The next sibling statement node.
245         :rtype: NodeNG or None
246         """"""
247         stmts = self.parent.child_sequence(self)
248         index = stmts.index(self)
249         try:
250             return stmts[index + 1]
251         except IndexError:
252             return None
253 
254     def previous_sibling(self):
255         """"""The previous sibling statement.
256 
257         :returns: The previous sibling statement node.
258         :rtype: NodeNG or None
259         """"""
260         stmts = self.parent.child_sequence(self)
261         index = stmts.index(self)
262         if index >= 1:
263             return stmts[index - 1]
264         return None
265 
266 
267 class BaseContainer(
268     mixins.ParentAssignTypeMixin, NodeNG, Instance, metaclass=abc.ABCMeta
269 ):
270     """"""Base class for Set, FrozenSet, Tuple and List.""""""
271 
272     _astroid_fields = (""elts"",)
273 
274     def __init__(
275         self,
276         lineno: int | None = None,
277         col_offset: int | None = None,
278         parent: NodeNG | None = None,
279         *,
280         end_lineno: int | None = None,
281         end_col_offset: int | None = None,
282     ) -> None:
283         """"""
284         :param lineno: The line that this node appears on in the source code.
285 
286         :param col_offset: The column that this node appears on in the
287             source code.
288 
289         :param parent: The parent node in the syntax tree.
290 
291         :param end_lineno: The last line this node appears on in the source code.
292 
293         :param end_col_offset: The end column this node appears on in the
294             source code. Note: This is after the last symbol.
295         """"""
296         self.elts: list[NodeNG] = []
297         """"""The elements in the node.""""""
298 
299         super().__init__(
300             lineno=lineno,
301             col_offset=col_offset,
302             end_lineno=end_lineno,
303             end_col_offset=end_col_offset,
304             parent=parent,
305         )
306 
307     def postinit(self, elts: list[NodeNG]) -> None:
308         """"""Do some setup after initialisation.
309 
310         :param elts: The list of elements the that node contains.
311         """"""
312         self.elts = elts
313 
314     @classmethod
315     def from_elements(cls, elts=None):
316         """"""Create a node of this type from the given list of elements.
317 
318         :param elts: The list of elements that the node should contain.
319         :type elts: list(NodeNG)
320 
321         :returns: A new node containing the given elements.
322         :rtype: NodeNG
323         """"""
324         node = cls()
325         if elts is None:
326             node.elts = []
327         else:
328             node.elts = [const_factory(e) if _is_const(e) else e for e in elts]
329         return node
330 
331     def itered(self):
332         """"""An iterator over the elements this node contains.
333 
334         :returns: The contents of this node.
335         :rtype: iterable(NodeNG)
336         """"""
337         return self.elts
338 
339     def bool_value(self, context=None):
340         """"""Determine the boolean value of this node.
341 
342         :returns: The boolean value of this node.
343         :rtype: bool or Uninferable
344         """"""
345         return bool(self.elts)
346 
347     @abc.abstractmethod
348     def pytype(self):
349         """"""Get the name of the type that this node represents.
350 
351         :returns: The name of the type.
352         :rtype: str
353         """"""
354 
355     def get_children(self):
356         yield from self.elts
357 
358 
359 class LookupMixIn:
360     """"""Mixin to look up a name in the right scope.""""""
361 
362     @lru_cache()  # noqa
363     def lookup(self, name: str) -> tuple[str, list[NodeNG]]:
364         """"""Lookup where the given variable is assigned.
365 
366         The lookup starts from self's scope. If self is not a frame itself
367         and the name is found in the inner frame locals, statements will be
368         filtered to remove ignorable statements according to self's location.
369 
370         :param name: The name of the variable to find assignments for.
371 
372         :returns: The scope node and the list of assignments associated to the
373             given name according to the scope where it has been found (locals,
374             globals or builtin).
375         """"""
376         return self.scope().scope_lookup(self, name)
377 
378     def ilookup(self, name):
379         """"""Lookup the inferred values of the given variable.
380 
381         :param name: The variable name to find values for.
382         :type name: str
383 
384         :returns: The inferred values of the statements returned from
385             :meth:`lookup`.
386         :rtype: iterable
387         """"""
388         frame, stmts = self.lookup(name)
389         context = InferenceContext()
390         return _infer_stmts(stmts, context, frame)
391 
392 
393 # Name classes
394 
395 
396 class AssignName(
397     mixins.NoChildrenMixin, LookupMixIn, mixins.ParentAssignTypeMixin, NodeNG
398 ):
399     """"""Variation of :class:`ast.Assign` representing assignment to a name.
400 
401     An :class:`AssignName` is the name of something that is assigned to.
402     This includes variables defined in a function signature or in a loop.
403 
404     >>> import astroid
405     >>> node = astroid.extract_node('variable = range(10)')
406     >>> node
407     <Assign l.1 at 0x7effe1db8550>
408     >>> list(node.get_children())
409     [<AssignName.variable l.1 at 0x7effe1db8748>, <Call l.1 at 0x7effe1db8630>]
410     >>> list(node.get_children())[0].as_string()
411     'variable'
412     """"""
413 
414     _other_fields = (""name"",)
415 
416     @decorators.deprecate_default_argument_values(name=""str"")
417     def __init__(
418         self,
419         name: str | None = None,
420         lineno: int | None = None,
421         col_offset: int | None = None,
422         parent: NodeNG | None = None,
423         *,
424         end_lineno: int | None = None,
425         end_col_offset: int | None = None,
426     ) -> None:
427         """"""
428         :param name: The name that is assigned to.
429 
430         :param lineno: The line that this node appears on in the source code.
431 
432         :param col_offset: The column that this node appears on in the
433             source code.
434 
435         :param parent: The parent node in the syntax tree.
436 
437         :param end_lineno: The last line this node appears on in the source code.
438 
439         :param end_col_offset: The end column this node appears on in the
440             source code. Note: This is after the last symbol.
441         """"""
442         self.name: str | None = name
443         """"""The name that is assigned to.""""""
444 
445         super().__init__(
446             lineno=lineno,
447             col_offset=col_offset,
448             end_lineno=end_lineno,
449             end_col_offset=end_col_offset,
450             parent=parent,
451         )
452 
453     assigned_stmts: ClassVar[AssignedStmtsCall[AssignName]]
454     """"""Returns the assigned statement (non inferred) according to the assignment type.
455     See astroid/protocols.py for actual implementation.
456     """"""
457 
458 
459 class DelName(
460     mixins.NoChildrenMixin, LookupMixIn, mixins.ParentAssignTypeMixin, NodeNG
461 ):
462     """"""Variation of :class:`ast.Delete` representing deletion of a name.
463 
464     A :class:`DelName` is the name of something that is deleted.
465 
466     >>> import astroid
467     >>> node = astroid.extract_node(""del variable #@"")
468     >>> list(node.get_children())
469     [<DelName.variable l.1 at 0x7effe1da4d30>]
470     >>> list(node.get_children())[0].as_string()
471     'variable'
472     """"""
473 
474     _other_fields = (""name"",)
475 
476     @decorators.deprecate_default_argument_values(name=""str"")
477     def __init__(
478         self,
479         name: str | None = None,
480         lineno: int | None = None,
481         col_offset: int | None = None,
482         parent: NodeNG | None = None,
483         *,
484         end_lineno: int | None = None,
485         end_col_offset: int | None = None,
486     ) -> None:
487         """"""
488         :param name: The name that is being deleted.
489 
490         :param lineno: The line that this node appears on in the source code.
491 
492         :param col_offset: The column that this node appears on in the
493             source code.
494 
495         :param parent: The parent node in the syntax tree.
496 
497         :param end_lineno: The last line this node appears on in the source code.
498 
499         :param end_col_offset: The end column this node appears on in the
500             source code. Note: This is after the last symbol.
501         """"""
502         self.name: str | None = name
503         """"""The name that is being deleted.""""""
504 
505         super().__init__(
506             lineno=lineno,
507             col_offset=col_offset,
508             end_lineno=end_lineno,
509             end_col_offset=end_col_offset,
510             parent=parent,
511         )
512 
513 
514 class Name(mixins.NoChildrenMixin, LookupMixIn, NodeNG):
515     """"""Class representing an :class:`ast.Name` node.
516 
517     A :class:`Name` node is something that is named, but not covered by
518     :class:`AssignName` or :class:`DelName`.
519 
520     >>> import astroid
521     >>> node = astroid.extract_node('range(10)')
522     >>> node
523     <Call l.1 at 0x7effe1db8710>
524     >>> list(node.get_children())
525     [<Name.range l.1 at 0x7effe1db86a0>, <Const.int l.1 at 0x7effe1db8518>]
526     >>> list(node.get_children())[0].as_string()
527     'range'
528     """"""
529 
530     _other_fields = (""name"",)
531 
532     @decorators.deprecate_default_argument_values(name=""str"")
533     def __init__(
534         self,
535         name: str | None = None,
536         lineno: int | None = None,
537         col_offset: int | None = None,
538         parent: NodeNG | None = None,
539         *,
540         end_lineno: int | None = None,
541         end_col_offset: int | None = None,
542     ) -> None:
543         """"""
544         :param name: The name that this node refers to.
545 
546         :param lineno: The line that this node appears on in the source code.
547 
548         :param col_offset: The column that this node appears on in the
549             source code.
550 
551         :param parent: The parent node in the syntax tree.
552 
553         :param end_lineno: The last line this node appears on in the source code.
554 
555         :param end_col_offset: The end column this node appears on in the
556             source code. Note: This is after the last symbol.
557         """"""
558         self.name: str | None = name
559         """"""The name that this node refers to.""""""
560 
561         super().__init__(
562             lineno=lineno,
563             col_offset=col_offset,
564             end_lineno=end_lineno,
565             end_col_offset=end_col_offset,
566             parent=parent,
567         )
568 
569     def _get_name_nodes(self):
570         yield self
571 
572         for child_node in self.get_children():
573             yield from child_node._get_name_nodes()
574 
575 
576 class Arguments(mixins.AssignTypeMixin, NodeNG):
577     """"""Class representing an :class:`ast.arguments` node.
578 
579     An :class:`Arguments` node represents that arguments in a
580     function definition.
581 
582     >>> import astroid
583     >>> node = astroid.extract_node('def foo(bar): pass')
584     >>> node
585     <FunctionDef.foo l.1 at 0x7effe1db8198>
586     >>> node.args
587     <Arguments l.1 at 0x7effe1db82e8>
588     """"""
589 
590     # Python 3.4+ uses a different approach regarding annotations,
591     # each argument is a new class, _ast.arg, which exposes an
592     # 'annotation' attribute. In astroid though, arguments are exposed
593     # as is in the Arguments node and the only way to expose annotations
594     # is by using something similar with Python 3.3:
595     #  - we expose 'varargannotation' and 'kwargannotation' of annotations
596     #    of varargs and kwargs.
597     #  - we expose 'annotation', a list with annotations for
598     #    for each normal argument. If an argument doesn't have an
599     #    annotation, its value will be None.
600     _astroid_fields = (
601         ""args"",
602         ""defaults"",
603         ""kwonlyargs"",
604         ""posonlyargs"",
605         ""posonlyargs_annotations"",
606         ""kw_defaults"",
607         ""annotations"",
608         ""varargannotation"",
609         ""kwargannotation"",
610         ""kwonlyargs_annotations"",
611         ""type_comment_args"",
612         ""type_comment_kwonlyargs"",
613         ""type_comment_posonlyargs"",
614     )
615 
616     _other_fields = (""vararg"", ""kwarg"")
617 
618     lineno: None
619     col_offset: None
620     end_lineno: None
621     end_col_offset: None
622 
623     def __init__(
624         self,
625         vararg: str | None = None,
626         kwarg: str | None = None,
627         parent: NodeNG | None = None,
628     ) -> None:
629         """"""
630         :param vararg: The name of the variable length arguments.
631 
632         :param kwarg: The name of the variable length keyword arguments.
633 
634         :param parent: The parent node in the syntax tree.
635         """"""
636         super().__init__(parent=parent)
637 
638         self.vararg: str | None = vararg  # can be None
639         """"""The name of the variable length arguments.""""""
640 
641         self.kwarg: str | None = kwarg  # can be None
642         """"""The name of the variable length keyword arguments.""""""
643 
644         self.args: list[AssignName] | None
645         """"""The names of the required arguments.
646 
647         Can be None if the associated function does not have a retrievable
648         signature and the arguments are therefore unknown.
649         This can happen with (builtin) functions implemented in C that have
650         incomplete signature information.
651         """"""
652         # TODO: Check if other attributes should also be None when
653         # .args is None.
654 
655         self.defaults: list[NodeNG]
656         """"""The default values for arguments that can be passed positionally.""""""
657 
658         self.kwonlyargs: list[AssignName]
659         """"""The keyword arguments that cannot be passed positionally.""""""
660 
661         self.posonlyargs: list[AssignName] = []
662         """"""The arguments that can only be passed positionally.""""""
663 
664         self.kw_defaults: list[NodeNG | None]
665         """"""The default values for keyword arguments that cannot be passed positionally.""""""
666 
667         self.annotations: list[NodeNG | None]
668         """"""The type annotations of arguments that can be passed positionally.""""""
669 
670         self.posonlyargs_annotations: list[NodeNG | None] = []
671         """"""The type annotations of arguments that can only be passed positionally.""""""
672 
673         self.kwonlyargs_annotations: list[NodeNG | None] = []
674         """"""The type annotations of arguments that cannot be passed positionally.""""""
675 
676         self.type_comment_args: list[NodeNG | None] = []
677         """"""The type annotation, passed by a type comment, of each argument.
678 
679         If an argument does not have a type comment,
680         the value for that argument will be None.
681         """"""
682 
683         self.type_comment_kwonlyargs: list[NodeNG | None] = []
684         """"""The type annotation, passed by a type comment, of each keyword only argument.
685 
686         If an argument does not have a type comment,
687         the value for that argument will be None.
688         """"""
689 
690         self.type_comment_posonlyargs: list[NodeNG | None] = []
691         """"""The type annotation, passed by a type comment, of each positional argument.
692 
693         If an argument does not have a type comment,
694         the value for that argument will be None.
695         """"""
696 
697         self.varargannotation: NodeNG | None = None  # can be None
698         """"""The type annotation for the variable length arguments.""""""
699 
700         self.kwargannotation: NodeNG | None = None  # can be None
701         """"""The type annotation for the variable length keyword arguments.""""""
702 
703     # pylint: disable=too-many-arguments
704     def postinit(
705         self,
706         args: list[AssignName] | None,
707         defaults: list[NodeNG],
708         kwonlyargs: list[AssignName],
709         kw_defaults: list[NodeNG | None],
710         annotations: list[NodeNG | None],
711         posonlyargs: list[AssignName] | None = None,
712         kwonlyargs_annotations: list[NodeNG | None] | None = None,
713         posonlyargs_annotations: list[NodeNG | None] | None = None,
714         varargannotation: NodeNG | None = None,
715         kwargannotation: NodeNG | None = None,
716         type_comment_args: list[NodeNG | None] | None = None,
717         type_comment_kwonlyargs: list[NodeNG | None] | None = None,
718         type_comment_posonlyargs: list[NodeNG | None] | None = None,
719     ) -> None:
720         """"""Do some setup after initialisation.
721 
722         :param args: The names of the required arguments.
723 
724         :param defaults: The default values for arguments that can be passed
725             positionally.
726 
727         :param kwonlyargs: The keyword arguments that cannot be passed
728             positionally.
729 
730         :param posonlyargs: The arguments that can only be passed
731             positionally.
732 
733         :param kw_defaults: The default values for keyword arguments that
734             cannot be passed positionally.
735 
736         :param annotations: The type annotations of arguments that can be
737             passed positionally.
738 
739         :param kwonlyargs_annotations: The type annotations of arguments that
740             cannot be passed positionally. This should always be passed in
741             Python 3.
742 
743         :param posonlyargs_annotations: The type annotations of arguments that
744             can only be passed positionally. This should always be passed in
745             Python 3.
746 
747         :param varargannotation: The type annotation for the variable length
748             arguments.
749 
750         :param kwargannotation: The type annotation for the variable length
751             keyword arguments.
752 
753         :param type_comment_args: The type annotation,
754             passed by a type comment, of each argument.
755 
756         :param type_comment_args: The type annotation,
757             passed by a type comment, of each keyword only argument.
758 
759         :param type_comment_args: The type annotation,
760             passed by a type comment, of each positional argument.
761         """"""
762         self.args = args
763         self.defaults = defaults
764         self.kwonlyargs = kwonlyargs
765         if posonlyargs is not None:
766             self.posonlyargs = posonlyargs
767         self.kw_defaults = kw_defaults
768         self.annotations = annotations
769         if kwonlyargs_annotations is not None:
770             self.kwonlyargs_annotations = kwonlyargs_annotations
771         if posonlyargs_annotations is not None:
772             self.posonlyargs_annotations = posonlyargs_annotations
773         self.varargannotation = varargannotation
774         self.kwargannotation = kwargannotation
775         if type_comment_args is not None:
776             self.type_comment_args = type_comment_args
777         if type_comment_kwonlyargs is not None:
778             self.type_comment_kwonlyargs = type_comment_kwonlyargs
779         if type_comment_posonlyargs is not None:
780             self.type_comment_posonlyargs = type_comment_posonlyargs
781 
782     assigned_stmts: ClassVar[AssignedStmtsCall[Arguments]]
783     """"""Returns the assigned statement (non inferred) according to the assignment type.
784     See astroid/protocols.py for actual implementation.
785     """"""
786 
787     def _infer_name(self, frame, name):
788         if self.parent is frame:
789             return name
790         return None
791 
792     @cached_property
793     def fromlineno(self):
794         """"""The first line that this node appears on in the source code.
795 
796         :type: int or None
797         """"""
798         lineno = super().fromlineno
799         return max(lineno, self.parent.fromlineno or 0)
800 
801     @cached_property
802     def arguments(self):
803         """"""Get all the arguments for this node, including positional only and positional and keyword""""""
804         return list(itertools.chain((self.posonlyargs or ()), self.args or ()))
805 
806     def format_args(self):
807         """"""Get the arguments formatted as string.
808 
809         :returns: The formatted arguments.
810         :rtype: str
811         """"""
812         result = []
813         positional_only_defaults = []
814         positional_or_keyword_defaults = self.defaults
815         if self.defaults:
816             args = self.args or []
817             positional_or_keyword_defaults = self.defaults[-len(args) :]
818             positional_only_defaults = self.defaults[: len(self.defaults) - len(args)]
819 
820         if self.posonlyargs:
821             result.append(
822                 _format_args(
823                     self.posonlyargs,
824                     positional_only_defaults,
825                     self.posonlyargs_annotations,
826                 )
827             )
828             result.append(""/"")
829         if self.args:
830             result.append(
831                 _format_args(
832                     self.args,
833                     positional_or_keyword_defaults,
834                     getattr(self, ""annotations"", None),
835                 )
836             )
837         if self.vararg:
838             result.append(f""*{self.vararg}"")
839         if self.kwonlyargs:
840             if not self.vararg:
841                 result.append(""*"")
842             result.append(
843                 _format_args(
844                     self.kwonlyargs, self.kw_defaults, self.kwonlyargs_annotations
845                 )
846             )
847         if self.kwarg:
848             result.append(f""**{self.kwarg}"")
849         return "", "".join(result)
850 
851     def default_value(self, argname):
852         """"""Get the default value for an argument.
853 
854         :param argname: The name of the argument to get the default value for.
855         :type argname: str
856 
857         :raises NoDefault: If there is no default value defined for the
858             given argument.
859         """"""
860         args = self.arguments
861         index = _find_arg(argname, args)[0]
862         if index is not None:
863             idx = index - (len(args) - len(self.defaults))
864             if idx >= 0:
865                 return self.defaults[idx]
866         index = _find_arg(argname, self.kwonlyargs)[0]
867         if index is not None and self.kw_defaults[index] is not None:
868             return self.kw_defaults[index]
869         raise NoDefault(func=self.parent, name=argname)
870 
871     def is_argument(self, name):
872         """"""Check if the given name is defined in the arguments.
873 
874         :param name: The name to check for.
875         :type name: str
876 
877         :returns: True if the given name is defined in the arguments,
878             False otherwise.
879         :rtype: bool
880         """"""
881         if name == self.vararg:
882             return True
883         if name == self.kwarg:
884             return True
885         return (
886             self.find_argname(name, rec=True)[1] is not None
887             or self.kwonlyargs
888             and _find_arg(name, self.kwonlyargs, rec=True)[1] is not None
889         )
890 
891     def find_argname(self, argname, rec=False):
892         """"""Get the index and :class:`AssignName` node for given name.
893 
894         :param argname: The name of the argument to search for.
895         :type argname: str
896 
897         :param rec: Whether or not to include arguments in unpacked tuples
898             in the search.
899         :type rec: bool
900 
901         :returns: The index and node for the argument.
902         :rtype: tuple(str or None, AssignName or None)
903         """"""
904         if self.arguments:
905             return _find_arg(argname, self.arguments, rec)
906         return None, None
907 
908     def get_children(self):
909         yield from self.posonlyargs or ()
910 
911         for elt in self.posonlyargs_annotations:
912             if elt is not None:
913                 yield elt
914 
915         yield from self.args or ()
916 
917         yield from self.defaults
918         yield from self.kwonlyargs
919 
920         for elt in self.kw_defaults:
921             if elt is not None:
922                 yield elt
923 
924         for elt in self.annotations:
925             if elt is not None:
926                 yield elt
927 
928         if self.varargannotation is not None:
929             yield self.varargannotation
930 
931         if self.kwargannotation is not None:
932             yield self.kwargannotation
933 
934         for elt in self.kwonlyargs_annotations:
935             if elt is not None:
936                 yield elt
937 
938 
939 def _find_arg(argname, args, rec=False):
940     for i, arg in enumerate(args):
941         if isinstance(arg, Tuple):
942             if rec:
943                 found = _find_arg(argname, arg.elts)
944                 if found[0] is not None:
945                     return found
946         elif arg.name == argname:
947             return i, arg
948     return None, None
949 
950 
951 def _format_args(args, defaults=None, annotations=None):
952     values = []
953     if args is None:
954         return """"
955     if annotations is None:
956         annotations = []
957     if defaults is not None:
958         default_offset = len(args) - len(defaults)
959     packed = itertools.zip_longest(args, annotations)
960     for i, (arg, annotation) in enumerate(packed):
961         if isinstance(arg, Tuple):
962             values.append(f""({_format_args(arg.elts)})"")
963         else:
964             argname = arg.name
965             default_sep = ""=""
966             if annotation is not None:
967                 argname += "": "" + annotation.as_string()
968                 default_sep = "" = ""
969             values.append(argname)
970 
971             if defaults is not None and i >= default_offset:
972                 if defaults[i - default_offset] is not None:
973                     values[-1] += default_sep + defaults[i - default_offset].as_string()
974     return "", "".join(values)
975 
976 
977 class AssignAttr(mixins.ParentAssignTypeMixin, NodeNG):
978     """"""Variation of :class:`ast.Assign` representing assignment to an attribute.
979 
980     >>> import astroid
981     >>> node = astroid.extract_node('self.attribute = range(10)')
982     >>> node
983     <Assign l.1 at 0x7effe1d521d0>
984     >>> list(node.get_children())
985     [<AssignAttr.attribute l.1 at 0x7effe1d52320>, <Call l.1 at 0x7effe1d522e8>]
986     >>> list(node.get_children())[0].as_string()
987     'self.attribute'
988     """"""
989 
990     _astroid_fields = (""expr"",)
991     _other_fields = (""attrname"",)
992 
993     @decorators.deprecate_default_argument_values(attrname=""str"")
994     def __init__(
995         self,
996         attrname: str | None = None,
997         lineno: int | None = None,
998         col_offset: int | None = None,
999         parent: NodeNG | None = None,
1000         *,
1001         end_lineno: int | None = None,
1002         end_col_offset: int | None = None,
1003     ) -> None:
1004         """"""
1005         :param attrname: The name of the attribute being assigned to.
1006 
1007         :param lineno: The line that this node appears on in the source code.
1008 
1009         :param col_offset: The column that this node appears on in the
1010             source code.
1011 
1012         :param parent: The parent node in the syntax tree.
1013 
1014         :param end_lineno: The last line this node appears on in the source code.
1015 
1016         :param end_col_offset: The end column this node appears on in the
1017             source code. Note: This is after the last symbol.
1018         """"""
1019         self.expr: NodeNG | None = None
1020         """"""What has the attribute that is being assigned to.""""""
1021 
1022         self.attrname: str | None = attrname
1023         """"""The name of the attribute being assigned to.""""""
1024 
1025         super().__init__(
1026             lineno=lineno,
1027             col_offset=col_offset,
1028             end_lineno=end_lineno,
1029             end_col_offset=end_col_offset,
1030             parent=parent,
1031         )
1032 
1033     def postinit(self, expr: NodeNG | None = None) -> None:
1034         """"""Do some setup after initialisation.
1035 
1036         :param expr: What has the attribute that is being assigned to.
1037         """"""
1038         self.expr = expr
1039 
1040     assigned_stmts: ClassVar[AssignedStmtsCall[AssignAttr]]
1041     """"""Returns the assigned statement (non inferred) according to the assignment type.
1042     See astroid/protocols.py for actual implementation.
1043     """"""
1044 
1045     def get_children(self):
1046         yield self.expr
1047 
1048 
1049 class Assert(Statement):
1050     """"""Class representing an :class:`ast.Assert` node.
1051 
1052     An :class:`Assert` node represents an assert statement.
1053 
1054     >>> import astroid
1055     >>> node = astroid.extract_node('assert len(things) == 10, ""Not enough things""')
1056     >>> node
1057     <Assert l.1 at 0x7effe1d527b8>
1058     """"""
1059 
1060     _astroid_fields = (""test"", ""fail"")
1061 
1062     def __init__(
1063         self,
1064         lineno: int | None = None,
1065         col_offset: int | None = None,
1066         parent: NodeNG | None = None,
1067         *,
1068         end_lineno: int | None = None,
1069         end_col_offset: int | None = None,
1070     ) -> None:
1071         """"""
1072         :param lineno: The line that this node appears on in the source code.
1073 
1074         :param col_offset: The column that this node appears on in the
1075             source code.
1076 
1077         :param parent: The parent node in the syntax tree.
1078 
1079         :param end_lineno: The last line this node appears on in the source code.
1080 
1081         :param end_col_offset: The end column this node appears on in the
1082             source code. Note: This is after the last symbol.
1083         """"""
1084         self.test: NodeNG | None = None
1085         """"""The test that passes or fails the assertion.""""""
1086 
1087         self.fail: NodeNG | None = None  # can be None
1088         """"""The message shown when the assertion fails.""""""
1089 
1090         super().__init__(
1091             lineno=lineno,
1092             col_offset=col_offset,
1093             end_lineno=end_lineno,
1094             end_col_offset=end_col_offset,
1095             parent=parent,
1096         )
1097 
1098     def postinit(self, test: NodeNG | None = None, fail: NodeNG | None = None) -> None:
1099         """"""Do some setup after initialisation.
1100 
1101         :param test: The test that passes or fails the assertion.
1102 
1103         :param fail: The message shown when the assertion fails.
1104         """"""
1105         self.fail = fail
1106         self.test = test
1107 
1108     def get_children(self):
1109         yield self.test
1110 
1111         if self.fail is not None:
1112             yield self.fail
1113 
1114 
1115 class Assign(mixins.AssignTypeMixin, Statement):
1116     """"""Class representing an :class:`ast.Assign` node.
1117 
1118     An :class:`Assign` is a statement where something is explicitly
1119     asssigned to.
1120 
1121     >>> import astroid
1122     >>> node = astroid.extract_node('variable = range(10)')
1123     >>> node
1124     <Assign l.1 at 0x7effe1db8550>
1125     """"""
1126 
1127     _astroid_fields = (""targets"", ""value"")
1128     _other_other_fields = (""type_annotation"",)
1129 
1130     def __init__(
1131         self,
1132         lineno: int | None = None,
1133         col_offset: int | None = None,
1134         parent: NodeNG | None = None,
1135         *,
1136         end_lineno: int | None = None,
1137         end_col_offset: int | None = None,
1138     ) -> None:
1139         """"""
1140         :param lineno: The line that this node appears on in the source code.
1141 
1142         :param col_offset: The column that this node appears on in the
1143             source code.
1144 
1145         :param parent: The parent node in the syntax tree.
1146 
1147         :param end_lineno: The last line this node appears on in the source code.
1148 
1149         :param end_col_offset: The end column this node appears on in the
1150             source code. Note: This is after the last symbol.
1151         """"""
1152         self.targets: list[NodeNG] = []
1153         """"""What is being assigned to.""""""
1154 
1155         self.value: NodeNG | None = None
1156         """"""The value being assigned to the variables.""""""
1157 
1158         self.type_annotation: NodeNG | None = None  # can be None
1159         """"""If present, this will contain the type annotation passed by a type comment""""""
1160 
1161         super().__init__(
1162             lineno=lineno,
1163             col_offset=col_offset,
1164             end_lineno=end_lineno,
1165             end_col_offset=end_col_offset,
1166             parent=parent,
1167         )
1168 
1169     def postinit(
1170         self,
1171         targets: list[NodeNG] | None = None,
1172         value: NodeNG | None = None,
1173         type_annotation: NodeNG | None = None,
1174     ) -> None:
1175         """"""Do some setup after initialisation.
1176 
1177         :param targets: What is being assigned to.
1178         :param value: The value being assigned to the variables.
1179         :param type_annotation:
1180         """"""
1181         if targets is not None:
1182             self.targets = targets
1183         self.value = value
1184         self.type_annotation = type_annotation
1185 
1186     assigned_stmts: ClassVar[AssignedStmtsCall[Assign]]
1187     """"""Returns the assigned statement (non inferred) according to the assignment type.
1188     See astroid/protocols.py for actual implementation.
1189     """"""
1190 
1191     def get_children(self):
1192         yield from self.targets
1193 
1194         yield self.value
1195 
1196     @decorators.cached
1197     def _get_assign_nodes(self):
1198         return [self] + list(self.value._get_assign_nodes())
1199 
1200     def _get_yield_nodes_skip_lambdas(self):
1201         yield from self.value._get_yield_nodes_skip_lambdas()
1202 
1203 
1204 class AnnAssign(mixins.AssignTypeMixin, Statement):
1205     """"""Class representing an :class:`ast.AnnAssign` node.
1206 
1207     An :class:`AnnAssign` is an assignment with a type annotation.
1208 
1209     >>> import astroid
1210     >>> node = astroid.extract_node('variable: List[int] = range(10)')
1211     >>> node
1212     <AnnAssign l.1 at 0x7effe1d4c630>
1213     """"""
1214 
1215     _astroid_fields = (""target"", ""annotation"", ""value"")
1216     _other_fields = (""simple"",)
1217 
1218     def __init__(
1219         self,
1220         lineno: int | None = None,
1221         col_offset: int | None = None,
1222         parent: NodeNG | None = None,
1223         *,
1224         end_lineno: int | None = None,
1225         end_col_offset: int | None = None,
1226     ) -> None:
1227         """"""
1228         :param lineno: The line that this node appears on in the source code.
1229 
1230         :param col_offset: The column that this node appears on in the
1231             source code.
1232 
1233         :param parent: The parent node in the syntax tree.
1234 
1235         :param end_lineno: The last line this node appears on in the source code.
1236 
1237         :param end_col_offset: The end column this node appears on in the
1238             source code. Note: This is after the last symbol.
1239         """"""
1240         self.target: NodeNG | None = None
1241         """"""What is being assigned to.""""""
1242 
1243         self.annotation: NodeNG | None = None
1244         """"""The type annotation of what is being assigned to.""""""
1245 
1246         self.value: NodeNG | None = None  # can be None
1247         """"""The value being assigned to the variables.""""""
1248 
1249         self.simple: int | None = None
1250         """"""Whether :attr:`target` is a pure name or a complex statement.""""""
1251 
1252         super().__init__(
1253             lineno=lineno,
1254             col_offset=col_offset,
1255             end_lineno=end_lineno,
1256             end_col_offset=end_col_offset,
1257             parent=parent,
1258         )
1259 
1260     def postinit(
1261         self,
1262         target: NodeNG,
1263         annotation: NodeNG,
1264         simple: int,
1265         value: NodeNG | None = None,
1266     ) -> None:
1267         """"""Do some setup after initialisation.
1268 
1269         :param target: What is being assigned to.
1270 
1271         :param annotation: The type annotation of what is being assigned to.
1272 
1273         :param simple: Whether :attr:`target` is a pure name
1274             or a complex statement.
1275 
1276         :param value: The value being assigned to the variables.
1277         """"""
1278         self.target = target
1279         self.annotation = annotation
1280         self.value = value
1281         self.simple = simple
1282 
1283     assigned_stmts: ClassVar[AssignedStmtsCall[AnnAssign]]
1284     """"""Returns the assigned statement (non inferred) according to the assignment type.
1285     See astroid/protocols.py for actual implementation.
1286     """"""
1287 
1288     def get_children(self):
1289         yield self.target
1290         yield self.annotation
1291 
1292         if self.value is not None:
1293             yield self.value
1294 
1295 
1296 class AugAssign(mixins.AssignTypeMixin, Statement):
1297     """"""Class representing an :class:`ast.AugAssign` node.
1298 
1299     An :class:`AugAssign` is an assignment paired with an operator.
1300 
1301     >>> import astroid
1302     >>> node = astroid.extract_node('variable += 1')
1303     >>> node
1304     <AugAssign l.1 at 0x7effe1db4d68>
1305     """"""
1306 
1307     _astroid_fields = (""target"", ""value"")
1308     _other_fields = (""op"",)
1309 
1310     @decorators.deprecate_default_argument_values(op=""str"")
1311     def __init__(
1312         self,
1313         op: str | None = None,
1314         lineno: int | None = None,
1315         col_offset: int | None = None,
1316         parent: NodeNG | None = None,
1317         *,
1318         end_lineno: int | None = None,
1319         end_col_offset: int | None = None,
1320     ) -> None:
1321         """"""
1322         :param op: The operator that is being combined with the assignment.
1323             This includes the equals sign.
1324 
1325         :param lineno: The line that this node appears on in the source code.
1326 
1327         :param col_offset: The column that this node appears on in the
1328             source code.
1329 
1330         :param parent: The parent node in the syntax tree.
1331 
1332         :param end_lineno: The last line this node appears on in the source code.
1333 
1334         :param end_col_offset: The end column this node appears on in the
1335             source code. Note: This is after the last symbol.
1336         """"""
1337         self.target: NodeNG | None = None
1338         """"""What is being assigned to.""""""
1339 
1340         self.op: str | None = op
1341         """"""The operator that is being combined with the assignment.
1342 
1343         This includes the equals sign.
1344         """"""
1345 
1346         self.value: NodeNG | None = None
1347         """"""The value being assigned to the variable.""""""
1348 
1349         super().__init__(
1350             lineno=lineno,
1351             col_offset=col_offset,
1352             end_lineno=end_lineno,
1353             end_col_offset=end_col_offset,
1354             parent=parent,
1355         )
1356 
1357     def postinit(
1358         self, target: NodeNG | None = None, value: NodeNG | None = None
1359     ) -> None:
1360         """"""Do some setup after initialisation.
1361 
1362         :param target: What is being assigned to.
1363 
1364         :param value: The value being assigned to the variable.
1365         """"""
1366         self.target = target
1367         self.value = value
1368 
1369     assigned_stmts: ClassVar[AssignedStmtsCall[AugAssign]]
1370     """"""Returns the assigned statement (non inferred) according to the assignment type.
1371     See astroid/protocols.py for actual implementation.
1372     """"""
1373 
1374     # This is set by inference.py
1375     def _infer_augassign(self, context=None):
1376         raise NotImplementedError
1377 
1378     def type_errors(self, context=None):
1379         """"""Get a list of type errors which can occur during inference.
1380 
1381         Each TypeError is represented by a :class:`BadBinaryOperationMessage` ,
1382         which holds the original exception.
1383 
1384         :returns: The list of possible type errors.
1385         :rtype: list(BadBinaryOperationMessage)
1386         """"""
1387         try:
1388             results = self._infer_augassign(context=context)
1389             return [
1390                 result
1391                 for result in results
1392                 if isinstance(result, util.BadBinaryOperationMessage)
1393             ]
1394         except InferenceError:
1395             return []
1396 
1397     def get_children(self):
1398         yield self.target
1399         yield self.value
1400 
1401     def _get_yield_nodes_skip_lambdas(self):
1402         """"""An AugAssign node can contain a Yield node in the value""""""
1403         yield from self.value._get_yield_nodes_skip_lambdas()
1404         yield from super()._get_yield_nodes_skip_lambdas()
1405 
1406 
1407 class BinOp(NodeNG):
1408     """"""Class representing an :class:`ast.BinOp` node.
1409 
1410     A :class:`BinOp` node is an application of a binary operator.
1411 
1412     >>> import astroid
1413     >>> node = astroid.extract_node('a + b')
1414     >>> node
1415     <BinOp l.1 at 0x7f23b2e8cfd0>
1416     """"""
1417 
1418     _astroid_fields = (""left"", ""right"")
1419     _other_fields = (""op"",)
1420 
1421     @decorators.deprecate_default_argument_values(op=""str"")
1422     def __init__(
1423         self,
1424         op: str | None = None,
1425         lineno: int | None = None,
1426         col_offset: int | None = None,
1427         parent: NodeNG | None = None,
1428         *,
1429         end_lineno: int | None = None,
1430         end_col_offset: int | None = None,
1431     ) -> None:
1432         """"""
1433         :param op: The operator.
1434 
1435         :param lineno: The line that this node appears on in the source code.
1436 
1437         :param col_offset: The column that this node appears on in the
1438             source code.
1439 
1440         :param parent: The parent node in the syntax tree.
1441 
1442         :param end_lineno: The last line this node appears on in the source code.
1443 
1444         :param end_col_offset: The end column this node appears on in the
1445             source code. Note: This is after the last symbol.
1446         """"""
1447         self.left: NodeNG | None = None
1448         """"""What is being applied to the operator on the left side.""""""
1449 
1450         self.op: str | None = op
1451         """"""The operator.""""""
1452 
1453         self.right: NodeNG | None = None
1454         """"""What is being applied to the operator on the right side.""""""
1455 
1456         super().__init__(
1457             lineno=lineno,
1458             col_offset=col_offset,
1459             end_lineno=end_lineno,
1460             end_col_offset=end_col_offset,
1461             parent=parent,
1462         )
1463 
1464     def postinit(self, left: NodeNG | None = None, right: NodeNG | None = None) -> None:
1465         """"""Do some setup after initialisation.
1466 
1467         :param left: What is being applied to the operator on the left side.
1468 
1469         :param right: What is being applied to the operator on the right side.
1470         """"""
1471         self.left = left
1472         self.right = right
1473 
1474     # This is set by inference.py
1475     def _infer_binop(self, context=None):
1476         raise NotImplementedError
1477 
1478     def type_errors(self, context=None):
1479         """"""Get a list of type errors which can occur during inference.
1480 
1481         Each TypeError is represented by a :class:`BadBinaryOperationMessage`,
1482         which holds the original exception.
1483 
1484         :returns: The list of possible type errors.
1485         :rtype: list(BadBinaryOperationMessage)
1486         """"""
1487         try:
1488             results = self._infer_binop(context=context)
1489             return [
1490                 result
1491                 for result in results
1492                 if isinstance(result, util.BadBinaryOperationMessage)
1493             ]
1494         except InferenceError:
1495             return []
1496 
1497     def get_children(self):
1498         yield self.left
1499         yield self.right
1500 
1501     def op_precedence(self):
1502         return OP_PRECEDENCE[self.op]
1503 
1504     def op_left_associative(self):
1505         # 2**3**4 == 2**(3**4)
1506         return self.op != ""**""
1507 
1508 
1509 class BoolOp(NodeNG):
1510     """"""Class representing an :class:`ast.BoolOp` node.
1511 
1512     A :class:`BoolOp` is an application of a boolean operator.
1513 
1514     >>> import astroid
1515     >>> node = astroid.extract_node('a and b')
1516     >>> node
1517     <BinOp l.1 at 0x7f23b2e71c50>
1518     """"""
1519 
1520     _astroid_fields = (""values"",)
1521     _other_fields = (""op"",)
1522 
1523     @decorators.deprecate_default_argument_values(op=""str"")
1524     def __init__(
1525         self,
1526         op: str | None = None,
1527         lineno: int | None = None,
1528         col_offset: int | None = None,
1529         parent: NodeNG | None = None,
1530         *,
1531         end_lineno: int | None = None,
1532         end_col_offset: int | None = None,
1533     ) -> None:
1534         """"""
1535         :param op: The operator.
1536 
1537         :param lineno: The line that this node appears on in the source code.
1538 
1539         :param col_offset: The column that this node appears on in the
1540             source code.
1541 
1542         :param parent: The parent node in the syntax tree.
1543 
1544         :param end_lineno: The last line this node appears on in the source code.
1545 
1546         :param end_col_offset: The end column this node appears on in the
1547             source code. Note: This is after the last symbol.
1548         """"""
1549         self.op: str | None = op
1550         """"""The operator.""""""
1551 
1552         self.values: list[NodeNG] = []
1553         """"""The values being applied to the operator.""""""
1554 
1555         super().__init__(
1556             lineno=lineno,
1557             col_offset=col_offset,
1558             end_lineno=end_lineno,
1559             end_col_offset=end_col_offset,
1560             parent=parent,
1561         )
1562 
1563     def postinit(self, values: list[NodeNG] | None = None) -> None:
1564         """"""Do some setup after initialisation.
1565 
1566         :param values: The values being applied to the operator.
1567         """"""
1568         if values is not None:
1569             self.values = values
1570 
1571     def get_children(self):
1572         yield from self.values
1573 
1574     def op_precedence(self):
1575         return OP_PRECEDENCE[self.op]
1576 
1577 
1578 class Break(mixins.NoChildrenMixin, Statement):
1579     """"""Class representing an :class:`ast.Break` node.
1580 
1581     >>> import astroid
1582     >>> node = astroid.extract_node('break')
1583     >>> node
1584     <Break l.1 at 0x7f23b2e9e5c0>
1585     """"""
1586 
1587 
1588 class Call(NodeNG):
1589     """"""Class representing an :class:`ast.Call` node.
1590 
1591     A :class:`Call` node is a call to a function, method, etc.
1592 
1593     >>> import astroid
1594     >>> node = astroid.extract_node('function()')
1595     >>> node
1596     <Call l.1 at 0x7f23b2e71eb8>
1597     """"""
1598 
1599     _astroid_fields = (""func"", ""args"", ""keywords"")
1600 
1601     def __init__(
1602         self,
1603         lineno: int | None = None,
1604         col_offset: int | None = None,
1605         parent: NodeNG | None = None,
1606         *,
1607         end_lineno: int | None = None,
1608         end_col_offset: int | None = None,
1609     ) -> None:
1610         """"""
1611         :param lineno: The line that this node appears on in the source code.
1612 
1613         :param col_offset: The column that this node appears on in the
1614             source code.
1615 
1616         :param parent: The parent node in the syntax tree.
1617 
1618         :param end_lineno: The last line this node appears on in the source code.
1619 
1620         :param end_col_offset: The end column this node appears on in the
1621             source code. Note: This is after the last symbol.
1622         """"""
1623         self.func: NodeNG | None = None
1624         """"""What is being called.""""""
1625 
1626         self.args: list[NodeNG] = []
1627         """"""The positional arguments being given to the call.""""""
1628 
1629         self.keywords: list[Keyword] = []
1630         """"""The keyword arguments being given to the call.""""""
1631 
1632         super().__init__(
1633             lineno=lineno,
1634             col_offset=col_offset,
1635             end_lineno=end_lineno,
1636             end_col_offset=end_col_offset,
1637             parent=parent,
1638         )
1639 
1640     def postinit(
1641         self,
1642         func: NodeNG | None = None,
1643         args: list[NodeNG] | None = None,
1644         keywords: list[Keyword] | None = None,
1645     ) -> None:
1646         """"""Do some setup after initialisation.
1647 
1648         :param func: What is being called.
1649 
1650         :param args: The positional arguments being given to the call.
1651 
1652         :param keywords: The keyword arguments being given to the call.
1653         """"""
1654         self.func = func
1655         if args is not None:
1656             self.args = args
1657         if keywords is not None:
1658             self.keywords = keywords
1659 
1660     @property
1661     def starargs(self) -> list[Starred]:
1662         """"""The positional arguments that unpack something.""""""
1663         return [arg for arg in self.args if isinstance(arg, Starred)]
1664 
1665     @property
1666     def kwargs(self) -> list[Keyword]:
1667         """"""The keyword arguments that unpack something.""""""
1668         return [keyword for keyword in self.keywords if keyword.arg is None]
1669 
1670     def get_children(self):
1671         yield self.func
1672 
1673         yield from self.args
1674 
1675         yield from self.keywords
1676 
1677 
1678 class Compare(NodeNG):
1679     """"""Class representing an :class:`ast.Compare` node.
1680 
1681     A :class:`Compare` node indicates a comparison.
1682 
1683     >>> import astroid
1684     >>> node = astroid.extract_node('a <= b <= c')
1685     >>> node
1686     <Compare l.1 at 0x7f23b2e9e6d8>
1687     >>> node.ops
1688     [('<=', <Name.b l.1 at 0x7f23b2e9e2b0>), ('<=', <Name.c l.1 at 0x7f23b2e9e390>)]
1689     """"""
1690 
1691     _astroid_fields = (""left"", ""ops"")
1692 
1693     def __init__(
1694         self,
1695         lineno: int | None = None,
1696         col_offset: int | None = None,
1697         parent: NodeNG | None = None,
1698         *,
1699         end_lineno: int | None = None,
1700         end_col_offset: int | None = None,
1701     ) -> None:
1702         """"""
1703         :param lineno: The line that this node appears on in the source code.
1704 
1705         :param col_offset: The column that this node appears on in the
1706             source code.
1707 
1708         :param parent: The parent node in the syntax tree.
1709 
1710         :param end_lineno: The last line this node appears on in the source code.
1711 
1712         :param end_col_offset: The end column this node appears on in the
1713             source code. Note: This is after the last symbol.
1714         """"""
1715         self.left: NodeNG | None = None
1716         """"""The value at the left being applied to a comparison operator.""""""
1717 
1718         self.ops: list[tuple[str, NodeNG]] = []
1719         """"""The remainder of the operators and their relevant right hand value.""""""
1720 
1721         super().__init__(
1722             lineno=lineno,
1723             col_offset=col_offset,
1724             end_lineno=end_lineno,
1725             end_col_offset=end_col_offset,
1726             parent=parent,
1727         )
1728 
1729     def postinit(
1730         self,
1731         left: NodeNG | None = None,
1732         ops: list[tuple[str, NodeNG]] | None = None,
1733     ) -> None:
1734         """"""Do some setup after initialisation.
1735 
1736         :param left: The value at the left being applied to a comparison
1737             operator.
1738 
1739         :param ops: The remainder of the operators
1740             and their relevant right hand value.
1741         """"""
1742         self.left = left
1743         if ops is not None:
1744             self.ops = ops
1745 
1746     def get_children(self):
1747         """"""Get the child nodes below this node.
1748 
1749         Overridden to handle the tuple fields and skip returning the operator
1750         strings.
1751 
1752         :returns: The children.
1753         :rtype: iterable(NodeNG)
1754         """"""
1755         yield self.left
1756         for _, comparator in self.ops:
1757             yield comparator  # we don't want the 'op'
1758 
1759     def last_child(self):
1760         """"""An optimized version of list(get_children())[-1]
1761 
1762         :returns: The last child.
1763         :rtype: NodeNG
1764         """"""
1765         # XXX maybe if self.ops:
1766         return self.ops[-1][1]
1767         # return self.left
1768 
1769 
1770 class Comprehension(NodeNG):
1771     """"""Class representing an :class:`ast.comprehension` node.
1772 
1773     A :class:`Comprehension` indicates the loop inside any type of
1774     comprehension including generator expressions.
1775 
1776     >>> import astroid
1777     >>> node = astroid.extract_node('[x for x in some_values]')
1778     >>> list(node.get_children())
1779     [<Name.x l.1 at 0x7f23b2e352b0>, <Comprehension l.1 at 0x7f23b2e35320>]
1780     >>> list(node.get_children())[1].as_string()
1781     'for x in some_values'
1782     """"""
1783 
1784     _astroid_fields = (""target"", ""iter"", ""ifs"")
1785     _other_fields = (""is_async"",)
1786 
1787     optional_assign = True
1788     """"""Whether this node optionally assigns a variable.""""""
1789 
1790     lineno: None
1791     col_offset: None
1792     end_lineno: None
1793     end_col_offset: None
1794 
1795     def __init__(self, parent: NodeNG | None = None) -> None:
1796         """"""
1797         :param parent: The parent node in the syntax tree.
1798         """"""
1799         self.target: NodeNG | None = None
1800         """"""What is assigned to by the comprehension.""""""
1801 
1802         self.iter: NodeNG | None = None
1803         """"""What is iterated over by the comprehension.""""""
1804 
1805         self.ifs: list[NodeNG] = []
1806         """"""The contents of any if statements that filter the comprehension.""""""
1807 
1808         self.is_async: bool | None = None
1809         """"""Whether this is an asynchronous comprehension or not.""""""
1810 
1811         super().__init__(parent=parent)
1812 
1813     # pylint: disable=redefined-builtin; same name as builtin ast module.
1814     def postinit(
1815         self,
1816         target: NodeNG | None = None,
1817         iter: NodeNG | None = None,
1818         ifs: list[NodeNG] | None = None,
1819         is_async: bool | None = None,
1820     ) -> None:
1821         """"""Do some setup after initialisation.
1822 
1823         :param target: What is assigned to by the comprehension.
1824 
1825         :param iter: What is iterated over by the comprehension.
1826 
1827         :param ifs: The contents of any if statements that filter
1828             the comprehension.
1829 
1830         :param is_async: Whether this is an asynchronous comprehension or not.
1831         """"""
1832         self.target = target
1833         self.iter = iter
1834         if ifs is not None:
1835             self.ifs = ifs
1836         self.is_async = is_async
1837 
1838     assigned_stmts: ClassVar[AssignedStmtsCall[Comprehension]]
1839     """"""Returns the assigned statement (non inferred) according to the assignment type.
1840     See astroid/protocols.py for actual implementation.
1841     """"""
1842 
1843     def assign_type(self):
1844         """"""The type of assignment that this node performs.
1845 
1846         :returns: The assignment type.
1847         :rtype: NodeNG
1848         """"""
1849         return self
1850 
1851     def _get_filtered_stmts(self, lookup_node, node, stmts, mystmt: Statement | None):
1852         """"""method used in filter_stmts""""""
1853         if self is mystmt:
1854             if isinstance(lookup_node, (Const, Name)):
1855                 return [lookup_node], True
1856 
1857         elif self.statement(future=True) is mystmt:
1858             # original node's statement is the assignment, only keeps
1859             # current node (gen exp, list comp)
1860 
1861             return [node], True
1862 
1863         return stmts, False
1864 
1865     def get_children(self):
1866         yield self.target
1867         yield self.iter
1868 
1869         yield from self.ifs
1870 
1871 
1872 class Const(mixins.NoChildrenMixin, NodeNG, Instance):
1873     """"""Class representing any constant including num, str, bool, None, bytes.
1874 
1875     >>> import astroid
1876     >>> node = astroid.extract_node('(5, ""This is a string."", True, None, b""bytes"")')
1877     >>> node
1878     <Tuple.tuple l.1 at 0x7f23b2e358d0>
1879     >>> list(node.get_children())
1880     [<Const.int l.1 at 0x7f23b2e35940>,
1881     <Const.str l.1 at 0x7f23b2e35978>,
1882     <Const.bool l.1 at 0x7f23b2e359b0>,
1883     <Const.NoneType l.1 at 0x7f23b2e359e8>,
1884     <Const.bytes l.1 at 0x7f23b2e35a20>]
1885     """"""
1886 
1887     _other_fields = (""value"", ""kind"")
1888 
1889     def __init__(
1890         self,
1891         value: Any,
1892         lineno: int | None = None,
1893         col_offset: int | None = None,
1894         parent: NodeNG | None = None,
1895         kind: str | None = None,
1896         *,
1897         end_lineno: int | None = None,
1898         end_col_offset: int | None = None,
1899     ) -> None:
1900         """"""
1901         :param value: The value that the constant represents.
1902 
1903         :param lineno: The line that this node appears on in the source code.
1904 
1905         :param col_offset: The column that this node appears on in the
1906             source code.
1907 
1908         :param parent: The parent node in the syntax tree.
1909 
1910         :param kind: The string prefix. ""u"" for u-prefixed strings and ``None`` otherwise. Python 3.8+ only.
1911 
1912         :param end_lineno: The last line this node appears on in the source code.
1913 
1914         :param end_col_offset: The end column this node appears on in the
1915             source code. Note: This is after the last symbol.
1916         """"""
1917         self.value: Any = value
1918         """"""The value that the constant represents.""""""
1919 
1920         self.kind: str | None = kind  # can be None
1921         """"""""The string prefix. ""u"" for u-prefixed strings and ``None`` otherwise. Python 3.8+ only.""""""
1922 
1923         super().__init__(
1924             lineno=lineno,
1925             col_offset=col_offset,
1926             end_lineno=end_lineno,
1927             end_col_offset=end_col_offset,
1928             parent=parent,
1929         )
1930 
1931     def __getattr__(self, name):
1932         # This is needed because of Proxy's __getattr__ method.
1933         # Calling object.__new__ on this class without calling
1934         # __init__ would result in an infinite loop otherwise
1935         # since __getattr__ is called when an attribute doesn't
1936         # exist and self._proxied indirectly calls self.value
1937         # and Proxy __getattr__ calls self.value
1938         if name == ""value"":
1939             raise AttributeError
1940         return super().__getattr__(name)
1941 
1942     def getitem(self, index, context=None):
1943         """"""Get an item from this node if subscriptable.
1944 
1945         :param index: The node to use as a subscript index.
1946         :type index: Const or Slice
1947 
1948         :raises AstroidTypeError: When the given index cannot be used as a
1949             subscript index, or if this node is not subscriptable.
1950         """"""
1951         if isinstance(index, Const):
1952             index_value = index.value
1953         elif isinstance(index, Slice):
1954             index_value = _infer_slice(index, context=context)
1955 
1956         else:
1957             raise AstroidTypeError(
1958                 f""Could not use type {type(index)} as subscript index""
1959             )
1960 
1961         try:
1962             if isinstance(self.value, (str, bytes)):
1963                 return Const(self.value[index_value])
1964         except IndexError as exc:
1965             raise AstroidIndexError(
1966                 message=""Index {index!r} out of range"",
1967                 node=self,
1968                 index=index,
1969                 context=context,
1970             ) from exc
1971         except TypeError as exc:
1972             raise AstroidTypeError(
1973                 message=""Type error {error!r}"", node=self, index=index, context=context
1974             ) from exc
1975 
1976         raise AstroidTypeError(f""{self!r} (value={self.value})"")
1977 
1978     def has_dynamic_getattr(self):
1979         """"""Check if the node has a custom __getattr__ or __getattribute__.
1980 
1981         :returns: True if the class has a custom
1982             __getattr__ or __getattribute__, False otherwise.
1983             For a :class:`Const` this is always ``False``.
1984         :rtype: bool
1985         """"""
1986         return False
1987 
1988     def itered(self):
1989         """"""An iterator over the elements this node contains.
1990 
1991         :returns: The contents of this node.
1992         :rtype: iterable(Const)
1993 
1994         :raises TypeError: If this node does not represent something that is iterable.
1995         """"""
1996         if isinstance(self.value, str):
1997             return [const_factory(elem) for elem in self.value]
1998         raise TypeError(f""Cannot iterate over type {type(self.value)!r}"")
1999 
2000     def pytype(self):
2001         """"""Get the name of the type that this node represents.
2002 
2003         :returns: The name of the type.
2004         :rtype: str
2005         """"""
2006         return self._proxied.qname()
2007 
2008     def bool_value(self, context=None):
2009         """"""Determine the boolean value of this node.
2010 
2011         :returns: The boolean value of this node.
2012         :rtype: bool
2013         """"""
2014         return bool(self.value)
2015 
2016 
2017 class Continue(mixins.NoChildrenMixin, Statement):
2018     """"""Class representing an :class:`ast.Continue` node.
2019 
2020     >>> import astroid
2021     >>> node = astroid.extract_node('continue')
2022     >>> node
2023     <Continue l.1 at 0x7f23b2e35588>
2024     """"""
2025 
2026 
2027 class Decorators(NodeNG):
2028     """"""A node representing a list of decorators.
2029 
2030     A :class:`Decorators` is the decorators that are applied to
2031     a method or function.
2032 
2033     >>> import astroid
2034     >>> node = astroid.extract_node('''
2035     @property
2036     def my_property(self):
2037         return 3
2038     ''')
2039     >>> node
2040     <FunctionDef.my_property l.2 at 0x7f23b2e35d30>
2041     >>> list(node.get_children())[0]
2042     <Decorators l.1 at 0x7f23b2e35d68>
2043     """"""
2044 
2045     _astroid_fields = (""nodes"",)
2046 
2047     def __init__(
2048         self,
2049         lineno: int | None = None,
2050         col_offset: int | None = None,
2051         parent: NodeNG | None = None,
2052         *,
2053         end_lineno: int | None = None,
2054         end_col_offset: int | None = None,
2055     ) -> None:
2056         """"""
2057         :param lineno: The line that this node appears on in the source code.
2058 
2059         :param col_offset: The column that this node appears on in the
2060             source code.
2061 
2062         :param parent: The parent node in the syntax tree.
2063 
2064         :param end_lineno: The last line this node appears on in the source code.
2065 
2066         :param end_col_offset: The end column this node appears on in the
2067             source code. Note: This is after the last symbol.
2068         """"""
2069         self.nodes: list[NodeNG]
2070         """"""The decorators that this node contains.
2071 
2072         :type: list(Name or Call) or None
2073         """"""
2074 
2075         super().__init__(
2076             lineno=lineno,
2077             col_offset=col_offset,
2078             end_lineno=end_lineno,
2079             end_col_offset=end_col_offset,
2080             parent=parent,
2081         )
2082 
2083     def postinit(self, nodes: list[NodeNG]) -> None:
2084         """"""Do some setup after initialisation.
2085 
2086         :param nodes: The decorators that this node contains.
2087         :type nodes: list(Name or Call)
2088         """"""
2089         self.nodes = nodes
2090 
2091     def scope(self) -> LocalsDictNodeNG:
2092         """"""The first parent node defining a new scope.
2093         These can be Module, FunctionDef, ClassDef, Lambda, or GeneratorExp nodes.
2094 
2095         :returns: The first parent scope node.
2096         """"""
2097         # skip the function node to go directly to the upper level scope
2098         if not self.parent:
2099             raise ParentMissingError(target=self)
2100         if not self.parent.parent:
2101             raise ParentMissingError(target=self.parent)
2102         return self.parent.parent.scope()
2103 
2104     def get_children(self):
2105         yield from self.nodes
2106 
2107 
2108 class DelAttr(mixins.ParentAssignTypeMixin, NodeNG):
2109     """"""Variation of :class:`ast.Delete` representing deletion of an attribute.
2110 
2111     >>> import astroid
2112     >>> node = astroid.extract_node('del self.attr')
2113     >>> node
2114     <Delete l.1 at 0x7f23b2e35f60>
2115     >>> list(node.get_children())[0]
2116     <DelAttr.attr l.1 at 0x7f23b2e411d0>
2117     """"""
2118 
2119     _astroid_fields = (""expr"",)
2120     _other_fields = (""attrname"",)
2121 
2122     @decorators.deprecate_default_argument_values(attrname=""str"")
2123     def __init__(
2124         self,
2125         attrname: str | None = None,
2126         lineno: int | None = None,
2127         col_offset: int | None = None,
2128         parent: NodeNG | None = None,
2129         *,
2130         end_lineno: int | None = None,
2131         end_col_offset: int | None = None,
2132     ) -> None:
2133         """"""
2134         :param attrname: The name of the attribute that is being deleted.
2135 
2136         :param lineno: The line that this node appears on in the source code.
2137 
2138         :param col_offset: The column that this node appears on in the
2139             source code.
2140 
2141         :param parent: The parent node in the syntax tree.
2142 
2143         :param end_lineno: The last line this node appears on in the source code.
2144 
2145         :param end_col_offset: The end column this node appears on in the
2146             source code. Note: This is after the last symbol.
2147         """"""
2148         self.expr: NodeNG | None = None
2149         """"""The name that this node represents.
2150 
2151         :type: Name or None
2152         """"""
2153 
2154         self.attrname: str | None = attrname
2155         """"""The name of the attribute that is being deleted.""""""
2156 
2157         super().__init__(
2158             lineno=lineno,
2159             col_offset=col_offset,
2160             end_lineno=end_lineno,
2161             end_col_offset=end_col_offset,
2162             parent=parent,
2163         )
2164 
2165     def postinit(self, expr: NodeNG | None = None) -> None:
2166         """"""Do some setup after initialisation.
2167 
2168         :param expr: The name that this node represents.
2169         :type expr: Name or None
2170         """"""
2171         self.expr = expr
2172 
2173     def get_children(self):
2174         yield self.expr
2175 
2176 
2177 class Delete(mixins.AssignTypeMixin, Statement):
2178     """"""Class representing an :class:`ast.Delete` node.
2179 
2180     A :class:`Delete` is a ``del`` statement this is deleting something.
2181 
2182     >>> import astroid
2183     >>> node = astroid.extract_node('del self.attr')
2184     >>> node
2185     <Delete l.1 at 0x7f23b2e35f60>
2186     """"""
2187 
2188     _astroid_fields = (""targets"",)
2189 
2190     def __init__(
2191         self,
2192         lineno: int | None = None,
2193         col_offset: int | None = None,
2194         parent: NodeNG | None = None,
2195         *,
2196         end_lineno: int | None = None,
2197         end_col_offset: int | None = None,
2198     ) -> None:
2199         """"""
2200         :param lineno: The line that this node appears on in the source code.
2201 
2202         :param col_offset: The column that this node appears on in the
2203             source code.
2204 
2205         :param parent: The parent node in the syntax tree.
2206 
2207         :param end_lineno: The last line this node appears on in the source code.
2208 
2209         :param end_col_offset: The end column this node appears on in the
2210             source code. Note: This is after the last symbol.
2211         """"""
2212         self.targets: list[NodeNG] = []
2213         """"""What is being deleted.""""""
2214 
2215         super().__init__(
2216             lineno=lineno,
2217             col_offset=col_offset,
2218             end_lineno=end_lineno,
2219             end_col_offset=end_col_offset,
2220             parent=parent,
2221         )
2222 
2223     def postinit(self, targets: list[NodeNG] | None = None) -> None:
2224         """"""Do some setup after initialisation.
2225 
2226         :param targets: What is being deleted.
2227         """"""
2228         if targets is not None:
2229             self.targets = targets
2230 
2231     def get_children(self):
2232         yield from self.targets
2233 
2234 
2235 class Dict(NodeNG, Instance):
2236     """"""Class representing an :class:`ast.Dict` node.
2237 
2238     A :class:`Dict` is a dictionary that is created with ``{}`` syntax.
2239 
2240     >>> import astroid
2241     >>> node = astroid.extract_node('{1: ""1""}')
2242     >>> node
2243     <Dict.dict l.1 at 0x7f23b2e35cc0>
2244     """"""
2245 
2246     _astroid_fields = (""items"",)
2247 
2248     def __init__(
2249         self,
2250         lineno: int | None = None,
2251         col_offset: int | None = None,
2252         parent: NodeNG | None = None,
2253         *,
2254         end_lineno: int | None = None,
2255         end_col_offset: int | None = None,
2256     ) -> None:
2257         """"""
2258         :param lineno: The line that this node appears on in the source code.
2259 
2260         :param col_offset: The column that this node appears on in the
2261             source code.
2262 
2263         :param parent: The parent node in the syntax tree.
2264 
2265         :param end_lineno: The last line this node appears on in the source code.
2266 
2267         :param end_col_offset: The end column this node appears on in the
2268             source code. Note: This is after the last symbol.
2269         """"""
2270         self.items: list[tuple[NodeNG, NodeNG]] = []
2271         """"""The key-value pairs contained in the dictionary.""""""
2272 
2273         super().__init__(
2274             lineno=lineno,
2275             col_offset=col_offset,
2276             end_lineno=end_lineno,
2277             end_col_offset=end_col_offset,
2278             parent=parent,
2279         )
2280 
2281     def postinit(self, items: list[tuple[NodeNG, NodeNG]]) -> None:
2282         """"""Do some setup after initialisation.
2283 
2284         :param items: The key-value pairs contained in the dictionary.
2285         """"""
2286         self.items = items
2287 
2288     @classmethod
2289     def from_elements(cls, items=None):
2290         """"""Create a :class:`Dict` of constants from a live dictionary.
2291 
2292         :param items: The items to store in the node.
2293         :type items: dict
2294 
2295         :returns: The created dictionary node.
2296         :rtype: Dict
2297         """"""
2298         node = cls()
2299         if items is None:
2300             node.items = []
2301         else:
2302             node.items = [
2303                 (const_factory(k), const_factory(v) if _is_const(v) else v)
2304                 for k, v in items.items()
2305                 # The keys need to be constants
2306                 if _is_const(k)
2307             ]
2308         return node
2309 
2310     def pytype(self):
2311         """"""Get the name of the type that this node represents.
2312 
2313         :returns: The name of the type.
2314         :rtype: str
2315         """"""
2316         return ""builtins.dict""
2317 
2318     def get_children(self):
2319         """"""Get the key and value nodes below this node.
2320 
2321         Children are returned in the order that they are defined in the source
2322         code, key first then the value.
2323 
2324         :returns: The children.
2325         :rtype: iterable(NodeNG)
2326         """"""
2327         for key, value in self.items:
2328             yield key
2329             yield value
2330 
2331     def last_child(self):
2332         """"""An optimized version of list(get_children())[-1]
2333 
2334         :returns: The last child, or None if no children exist.
2335         :rtype: NodeNG or None
2336         """"""
2337         if self.items:
2338             return self.items[-1][1]
2339         return None
2340 
2341     def itered(self):
2342         """"""An iterator over the keys this node contains.
2343 
2344         :returns: The keys of this node.
2345         :rtype: iterable(NodeNG)
2346         """"""
2347         return [key for (key, _) in self.items]
2348 
2349     def getitem(self, index, context=None):
2350         """"""Get an item from this node.
2351 
2352         :param index: The node to use as a subscript index.
2353         :type index: Const or Slice
2354 
2355         :raises AstroidTypeError: When the given index cannot be used as a
2356             subscript index, or if this node is not subscriptable.
2357         :raises AstroidIndexError: If the given index does not exist in the
2358             dictionary.
2359         """"""
2360         for key, value in self.items:
2361             # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.
2362             if isinstance(key, DictUnpack):
2363                 try:
2364                     return value.getitem(index, context)
2365                 except (AstroidTypeError, AstroidIndexError):
2366                     continue
2367             for inferredkey in key.infer(context):
2368                 if inferredkey is util.Uninferable:
2369                     continue
2370                 if isinstance(inferredkey, Const) and isinstance(index, Const):
2371                     if inferredkey.value == index.value:
2372                         return value
2373 
2374         raise AstroidIndexError(index)
2375 
2376     def bool_value(self, context=None):
2377         """"""Determine the boolean value of this node.
2378 
2379         :returns: The boolean value of this node.
2380         :rtype: bool
2381         """"""
2382         return bool(self.items)
2383 
2384 
2385 class Expr(Statement):
2386     """"""Class representing an :class:`ast.Expr` node.
2387 
2388     An :class:`Expr` is any expression that does not have its value used or
2389     stored.
2390 
2391     >>> import astroid
2392     >>> node = astroid.extract_node('method()')
2393     >>> node
2394     <Call l.1 at 0x7f23b2e352b0>
2395     >>> node.parent
2396     <Expr l.1 at 0x7f23b2e35278>
2397     """"""
2398 
2399     _astroid_fields = (""value"",)
2400 
2401     def __init__(
2402         self,
2403         lineno: int | None = None,
2404         col_offset: int | None = None,
2405         parent: NodeNG | None = None,
2406         *,
2407         end_lineno: int | None = None,
2408         end_col_offset: int | None = None,
2409     ) -> None:
2410         """"""
2411         :param lineno: The line that this node appears on in the source code.
2412 
2413         :param col_offset: The column that this node appears on in the
2414             source code.
2415 
2416         :param parent: The parent node in the syntax tree.
2417 
2418         :param end_lineno: The last line this node appears on in the source code.
2419 
2420         :param end_col_offset: The end column this node appears on in the
2421             source code. Note: This is after the last symbol.
2422         """"""
2423         self.value: NodeNG | None = None
2424         """"""What the expression does.""""""
2425 
2426         super().__init__(
2427             lineno=lineno,
2428             col_offset=col_offset,
2429             end_lineno=end_lineno,
2430             end_col_offset=end_col_offset,
2431             parent=parent,
2432         )
2433 
2434     def postinit(self, value: NodeNG | None = None) -> None:
2435         """"""Do some setup after initialisation.
2436 
2437         :param value: What the expression does.
2438         """"""
2439         self.value = value
2440 
2441     def get_children(self):
2442         yield self.value
2443 
2444     def _get_yield_nodes_skip_lambdas(self):
2445         if not self.value.is_lambda:
2446             yield from self.value._get_yield_nodes_skip_lambdas()
2447 
2448 
2449 class Ellipsis(mixins.NoChildrenMixin, NodeNG):  # pylint: disable=redefined-builtin
2450     """"""Class representing an :class:`ast.Ellipsis` node.
2451 
2452     An :class:`Ellipsis` is the ``...`` syntax.
2453 
2454     Deprecated since v2.6.0 - Use :class:`Const` instead.
2455     Will be removed with the release v2.7.0
2456     """"""
2457 
2458 
2459 class EmptyNode(mixins.NoChildrenMixin, NodeNG):
2460     """"""Holds an arbitrary object in the :attr:`LocalsDictNodeNG.locals`.""""""
2461 
2462     object = None
2463 
2464 
2465 class ExceptHandler(mixins.MultiLineBlockMixin, mixins.AssignTypeMixin, Statement):
2466     """"""Class representing an :class:`ast.ExceptHandler`. node.
2467 
2468     An :class:`ExceptHandler` is an ``except`` block on a try-except.
2469 
2470     >>> import astroid
2471     >>> node = astroid.extract_node('''
2472         try:
2473             do_something()
2474         except Exception as error:
2475             print(""Error!"")
2476         ''')
2477     >>> node
2478     <TryExcept l.2 at 0x7f23b2e9d908>
2479     >>> node.handlers
2480     [<ExceptHandler l.4 at 0x7f23b2e9e860>]
2481     """"""
2482 
2483     _astroid_fields = (""type"", ""name"", ""body"")
2484     _multi_line_block_fields = (""body"",)
2485 
2486     def __init__(
2487         self,
2488         lineno: int | None = None,
2489         col_offset: int | None = None,
2490         parent: NodeNG | None = None,
2491         *,
2492         end_lineno: int | None = None,
2493         end_col_offset: int | None = None,
2494     ) -> None:
2495         """"""
2496         :param lineno: The line that this node appears on in the source code.
2497 
2498         :param col_offset: The column that this node appears on in the
2499             source code.
2500 
2501         :param parent: The parent node in the syntax tree.
2502 
2503         :param end_lineno: The last line this node appears on in the source code.
2504 
2505         :param end_col_offset: The end column this node appears on in the
2506             source code. Note: This is after the last symbol.
2507         """"""
2508         self.type: NodeNG | None = None  # can be None
2509         """"""The types that the block handles.
2510 
2511         :type: Tuple or NodeNG or None
2512         """"""
2513 
2514         self.name: AssignName | None = None  # can be None
2515         """"""The name that the caught exception is assigned to.""""""
2516 
2517         self.body: list[NodeNG] = []
2518         """"""The contents of the block.""""""
2519 
2520         super().__init__(
2521             lineno=lineno,
2522             col_offset=col_offset,
2523             end_lineno=end_lineno,
2524             end_col_offset=end_col_offset,
2525             parent=parent,
2526         )
2527 
2528     assigned_stmts: ClassVar[AssignedStmtsCall[ExceptHandler]]
2529     """"""Returns the assigned statement (non inferred) according to the assignment type.
2530     See astroid/protocols.py for actual implementation.
2531     """"""
2532 
2533     def get_children(self):
2534         if self.type is not None:
2535             yield self.type
2536 
2537         if self.name is not None:
2538             yield self.name
2539 
2540         yield from self.body
2541 
2542     # pylint: disable=redefined-builtin; had to use the same name as builtin ast module.
2543     def postinit(
2544         self,
2545         type: NodeNG | None = None,
2546         name: AssignName | None = None,
2547         body: list[NodeNG] | None = None,
2548     ) -> None:
2549         """"""Do some setup after initialisation.
2550 
2551         :param type: The types that the block handles.
2552         :type type: Tuple or NodeNG or None
2553 
2554         :param name: The name that the caught exception is assigned to.
2555 
2556         :param body:The contents of the block.
2557         """"""
2558         self.type = type
2559         self.name = name
2560         if body is not None:
2561             self.body = body
2562 
2563     @cached_property
2564     def blockstart_tolineno(self):
2565         """"""The line on which the beginning of this block ends.
2566 
2567         :type: int
2568         """"""
2569         if self.name:
2570             return self.name.tolineno
2571         if self.type:
2572             return self.type.tolineno
2573         return self.lineno
2574 
2575     def catch(self, exceptions: list[str] | None) -> bool:
2576         """"""Check if this node handles any of the given
2577 
2578         :param exceptions: The names of the exceptions to check for.
2579         """"""
2580         if self.type is None or exceptions is None:
2581             return True
2582         return any(node.name in exceptions for node in self.type._get_name_nodes())
2583 
2584 
2585 class ExtSlice(NodeNG):
2586     """"""Class representing an :class:`ast.ExtSlice` node.
2587 
2588     An :class:`ExtSlice` is a complex slice expression.
2589 
2590     Deprecated since v2.6.0 - Now part of the :class:`Subscript` node.
2591     Will be removed with the release of v2.7.0
2592     """"""
2593 
2594 
2595 class For(
2596     mixins.MultiLineBlockMixin,
2597     mixins.BlockRangeMixIn,
2598     mixins.AssignTypeMixin,
2599     Statement,
2600 ):
2601     """"""Class representing an :class:`ast.For` node.
2602 
2603     >>> import astroid
2604     >>> node = astroid.extract_node('for thing in things: print(thing)')
2605     >>> node
2606     <For l.1 at 0x7f23b2e8cf28>
2607     """"""
2608 
2609     _astroid_fields = (""target"", ""iter"", ""body"", ""orelse"")
2610     _other_other_fields = (""type_annotation"",)
2611     _multi_line_block_fields = (""body"", ""orelse"")
2612 
2613     optional_assign = True
2614     """"""Whether this node optionally assigns a variable.
2615 
2616     This is always ``True`` for :class:`For` nodes.
2617     """"""
2618 
2619     def __init__(
2620         self,
2621         lineno: int | None = None,
2622         col_offset: int | None = None,
2623         parent: NodeNG | None = None,
2624         *,
2625         end_lineno: int | None = None,
2626         end_col_offset: int | None = None,
2627     ) -> None:
2628         """"""
2629         :param lineno: The line that this node appears on in the source code.
2630 
2631         :param col_offset: The column that this node appears on in the
2632             source code.
2633 
2634         :param parent: The parent node in the syntax tree.
2635 
2636         :param end_lineno: The last line this node appears on in the source code.
2637 
2638         :param end_col_offset: The end column this node appears on in the
2639             source code. Note: This is after the last symbol.
2640         """"""
2641         self.target: NodeNG | None = None
2642         """"""What the loop assigns to.""""""
2643 
2644         self.iter: NodeNG | None = None
2645         """"""What the loop iterates over.""""""
2646 
2647         self.body: list[NodeNG] = []
2648         """"""The contents of the body of the loop.""""""
2649 
2650         self.orelse: list[NodeNG] = []
2651         """"""The contents of the ``else`` block of the loop.""""""
2652 
2653         self.type_annotation: NodeNG | None = None  # can be None
2654         """"""If present, this will contain the type annotation passed by a type comment""""""
2655 
2656         super().__init__(
2657             lineno=lineno,
2658             col_offset=col_offset,
2659             end_lineno=end_lineno,
2660             end_col_offset=end_col_offset,
2661             parent=parent,
2662         )
2663 
2664     # pylint: disable=redefined-builtin; had to use the same name as builtin ast module.
2665     def postinit(
2666         self,
2667         target: NodeNG | None = None,
2668         iter: NodeNG | None = None,
2669         body: list[NodeNG] | None = None,
2670         orelse: list[NodeNG] | None = None,
2671         type_annotation: NodeNG | None = None,
2672     ) -> None:
2673         """"""Do some setup after initialisation.
2674 
2675         :param target: What the loop assigns to.
2676 
2677         :param iter: What the loop iterates over.
2678 
2679         :param body: The contents of the body of the loop.
2680 
2681         :param orelse: The contents of the ``else`` block of the loop.
2682         """"""
2683         self.target = target
2684         self.iter = iter
2685         if body is not None:
2686             self.body = body
2687         if orelse is not None:
2688             self.orelse = orelse
2689         self.type_annotation = type_annotation
2690 
2691     assigned_stmts: ClassVar[AssignedStmtsCall[For]]
2692     """"""Returns the assigned statement (non inferred) according to the assignment type.
2693     See astroid/protocols.py for actual implementation.
2694     """"""
2695 
2696     @cached_property
2697     def blockstart_tolineno(self):
2698         """"""The line on which the beginning of this block ends.
2699 
2700         :type: int
2701         """"""
2702         return self.iter.tolineno
2703 
2704     def get_children(self):
2705         yield self.target
2706         yield self.iter
2707 
2708         yield from self.body
2709         yield from self.orelse
2710 
2711 
2712 class AsyncFor(For):
2713     """"""Class representing an :class:`ast.AsyncFor` node.
2714 
2715     An :class:`AsyncFor` is an asynchronous :class:`For` built with
2716     the ``async`` keyword.
2717 
2718     >>> import astroid
2719     >>> node = astroid.extract_node('''
2720     async def func(things):
2721         async for thing in things:
2722             print(thing)
2723     ''')
2724     >>> node
2725     <AsyncFunctionDef.func l.2 at 0x7f23b2e416d8>
2726     >>> node.body[0]
2727     <AsyncFor l.3 at 0x7f23b2e417b8>
2728     """"""
2729 
2730 
2731 class Await(NodeNG):
2732     """"""Class representing an :class:`ast.Await` node.
2733 
2734     An :class:`Await` is the ``await`` keyword.
2735 
2736     >>> import astroid
2737     >>> node = astroid.extract_node('''
2738     async def func(things):
2739         await other_func()
2740     ''')
2741     >>> node
2742     <AsyncFunctionDef.func l.2 at 0x7f23b2e41748>
2743     >>> node.body[0]
2744     <Expr l.3 at 0x7f23b2e419e8>
2745     >>> list(node.body[0].get_children())[0]
2746     <Await l.3 at 0x7f23b2e41a20>
2747     """"""
2748 
2749     _astroid_fields = (""value"",)
2750 
2751     def __init__(
2752         self,
2753         lineno: int | None = None,
2754         col_offset: int | None = None,
2755         parent: NodeNG | None = None,
2756         *,
2757         end_lineno: int | None = None,
2758         end_col_offset: int | None = None,
2759     ) -> None:
2760         """"""
2761         :param lineno: The line that this node appears on in the source code.
2762 
2763         :param col_offset: The column that this node appears on in the
2764             source code.
2765 
2766         :param parent: The parent node in the syntax tree.
2767 
2768         :param end_lineno: The last line this node appears on in the source code.
2769 
2770         :param end_col_offset: The end column this node appears on in the
2771             source code. Note: This is after the last symbol.
2772         """"""
2773         self.value: NodeNG | None = None
2774         """"""What to wait for.""""""
2775 
2776         super().__init__(
2777             lineno=lineno,
2778             col_offset=col_offset,
2779             end_lineno=end_lineno,
2780             end_col_offset=end_col_offset,
2781             parent=parent,
2782         )
2783 
2784     def postinit(self, value: NodeNG | None = None) -> None:
2785         """"""Do some setup after initialisation.
2786 
2787         :param value: What to wait for.
2788         """"""
2789         self.value = value
2790 
2791     def get_children(self):
2792         yield self.value
2793 
2794 
2795 class ImportFrom(mixins.NoChildrenMixin, mixins.ImportFromMixin, Statement):
2796     """"""Class representing an :class:`ast.ImportFrom` node.
2797 
2798     >>> import astroid
2799     >>> node = astroid.extract_node('from my_package import my_module')
2800     >>> node
2801     <ImportFrom l.1 at 0x7f23b2e415c0>
2802     """"""
2803 
2804     _other_fields = (""modname"", ""names"", ""level"")
2805 
2806     def __init__(
2807         self,
2808         fromname: str | None,
2809         names: list[tuple[str, str | None]],
2810         level: int | None = 0,
2811         lineno: int | None = None,
2812         col_offset: int | None = None,
2813         parent: NodeNG | None = None,
2814         *,
2815         end_lineno: int | None = None,
2816         end_col_offset: int | None = None,
2817     ) -> None:
2818         """"""
2819         :param fromname: The module that is being imported from.
2820 
2821         :param names: What is being imported from the module.
2822 
2823         :param level: The level of relative import.
2824 
2825         :param lineno: The line that this node appears on in the source code.
2826 
2827         :param col_offset: The column that this node appears on in the
2828             source code.
2829 
2830         :param parent: The parent node in the syntax tree.
2831 
2832         :param end_lineno: The last line this node appears on in the source code.
2833 
2834         :param end_col_offset: The end column this node appears on in the
2835             source code. Note: This is after the last symbol.
2836         """"""
2837         self.modname: str | None = fromname  # can be None
2838         """"""The module that is being imported from.
2839 
2840         This is ``None`` for relative imports.
2841         """"""
2842 
2843         self.names: list[tuple[str, str | None]] = names
2844         """"""What is being imported from the module.
2845 
2846         Each entry is a :class:`tuple` of the name being imported,
2847         and the alias that the name is assigned to (if any).
2848         """"""
2849 
2850         # TODO When is 'level' None?
2851         self.level: int | None = level  # can be None
2852         """"""The level of relative import.
2853 
2854         Essentially this is the number of dots in the import.
2855         This is always 0 for absolute imports.
2856         """"""
2857 
2858         super().__init__(
2859             lineno=lineno,
2860             col_offset=col_offset,
2861             end_lineno=end_lineno,
2862             end_col_offset=end_col_offset,
2863             parent=parent,
2864         )
2865 
2866 
2867 class Attribute(NodeNG):
2868     """"""Class representing an :class:`ast.Attribute` node.""""""
2869 
2870     _astroid_fields = (""expr"",)
2871     _other_fields = (""attrname"",)
2872 
2873     @decorators.deprecate_default_argument_values(attrname=""str"")
2874     def __init__(
2875         self,
2876         attrname: str | None = None,
2877         lineno: int | None = None,
2878         col_offset: int | None = None,
2879         parent: NodeNG | None = None,
2880         *,
2881         end_lineno: int | None = None,
2882         end_col_offset: int | None = None,
2883     ) -> None:
2884         """"""
2885         :param attrname: The name of the attribute.
2886 
2887         :param lineno: The line that this node appears on in the source code.
2888 
2889         :param col_offset: The column that this node appears on in the
2890             source code.
2891 
2892         :param parent: The parent node in the syntax tree.
2893 
2894         :param end_lineno: The last line this node appears on in the source code.
2895 
2896         :param end_col_offset: The end column this node appears on in the
2897             source code. Note: This is after the last symbol.
2898         """"""
2899         self.expr: NodeNG | None = None
2900         """"""The name that this node represents.
2901 
2902         :type: Name or None
2903         """"""
2904 
2905         self.attrname: str | None = attrname
2906         """"""The name of the attribute.""""""
2907 
2908         super().__init__(
2909             lineno=lineno,
2910             col_offset=col_offset,
2911             end_lineno=end_lineno,
2912             end_col_offset=end_col_offset,
2913             parent=parent,
2914         )
2915 
2916     def postinit(self, expr: NodeNG | None = None) -> None:
2917         """"""Do some setup after initialisation.
2918 
2919         :param expr: The name that this node represents.
2920         :type expr: Name or None
2921         """"""
2922         self.expr = expr
2923 
2924     def get_children(self):
2925         yield self.expr
2926 
2927 
2928 class Global(mixins.NoChildrenMixin, Statement):
2929     """"""Class representing an :class:`ast.Global` node.
2930 
2931     >>> import astroid
2932     >>> node = astroid.extract_node('global a_global')
2933     >>> node
2934     <Global l.1 at 0x7f23b2e9de10>
2935     """"""
2936 
2937     _other_fields = (""names"",)
2938 
2939     def __init__(
2940         self,
2941         names: list[str],
2942         lineno: int | None = None,
2943         col_offset: int | None = None,
2944         parent: NodeNG | None = None,
2945         *,
2946         end_lineno: int | None = None,
2947         end_col_offset: int | None = None,
2948     ) -> None:
2949         """"""
2950         :param names: The names being declared as global.
2951 
2952         :param lineno: The line that this node appears on in the source code.
2953 
2954         :param col_offset: The column that this node appears on in the
2955             source code.
2956 
2957         :param parent: The parent node in the syntax tree.
2958 
2959         :param end_lineno: The last line this node appears on in the source code.
2960 
2961         :param end_col_offset: The end column this node appears on in the
2962             source code. Note: This is after the last symbol.
2963         """"""
2964         self.names: list[str] = names
2965         """"""The names being declared as global.""""""
2966 
2967         super().__init__(
2968             lineno=lineno,
2969             col_offset=col_offset,
2970             end_lineno=end_lineno,
2971             end_col_offset=end_col_offset,
2972             parent=parent,
2973         )
2974 
2975     def _infer_name(self, frame, name):
2976         return name
2977 
2978 
2979 class If(mixins.MultiLineBlockMixin, mixins.BlockRangeMixIn, Statement):
2980     """"""Class representing an :class:`ast.If` node.
2981 
2982     >>> import astroid
2983     >>> node = astroid.extract_node('if condition: print(True)')
2984     >>> node
2985     <If l.1 at 0x7f23b2e9dd30>
2986     """"""
2987 
2988     _astroid_fields = (""test"", ""body"", ""orelse"")
2989     _multi_line_block_fields = (""body"", ""orelse"")
2990 
2991     def __init__(
2992         self,
2993         lineno: int | None = None,
2994         col_offset: int | None = None,
2995         parent: NodeNG | None = None,
2996         *,
2997         end_lineno: int | None = None,
2998         end_col_offset: int | None = None,
2999     ) -> None:
3000         """"""
3001         :param lineno: The line that this node appears on in the source code.
3002 
3003         :param col_offset: The column that this node appears on in the
3004             source code.
3005 
3006         :param parent: The parent node in the syntax tree.
3007 
3008         :param end_lineno: The last line this node appears on in the source code.
3009 
3010         :param end_col_offset: The end column this node appears on in the
3011             source code. Note: This is after the last symbol.
3012         """"""
3013         self.test: NodeNG | None = None
3014         """"""The condition that the statement tests.""""""
3015 
3016         self.body: list[NodeNG] = []
3017         """"""The contents of the block.""""""
3018 
3019         self.orelse: list[NodeNG] = []
3020         """"""The contents of the ``else`` block.""""""
3021 
3022         self.is_orelse: bool = False
3023         """"""Whether the if-statement is the orelse-block of another if statement.""""""
3024 
3025         super().__init__(
3026             lineno=lineno,
3027             col_offset=col_offset,
3028             end_lineno=end_lineno,
3029             end_col_offset=end_col_offset,
3030             parent=parent,
3031         )
3032 
3033     def postinit(
3034         self,
3035         test: NodeNG | None = None,
3036         body: list[NodeNG] | None = None,
3037         orelse: list[NodeNG] | None = None,
3038     ) -> None:
3039         """"""Do some setup after initialisation.
3040 
3041         :param test: The condition that the statement tests.
3042 
3043         :param body: The contents of the block.
3044 
3045         :param orelse: The contents of the ``else`` block.
3046         """"""
3047         self.test = test
3048         if body is not None:
3049             self.body = body
3050         if orelse is not None:
3051             self.orelse = orelse
3052         if isinstance(self.parent, If) and self in self.parent.orelse:
3053             self.is_orelse = True
3054 
3055     @cached_property
3056     def blockstart_tolineno(self):
3057         """"""The line on which the beginning of this block ends.
3058 
3059         :type: int
3060         """"""
3061         return self.test.tolineno
3062 
3063     def block_range(self, lineno):
3064         """"""Get a range from the given line number to where this node ends.
3065 
3066         :param lineno: The line number to start the range at.
3067         :type lineno: int
3068 
3069         :returns: The range of line numbers that this node belongs to,
3070             starting at the given line number.
3071         :rtype: tuple(int, int)
3072         """"""
3073         if lineno == self.body[0].fromlineno:
3074             return lineno, lineno
3075         if lineno <= self.body[-1].tolineno:
3076             return lineno, self.body[-1].tolineno
3077         return self._elsed_block_range(lineno, self.orelse, self.body[0].fromlineno - 1)
3078 
3079     def get_children(self):
3080         yield self.test
3081 
3082         yield from self.body
3083         yield from self.orelse
3084 
3085     def has_elif_block(self):
3086         return len(self.orelse) == 1 and isinstance(self.orelse[0], If)
3087 
3088     def _get_yield_nodes_skip_lambdas(self):
3089         """"""An If node can contain a Yield node in the test""""""
3090         yield from self.test._get_yield_nodes_skip_lambdas()
3091         yield from super()._get_yield_nodes_skip_lambdas()
3092 
3093     def is_sys_guard(self) -> bool:
3094         """"""Return True if IF stmt is a sys.version_info guard.
3095 
3096         >>> import astroid
3097         >>> node = astroid.extract_node('''
3098         import sys
3099         if sys.version_info > (3, 8):
3100             from typing import Literal
3101         else:
3102             from typing_extensions import Literal
3103         ''')
3104         >>> node.is_sys_guard()
3105         True
3106         """"""
3107         warnings.warn(
3108             ""The 'is_sys_guard' function is deprecated and will be removed in astroid 3.0.0 ""
3109             ""It has been moved to pylint and can be imported from 'pylint.checkers.utils' ""
3110             ""starting with pylint 2.12"",
3111             DeprecationWarning,
3112         )
3113         if isinstance(self.test, Compare):
3114             value = self.test.left
3115             if isinstance(value, Subscript):
3116                 value = value.value
3117             if isinstance(value, Attribute) and value.as_string() == ""sys.version_info"":
3118                 return True
3119 
3120         return False
3121 
3122     def is_typing_guard(self) -> bool:
3123         """"""Return True if IF stmt is a typing guard.
3124 
3125         >>> import astroid
3126         >>> node = astroid.extract_node('''
3127         from typing import TYPE_CHECKING
3128         if TYPE_CHECKING:
3129             from xyz import a
3130         ''')
3131         >>> node.is_typing_guard()
3132         True
3133         """"""
3134         warnings.warn(
3135             ""The 'is_typing_guard' function is deprecated and will be removed in astroid 3.0.0 ""
3136             ""It has been moved to pylint and can be imported from 'pylint.checkers.utils' ""
3137             ""starting with pylint 2.12"",
3138             DeprecationWarning,
3139         )
3140         return isinstance(
3141             self.test, (Name, Attribute)
3142         ) and self.test.as_string().endswith(""TYPE_CHECKING"")
3143 
3144 
3145 class IfExp(NodeNG):
3146     """"""Class representing an :class:`ast.IfExp` node.
3147     >>> import astroid
3148     >>> node = astroid.extract_node('value if condition else other')
3149     >>> node
3150     <IfExp l.1 at 0x7f23b2e9dbe0>
3151     """"""
3152 
3153     _astroid_fields = (""test"", ""body"", ""orelse"")
3154 
3155     def __init__(
3156         self,
3157         lineno: int | None = None,
3158         col_offset: int | None = None,
3159         parent: NodeNG | None = None,
3160         *,
3161         end_lineno: int | None = None,
3162         end_col_offset: int | None = None,
3163     ) -> None:
3164         """"""
3165         :param lineno: The line that this node appears on in the source code.
3166 
3167         :param col_offset: The column that this node appears on in the
3168             source code.
3169 
3170         :param parent: The parent node in the syntax tree.
3171 
3172         :param end_lineno: The last line this node appears on in the source code.
3173 
3174         :param end_col_offset: The end column this node appears on in the
3175             source code. Note: This is after the last symbol.
3176         """"""
3177         self.test: NodeNG | None = None
3178         """"""The condition that the statement tests.""""""
3179 
3180         self.body: NodeNG | None = None
3181         """"""The contents of the block.""""""
3182 
3183         self.orelse: NodeNG | None = None
3184         """"""The contents of the ``else`` block.""""""
3185 
3186         super().__init__(
3187             lineno=lineno,
3188             col_offset=col_offset,
3189             end_lineno=end_lineno,
3190             end_col_offset=end_col_offset,
3191             parent=parent,
3192         )
3193 
3194     def postinit(
3195         self,
3196         test: NodeNG | None = None,
3197         body: NodeNG | None = None,
3198         orelse: NodeNG | None = None,
3199     ) -> None:
3200         """"""Do some setup after initialisation.
3201 
3202         :param test: The condition that the statement tests.
3203 
3204         :param body: The contents of the block.
3205 
3206         :param orelse: The contents of the ``else`` block.
3207         """"""
3208         self.test = test
3209         self.body = body
3210         self.orelse = orelse
3211 
3212     def get_children(self):
3213         yield self.test
3214         yield self.body
3215         yield self.orelse
3216 
3217     def op_left_associative(self):
3218         # `1 if True else 2 if False else 3` is parsed as
3219         # `1 if True else (2 if False else 3)`
3220         return False
3221 
3222 
3223 class Import(mixins.NoChildrenMixin, mixins.ImportFromMixin, Statement):
3224     """"""Class representing an :class:`ast.Import` node.
3225     >>> import astroid
3226     >>> node = astroid.extract_node('import astroid')
3227     >>> node
3228     <Import l.1 at 0x7f23b2e4e5c0>
3229     """"""
3230 
3231     _other_fields = (""names"",)
3232 
3233     @decorators.deprecate_default_argument_values(names=""list[tuple[str, str | None]]"")
3234     def __init__(
3235         self,
3236         names: list[tuple[str, str | None]] | None = None,
3237         lineno: int | None = None,
3238         col_offset: int | None = None,
3239         parent: NodeNG | None = None,
3240         *,
3241         end_lineno: int | None = None,
3242         end_col_offset: int | None = None,
3243     ) -> None:
3244         """"""
3245         :param names: The names being imported.
3246 
3247         :param lineno: The line that this node appears on in the source code.
3248 
3249         :param col_offset: The column that this node appears on in the
3250             source code.
3251 
3252         :param parent: The parent node in the syntax tree.
3253 
3254         :param end_lineno: The last line this node appears on in the source code.
3255 
3256         :param end_col_offset: The end column this node appears on in the
3257             source code. Note: This is after the last symbol.
3258         """"""
3259         self.names: list[tuple[str, str | None]] = names or []
3260         """"""The names being imported.
3261 
3262         Each entry is a :class:`tuple` of the name being imported,
3263         and the alias that the name is assigned to (if any).
3264         """"""
3265 
3266         super().__init__(
3267             lineno=lineno,
3268             col_offset=col_offset,
3269             end_lineno=end_lineno,
3270             end_col_offset=end_col_offset,
3271             parent=parent,
3272         )
3273 
3274 
3275 class Index(NodeNG):
3276     """"""Class representing an :class:`ast.Index` node.
3277 
3278     An :class:`Index` is a simple subscript.
3279 
3280     Deprecated since v2.6.0 - Now part of the :class:`Subscript` node.
3281     Will be removed with the release of v2.7.0
3282     """"""
3283 
3284 
3285 class Keyword(NodeNG):
3286     """"""Class representing an :class:`ast.keyword` node.
3287 
3288     >>> import astroid
3289     >>> node = astroid.extract_node('function(a_kwarg=True)')
3290     >>> node
3291     <Call l.1 at 0x7f23b2e9e320>
3292     >>> node.keywords
3293     [<Keyword l.1 at 0x7f23b2e9e9b0>]
3294     """"""
3295 
3296     _astroid_fields = (""value"",)
3297     _other_fields = (""arg"",)
3298 
3299     def __init__(
3300         self,
3301         arg: str | None = None,
3302         lineno: int | None = None,
3303         col_offset: int | None = None,
3304         parent: NodeNG | None = None,
3305         *,
3306         end_lineno: int | None = None,
3307         end_col_offset: int | None = None,
3308     ) -> None:
3309         """"""
3310         :param arg: The argument being assigned to.
3311 
3312         :param lineno: The line that this node appears on in the source code.
3313 
3314         :param col_offset: The column that this node appears on in the
3315             source code.
3316 
3317         :param parent: The parent node in the syntax tree.
3318 
3319         :param end_lineno: The last line this node appears on in the source code.
3320 
3321         :param end_col_offset: The end column this node appears on in the
3322             source code. Note: This is after the last symbol.
3323         """"""
3324         self.arg: str | None = arg  # can be None
3325         """"""The argument being assigned to.""""""
3326 
3327         self.value: NodeNG | None = None
3328         """"""The value being assigned to the keyword argument.""""""
3329 
3330         super().__init__(
3331             lineno=lineno,
3332             col_offset=col_offset,
3333             end_lineno=end_lineno,
3334             end_col_offset=end_col_offset,
3335             parent=parent,
3336         )
3337 
3338     def postinit(self, value: NodeNG | None = None) -> None:
3339         """"""Do some setup after initialisation.
3340 
3341         :param value: The value being assigned to the keyword argument.
3342         """"""
3343         self.value = value
3344 
3345     def get_children(self):
3346         yield self.value
3347 
3348 
3349 class List(BaseContainer):
3350     """"""Class representing an :class:`ast.List` node.
3351 
3352     >>> import astroid
3353     >>> node = astroid.extract_node('[1, 2, 3]')
3354     >>> node
3355     <List.list l.1 at 0x7f23b2e9e128>
3356     """"""
3357 
3358     _other_fields = (""ctx"",)
3359 
3360     def __init__(
3361         self,
3362         ctx: Context | None = None,
3363         lineno: int | None = None,
3364         col_offset: int | None = None,
3365         parent: NodeNG | None = None,
3366         *,
3367         end_lineno: int | None = None,
3368         end_col_offset: int | None = None,
3369     ) -> None:
3370         """"""
3371         :param ctx: Whether the list is assigned to or loaded from.
3372 
3373         :param lineno: The line that this node appears on in the source code.
3374 
3375         :param col_offset: The column that this node appears on in the
3376             source code.
3377 
3378         :param parent: The parent node in the syntax tree.
3379 
3380         :param end_lineno: The last line this node appears on in the source code.
3381 
3382         :param end_col_offset: The end column this node appears on in the
3383             source code. Note: This is after the last symbol.
3384         """"""
3385         self.ctx: Context | None = ctx
3386         """"""Whether the list is assigned to or loaded from.""""""
3387 
3388         super().__init__(
3389             lineno=lineno,
3390             col_offset=col_offset,
3391             end_lineno=end_lineno,
3392             end_col_offset=end_col_offset,
3393             parent=parent,
3394         )
3395 
3396     assigned_stmts: ClassVar[AssignedStmtsCall[List]]
3397     """"""Returns the assigned statement (non inferred) according to the assignment type.
3398     See astroid/protocols.py for actual implementation.
3399     """"""
3400 
3401     def pytype(self):
3402         """"""Get the name of the type that this node represents.
3403 
3404         :returns: The name of the type.
3405         :rtype: str
3406         """"""
3407         return ""builtins.list""
3408 
3409     def getitem(self, index, context=None):
3410         """"""Get an item from this node.
3411 
3412         :param index: The node to use as a subscript index.
3413         :type index: Const or Slice
3414         """"""
3415         return _container_getitem(self, self.elts, index, context=context)
3416 
3417 
3418 class Nonlocal(mixins.NoChildrenMixin, Statement):
3419     """"""Class representing an :class:`ast.Nonlocal` node.
3420 
3421     >>> import astroid
3422     >>> node = astroid.extract_node('''
3423     def function():
3424         nonlocal var
3425     ''')
3426     >>> node
3427     <FunctionDef.function l.2 at 0x7f23b2e9e208>
3428     >>> node.body[0]
3429     <Nonlocal l.3 at 0x7f23b2e9e908>
3430     """"""
3431 
3432     _other_fields = (""names"",)
3433 
3434     def __init__(
3435         self,
3436         names: list[str],
3437         lineno: int | None = None,
3438         col_offset: int | None = None,
3439         parent: NodeNG | None = None,
3440         *,
3441         end_lineno: int | None = None,
3442         end_col_offset: int | None = None,
3443     ) -> None:
3444         """"""
3445         :param names: The names being declared as not local.
3446 
3447         :param lineno: The line that this node appears on in the source code.
3448 
3449         :param col_offset: The column that this node appears on in the
3450             source code.
3451 
3452         :param parent: The parent node in the syntax tree.
3453 
3454         :param end_lineno: The last line this node appears on in the source code.
3455 
3456         :param end_col_offset: The end column this node appears on in the
3457             source code. Note: This is after the last symbol.
3458         """"""
3459         self.names: list[str] = names
3460         """"""The names being declared as not local.""""""
3461 
3462         super().__init__(
3463             lineno=lineno,
3464             col_offset=col_offset,
3465             end_lineno=end_lineno,
3466             end_col_offset=end_col_offset,
3467             parent=parent,
3468         )
3469 
3470     def _infer_name(self, frame, name):
3471         return name
3472 
3473 
3474 class Pass(mixins.NoChildrenMixin, Statement):
3475     """"""Class representing an :class:`ast.Pass` node.
3476 
3477     >>> import astroid
3478     >>> node = astroid.extract_node('pass')
3479     >>> node
3480     <Pass l.1 at 0x7f23b2e9e748>
3481     """"""
3482 
3483 
3484 class Raise(Statement):
3485     """"""Class representing an :class:`ast.Raise` node.
3486 
3487     >>> import astroid
3488     >>> node = astroid.extract_node('raise RuntimeError(""Something bad happened!"")')
3489     >>> node
3490     <Raise l.1 at 0x7f23b2e9e828>
3491     """"""
3492 
3493     _astroid_fields = (""exc"", ""cause"")
3494 
3495     def __init__(
3496         self,
3497         lineno: int | None = None,
3498         col_offset: int | None = None,
3499         parent: NodeNG | None = None,
3500         *,
3501         end_lineno: int | None = None,
3502         end_col_offset: int | None = None,
3503     ) -> None:
3504         """"""
3505         :param lineno: The line that this node appears on in the source code.
3506 
3507         :param col_offset: The column that this node appears on in the
3508             source code.
3509 
3510         :param parent: The parent node in the syntax tree.
3511 
3512         :param end_lineno: The last line this node appears on in the source code.
3513 
3514         :param end_col_offset: The end column this node appears on in the
3515             source code. Note: This is after the last symbol.
3516         """"""
3517         self.exc: NodeNG | None = None  # can be None
3518         """"""What is being raised.""""""
3519 
3520         self.cause: NodeNG | None = None  # can be None
3521         """"""The exception being used to raise this one.""""""
3522 
3523         super().__init__(
3524             lineno=lineno,
3525             col_offset=col_offset,
3526             end_lineno=end_lineno,
3527             end_col_offset=end_col_offset,
3528             parent=parent,
3529         )
3530 
3531     def postinit(
3532         self,
3533         exc: NodeNG | None = None,
3534         cause: NodeNG | None = None,
3535     ) -> None:
3536         """"""Do some setup after initialisation.
3537 
3538         :param exc: What is being raised.
3539 
3540         :param cause: The exception being used to raise this one.
3541         """"""
3542         self.exc = exc
3543         self.cause = cause
3544 
3545     def raises_not_implemented(self):
3546         """"""Check if this node raises a :class:`NotImplementedError`.
3547 
3548         :returns: True if this node raises a :class:`NotImplementedError`,
3549             False otherwise.
3550         :rtype: bool
3551         """"""
3552         if not self.exc:
3553             return False
3554         return any(
3555             name.name == ""NotImplementedError"" for name in self.exc._get_name_nodes()
3556         )
3557 
3558     def get_children(self):
3559         if self.exc is not None:
3560             yield self.exc
3561 
3562         if self.cause is not None:
3563             yield self.cause
3564 
3565 
3566 class Return(Statement):
3567     """"""Class representing an :class:`ast.Return` node.
3568 
3569     >>> import astroid
3570     >>> node = astroid.extract_node('return True')
3571     >>> node
3572     <Return l.1 at 0x7f23b8211908>
3573     """"""
3574 
3575     _astroid_fields = (""value"",)
3576 
3577     def __init__(
3578         self,
3579         lineno: int | None = None,
3580         col_offset: int | None = None,
3581         parent: NodeNG | None = None,
3582         *,
3583         end_lineno: int | None = None,
3584         end_col_offset: int | None = None,
3585     ) -> None:
3586         """"""
3587         :param lineno: The line that this node appears on in the source code.
3588 
3589         :param col_offset: The column that this node appears on in the
3590             source code.
3591 
3592         :param parent: The parent node in the syntax tree.
3593 
3594         :param end_lineno: The last line this node appears on in the source code.
3595 
3596         :param end_col_offset: The end column this node appears on in the
3597             source code. Note: This is after the last symbol.
3598         """"""
3599         self.value: NodeNG | None = None  # can be None
3600         """"""The value being returned.""""""
3601 
3602         super().__init__(
3603             lineno=lineno,
3604             col_offset=col_offset,
3605             end_lineno=end_lineno,
3606             end_col_offset=end_col_offset,
3607             parent=parent,
3608         )
3609 
3610     def postinit(self, value: NodeNG | None = None) -> None:
3611         """"""Do some setup after initialisation.
3612 
3613         :param value: The value being returned.
3614         """"""
3615         self.value = value
3616 
3617     def get_children(self):
3618         if self.value is not None:
3619             yield self.value
3620 
3621     def is_tuple_return(self):
3622         return isinstance(self.value, Tuple)
3623 
3624     def _get_return_nodes_skip_functions(self):
3625         yield self
3626 
3627 
3628 class Set(BaseContainer):
3629     """"""Class representing an :class:`ast.Set` node.
3630 
3631     >>> import astroid
3632     >>> node = astroid.extract_node('{1, 2, 3}')
3633     >>> node
3634     <Set.set l.1 at 0x7f23b2e71d68>
3635     """"""
3636 
3637     def pytype(self):
3638         """"""Get the name of the type that this node represents.
3639 
3640         :returns: The name of the type.
3641         :rtype: str
3642         """"""
3643         return ""builtins.set""
3644 
3645 
3646 class Slice(NodeNG):
3647     """"""Class representing an :class:`ast.Slice` node.
3648 
3649     >>> import astroid
3650     >>> node = astroid.extract_node('things[1:3]')
3651     >>> node
3652     <Subscript l.1 at 0x7f23b2e71f60>
3653     >>> node.slice
3654     <Slice l.1 at 0x7f23b2e71e80>
3655     """"""
3656 
3657     _astroid_fields = (""lower"", ""upper"", ""step"")
3658 
3659     def __init__(
3660         self,
3661         lineno: int | None = None,
3662         col_offset: int | None = None,
3663         parent: NodeNG | None = None,
3664         *,
3665         end_lineno: int | None = None,
3666         end_col_offset: int | None = None,
3667     ) -> None:
3668         """"""
3669         :param lineno: The line that this node appears on in the source code.
3670 
3671         :param col_offset: The column that this node appears on in the
3672             source code.
3673 
3674         :param parent: The parent node in the syntax tree.
3675 
3676         :param end_lineno: The last line this node appears on in the source code.
3677 
3678         :param end_col_offset: The end column this node appears on in the
3679             source code. Note: This is after the last symbol.
3680         """"""
3681         self.lower: NodeNG | None = None  # can be None
3682         """"""The lower index in the slice.""""""
3683 
3684         self.upper: NodeNG | None = None  # can be None
3685         """"""The upper index in the slice.""""""
3686 
3687         self.step: NodeNG | None = None  # can be None
3688         """"""The step to take between indexes.""""""
3689 
3690         super().__init__(
3691             lineno=lineno,
3692             col_offset=col_offset,
3693             end_lineno=end_lineno,
3694             end_col_offset=end_col_offset,
3695             parent=parent,
3696         )
3697 
3698     def postinit(
3699         self,
3700         lower: NodeNG | None = None,
3701         upper: NodeNG | None = None,
3702         step: NodeNG | None = None,
3703     ) -> None:
3704         """"""Do some setup after initialisation.
3705 
3706         :param lower: The lower index in the slice.
3707 
3708         :param upper: The upper index in the slice.
3709 
3710         :param step: The step to take between index.
3711         """"""
3712         self.lower = lower
3713         self.upper = upper
3714         self.step = step
3715 
3716     def _wrap_attribute(self, attr):
3717         """"""Wrap the empty attributes of the Slice in a Const node.""""""
3718         if not attr:
3719             const = const_factory(attr)
3720             const.parent = self
3721             return const
3722         return attr
3723 
3724     @cached_property
3725     def _proxied(self):
3726         builtins = AstroidManager().builtins_module
3727         return builtins.getattr(""slice"")[0]
3728 
3729     def pytype(self):
3730         """"""Get the name of the type that this node represents.
3731 
3732         :returns: The name of the type.
3733         :rtype: str
3734         """"""
3735         return ""builtins.slice""
3736 
3737     def igetattr(self, attrname, context=None):
3738         """"""Infer the possible values of the given attribute on the slice.
3739 
3740         :param attrname: The name of the attribute to infer.
3741         :type attrname: str
3742 
3743         :returns: The inferred possible values.
3744         :rtype: iterable(NodeNG)
3745         """"""
3746         if attrname == ""start"":
3747             yield self._wrap_attribute(self.lower)
3748         elif attrname == ""stop"":
3749             yield self._wrap_attribute(self.upper)
3750         elif attrname == ""step"":
3751             yield self._wrap_attribute(self.step)
3752         else:
3753             yield from self.getattr(attrname, context=context)
3754 
3755     def getattr(self, attrname, context=None):
3756         return self._proxied.getattr(attrname, context)
3757 
3758     def get_children(self):
3759         if self.lower is not None:
3760             yield self.lower
3761 
3762         if self.upper is not None:
3763             yield self.upper
3764 
3765         if self.step is not None:
3766             yield self.step
3767 
3768 
3769 class Starred(mixins.ParentAssignTypeMixin, NodeNG):
3770     """"""Class representing an :class:`ast.Starred` node.
3771 
3772     >>> import astroid
3773     >>> node = astroid.extract_node('*args')
3774     >>> node
3775     <Starred l.1 at 0x7f23b2e41978>
3776     """"""
3777 
3778     _astroid_fields = (""value"",)
3779     _other_fields = (""ctx"",)
3780 
3781     def __init__(
3782         self,
3783         ctx: Context | None = None,
3784         lineno: int | None = None,
3785         col_offset: int | None = None,
3786         parent: NodeNG | None = None,
3787         *,
3788         end_lineno: int | None = None,
3789         end_col_offset: int | None = None,
3790     ) -> None:
3791         """"""
3792         :param ctx: Whether the list is assigned to or loaded from.
3793 
3794         :param lineno: The line that this node appears on in the source code.
3795 
3796         :param col_offset: The column that this node appears on in the
3797             source code.
3798 
3799         :param parent: The parent node in the syntax tree.
3800 
3801         :param end_lineno: The last line this node appears on in the source code.
3802 
3803         :param end_col_offset: The end column this node appears on in the
3804             source code. Note: This is after the last symbol.
3805         """"""
3806         self.value: NodeNG | None = None
3807         """"""What is being unpacked.""""""
3808 
3809         self.ctx: Context | None = ctx
3810         """"""Whether the starred item is assigned to or loaded from.""""""
3811 
3812         super().__init__(
3813             lineno=lineno,
3814             col_offset=col_offset,
3815             end_lineno=end_lineno,
3816             end_col_offset=end_col_offset,
3817             parent=parent,
3818         )
3819 
3820     def postinit(self, value: NodeNG | None = None) -> None:
3821         """"""Do some setup after initialisation.
3822 
3823         :param value: What is being unpacked.
3824         """"""
3825         self.value = value
3826 
3827     assigned_stmts: ClassVar[AssignedStmtsCall[Starred]]
3828     """"""Returns the assigned statement (non inferred) according to the assignment type.
3829     See astroid/protocols.py for actual implementation.
3830     """"""
3831 
3832     def get_children(self):
3833         yield self.value
3834 
3835 
3836 class Subscript(NodeNG):
3837     """"""Class representing an :class:`ast.Subscript` node.
3838 
3839     >>> import astroid
3840     >>> node = astroid.extract_node('things[1:3]')
3841     >>> node
3842     <Subscript l.1 at 0x7f23b2e71f60>
3843     """"""
3844 
3845     _astroid_fields = (""value"", ""slice"")
3846     _other_fields = (""ctx"",)
3847 
3848     def __init__(
3849         self,
3850         ctx: Context | None = None,
3851         lineno: int | None = None,
3852         col_offset: int | None = None,
3853         parent: NodeNG | None = None,
3854         *,
3855         end_lineno: int | None = None,
3856         end_col_offset: int | None = None,
3857     ) -> None:
3858         """"""
3859         :param ctx: Whether the subscripted item is assigned to or loaded from.
3860 
3861         :param lineno: The line that this node appears on in the source code.
3862 
3863         :param col_offset: The column that this node appears on in the
3864             source code.
3865 
3866         :param parent: The parent node in the syntax tree.
3867 
3868         :param end_lineno: The last line this node appears on in the source code.
3869 
3870         :param end_col_offset: The end column this node appears on in the
3871             source code. Note: This is after the last symbol.
3872         """"""
3873         self.value: NodeNG | None = None
3874         """"""What is being indexed.""""""
3875 
3876         self.slice: NodeNG | None = None
3877         """"""The slice being used to lookup.""""""
3878 
3879         self.ctx: Context | None = ctx
3880         """"""Whether the subscripted item is assigned to or loaded from.""""""
3881 
3882         super().__init__(
3883             lineno=lineno,
3884             col_offset=col_offset,
3885             end_lineno=end_lineno,
3886             end_col_offset=end_col_offset,
3887             parent=parent,
3888         )
3889 
3890     # pylint: disable=redefined-builtin; had to use the same name as builtin ast module.
3891     def postinit(
3892         self, value: NodeNG | None = None, slice: NodeNG | None = None
3893     ) -> None:
3894         """"""Do some setup after initialisation.
3895 
3896         :param value: What is being indexed.
3897 
3898         :param slice: The slice being used to lookup.
3899         """"""
3900         self.value = value
3901         self.slice = slice
3902 
3903     def get_children(self):
3904         yield self.value
3905         yield self.slice
3906 
3907 
3908 class TryExcept(mixins.MultiLineBlockMixin, mixins.BlockRangeMixIn, Statement):
3909     """"""Class representing an :class:`ast.TryExcept` node.
3910 
3911     >>> import astroid
3912     >>> node = astroid.extract_node('''
3913         try:
3914             do_something()
3915         except Exception as error:
3916             print(""Error!"")
3917         ''')
3918     >>> node
3919     <TryExcept l.2 at 0x7f23b2e9d908>
3920     """"""
3921 
3922     _astroid_fields = (""body"", ""handlers"", ""orelse"")
3923     _multi_line_block_fields = (""body"", ""handlers"", ""orelse"")
3924 
3925     def __init__(
3926         self,
3927         lineno: int | None = None,
3928         col_offset: int | None = None,
3929         parent: NodeNG | None = None,
3930         *,
3931         end_lineno: int | None = None,
3932         end_col_offset: int | None = None,
3933     ) -> None:
3934         """"""
3935         :param lineno: The line that this node appears on in the source code.
3936 
3937         :param col_offset: The column that this node appears on in the
3938             source code.
3939 
3940         :param parent: The parent node in the syntax tree.
3941 
3942         :param end_lineno: The last line this node appears on in the source code.
3943 
3944         :param end_col_offset: The end column this node appears on in the
3945             source code. Note: This is after the last symbol.
3946         """"""
3947         self.body: list[NodeNG] = []
3948         """"""The contents of the block to catch exceptions from.""""""
3949 
3950         self.handlers: list[ExceptHandler] = []
3951         """"""The exception handlers.""""""
3952 
3953         self.orelse: list[NodeNG] = []
3954         """"""The contents of the ``else`` block.""""""
3955 
3956         super().__init__(
3957             lineno=lineno,
3958             col_offset=col_offset,
3959             end_lineno=end_lineno,
3960             end_col_offset=end_col_offset,
3961             parent=parent,
3962         )
3963 
3964     def postinit(
3965         self,
3966         body: list[NodeNG] | None = None,
3967         handlers: list[ExceptHandler] | None = None,
3968         orelse: list[NodeNG] | None = None,
3969     ) -> None:
3970         """"""Do some setup after initialisation.
3971 
3972         :param body: The contents of the block to catch exceptions from.
3973 
3974         :param handlers: The exception handlers.
3975 
3976         :param orelse: The contents of the ``else`` block.
3977         """"""
3978         if body is not None:
3979             self.body = body
3980         if handlers is not None:
3981             self.handlers = handlers
3982         if orelse is not None:
3983             self.orelse = orelse
3984 
3985     def _infer_name(self, frame, name):
3986         return name
3987 
3988     def block_range(self, lineno):
3989         """"""Get a range from the given line number to where this node ends.
3990 
3991         :param lineno: The line number to start the range at.
3992         :type lineno: int
3993 
3994         :returns: The range of line numbers that this node belongs to,
3995             starting at the given line number.
3996         :rtype: tuple(int, int)
3997         """"""
3998         last = None
3999         for exhandler in self.handlers:
4000             if exhandler.type and lineno == exhandler.type.fromlineno:
4001                 return lineno, lineno
4002             if exhandler.body[0].fromlineno <= lineno <= exhandler.body[-1].tolineno:
4003                 return lineno, exhandler.body[-1].tolineno
4004             if last is None:
4005                 last = exhandler.body[0].fromlineno - 1
4006         return self._elsed_block_range(lineno, self.orelse, last)
4007 
4008     def get_children(self):
4009         yield from self.body
4010 
4011         yield from self.handlers or ()
4012         yield from self.orelse or ()
4013 
4014 
4015 class TryFinally(mixins.MultiLineBlockMixin, mixins.BlockRangeMixIn, Statement):
4016     """"""Class representing an :class:`ast.TryFinally` node.
4017 
4018     >>> import astroid
4019     >>> node = astroid.extract_node('''
4020     try:
4021         do_something()
4022     except Exception as error:
4023         print(""Error!"")
4024     finally:
4025         print(""Cleanup!"")
4026     ''')
4027     >>> node
4028     <TryFinally l.2 at 0x7f23b2e41d68>
4029     """"""
4030 
4031     _astroid_fields = (""body"", ""finalbody"")
4032     _multi_line_block_fields = (""body"", ""finalbody"")
4033 
4034     def __init__(
4035         self,
4036         lineno: int | None = None,
4037         col_offset: int | None = None,
4038         parent: NodeNG | None = None,
4039         *,
4040         end_lineno: int | None = None,
4041         end_col_offset: int | None = None,
4042     ) -> None:
4043         """"""
4044         :param lineno: The line that this node appears on in the source code.
4045 
4046         :param col_offset: The column that this node appears on in the
4047             source code.
4048 
4049         :param parent: The parent node in the syntax tree.
4050 
4051         :param end_lineno: The last line this node appears on in the source code.
4052 
4053         :param end_col_offset: The end column this node appears on in the
4054             source code. Note: This is after the last symbol.
4055         """"""
4056         self.body: list[NodeNG | TryExcept] = []
4057         """"""The try-except that the finally is attached to.""""""
4058 
4059         self.finalbody: list[NodeNG] = []
4060         """"""The contents of the ``finally`` block.""""""
4061 
4062         super().__init__(
4063             lineno=lineno,
4064             col_offset=col_offset,
4065             end_lineno=end_lineno,
4066             end_col_offset=end_col_offset,
4067             parent=parent,
4068         )
4069 
4070     def postinit(
4071         self,
4072         body: list[NodeNG | TryExcept] | None = None,
4073         finalbody: list[NodeNG] | None = None,
4074     ) -> None:
4075         """"""Do some setup after initialisation.
4076 
4077         :param body: The try-except that the finally is attached to.
4078 
4079         :param finalbody: The contents of the ``finally`` block.
4080         """"""
4081         if body is not None:
4082             self.body = body
4083         if finalbody is not None:
4084             self.finalbody = finalbody
4085 
4086     def block_range(self, lineno):
4087         """"""Get a range from the given line number to where this node ends.
4088 
4089         :param lineno: The line number to start the range at.
4090         :type lineno: int
4091 
4092         :returns: The range of line numbers that this node belongs to,
4093             starting at the given line number.
4094         :rtype: tuple(int, int)
4095         """"""
4096         child = self.body[0]
4097         # py2.5 try: except: finally:
4098         if (
4099             isinstance(child, TryExcept)
4100             and child.fromlineno == self.fromlineno
4101             and child.tolineno >= lineno > self.fromlineno
4102         ):
4103             return child.block_range(lineno)
4104         return self._elsed_block_range(lineno, self.finalbody)
4105 
4106     def get_children(self):
4107         yield from self.body
4108         yield from self.finalbody
4109 
4110 
4111 class Tuple(BaseContainer):
4112     """"""Class representing an :class:`ast.Tuple` node.
4113 
4114     >>> import astroid
4115     >>> node = astroid.extract_node('(1, 2, 3)')
4116     >>> node
4117     <Tuple.tuple l.1 at 0x7f23b2e41780>
4118     """"""
4119 
4120     _other_fields = (""ctx"",)
4121 
4122     def __init__(
4123         self,
4124         ctx: Context | None = None,
4125         lineno: int | None = None,
4126         col_offset: int | None = None,
4127         parent: NodeNG | None = None,
4128         *,
4129         end_lineno: int | None = None,
4130         end_col_offset: int | None = None,
4131     ) -> None:
4132         """"""
4133         :param ctx: Whether the tuple is assigned to or loaded from.
4134 
4135         :param lineno: The line that this node appears on in the source code.
4136 
4137         :param col_offset: The column that this node appears on in the
4138             source code.
4139 
4140         :param parent: The parent node in the syntax tree.
4141 
4142         :param end_lineno: The last line this node appears on in the source code.
4143 
4144         :param end_col_offset: The end column this node appears on in the
4145             source code. Note: This is after the last symbol.
4146         """"""
4147         self.ctx: Context | None = ctx
4148         """"""Whether the tuple is assigned to or loaded from.""""""
4149 
4150         super().__init__(
4151             lineno=lineno,
4152             col_offset=col_offset,
4153             end_lineno=end_lineno,
4154             end_col_offset=end_col_offset,
4155             parent=parent,
4156         )
4157 
4158     assigned_stmts: ClassVar[AssignedStmtsCall[Tuple]]
4159     """"""Returns the assigned statement (non inferred) according to the assignment type.
4160     See astroid/protocols.py for actual implementation.
4161     """"""
4162 
4163     def pytype(self):
4164         """"""Get the name of the type that this node represents.
4165 
4166         :returns: The name of the type.
4167         :rtype: str
4168         """"""
4169         return ""builtins.tuple""
4170 
4171     def getitem(self, index, context=None):
4172         """"""Get an item from this node.
4173 
4174         :param index: The node to use as a subscript index.
4175         :type index: Const or Slice
4176         """"""
4177         return _container_getitem(self, self.elts, index, context=context)
4178 
4179 
4180 class UnaryOp(NodeNG):
4181     """"""Class representing an :class:`ast.UnaryOp` node.
4182 
4183     >>> import astroid
4184     >>> node = astroid.extract_node('-5')
4185     >>> node
4186     <UnaryOp l.1 at 0x7f23b2e4e198>
4187     """"""
4188 
4189     _astroid_fields = (""operand"",)
4190     _other_fields = (""op"",)
4191 
4192     @decorators.deprecate_default_argument_values(op=""str"")
4193     def __init__(
4194         self,
4195         op: str | None = None,
4196         lineno: int | None = None,
4197         col_offset: int | None = None,
4198         parent: NodeNG | None = None,
4199         *,
4200         end_lineno: int | None = None,
4201         end_col_offset: int | None = None,
4202     ) -> None:
4203         """"""
4204         :param op: The operator.
4205 
4206         :param lineno: The line that this node appears on in the source code.
4207 
4208         :param col_offset: The column that this node appears on in the
4209             source code.
4210 
4211         :param parent: The parent node in the syntax tree.
4212 
4213         :param end_lineno: The last line this node appears on in the source code.
4214 
4215         :param end_col_offset: The end column this node appears on in the
4216             source code. Note: This is after the last symbol.
4217         """"""
4218         self.op: str | None = op
4219         """"""The operator.""""""
4220 
4221         self.operand: NodeNG | None = None
4222         """"""What the unary operator is applied to.""""""
4223 
4224         super().__init__(
4225             lineno=lineno,
4226             col_offset=col_offset,
4227             end_lineno=end_lineno,
4228             end_col_offset=end_col_offset,
4229             parent=parent,
4230         )
4231 
4232     def postinit(self, operand: NodeNG | None = None) -> None:
4233         """"""Do some setup after initialisation.
4234 
4235         :param operand: What the unary operator is applied to.
4236         """"""
4237         self.operand = operand
4238 
4239     # This is set by inference.py
4240     def _infer_unaryop(self, context=None):
4241         raise NotImplementedError
4242 
4243     def type_errors(self, context=None):
4244         """"""Get a list of type errors which can occur during inference.
4245 
4246         Each TypeError is represented by a :class:`BadBinaryOperationMessage`,
4247         which holds the original exception.
4248 
4249         :returns: The list of possible type errors.
4250         :rtype: list(BadBinaryOperationMessage)
4251         """"""
4252         try:
4253             results = self._infer_unaryop(context=context)
4254             return [
4255                 result
4256                 for result in results
4257                 if isinstance(result, util.BadUnaryOperationMessage)
4258             ]
4259         except InferenceError:
4260             return []
4261 
4262     def get_children(self):
4263         yield self.operand
4264 
4265     def op_precedence(self):
4266         if self.op == ""not"":
4267             return OP_PRECEDENCE[self.op]
4268 
4269         return super().op_precedence()
4270 
4271 
4272 class While(mixins.MultiLineBlockMixin, mixins.BlockRangeMixIn, Statement):
4273     """"""Class representing an :class:`ast.While` node.
4274 
4275     >>> import astroid
4276     >>> node = astroid.extract_node('''
4277     while condition():
4278         print(""True"")
4279     ''')
4280     >>> node
4281     <While l.2 at 0x7f23b2e4e390>
4282     """"""
4283 
4284     _astroid_fields = (""test"", ""body"", ""orelse"")
4285     _multi_line_block_fields = (""body"", ""orelse"")
4286 
4287     def __init__(
4288         self,
4289         lineno: int | None = None,
4290         col_offset: int | None = None,
4291         parent: NodeNG | None = None,
4292         *,
4293         end_lineno: int | None = None,
4294         end_col_offset: int | None = None,
4295     ) -> None:
4296         """"""
4297         :param lineno: The line that this node appears on in the source code.
4298 
4299         :param col_offset: The column that this node appears on in the
4300             source code.
4301 
4302         :param parent: The parent node in the syntax tree.
4303 
4304         :param end_lineno: The last line this node appears on in the source code.
4305 
4306         :param end_col_offset: The end column this node appears on in the
4307             source code. Note: This is after the last symbol.
4308         """"""
4309         self.test: NodeNG | None = None
4310         """"""The condition that the loop tests.""""""
4311 
4312         self.body: list[NodeNG] = []
4313         """"""The contents of the loop.""""""
4314 
4315         self.orelse: list[NodeNG] = []
4316         """"""The contents of the ``else`` block.""""""
4317 
4318         super().__init__(
4319             lineno=lineno,
4320             col_offset=col_offset,
4321             end_lineno=end_lineno,
4322             end_col_offset=end_col_offset,
4323             parent=parent,
4324         )
4325 
4326     def postinit(
4327         self,
4328         test: NodeNG | None = None,
4329         body: list[NodeNG] | None = None,
4330         orelse: list[NodeNG] | None = None,
4331     ) -> None:
4332         """"""Do some setup after initialisation.
4333 
4334         :param test: The condition that the loop tests.
4335 
4336         :param body: The contents of the loop.
4337 
4338         :param orelse: The contents of the ``else`` block.
4339         """"""
4340         self.test = test
4341         if body is not None:
4342             self.body = body
4343         if orelse is not None:
4344             self.orelse = orelse
4345 
4346     @cached_property
4347     def blockstart_tolineno(self):
4348         """"""The line on which the beginning of this block ends.
4349 
4350         :type: int
4351         """"""
4352         return self.test.tolineno
4353 
4354     def block_range(self, lineno):
4355         """"""Get a range from the given line number to where this node ends.
4356 
4357         :param lineno: The line number to start the range at.
4358         :type lineno: int
4359 
4360         :returns: The range of line numbers that this node belongs to,
4361             starting at the given line number.
4362         :rtype: tuple(int, int)
4363         """"""
4364         return self._elsed_block_range(lineno, self.orelse)
4365 
4366     def get_children(self):
4367         yield self.test
4368 
4369         yield from self.body
4370         yield from self.orelse
4371 
4372     def _get_yield_nodes_skip_lambdas(self):
4373         """"""A While node can contain a Yield node in the test""""""
4374         yield from self.test._get_yield_nodes_skip_lambdas()
4375         yield from super()._get_yield_nodes_skip_lambdas()
4376 
4377 
4378 class With(
4379     mixins.MultiLineBlockMixin,
4380     mixins.BlockRangeMixIn,
4381     mixins.AssignTypeMixin,
4382     Statement,
4383 ):
4384     """"""Class representing an :class:`ast.With` node.
4385 
4386     >>> import astroid
4387     >>> node = astroid.extract_node('''
4388     with open(file_path) as file_:
4389         print(file_.read())
4390     ''')
4391     >>> node
4392     <With l.2 at 0x7f23b2e4e710>
4393     """"""
4394 
4395     _astroid_fields = (""items"", ""body"")
4396     _other_other_fields = (""type_annotation"",)
4397     _multi_line_block_fields = (""body"",)
4398 
4399     def __init__(
4400         self,
4401         lineno: int | None = None,
4402         col_offset: int | None = None,
4403         parent: NodeNG | None = None,
4404         *,
4405         end_lineno: int | None = None,
4406         end_col_offset: int | None = None,
4407     ) -> None:
4408         """"""
4409         :param lineno: The line that this node appears on in the source code.
4410 
4411         :param col_offset: The column that this node appears on in the
4412             source code.
4413 
4414         :param parent: The parent node in the syntax tree.
4415 
4416         :param end_lineno: The last line this node appears on in the source code.
4417 
4418         :param end_col_offset: The end column this node appears on in the
4419             source code. Note: This is after the last symbol.
4420         """"""
4421         self.items: list[tuple[NodeNG, NodeNG | None]] = []
4422         """"""The pairs of context managers and the names they are assigned to.""""""
4423 
4424         self.body: list[NodeNG] = []
4425         """"""The contents of the ``with`` block.""""""
4426 
4427         self.type_annotation: NodeNG | None = None  # can be None
4428         """"""If present, this will contain the type annotation passed by a type comment""""""
4429 
4430         super().__init__(
4431             lineno=lineno,
4432             col_offset=col_offset,
4433             end_lineno=end_lineno,
4434             end_col_offset=end_col_offset,
4435             parent=parent,
4436         )
4437 
4438     def postinit(
4439         self,
4440         items: list[tuple[NodeNG, NodeNG | None]] | None = None,
4441         body: list[NodeNG] | None = None,
4442         type_annotation: NodeNG | None = None,
4443     ) -> None:
4444         """"""Do some setup after initialisation.
4445 
4446         :param items: The pairs of context managers and the names
4447             they are assigned to.
4448 
4449         :param body: The contents of the ``with`` block.
4450         """"""
4451         if items is not None:
4452             self.items = items
4453         if body is not None:
4454             self.body = body
4455         self.type_annotation = type_annotation
4456 
4457     assigned_stmts: ClassVar[AssignedStmtsCall[With]]
4458     """"""Returns the assigned statement (non inferred) according to the assignment type.
4459     See astroid/protocols.py for actual implementation.
4460     """"""
4461 
4462     @cached_property
4463     def blockstart_tolineno(self):
4464         """"""The line on which the beginning of this block ends.
4465 
4466         :type: int
4467         """"""
4468         return self.items[-1][0].tolineno
4469 
4470     def get_children(self):
4471         """"""Get the child nodes below this node.
4472 
4473         :returns: The children.
4474         :rtype: iterable(NodeNG)
4475         """"""
4476         for expr, var in self.items:
4477             yield expr
4478             if var:
4479                 yield var
4480         yield from self.body
4481 
4482 
4483 class AsyncWith(With):
4484     """"""Asynchronous ``with`` built with the ``async`` keyword.""""""
4485 
4486 
4487 class Yield(NodeNG):
4488     """"""Class representing an :class:`ast.Yield` node.
4489 
4490     >>> import astroid
4491     >>> node = astroid.extract_node('yield True')
4492     >>> node
4493     <Yield l.1 at 0x7f23b2e4e5f8>
4494     """"""
4495 
4496     _astroid_fields = (""value"",)
4497 
4498     def __init__(
4499         self,
4500         lineno: int | None = None,
4501         col_offset: int | None = None,
4502         parent: NodeNG | None = None,
4503         *,
4504         end_lineno: int | None = None,
4505         end_col_offset: int | None = None,
4506     ) -> None:
4507         """"""
4508         :param lineno: The line that this node appears on in the source code.
4509 
4510         :param col_offset: The column that this node appears on in the
4511             source code.
4512 
4513         :param parent: The parent node in the syntax tree.
4514 
4515         :param end_lineno: The last line this node appears on in the source code.
4516 
4517         :param end_col_offset: The end column this node appears on in the
4518             source code. Note: This is after the last symbol.
4519         """"""
4520         self.value: NodeNG | None = None  # can be None
4521         """"""The value to yield.""""""
4522 
4523         super().__init__(
4524             lineno=lineno,
4525             col_offset=col_offset,
4526             end_lineno=end_lineno,
4527             end_col_offset=end_col_offset,
4528             parent=parent,
4529         )
4530 
4531     def postinit(self, value: NodeNG | None = None) -> None:
4532         """"""Do some setup after initialisation.
4533 
4534         :param value: The value to yield.
4535         """"""
4536         self.value = value
4537 
4538     def get_children(self):
4539         if self.value is not None:
4540             yield self.value
4541 
4542     def _get_yield_nodes_skip_lambdas(self):
4543         yield self
4544 
4545 
4546 class YieldFrom(Yield):  # TODO value is required, not optional
4547     """"""Class representing an :class:`ast.YieldFrom` node.""""""
4548 
4549 
4550 class DictUnpack(mixins.NoChildrenMixin, NodeNG):
4551     """"""Represents the unpacking of dicts into dicts using :pep:`448`.""""""
4552 
4553 
4554 class FormattedValue(NodeNG):
4555     """"""Class representing an :class:`ast.FormattedValue` node.
4556 
4557     Represents a :pep:`498` format string.
4558 
4559     >>> import astroid
4560     >>> node = astroid.extract_node('f""Format {type_}""')
4561     >>> node
4562     <JoinedStr l.1 at 0x7f23b2e4ed30>
4563     >>> node.values
4564     [<Const.str l.1 at 0x7f23b2e4eda0>, <FormattedValue l.1 at 0x7f23b2e4edd8>]
4565     """"""
4566 
4567     _astroid_fields = (""value"", ""format_spec"")
4568     _other_fields = (""conversion"",)
4569 
4570     def __init__(
4571         self,
4572         lineno: int | None = None,
4573         col_offset: int | None = None,
4574         parent: NodeNG | None = None,
4575         *,
4576         end_lineno: int | None = None,
4577         end_col_offset: int | None = None,
4578     ) -> None:
4579         """"""
4580         :param lineno: The line that this node appears on in the source code.
4581 
4582         :param col_offset: The column that this node appears on in the
4583             source code.
4584 
4585         :param parent: The parent node in the syntax tree.
4586 
4587         :param end_lineno: The last line this node appears on in the source code.
4588 
4589         :param end_col_offset: The end column this node appears on in the
4590             source code. Note: This is after the last symbol.
4591         """"""
4592         self.value: NodeNG
4593         """"""The value to be formatted into the string.""""""
4594 
4595         self.conversion: int | None = None  # can be None
4596         """"""The type of formatting to be applied to the value.
4597 
4598         .. seealso::
4599             :class:`ast.FormattedValue`
4600         """"""
4601 
4602         self.format_spec: NodeNG | None = None  # can be None
4603         """"""The formatting to be applied to the value.
4604 
4605         .. seealso::
4606             :class:`ast.FormattedValue`
4607 
4608         :type: JoinedStr or None
4609         """"""
4610 
4611         super().__init__(
4612             lineno=lineno,
4613             col_offset=col_offset,
4614             end_lineno=end_lineno,
4615             end_col_offset=end_col_offset,
4616             parent=parent,
4617         )
4618 
4619     def postinit(
4620         self,
4621         value: NodeNG,
4622         conversion: int | None = None,
4623         format_spec: NodeNG | None = None,
4624     ) -> None:
4625         """"""Do some setup after initialisation.
4626 
4627         :param value: The value to be formatted into the string.
4628 
4629         :param conversion: The type of formatting to be applied to the value.
4630 
4631         :param format_spec: The formatting to be applied to the value.
4632         :type format_spec: JoinedStr or None
4633         """"""
4634         self.value = value
4635         self.conversion = conversion
4636         self.format_spec = format_spec
4637 
4638     def get_children(self):
4639         yield self.value
4640 
4641         if self.format_spec is not None:
4642             yield self.format_spec
4643 
4644 
4645 class JoinedStr(NodeNG):
4646     """"""Represents a list of string expressions to be joined.
4647 
4648     >>> import astroid
4649     >>> node = astroid.extract_node('f""Format {type_}""')
4650     >>> node
4651     <JoinedStr l.1 at 0x7f23b2e4ed30>
4652     """"""
4653 
4654     _astroid_fields = (""values"",)
4655 
4656     def __init__(
4657         self,
4658         lineno: int | None = None,
4659         col_offset: int | None = None,
4660         parent: NodeNG | None = None,
4661         *,
4662         end_lineno: int | None = None,
4663         end_col_offset: int | None = None,
4664     ) -> None:
4665         """"""
4666         :param lineno: The line that this node appears on in the source code.
4667 
4668         :param col_offset: The column that this node appears on in the
4669             source code.
4670 
4671         :param parent: The parent node in the syntax tree.
4672 
4673         :param end_lineno: The last line this node appears on in the source code.
4674 
4675         :param end_col_offset: The end column this node appears on in the
4676             source code. Note: This is after the last symbol.
4677         """"""
4678         self.values: list[NodeNG] = []
4679         """"""The string expressions to be joined.
4680 
4681         :type: list(FormattedValue or Const)
4682         """"""
4683 
4684         super().__init__(
4685             lineno=lineno,
4686             col_offset=col_offset,
4687             end_lineno=end_lineno,
4688             end_col_offset=end_col_offset,
4689             parent=parent,
4690         )
4691 
4692     def postinit(self, values: list[NodeNG] | None = None) -> None:
4693         """"""Do some setup after initialisation.
4694 
4695         :param value: The string expressions to be joined.
4696 
4697         :type: list(FormattedValue or Const)
4698         """"""
4699         if values is not None:
4700             self.values = values
4701 
4702     def get_children(self):
4703         yield from self.values
4704 
4705 
4706 class NamedExpr(mixins.AssignTypeMixin, NodeNG):
4707     """"""Represents the assignment from the assignment expression
4708 
4709     >>> import astroid
4710     >>> module = astroid.parse('if a := 1: pass')
4711     >>> module.body[0].test
4712     <NamedExpr l.1 at 0x7f23b2e4ed30>
4713     """"""
4714 
4715     _astroid_fields = (""target"", ""value"")
4716 
4717     optional_assign = True
4718     """"""Whether this node optionally assigns a variable.
4719 
4720     Since NamedExpr are not always called they do not always assign.""""""
4721 
4722     def __init__(
4723         self,
4724         lineno: int | None = None,
4725         col_offset: int | None = None,
4726         parent: NodeNG | None = None,
4727         *,
4728         end_lineno: int | None = None,
4729         end_col_offset: int | None = None,
4730     ) -> None:
4731         """"""
4732         :param lineno: The line that this node appears on in the source code.
4733 
4734         :param col_offset: The column that this node appears on in the
4735             source code.
4736 
4737         :param parent: The parent node in the syntax tree.
4738 
4739         :param end_lineno: The last line this node appears on in the source code.
4740 
4741         :param end_col_offset: The end column this node appears on in the
4742             source code. Note: This is after the last symbol.
4743         """"""
4744         self.target: NodeNG
4745         """"""The assignment target
4746 
4747         :type: Name
4748         """"""
4749 
4750         self.value: NodeNG
4751         """"""The value that gets assigned in the expression""""""
4752 
4753         super().__init__(
4754             lineno=lineno,
4755             col_offset=col_offset,
4756             end_lineno=end_lineno,
4757             end_col_offset=end_col_offset,
4758             parent=parent,
4759         )
4760 
4761     def postinit(self, target: NodeNG, value: NodeNG) -> None:
4762         self.target = target
4763         self.value = value
4764 
4765     assigned_stmts: ClassVar[AssignedStmtsCall[NamedExpr]]
4766     """"""Returns the assigned statement (non inferred) according to the assignment type.
4767     See astroid/protocols.py for actual implementation.
4768     """"""
4769 
4770     def frame(
4771         self, *, future: Literal[None, True] = None
4772     ) -> nodes.FunctionDef | nodes.Module | nodes.ClassDef | nodes.Lambda:
4773         """"""The first parent frame node.
4774 
4775         A frame node is a :class:`Module`, :class:`FunctionDef`,
4776         or :class:`ClassDef`.
4777 
4778         :returns: The first parent frame node.
4779         """"""
4780         if not self.parent:
4781             raise ParentMissingError(target=self)
4782 
4783         # For certain parents NamedExpr evaluate to the scope of the parent
4784         if isinstance(self.parent, (Arguments, Keyword, Comprehension)):
4785             if not self.parent.parent:
4786                 raise ParentMissingError(target=self.parent)
4787             if not self.parent.parent.parent:
4788                 raise ParentMissingError(target=self.parent.parent)
4789             return self.parent.parent.parent.frame(future=True)
4790 
4791         return self.parent.frame(future=True)
4792 
4793     def scope(self) -> LocalsDictNodeNG:
4794         """"""The first parent node defining a new scope.
4795         These can be Module, FunctionDef, ClassDef, Lambda, or GeneratorExp nodes.
4796 
4797         :returns: The first parent scope node.
4798         """"""
4799         if not self.parent:
4800             raise ParentMissingError(target=self)
4801 
4802         # For certain parents NamedExpr evaluate to the scope of the parent
4803         if isinstance(self.parent, (Arguments, Keyword, Comprehension)):
4804             if not self.parent.parent:
4805                 raise ParentMissingError(target=self.parent)
4806             if not self.parent.parent.parent:
4807                 raise ParentMissingError(target=self.parent.parent)
4808             return self.parent.parent.parent.scope()
4809 
4810         return self.parent.scope()
4811 
4812     def set_local(self, name: str, stmt: AssignName) -> None:
4813         """"""Define that the given name is declared in the given statement node.
4814         NamedExpr's in Arguments, Keyword or Comprehension are evaluated in their
4815         parent's parent scope. So we add to their frame's locals.
4816 
4817         .. seealso:: :meth:`scope`
4818 
4819         :param name: The name that is being defined.
4820 
4821         :param stmt: The statement that defines the given name.
4822         """"""
4823         self.frame(future=True).set_local(name, stmt)
4824 
4825 
4826 class Unknown(mixins.AssignTypeMixin, NodeNG):
4827     """"""This node represents a node in a constructed AST where
4828     introspection is not possible.  At the moment, it's only used in
4829     the args attribute of FunctionDef nodes where function signature
4830     introspection failed.
4831     """"""
4832 
4833     name = ""Unknown""
4834 
4835     def qname(self):
4836         return ""Unknown""
4837 
4838     def _infer(self, context=None, **kwargs):
4839         """"""Inference on an Unknown node immediately terminates.""""""
4840         yield util.Uninferable
4841 
4842 
4843 class EvaluatedObject(NodeNG):
4844     """"""Contains an object that has already been inferred
4845 
4846     This class is useful to pre-evaluate a particular node,
4847     with the resulting class acting as the non-evaluated node.
4848     """"""
4849 
4850     name = ""EvaluatedObject""
4851     _astroid_fields = (""original"",)
4852     _other_fields = (""value"",)
4853 
4854     def __init__(
4855         self, original: NodeNG, value: NodeNG | type[util.Uninferable]
4856     ) -> None:
4857         self.original: NodeNG = original
4858         """"""The original node that has already been evaluated""""""
4859 
4860         self.value: NodeNG | type[util.Uninferable] = value
4861         """"""The inferred value""""""
4862 
4863         super().__init__(
4864             lineno=self.original.lineno,
4865             col_offset=self.original.col_offset,
4866             parent=self.original.parent,
4867         )
4868 
4869     def _infer(
4870         self, context: InferenceContext | None = None
4871     ) -> Iterator[NodeNG | type[util.Uninferable]]:
4872         yield self.value
4873 
4874 
4875 # Pattern matching #######################################################
4876 
4877 
4878 class Match(Statement):
4879     """"""Class representing a :class:`ast.Match` node.
4880 
4881     >>> import astroid
4882     >>> node = astroid.extract_node('''
4883     match x:
4884         case 200:
4885             ...
4886         case _:
4887             ...
4888     ''')
4889     >>> node
4890     <Match l.2 at 0x10c24e170>
4891     """"""
4892 
4893     _astroid_fields = (""subject"", ""cases"")
4894 
4895     def __init__(
4896         self,
4897         lineno: int | None = None,
4898         col_offset: int | None = None,
4899         parent: NodeNG | None = None,
4900         *,
4901         end_lineno: int | None = None,
4902         end_col_offset: int | None = None,
4903     ) -> None:
4904         self.subject: NodeNG
4905         self.cases: list[MatchCase]
4906         super().__init__(
4907             lineno=lineno,
4908             col_offset=col_offset,
4909             end_lineno=end_lineno,
4910             end_col_offset=end_col_offset,
4911             parent=parent,
4912         )
4913 
4914     def postinit(
4915         self,
4916         *,
4917         subject: NodeNG,
4918         cases: list[MatchCase],
4919     ) -> None:
4920         self.subject = subject
4921         self.cases = cases
4922 
4923 
4924 class Pattern(NodeNG):
4925     """"""Base class for all Pattern nodes.""""""
4926 
4927 
4928 class MatchCase(mixins.MultiLineBlockMixin, NodeNG):
4929     """"""Class representing a :class:`ast.match_case` node.
4930 
4931     >>> import astroid
4932     >>> node = astroid.extract_node('''
4933     match x:
4934         case 200:
4935             ...
4936     ''')
4937     >>> node.cases[0]
4938     <MatchCase l.3 at 0x10c24e590>
4939     """"""
4940 
4941     _astroid_fields = (""pattern"", ""guard"", ""body"")
4942     _multi_line_block_fields = (""body"",)
4943 
4944     lineno: None
4945     col_offset: None
4946     end_lineno: None
4947     end_col_offset: None
4948 
4949     def __init__(self, *, parent: NodeNG | None = None) -> None:
4950         self.pattern: Pattern
4951         self.guard: NodeNG | None
4952         self.body: list[NodeNG]
4953         super().__init__(parent=parent)
4954 
4955     def postinit(
4956         self,
4957         *,
4958         pattern: Pattern,
4959         guard: NodeNG | None,
4960         body: list[NodeNG],
4961     ) -> None:
4962         self.pattern = pattern
4963         self.guard = guard
4964         self.body = body
4965 
4966 
4967 class MatchValue(Pattern):
4968     """"""Class representing a :class:`ast.MatchValue` node.
4969 
4970     >>> import astroid
4971     >>> node = astroid.extract_node('''
4972     match x:
4973         case 200:
4974             ...
4975     ''')
4976     >>> node.cases[0].pattern
4977     <MatchValue l.3 at 0x10c24e200>
4978     """"""
4979 
4980     _astroid_fields = (""value"",)
4981 
4982     def __init__(
4983         self,
4984         lineno: int | None = None,
4985         col_offset: int | None = None,
4986         parent: NodeNG | None = None,
4987         *,
4988         end_lineno: int | None = None,
4989         end_col_offset: int | None = None,
4990     ) -> None:
4991         self.value: NodeNG
4992         super().__init__(
4993             lineno=lineno,
4994             col_offset=col_offset,
4995             end_lineno=end_lineno,
4996             end_col_offset=end_col_offset,
4997             parent=parent,
4998         )
4999 
5000     def postinit(self, *, value: NodeNG) -> None:
5001         self.value = value
5002 
5003 
5004 class MatchSingleton(Pattern):
5005     """"""Class representing a :class:`ast.MatchSingleton` node.
5006 
5007     >>> import astroid
5008     >>> node = astroid.extract_node('''
5009     match x:
5010         case True:
5011             ...
5012         case False:
5013             ...
5014         case None:
5015             ...
5016     ''')
5017     >>> node.cases[0].pattern
5018     <MatchSingleton l.3 at 0x10c2282e0>
5019     >>> node.cases[1].pattern
5020     <MatchSingleton l.5 at 0x10c228af0>
5021     >>> node.cases[2].pattern
5022     <MatchSingleton l.7 at 0x10c229f90>
5023     """"""
5024 
5025     _other_fields = (""value"",)
5026 
5027     def __init__(
5028         self,
5029         *,
5030         value: Literal[True, False, None],
5031         lineno: int | None = None,
5032         col_offset: int | None = None,
5033         end_lineno: int | None = None,
5034         end_col_offset: int | None = None,
5035         parent: NodeNG | None = None,
5036     ) -> None:
5037         self.value = value
5038         super().__init__(
5039             lineno=lineno,
5040             col_offset=col_offset,
5041             end_lineno=end_lineno,
5042             end_col_offset=end_col_offset,
5043             parent=parent,
5044         )
5045 
5046 
5047 class MatchSequence(Pattern):
5048     """"""Class representing a :class:`ast.MatchSequence` node.
5049 
5050     >>> import astroid
5051     >>> node = astroid.extract_node('''
5052     match x:
5053         case [1, 2]:
5054             ...
5055         case (1, 2, *_):
5056             ...
5057     ''')
5058     >>> node.cases[0].pattern
5059     <MatchSequence l.3 at 0x10ca80d00>
5060     >>> node.cases[1].pattern
5061     <MatchSequence l.5 at 0x10ca80b20>
5062     """"""
5063 
5064     _astroid_fields = (""patterns"",)
5065 
5066     def __init__(
5067         self,
5068         lineno: int | None = None,
5069         col_offset: int | None = None,
5070         parent: NodeNG | None = None,
5071         *,
5072         end_lineno: int | None = None,
5073         end_col_offset: int | None = None,
5074     ) -> None:
5075         self.patterns: list[Pattern]
5076         super().__init__(
5077             lineno=lineno,
5078             col_offset=col_offset,
5079             end_lineno=end_lineno,
5080             end_col_offset=end_col_offset,
5081             parent=parent,
5082         )
5083 
5084     def postinit(self, *, patterns: list[Pattern]) -> None:
5085         self.patterns = patterns
5086 
5087 
5088 class MatchMapping(mixins.AssignTypeMixin, Pattern):
5089     """"""Class representing a :class:`ast.MatchMapping` node.
5090 
5091     >>> import astroid
5092     >>> node = astroid.extract_node('''
5093     match x:
5094         case {1: ""Hello"", 2: ""World"", 3: _, **rest}:
5095             ...
5096     ''')
5097     >>> node.cases[0].pattern
5098     <MatchMapping l.3 at 0x10c8a8850>
5099     """"""
5100 
5101     _astroid_fields = (""keys"", ""patterns"", ""rest"")
5102 
5103     def __init__(
5104         self,
5105         lineno: int | None = None,
5106         col_offset: int | None = None,
5107         parent: NodeNG | None = None,
5108         *,
5109         end_lineno: int | None = None,
5110         end_col_offset: int | None = None,
5111     ) -> None:
5112         self.keys: list[NodeNG]
5113         self.patterns: list[Pattern]
5114         self.rest: AssignName | None
5115         super().__init__(
5116             lineno=lineno,
5117             col_offset=col_offset,
5118             end_lineno=end_lineno,
5119             end_col_offset=end_col_offset,
5120             parent=parent,
5121         )
5122 
5123     def postinit(
5124         self,
5125         *,
5126         keys: list[NodeNG],
5127         patterns: list[Pattern],
5128         rest: AssignName | None,
5129     ) -> None:
5130         self.keys = keys
5131         self.patterns = patterns
5132         self.rest = rest
5133 
5134     assigned_stmts: ClassVar[
5135         Callable[
5136             [
5137                 MatchMapping,
5138                 AssignName,
5139                 InferenceContext | None,
5140                 None,
5141             ],
5142             Generator[NodeNG, None, None],
5143         ]
5144     ]
5145     """"""Returns the assigned statement (non inferred) according to the assignment type.
5146     See astroid/protocols.py for actual implementation.
5147     """"""
5148 
5149 
5150 class MatchClass(Pattern):
5151     """"""Class representing a :class:`ast.MatchClass` node.
5152 
5153     >>> import astroid
5154     >>> node = astroid.extract_node('''
5155     match x:
5156         case Point2D(0, 0):
5157             ...
5158         case Point3D(x=0, y=0, z=0):
5159             ...
5160     ''')
5161     >>> node.cases[0].pattern
5162     <MatchClass l.3 at 0x10ca83940>
5163     >>> node.cases[1].pattern
5164     <MatchClass l.5 at 0x10ca80880>
5165     """"""
5166 
5167     _astroid_fields = (""cls"", ""patterns"", ""kwd_patterns"")
5168     _other_fields = (""kwd_attrs"",)
5169 
5170     def __init__(
5171         self,
5172         lineno: int | None = None,
5173         col_offset: int | None = None,
5174         parent: NodeNG | None = None,
5175         *,
5176         end_lineno: int | None = None,
5177         end_col_offset: int | None = None,
5178     ) -> None:
5179         self.cls: NodeNG
5180         self.patterns: list[Pattern]
5181         self.kwd_attrs: list[str]
5182         self.kwd_patterns: list[Pattern]
5183         super().__init__(
5184             lineno=lineno,
5185             col_offset=col_offset,
5186             end_lineno=end_lineno,
5187             end_col_offset=end_col_offset,
5188             parent=parent,
5189         )
5190 
5191     def postinit(
5192         self,
5193         *,
5194         cls: NodeNG,
5195         patterns: list[Pattern],
5196         kwd_attrs: list[str],
5197         kwd_patterns: list[Pattern],
5198     ) -> None:
5199         self.cls = cls
5200         self.patterns = patterns
5201         self.kwd_attrs = kwd_attrs
5202         self.kwd_patterns = kwd_patterns
5203 
5204 
5205 class MatchStar(mixins.AssignTypeMixin, Pattern):
5206     """"""Class representing a :class:`ast.MatchStar` node.
5207 
5208     >>> import astroid
5209     >>> node = astroid.extract_node('''
5210     match x:
5211         case [1, *_]:
5212             ...
5213     ''')
5214     >>> node.cases[0].pattern.patterns[1]
5215     <MatchStar l.3 at 0x10ca809a0>
5216     """"""
5217 
5218     _astroid_fields = (""name"",)
5219 
5220     def __init__(
5221         self,
5222         lineno: int | None = None,
5223         col_offset: int | None = None,
5224         parent: NodeNG | None = None,
5225         *,
5226         end_lineno: int | None = None,
5227         end_col_offset: int | None = None,
5228     ) -> None:
5229         self.name: AssignName | None
5230         super().__init__(
5231             lineno=lineno,
5232             col_offset=col_offset,
5233             end_lineno=end_lineno,
5234             end_col_offset=end_col_offset,
5235             parent=parent,
5236         )
5237 
5238     def postinit(self, *, name: AssignName | None) -> None:
5239         self.name = name
5240 
5241     assigned_stmts: ClassVar[
5242         Callable[
5243             [
5244                 MatchStar,
5245                 AssignName,
5246                 InferenceContext | None,
5247                 None,
5248             ],
5249             Generator[NodeNG, None, None],
5250         ]
5251     ]
5252     """"""Returns the assigned statement (non inferred) according to the assignment type.
5253     See astroid/protocols.py for actual implementation.
5254     """"""
5255 
5256 
5257 class MatchAs(mixins.AssignTypeMixin, Pattern):
5258     """"""Class representing a :class:`ast.MatchAs` node.
5259 
5260     >>> import astroid
5261     >>> node = astroid.extract_node('''
5262     match x:
5263         case [1, a]:
5264             ...
5265         case {'key': b}:
5266             ...
5267         case Point2D(0, 0) as c:
5268             ...
5269         case d:
5270             ...
5271     ''')
5272     >>> node.cases[0].pattern.patterns[1]
5273     <MatchAs l.3 at 0x10d0b2da0>
5274     >>> node.cases[1].pattern.patterns[0]
5275     <MatchAs l.5 at 0x10d0b2920>
5276     >>> node.cases[2].pattern
5277     <MatchAs l.7 at 0x10d0b06a0>
5278     >>> node.cases[3].pattern
5279     <MatchAs l.9 at 0x10d09b880>
5280     """"""
5281 
5282     _astroid_fields = (""pattern"", ""name"")
5283 
5284     def __init__(
5285         self,
5286         lineno: int | None = None,
5287         col_offset: int | None = None,
5288         parent: NodeNG | None = None,
5289         *,
5290         end_lineno: int | None = None,
5291         end_col_offset: int | None = None,
5292     ) -> None:
5293         self.pattern: Pattern | None
5294         self.name: AssignName | None
5295         super().__init__(
5296             lineno=lineno,
5297             col_offset=col_offset,
5298             end_lineno=end_lineno,
5299             end_col_offset=end_col_offset,
5300             parent=parent,
5301         )
5302 
5303     def postinit(
5304         self,
5305         *,
5306         pattern: Pattern | None,
5307         name: AssignName | None,
5308     ) -> None:
5309         self.pattern = pattern
5310         self.name = name
5311 
5312     assigned_stmts: ClassVar[
5313         Callable[
5314             [
5315                 MatchAs,
5316                 AssignName,
5317                 InferenceContext | None,
5318                 None,
5319             ],
5320             Generator[NodeNG, None, None],
5321         ]
5322     ]
5323     """"""Returns the assigned statement (non inferred) according to the assignment type.
5324     See astroid/protocols.py for actual implementation.
5325     """"""
5326 
5327 
5328 class MatchOr(Pattern):
5329     """"""Class representing a :class:`ast.MatchOr` node.
5330 
5331     >>> import astroid
5332     >>> node = astroid.extract_node('''
5333     match x:
5334         case 400 | 401 | 402:
5335             ...
5336     ''')
5337     >>> node.cases[0].pattern
5338     <MatchOr l.3 at 0x10d0b0b50>
5339     """"""
5340 
5341     _astroid_fields = (""patterns"",)
5342 
5343     def __init__(
5344         self,
5345         lineno: int | None = None,
5346         col_offset: int | None = None,
5347         parent: NodeNG | None = None,
5348         *,
5349         end_lineno: int | None = None,
5350         end_col_offset: int | None = None,
5351     ) -> None:
5352         self.patterns: list[Pattern]
5353         super().__init__(
5354             lineno=lineno,
5355             col_offset=col_offset,
5356             end_lineno=end_lineno,
5357             end_col_offset=end_col_offset,
5358             parent=parent,
5359         )
5360 
5361     def postinit(self, *, patterns: list[Pattern]) -> None:
5362         self.patterns = patterns
5363 
5364 
5365 # constants ##############################################################
5366 
5367 CONST_CLS = {
5368     list: List,
5369     tuple: Tuple,
5370     dict: Dict,
5371     set: Set,
5372     type(None): Const,
5373     type(NotImplemented): Const,
5374     type(...): Const,
5375 }
5376 
5377 
5378 def _update_const_classes():
5379     """"""update constant classes, so the keys of CONST_CLS can be reused""""""
5380     klasses = (bool, int, float, complex, str, bytes)
5381     for kls in klasses:
5382         CONST_CLS[kls] = Const
5383 
5384 
5385 _update_const_classes()
5386 
5387 
5388 def _two_step_initialization(cls, value):
5389     instance = cls()
5390     instance.postinit(value)
5391     return instance
5392 
5393 
5394 def _dict_initialization(cls, value):
5395     if isinstance(value, dict):
5396         value = tuple(value.items())
5397     return _two_step_initialization(cls, value)
5398 
5399 
5400 _CONST_CLS_CONSTRUCTORS = {
5401     List: _two_step_initialization,
5402     Tuple: _two_step_initialization,
5403     Dict: _dict_initialization,
5404     Set: _two_step_initialization,
5405     Const: lambda cls, value: cls(value),
5406 }
5407 
5408 
5409 def const_factory(value):
5410     """"""return an astroid node for a python value""""""
5411     # XXX we should probably be stricter here and only consider stuff in
5412     # CONST_CLS or do better treatment: in case where value is not in CONST_CLS,
5413     # we should rather recall the builder on this value than returning an empty
5414     # node (another option being that const_factory shouldn't be called with something
5415     # not in CONST_CLS)
5416     assert not isinstance(value, NodeNG)
5417 
5418     # Hack for ignoring elements of a sequence
5419     # or a mapping, in order to avoid transforming
5420     # each element to an AST. This is fixed in 2.0
5421     # and this approach is a temporary hack.
5422     if isinstance(value, (list, set, tuple, dict)):
5423         elts = []
5424     else:
5425         elts = value
5426 
5427     try:
5428         initializer_cls = CONST_CLS[value.__class__]
5429         initializer = _CONST_CLS_CONSTRUCTORS[initializer_cls]
5430         return initializer(initializer_cls, elts)
5431     except (KeyError, AttributeError):
5432         node = EmptyNode()
5433         node.object = value
5434         return node
5435 
[end of astroid/nodes/node_classes.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pylint-dev/astroid,39c2a9805970ca57093d32bbaf0e6a63e05041d8,"getitem does not infer the actual unpacked value
When trying to call `Dict.getitem()` on a context where we have a dict unpacking of anything beside a real dict, astroid currently raises an `AttributeError: 'getitem'`, which has 2 problems:

- The object might be a reference against something constant, this pattern is usually seen when we have different sets of dicts that extend each other, and all of their values are inferrable. 
- We can have something that is uninferable, but in that case instead of an `AttributeError` I think it makes sense to raise the usual `AstroidIndexError` which is supposed to be already handled by the downstream.


Here is a short reproducer;

```py
from astroid import parse


source = """"""
X = {
    'A': 'B'
}

Y = {
    **X
}

KEY = 'A'
""""""

tree = parse(source)

first_dict = tree.body[0].value
second_dict = tree.body[1].value
key = tree.body[2].value

print(f'{first_dict.getitem(key).value = }')
print(f'{second_dict.getitem(key).value = }')


```

The current output;

```
 $ python t1.py                                                                                                 3ms
first_dict.getitem(key).value = 'B'
Traceback (most recent call last):
  File ""/home/isidentical/projects/astroid/t1.py"", line 23, in <module>
    print(f'{second_dict.getitem(key).value = }')
  File ""/home/isidentical/projects/astroid/astroid/nodes/node_classes.py"", line 2254, in getitem
    return value.getitem(index, context)
AttributeError: 'Name' object has no attribute 'getitem'
```

Expeceted output;
```
 $ python t1.py                                                                                                 4ms
first_dict.getitem(key).value = 'B'
second_dict.getitem(key).value = 'B'

```

",,2021-10-03T15:58:07Z,"<patch>
diff --git a/astroid/nodes/node_classes.py b/astroid/nodes/node_classes.py
--- a/astroid/nodes/node_classes.py
+++ b/astroid/nodes/node_classes.py
@@ -2346,24 +2346,33 @@ def itered(self):
         """"""
         return [key for (key, _) in self.items]
 
-    def getitem(self, index, context=None):
+    def getitem(
+        self, index: Const | Slice, context: InferenceContext | None = None
+    ) -> NodeNG:
         """"""Get an item from this node.
 
         :param index: The node to use as a subscript index.
-        :type index: Const or Slice
 
         :raises AstroidTypeError: When the given index cannot be used as a
             subscript index, or if this node is not subscriptable.
         :raises AstroidIndexError: If the given index does not exist in the
             dictionary.
         """"""
+        # pylint: disable-next=import-outside-toplevel; circular import
+        from astroid.helpers import safe_infer
+
         for key, value in self.items:
             # TODO(cpopa): no support for overriding yet, {1:2, **{1: 3}}.
             if isinstance(key, DictUnpack):
+                inferred_value = safe_infer(value, context)
+                if not isinstance(inferred_value, Dict):
+                    continue
+
                 try:
-                    return value.getitem(index, context)
+                    return inferred_value.getitem(index, context)
                 except (AstroidTypeError, AstroidIndexError):
                     continue
+
             for inferredkey in key.infer(context):
                 if inferredkey is util.Uninferable:
                     continue

</patch>","diff --git a/tests/unittest_python3.py b/tests/unittest_python3.py
--- a/tests/unittest_python3.py
+++ b/tests/unittest_python3.py
@@ -5,7 +5,9 @@
 import unittest
 from textwrap import dedent
 
-from astroid import nodes
+import pytest
+
+from astroid import exceptions, nodes
 from astroid.builder import AstroidBuilder, extract_node
 from astroid.test_utils import require_version
 
@@ -285,6 +287,33 @@ def test_unpacking_in_dict_getitem(self) -> None:
             self.assertIsInstance(value, nodes.Const)
             self.assertEqual(value.value, expected)
 
+    @staticmethod
+    def test_unpacking_in_dict_getitem_with_ref() -> None:
+        node = extract_node(
+            """"""
+        a = {1: 2}
+        {**a, 2: 3}  #@
+        """"""
+        )
+        assert isinstance(node, nodes.Dict)
+
+        for key, expected in ((1, 2), (2, 3)):
+            value = node.getitem(nodes.Const(key))
+            assert isinstance(value, nodes.Const)
+            assert value.value == expected
+
+    @staticmethod
+    def test_unpacking_in_dict_getitem_uninferable() -> None:
+        node = extract_node(""{**a, 2: 3}"")
+        assert isinstance(node, nodes.Dict)
+
+        with pytest.raises(exceptions.AstroidIndexError):
+            node.getitem(nodes.Const(1))
+
+        value = node.getitem(nodes.Const(2))
+        assert isinstance(value, nodes.Const)
+        assert value.value == 3
+
     def test_format_string(self) -> None:
         code = ""f'{greetings} {person}'""
         node = extract_node(code)
",2.12,"[""tests/unittest_python3.py::Python3TC::test_unpacking_in_dict_getitem_uninferable"", ""tests/unittest_python3.py::Python3TC::test_unpacking_in_dict_getitem_with_ref""]","[""tests/unittest_python3.py::Python3TC::test_annotation_as_string"", ""tests/unittest_python3.py::Python3TC::test_annotation_support"", ""tests/unittest_python3.py::Python3TC::test_as_string"", ""tests/unittest_python3.py::Python3TC::test_async_comprehensions"", ""tests/unittest_python3.py::Python3TC::test_async_comprehensions_as_string"", ""tests/unittest_python3.py::Python3TC::test_async_comprehensions_outside_coroutine"", ""tests/unittest_python3.py::Python3TC::test_format_string"", ""tests/unittest_python3.py::Python3TC::test_kwonlyargs_annotations_supper"", ""tests/unittest_python3.py::Python3TC::test_metaclass_ancestors"", ""tests/unittest_python3.py::Python3TC::test_metaclass_error"", ""tests/unittest_python3.py::Python3TC::test_metaclass_imported"", ""tests/unittest_python3.py::Python3TC::test_metaclass_multiple_keywords"", ""tests/unittest_python3.py::Python3TC::test_metaclass_yes_leak"", ""tests/unittest_python3.py::Python3TC::test_nested_unpacking_in_dicts"", ""tests/unittest_python3.py::Python3TC::test_old_syntax_works"", ""tests/unittest_python3.py::Python3TC::test_parent_metaclass"", ""tests/unittest_python3.py::Python3TC::test_simple_metaclass"", ""tests/unittest_python3.py::Python3TC::test_starred_notation"", ""tests/unittest_python3.py::Python3TC::test_underscores_in_numeral_literal"", ""tests/unittest_python3.py::Python3TC::test_unpacking_in_dict_getitem"", ""tests/unittest_python3.py::Python3TC::test_unpacking_in_dicts"", ""tests/unittest_python3.py::Python3TC::test_yield_from"", ""tests/unittest_python3.py::Python3TC::test_yield_from_as_string"", ""tests/unittest_python3.py::Python3TC::test_yield_from_is_generator""]",52f6d2d7722db383af035be929f18af5e9fe8cd5
pylint-dev__astroid-1866,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
""TypeError: unsupported format string passed to NoneType.__format__"" while running type inference in version 2.12.x
### Steps to reproduce

I have no concise reproducer. Exception happens every time I run pylint on some internal code, with astroid 2.12.10 and 2.12.12 (debian bookworm). It does _not_ happen with earlier versions of astroid (not with version 2.9). The pylinted code itself is ""valid"", it runs in production here.

### Current behavior

When running pylint on some code, I get this exception:
```
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/pylint/utils/ast_walker.py"", line 90, in walk
    callback(astroid)
  File ""/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py"", line 183, in visit_functiondef
    inferred = _safe_infer_call_result(node, node)
  File ""/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py"", line 42, in _safe_infer_call_result
    value = next(inferit)
  File ""/usr/lib/python3/dist-packages/astroid/nodes/scoped_nodes/scoped_nodes.py"", line 1749, in infer_call_result
    yield from returnnode.value.infer(context)
  File ""/usr/lib/python3/dist-packages/astroid/nodes/node_ng.py"", line 159, in infer
    results = list(self._explicit_inference(self, context, **kwargs))
  File ""/usr/lib/python3/dist-packages/astroid/inference_tip.py"", line 45, in _inference_tip_cached
    result = _cache[func, node] = list(func(*args, **kwargs))
  File ""/usr/lib/python3/dist-packages/astroid/brain/brain_builtin_inference.py"", line 956, in _infer_str_format_call
    formatted_string = format_template.format(*pos_values, **keyword_values)
TypeError: unsupported format string passed to NoneType.__format__
```

### Expected behavior

TypeError exception should not happen

### `python -c ""from astroid import __pkginfo__; print(__pkginfo__.version)""` output

2.12.10,
2.12.12

</issue>
<code>
[start of README.rst]
1 Astroid
2 =======
3 
4 .. image:: https://coveralls.io/repos/github/PyCQA/astroid/badge.svg?branch=main
5     :target: https://coveralls.io/github/PyCQA/astroid?branch=main
6     :alt: Coverage badge from coveralls.io
7 
8 .. image:: https://readthedocs.org/projects/astroid/badge/?version=latest
9     :target: http://astroid.readthedocs.io/en/latest/?badge=latest
10     :alt: Documentation Status
11 
12 .. image:: https://img.shields.io/badge/code%20style-black-000000.svg
13     :target: https://github.com/ambv/black
14 
15 .. image:: https://results.pre-commit.ci/badge/github/PyCQA/astroid/main.svg
16    :target: https://results.pre-commit.ci/latest/github/PyCQA/astroid/main
17    :alt: pre-commit.ci status
18 
19 .. |tidelift_logo| image:: https://raw.githubusercontent.com/PyCQA/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png
20    :width: 200
21    :alt: Tidelift
22 
23 .. list-table::
24    :widths: 10 100
25 
26    * - |tidelift_logo|
27      - Professional support for astroid is available as part of the
28        `Tidelift Subscription`_.  Tidelift gives software development teams a single source for
29        purchasing and maintaining their software, with professional grade assurances
30        from the experts who know it best, while seamlessly integrating with existing
31        tools.
32 
33 .. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme
34 
35 
36 
37 What's this?
38 ------------
39 
40 The aim of this module is to provide a common base representation of
41 python source code. It is currently the library powering pylint's capabilities.
42 
43 It provides a compatible representation which comes from the `_ast`
44 module.  It rebuilds the tree generated by the builtin _ast module by
45 recursively walking down the AST and building an extended ast. The new
46 node classes have additional methods and attributes for different
47 usages. They include some support for static inference and local name
48 scopes. Furthermore, astroid can also build partial trees by inspecting living
49 objects.
50 
51 
52 Installation
53 ------------
54 
55 Extract the tarball, jump into the created directory and run::
56 
57     pip install .
58 
59 
60 If you want to do an editable installation, you can run::
61 
62     pip install -e .
63 
64 
65 If you have any questions, please mail the code-quality@python.org
66 mailing list for support. See
67 http://mail.python.org/mailman/listinfo/code-quality for subscription
68 information and archives.
69 
70 Documentation
71 -------------
72 http://astroid.readthedocs.io/en/latest/
73 
74 
75 Python Versions
76 ---------------
77 
78 astroid 2.0 is currently available for Python 3 only. If you want Python 2
79 support, use an older version of astroid (though note that these versions
80 are no longer supported).
81 
82 Test
83 ----
84 
85 Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use
86 either `tox` or `pytest`::
87 
88     tox
89     pytest
90 
[end of README.rst]
[start of astroid/brain/brain_builtin_inference.py]
1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html
2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE
3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt
4 
5 """"""Astroid hooks for various builtins.""""""
6 
7 from __future__ import annotations
8 
9 import itertools
10 from collections.abc import Iterator
11 from functools import partial
12 
13 from astroid import arguments, helpers, inference_tip, nodes, objects, util
14 from astroid.builder import AstroidBuilder
15 from astroid.context import InferenceContext
16 from astroid.exceptions import (
17     AstroidTypeError,
18     AttributeInferenceError,
19     InferenceError,
20     MroError,
21     UseInferenceDefault,
22 )
23 from astroid.manager import AstroidManager
24 from astroid.nodes import scoped_nodes
25 
26 OBJECT_DUNDER_NEW = ""object.__new__""
27 
28 STR_CLASS = """"""
29 class whatever(object):
30     def join(self, iterable):
31         return {rvalue}
32     def replace(self, old, new, count=None):
33         return {rvalue}
34     def format(self, *args, **kwargs):
35         return {rvalue}
36     def encode(self, encoding='ascii', errors=None):
37         return b''
38     def decode(self, encoding='ascii', errors=None):
39         return u''
40     def capitalize(self):
41         return {rvalue}
42     def title(self):
43         return {rvalue}
44     def lower(self):
45         return {rvalue}
46     def upper(self):
47         return {rvalue}
48     def swapcase(self):
49         return {rvalue}
50     def index(self, sub, start=None, end=None):
51         return 0
52     def find(self, sub, start=None, end=None):
53         return 0
54     def count(self, sub, start=None, end=None):
55         return 0
56     def strip(self, chars=None):
57         return {rvalue}
58     def lstrip(self, chars=None):
59         return {rvalue}
60     def rstrip(self, chars=None):
61         return {rvalue}
62     def rjust(self, width, fillchar=None):
63         return {rvalue}
64     def center(self, width, fillchar=None):
65         return {rvalue}
66     def ljust(self, width, fillchar=None):
67         return {rvalue}
68 """"""
69 
70 
71 BYTES_CLASS = """"""
72 class whatever(object):
73     def join(self, iterable):
74         return {rvalue}
75     def replace(self, old, new, count=None):
76         return {rvalue}
77     def decode(self, encoding='ascii', errors=None):
78         return u''
79     def capitalize(self):
80         return {rvalue}
81     def title(self):
82         return {rvalue}
83     def lower(self):
84         return {rvalue}
85     def upper(self):
86         return {rvalue}
87     def swapcase(self):
88         return {rvalue}
89     def index(self, sub, start=None, end=None):
90         return 0
91     def find(self, sub, start=None, end=None):
92         return 0
93     def count(self, sub, start=None, end=None):
94         return 0
95     def strip(self, chars=None):
96         return {rvalue}
97     def lstrip(self, chars=None):
98         return {rvalue}
99     def rstrip(self, chars=None):
100         return {rvalue}
101     def rjust(self, width, fillchar=None):
102         return {rvalue}
103     def center(self, width, fillchar=None):
104         return {rvalue}
105     def ljust(self, width, fillchar=None):
106         return {rvalue}
107 """"""
108 
109 
110 def _extend_string_class(class_node, code, rvalue):
111     """"""function to extend builtin str/unicode class""""""
112     code = code.format(rvalue=rvalue)
113     fake = AstroidBuilder(AstroidManager()).string_build(code)[""whatever""]
114     for method in fake.mymethods():
115         method.parent = class_node
116         method.lineno = None
117         method.col_offset = None
118         if ""__class__"" in method.locals:
119             method.locals[""__class__""] = [class_node]
120         class_node.locals[method.name] = [method]
121         method.parent = class_node
122 
123 
124 def _extend_builtins(class_transforms):
125     builtin_ast = AstroidManager().builtins_module
126     for class_name, transform in class_transforms.items():
127         transform(builtin_ast[class_name])
128 
129 
130 _extend_builtins(
131     {
132         ""bytes"": partial(_extend_string_class, code=BYTES_CLASS, rvalue=""b''""),
133         ""str"": partial(_extend_string_class, code=STR_CLASS, rvalue=""''""),
134     }
135 )
136 
137 
138 def _builtin_filter_predicate(node, builtin_name):
139     if (
140         builtin_name == ""type""
141         and node.root().name == ""re""
142         and isinstance(node.func, nodes.Name)
143         and node.func.name == ""type""
144         and isinstance(node.parent, nodes.Assign)
145         and len(node.parent.targets) == 1
146         and isinstance(node.parent.targets[0], nodes.AssignName)
147         and node.parent.targets[0].name in {""Pattern"", ""Match""}
148     ):
149         # Handle re.Pattern and re.Match in brain_re
150         # Match these patterns from stdlib/re.py
151         # ```py
152         # Pattern = type(...)
153         # Match = type(...)
154         # ```
155         return False
156     if isinstance(node.func, nodes.Name) and node.func.name == builtin_name:
157         return True
158     if isinstance(node.func, nodes.Attribute):
159         return (
160             node.func.attrname == ""fromkeys""
161             and isinstance(node.func.expr, nodes.Name)
162             and node.func.expr.name == ""dict""
163         )
164     return False
165 
166 
167 def register_builtin_transform(transform, builtin_name):
168     """"""Register a new transform function for the given *builtin_name*.
169 
170     The transform function must accept two parameters, a node and
171     an optional context.
172     """"""
173 
174     def _transform_wrapper(node, context=None):
175         result = transform(node, context=context)
176         if result:
177             if not result.parent:
178                 # Let the transformation function determine
179                 # the parent for its result. Otherwise,
180                 # we set it to be the node we transformed from.
181                 result.parent = node
182 
183             if result.lineno is None:
184                 result.lineno = node.lineno
185             # Can be a 'Module' see https://github.com/PyCQA/pylint/issues/4671
186             # We don't have a regression test on this one: tread carefully
187             if hasattr(result, ""col_offset"") and result.col_offset is None:
188                 result.col_offset = node.col_offset
189         return iter([result])
190 
191     AstroidManager().register_transform(
192         nodes.Call,
193         inference_tip(_transform_wrapper),
194         partial(_builtin_filter_predicate, builtin_name=builtin_name),
195     )
196 
197 
198 def _container_generic_inference(node, context, node_type, transform):
199     args = node.args
200     if not args:
201         return node_type()
202     if len(node.args) > 1:
203         raise UseInferenceDefault()
204 
205     (arg,) = args
206     transformed = transform(arg)
207     if not transformed:
208         try:
209             inferred = next(arg.infer(context=context))
210         except (InferenceError, StopIteration) as exc:
211             raise UseInferenceDefault from exc
212         if inferred is util.Uninferable:
213             raise UseInferenceDefault
214         transformed = transform(inferred)
215     if not transformed or transformed is util.Uninferable:
216         raise UseInferenceDefault
217     return transformed
218 
219 
220 def _container_generic_transform(  # pylint: disable=inconsistent-return-statements
221     arg, context, klass, iterables, build_elts
222 ):
223     if isinstance(arg, klass):
224         return arg
225     if isinstance(arg, iterables):
226         if all(isinstance(elt, nodes.Const) for elt in arg.elts):
227             elts = [elt.value for elt in arg.elts]
228         else:
229             # TODO: Does not handle deduplication for sets.
230             elts = []
231             for element in arg.elts:
232                 if not element:
233                     continue
234                 inferred = helpers.safe_infer(element, context=context)
235                 if inferred:
236                     evaluated_object = nodes.EvaluatedObject(
237                         original=element, value=inferred
238                     )
239                     elts.append(evaluated_object)
240     elif isinstance(arg, nodes.Dict):
241         # Dicts need to have consts as strings already.
242         if not all(isinstance(elt[0], nodes.Const) for elt in arg.items):
243             raise UseInferenceDefault()
244         elts = [item[0].value for item in arg.items]
245     elif isinstance(arg, nodes.Const) and isinstance(arg.value, (str, bytes)):
246         elts = arg.value
247     else:
248         return
249     return klass.from_elements(elts=build_elts(elts))
250 
251 
252 def _infer_builtin_container(
253     node, context, klass=None, iterables=None, build_elts=None
254 ):
255     transform_func = partial(
256         _container_generic_transform,
257         context=context,
258         klass=klass,
259         iterables=iterables,
260         build_elts=build_elts,
261     )
262 
263     return _container_generic_inference(node, context, klass, transform_func)
264 
265 
266 # pylint: disable=invalid-name
267 infer_tuple = partial(
268     _infer_builtin_container,
269     klass=nodes.Tuple,
270     iterables=(
271         nodes.List,
272         nodes.Set,
273         objects.FrozenSet,
274         objects.DictItems,
275         objects.DictKeys,
276         objects.DictValues,
277     ),
278     build_elts=tuple,
279 )
280 
281 infer_list = partial(
282     _infer_builtin_container,
283     klass=nodes.List,
284     iterables=(
285         nodes.Tuple,
286         nodes.Set,
287         objects.FrozenSet,
288         objects.DictItems,
289         objects.DictKeys,
290         objects.DictValues,
291     ),
292     build_elts=list,
293 )
294 
295 infer_set = partial(
296     _infer_builtin_container,
297     klass=nodes.Set,
298     iterables=(nodes.List, nodes.Tuple, objects.FrozenSet, objects.DictKeys),
299     build_elts=set,
300 )
301 
302 infer_frozenset = partial(
303     _infer_builtin_container,
304     klass=objects.FrozenSet,
305     iterables=(nodes.List, nodes.Tuple, nodes.Set, objects.FrozenSet, objects.DictKeys),
306     build_elts=frozenset,
307 )
308 
309 
310 def _get_elts(arg, context):
311     def is_iterable(n):
312         return isinstance(n, (nodes.List, nodes.Tuple, nodes.Set))
313 
314     try:
315         inferred = next(arg.infer(context))
316     except (InferenceError, StopIteration) as exc:
317         raise UseInferenceDefault from exc
318     if isinstance(inferred, nodes.Dict):
319         items = inferred.items
320     elif is_iterable(inferred):
321         items = []
322         for elt in inferred.elts:
323             # If an item is not a pair of two items,
324             # then fallback to the default inference.
325             # Also, take in consideration only hashable items,
326             # tuples and consts. We are choosing Names as well.
327             if not is_iterable(elt):
328                 raise UseInferenceDefault()
329             if len(elt.elts) != 2:
330                 raise UseInferenceDefault()
331             if not isinstance(elt.elts[0], (nodes.Tuple, nodes.Const, nodes.Name)):
332                 raise UseInferenceDefault()
333             items.append(tuple(elt.elts))
334     else:
335         raise UseInferenceDefault()
336     return items
337 
338 
339 def infer_dict(node, context=None):
340     """"""Try to infer a dict call to a Dict node.
341 
342     The function treats the following cases:
343 
344         * dict()
345         * dict(mapping)
346         * dict(iterable)
347         * dict(iterable, **kwargs)
348         * dict(mapping, **kwargs)
349         * dict(**kwargs)
350 
351     If a case can't be inferred, we'll fallback to default inference.
352     """"""
353     call = arguments.CallSite.from_call(node, context=context)
354     if call.has_invalid_arguments() or call.has_invalid_keywords():
355         raise UseInferenceDefault
356 
357     args = call.positional_arguments
358     kwargs = list(call.keyword_arguments.items())
359 
360     if not args and not kwargs:
361         # dict()
362         return nodes.Dict()
363     if kwargs and not args:
364         # dict(a=1, b=2, c=4)
365         items = [(nodes.Const(key), value) for key, value in kwargs]
366     elif len(args) == 1 and kwargs:
367         # dict(some_iterable, b=2, c=4)
368         elts = _get_elts(args[0], context)
369         keys = [(nodes.Const(key), value) for key, value in kwargs]
370         items = elts + keys
371     elif len(args) == 1:
372         items = _get_elts(args[0], context)
373     else:
374         raise UseInferenceDefault()
375     value = nodes.Dict(
376         col_offset=node.col_offset, lineno=node.lineno, parent=node.parent
377     )
378     value.postinit(items)
379     return value
380 
381 
382 def infer_super(node, context=None):
383     """"""Understand super calls.
384 
385     There are some restrictions for what can be understood:
386 
387         * unbounded super (one argument form) is not understood.
388 
389         * if the super call is not inside a function (classmethod or method),
390           then the default inference will be used.
391 
392         * if the super arguments can't be inferred, the default inference
393           will be used.
394     """"""
395     if len(node.args) == 1:
396         # Ignore unbounded super.
397         raise UseInferenceDefault
398 
399     scope = node.scope()
400     if not isinstance(scope, nodes.FunctionDef):
401         # Ignore non-method uses of super.
402         raise UseInferenceDefault
403     if scope.type not in (""classmethod"", ""method""):
404         # Not interested in staticmethods.
405         raise UseInferenceDefault
406 
407     cls = scoped_nodes.get_wrapping_class(scope)
408     if not node.args:
409         mro_pointer = cls
410         # In we are in a classmethod, the interpreter will fill
411         # automatically the class as the second argument, not an instance.
412         if scope.type == ""classmethod"":
413             mro_type = cls
414         else:
415             mro_type = cls.instantiate_class()
416     else:
417         try:
418             mro_pointer = next(node.args[0].infer(context=context))
419         except (InferenceError, StopIteration) as exc:
420             raise UseInferenceDefault from exc
421         try:
422             mro_type = next(node.args[1].infer(context=context))
423         except (InferenceError, StopIteration) as exc:
424             raise UseInferenceDefault from exc
425 
426     if mro_pointer is util.Uninferable or mro_type is util.Uninferable:
427         # No way we could understand this.
428         raise UseInferenceDefault
429 
430     super_obj = objects.Super(
431         mro_pointer=mro_pointer, mro_type=mro_type, self_class=cls, scope=scope
432     )
433     super_obj.parent = node
434     return super_obj
435 
436 
437 def _infer_getattr_args(node, context):
438     if len(node.args) not in (2, 3):
439         # Not a valid getattr call.
440         raise UseInferenceDefault
441 
442     try:
443         obj = next(node.args[0].infer(context=context))
444         attr = next(node.args[1].infer(context=context))
445     except (InferenceError, StopIteration) as exc:
446         raise UseInferenceDefault from exc
447 
448     if obj is util.Uninferable or attr is util.Uninferable:
449         # If one of the arguments is something we can't infer,
450         # then also make the result of the getattr call something
451         # which is unknown.
452         return util.Uninferable, util.Uninferable
453 
454     is_string = isinstance(attr, nodes.Const) and isinstance(attr.value, str)
455     if not is_string:
456         raise UseInferenceDefault
457 
458     return obj, attr.value
459 
460 
461 def infer_getattr(node, context=None):
462     """"""Understand getattr calls
463 
464     If one of the arguments is an Uninferable object, then the
465     result will be an Uninferable object. Otherwise, the normal attribute
466     lookup will be done.
467     """"""
468     obj, attr = _infer_getattr_args(node, context)
469     if (
470         obj is util.Uninferable
471         or attr is util.Uninferable
472         or not hasattr(obj, ""igetattr"")
473     ):
474         return util.Uninferable
475 
476     try:
477         return next(obj.igetattr(attr, context=context))
478     except (StopIteration, InferenceError, AttributeInferenceError):
479         if len(node.args) == 3:
480             # Try to infer the default and return it instead.
481             try:
482                 return next(node.args[2].infer(context=context))
483             except (StopIteration, InferenceError) as exc:
484                 raise UseInferenceDefault from exc
485 
486     raise UseInferenceDefault
487 
488 
489 def infer_hasattr(node, context=None):
490     """"""Understand hasattr calls
491 
492     This always guarantees three possible outcomes for calling
493     hasattr: Const(False) when we are sure that the object
494     doesn't have the intended attribute, Const(True) when
495     we know that the object has the attribute and Uninferable
496     when we are unsure of the outcome of the function call.
497     """"""
498     try:
499         obj, attr = _infer_getattr_args(node, context)
500         if (
501             obj is util.Uninferable
502             or attr is util.Uninferable
503             or not hasattr(obj, ""getattr"")
504         ):
505             return util.Uninferable
506         obj.getattr(attr, context=context)
507     except UseInferenceDefault:
508         # Can't infer something from this function call.
509         return util.Uninferable
510     except AttributeInferenceError:
511         # Doesn't have it.
512         return nodes.Const(False)
513     return nodes.Const(True)
514 
515 
516 def infer_callable(node, context=None):
517     """"""Understand callable calls
518 
519     This follows Python's semantics, where an object
520     is callable if it provides an attribute __call__,
521     even though that attribute is something which can't be
522     called.
523     """"""
524     if len(node.args) != 1:
525         # Invalid callable call.
526         raise UseInferenceDefault
527 
528     argument = node.args[0]
529     try:
530         inferred = next(argument.infer(context=context))
531     except (InferenceError, StopIteration):
532         return util.Uninferable
533     if inferred is util.Uninferable:
534         return util.Uninferable
535     return nodes.Const(inferred.callable())
536 
537 
538 def infer_property(
539     node: nodes.Call, context: InferenceContext | None = None
540 ) -> objects.Property:
541     """"""Understand `property` class
542 
543     This only infers the output of `property`
544     call, not the arguments themselves.
545     """"""
546     if len(node.args) < 1:
547         # Invalid property call.
548         raise UseInferenceDefault
549 
550     getter = node.args[0]
551     try:
552         inferred = next(getter.infer(context=context))
553     except (InferenceError, StopIteration) as exc:
554         raise UseInferenceDefault from exc
555 
556     if not isinstance(inferred, (nodes.FunctionDef, nodes.Lambda)):
557         raise UseInferenceDefault
558 
559     prop_func = objects.Property(
560         function=inferred,
561         name=inferred.name,
562         lineno=node.lineno,
563         parent=node,
564         col_offset=node.col_offset,
565     )
566     prop_func.postinit(
567         body=[],
568         args=inferred.args,
569         doc_node=getattr(inferred, ""doc_node"", None),
570     )
571     return prop_func
572 
573 
574 def infer_bool(node, context=None):
575     """"""Understand bool calls.""""""
576     if len(node.args) > 1:
577         # Invalid bool call.
578         raise UseInferenceDefault
579 
580     if not node.args:
581         return nodes.Const(False)
582 
583     argument = node.args[0]
584     try:
585         inferred = next(argument.infer(context=context))
586     except (InferenceError, StopIteration):
587         return util.Uninferable
588     if inferred is util.Uninferable:
589         return util.Uninferable
590 
591     bool_value = inferred.bool_value(context=context)
592     if bool_value is util.Uninferable:
593         return util.Uninferable
594     return nodes.Const(bool_value)
595 
596 
597 def infer_type(node, context=None):
598     """"""Understand the one-argument form of *type*.""""""
599     if len(node.args) != 1:
600         raise UseInferenceDefault
601 
602     return helpers.object_type(node.args[0], context)
603 
604 
605 def infer_slice(node, context=None):
606     """"""Understand `slice` calls.""""""
607     args = node.args
608     if not 0 < len(args) <= 3:
609         raise UseInferenceDefault
610 
611     infer_func = partial(helpers.safe_infer, context=context)
612     args = [infer_func(arg) for arg in args]
613     for arg in args:
614         if not arg or arg is util.Uninferable:
615             raise UseInferenceDefault
616         if not isinstance(arg, nodes.Const):
617             raise UseInferenceDefault
618         if not isinstance(arg.value, (type(None), int)):
619             raise UseInferenceDefault
620 
621     if len(args) < 3:
622         # Make sure we have 3 arguments.
623         args.extend([None] * (3 - len(args)))
624 
625     slice_node = nodes.Slice(
626         lineno=node.lineno, col_offset=node.col_offset, parent=node.parent
627     )
628     slice_node.postinit(*args)
629     return slice_node
630 
631 
632 def _infer_object__new__decorator(node, context=None):
633     # Instantiate class immediately
634     # since that's what @object.__new__ does
635     return iter((node.instantiate_class(),))
636 
637 
638 def _infer_object__new__decorator_check(node):
639     """"""Predicate before inference_tip
640 
641     Check if the given ClassDef has an @object.__new__ decorator
642     """"""
643     if not node.decorators:
644         return False
645 
646     for decorator in node.decorators.nodes:
647         if isinstance(decorator, nodes.Attribute):
648             if decorator.as_string() == OBJECT_DUNDER_NEW:
649                 return True
650     return False
651 
652 
653 def infer_issubclass(callnode, context=None):
654     """"""Infer issubclass() calls
655 
656     :param nodes.Call callnode: an `issubclass` call
657     :param InferenceContext context: the context for the inference
658     :rtype nodes.Const: Boolean Const value of the `issubclass` call
659     :raises UseInferenceDefault: If the node cannot be inferred
660     """"""
661     call = arguments.CallSite.from_call(callnode, context=context)
662     if call.keyword_arguments:
663         # issubclass doesn't support keyword arguments
664         raise UseInferenceDefault(""TypeError: issubclass() takes no keyword arguments"")
665     if len(call.positional_arguments) != 2:
666         raise UseInferenceDefault(
667             f""Expected two arguments, got {len(call.positional_arguments)}""
668         )
669     # The left hand argument is the obj to be checked
670     obj_node, class_or_tuple_node = call.positional_arguments
671 
672     try:
673         obj_type = next(obj_node.infer(context=context))
674     except (InferenceError, StopIteration) as exc:
675         raise UseInferenceDefault from exc
676     if not isinstance(obj_type, nodes.ClassDef):
677         raise UseInferenceDefault(""TypeError: arg 1 must be class"")
678 
679     # The right hand argument is the class(es) that the given
680     # object is to be checked against.
681     try:
682         class_container = _class_or_tuple_to_container(
683             class_or_tuple_node, context=context
684         )
685     except InferenceError as exc:
686         raise UseInferenceDefault from exc
687     try:
688         issubclass_bool = helpers.object_issubclass(obj_type, class_container, context)
689     except AstroidTypeError as exc:
690         raise UseInferenceDefault(""TypeError: "" + str(exc)) from exc
691     except MroError as exc:
692         raise UseInferenceDefault from exc
693     return nodes.Const(issubclass_bool)
694 
695 
696 def infer_isinstance(callnode, context=None):
697     """"""Infer isinstance calls
698 
699     :param nodes.Call callnode: an isinstance call
700     :param InferenceContext context: context for call
701         (currently unused but is a common interface for inference)
702     :rtype nodes.Const: Boolean Const value of isinstance call
703 
704     :raises UseInferenceDefault: If the node cannot be inferred
705     """"""
706     call = arguments.CallSite.from_call(callnode, context=context)
707     if call.keyword_arguments:
708         # isinstance doesn't support keyword arguments
709         raise UseInferenceDefault(""TypeError: isinstance() takes no keyword arguments"")
710     if len(call.positional_arguments) != 2:
711         raise UseInferenceDefault(
712             f""Expected two arguments, got {len(call.positional_arguments)}""
713         )
714     # The left hand argument is the obj to be checked
715     obj_node, class_or_tuple_node = call.positional_arguments
716     # The right hand argument is the class(es) that the given
717     # obj is to be check is an instance of
718     try:
719         class_container = _class_or_tuple_to_container(
720             class_or_tuple_node, context=context
721         )
722     except InferenceError as exc:
723         raise UseInferenceDefault from exc
724     try:
725         isinstance_bool = helpers.object_isinstance(obj_node, class_container, context)
726     except AstroidTypeError as exc:
727         raise UseInferenceDefault(""TypeError: "" + str(exc)) from exc
728     except MroError as exc:
729         raise UseInferenceDefault from exc
730     if isinstance_bool is util.Uninferable:
731         raise UseInferenceDefault
732     return nodes.Const(isinstance_bool)
733 
734 
735 def _class_or_tuple_to_container(node, context=None):
736     # Move inferences results into container
737     # to simplify later logic
738     # raises InferenceError if any of the inferences fall through
739     try:
740         node_infer = next(node.infer(context=context))
741     except StopIteration as e:
742         raise InferenceError(node=node, context=context) from e
743     # arg2 MUST be a type or a TUPLE of types
744     # for isinstance
745     if isinstance(node_infer, nodes.Tuple):
746         try:
747             class_container = [
748                 next(node.infer(context=context)) for node in node_infer.elts
749             ]
750         except StopIteration as e:
751             raise InferenceError(node=node, context=context) from e
752         class_container = [
753             klass_node for klass_node in class_container if klass_node is not None
754         ]
755     else:
756         class_container = [node_infer]
757     return class_container
758 
759 
760 def infer_len(node, context=None):
761     """"""Infer length calls
762 
763     :param nodes.Call node: len call to infer
764     :param context.InferenceContext: node context
765     :rtype nodes.Const: a Const node with the inferred length, if possible
766     """"""
767     call = arguments.CallSite.from_call(node, context=context)
768     if call.keyword_arguments:
769         raise UseInferenceDefault(""TypeError: len() must take no keyword arguments"")
770     if len(call.positional_arguments) != 1:
771         raise UseInferenceDefault(
772             ""TypeError: len() must take exactly one argument ""
773             ""({len}) given"".format(len=len(call.positional_arguments))
774         )
775     [argument_node] = call.positional_arguments
776 
777     try:
778         return nodes.Const(helpers.object_len(argument_node, context=context))
779     except (AstroidTypeError, InferenceError) as exc:
780         raise UseInferenceDefault(str(exc)) from exc
781 
782 
783 def infer_str(node, context=None):
784     """"""Infer str() calls
785 
786     :param nodes.Call node: str() call to infer
787     :param context.InferenceContext: node context
788     :rtype nodes.Const: a Const containing an empty string
789     """"""
790     call = arguments.CallSite.from_call(node, context=context)
791     if call.keyword_arguments:
792         raise UseInferenceDefault(""TypeError: str() must take no keyword arguments"")
793     try:
794         return nodes.Const("""")
795     except (AstroidTypeError, InferenceError) as exc:
796         raise UseInferenceDefault(str(exc)) from exc
797 
798 
799 def infer_int(node, context=None):
800     """"""Infer int() calls
801 
802     :param nodes.Call node: int() call to infer
803     :param context.InferenceContext: node context
804     :rtype nodes.Const: a Const containing the integer value of the int() call
805     """"""
806     call = arguments.CallSite.from_call(node, context=context)
807     if call.keyword_arguments:
808         raise UseInferenceDefault(""TypeError: int() must take no keyword arguments"")
809 
810     if call.positional_arguments:
811         try:
812             first_value = next(call.positional_arguments[0].infer(context=context))
813         except (InferenceError, StopIteration) as exc:
814             raise UseInferenceDefault(str(exc)) from exc
815 
816         if first_value is util.Uninferable:
817             raise UseInferenceDefault
818 
819         if isinstance(first_value, nodes.Const) and isinstance(
820             first_value.value, (int, str)
821         ):
822             try:
823                 actual_value = int(first_value.value)
824             except ValueError:
825                 return nodes.Const(0)
826             return nodes.Const(actual_value)
827 
828     return nodes.Const(0)
829 
830 
831 def infer_dict_fromkeys(node, context=None):
832     """"""Infer dict.fromkeys
833 
834     :param nodes.Call node: dict.fromkeys() call to infer
835     :param context.InferenceContext context: node context
836     :rtype nodes.Dict:
837         a Dictionary containing the values that astroid was able to infer.
838         In case the inference failed for any reason, an empty dictionary
839         will be inferred instead.
840     """"""
841 
842     def _build_dict_with_elements(elements):
843         new_node = nodes.Dict(
844             col_offset=node.col_offset, lineno=node.lineno, parent=node.parent
845         )
846         new_node.postinit(elements)
847         return new_node
848 
849     call = arguments.CallSite.from_call(node, context=context)
850     if call.keyword_arguments:
851         raise UseInferenceDefault(""TypeError: int() must take no keyword arguments"")
852     if len(call.positional_arguments) not in {1, 2}:
853         raise UseInferenceDefault(
854             ""TypeError: Needs between 1 and 2 positional arguments""
855         )
856 
857     default = nodes.Const(None)
858     values = call.positional_arguments[0]
859     try:
860         inferred_values = next(values.infer(context=context))
861     except (InferenceError, StopIteration):
862         return _build_dict_with_elements([])
863     if inferred_values is util.Uninferable:
864         return _build_dict_with_elements([])
865 
866     # Limit to a couple of potential values, as this can become pretty complicated
867     accepted_iterable_elements = (nodes.Const,)
868     if isinstance(inferred_values, (nodes.List, nodes.Set, nodes.Tuple)):
869         elements = inferred_values.elts
870         for element in elements:
871             if not isinstance(element, accepted_iterable_elements):
872                 # Fallback to an empty dict
873                 return _build_dict_with_elements([])
874 
875         elements_with_value = [(element, default) for element in elements]
876         return _build_dict_with_elements(elements_with_value)
877     if isinstance(inferred_values, nodes.Const) and isinstance(
878         inferred_values.value, (str, bytes)
879     ):
880         elements = [
881             (nodes.Const(element), default) for element in inferred_values.value
882         ]
883         return _build_dict_with_elements(elements)
884     if isinstance(inferred_values, nodes.Dict):
885         keys = inferred_values.itered()
886         for key in keys:
887             if not isinstance(key, accepted_iterable_elements):
888                 # Fallback to an empty dict
889                 return _build_dict_with_elements([])
890 
891         elements_with_value = [(element, default) for element in keys]
892         return _build_dict_with_elements(elements_with_value)
893 
894     # Fallback to an empty dictionary
895     return _build_dict_with_elements([])
896 
897 
898 def _infer_copy_method(
899     node: nodes.Call, context: InferenceContext | None = None
900 ) -> Iterator[nodes.NodeNG]:
901     assert isinstance(node.func, nodes.Attribute)
902     inferred_orig, inferred_copy = itertools.tee(node.func.expr.infer(context=context))
903     if all(
904         isinstance(
905             inferred_node, (nodes.Dict, nodes.List, nodes.Set, objects.FrozenSet)
906         )
907         for inferred_node in inferred_orig
908     ):
909         return inferred_copy
910 
911     raise UseInferenceDefault()
912 
913 
914 def _is_str_format_call(node: nodes.Call) -> bool:
915     """"""Catch calls to str.format().""""""
916     if not isinstance(node.func, nodes.Attribute) or not node.func.attrname == ""format"":
917         return False
918 
919     if isinstance(node.func.expr, nodes.Name):
920         value = helpers.safe_infer(node.func.expr)
921     else:
922         value = node.func.expr
923 
924     return isinstance(value, nodes.Const) and isinstance(value.value, str)
925 
926 
927 def _infer_str_format_call(
928     node: nodes.Call, context: InferenceContext | None = None
929 ) -> Iterator[nodes.Const | type[util.Uninferable]]:
930     """"""Return a Const node based on the template and passed arguments.""""""
931     call = arguments.CallSite.from_call(node, context=context)
932     if isinstance(node.func.expr, nodes.Name):
933         value: nodes.Const = helpers.safe_infer(node.func.expr)
934     else:
935         value = node.func.expr
936 
937     format_template = value.value
938 
939     # Get the positional arguments passed
940     inferred_positional = [
941         helpers.safe_infer(i, context) for i in call.positional_arguments
942     ]
943     if not all(isinstance(i, nodes.Const) for i in inferred_positional):
944         return iter([util.Uninferable])
945     pos_values: list[str] = [i.value for i in inferred_positional]
946 
947     # Get the keyword arguments passed
948     inferred_keyword = {
949         k: helpers.safe_infer(v, context) for k, v in call.keyword_arguments.items()
950     }
951     if not all(isinstance(i, nodes.Const) for i in inferred_keyword.values()):
952         return iter([util.Uninferable])
953     keyword_values: dict[str, str] = {k: v.value for k, v in inferred_keyword.items()}
954 
955     try:
956         formatted_string = format_template.format(*pos_values, **keyword_values)
957     except (IndexError, KeyError):
958         # If there is an IndexError there are too few arguments to interpolate
959         return iter([util.Uninferable])
960 
961     return iter([nodes.const_factory(formatted_string)])
962 
963 
964 # Builtins inference
965 register_builtin_transform(infer_bool, ""bool"")
966 register_builtin_transform(infer_super, ""super"")
967 register_builtin_transform(infer_callable, ""callable"")
968 register_builtin_transform(infer_property, ""property"")
969 register_builtin_transform(infer_getattr, ""getattr"")
970 register_builtin_transform(infer_hasattr, ""hasattr"")
971 register_builtin_transform(infer_tuple, ""tuple"")
972 register_builtin_transform(infer_set, ""set"")
973 register_builtin_transform(infer_list, ""list"")
974 register_builtin_transform(infer_dict, ""dict"")
975 register_builtin_transform(infer_frozenset, ""frozenset"")
976 register_builtin_transform(infer_type, ""type"")
977 register_builtin_transform(infer_slice, ""slice"")
978 register_builtin_transform(infer_isinstance, ""isinstance"")
979 register_builtin_transform(infer_issubclass, ""issubclass"")
980 register_builtin_transform(infer_len, ""len"")
981 register_builtin_transform(infer_str, ""str"")
982 register_builtin_transform(infer_int, ""int"")
983 register_builtin_transform(infer_dict_fromkeys, ""dict.fromkeys"")
984 
985 
986 # Infer object.__new__ calls
987 AstroidManager().register_transform(
988     nodes.ClassDef,
989     inference_tip(_infer_object__new__decorator),
990     _infer_object__new__decorator_check,
991 )
992 
993 AstroidManager().register_transform(
994     nodes.Call,
995     inference_tip(_infer_copy_method),
996     lambda node: isinstance(node.func, nodes.Attribute)
997     and node.func.attrname == ""copy"",
998 )
999 
1000 AstroidManager().register_transform(
1001     nodes.Call, inference_tip(_infer_str_format_call), _is_str_format_call
1002 )
1003 
[end of astroid/brain/brain_builtin_inference.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pylint-dev/astroid,6cf238d089cf4b6753c94cfc089b4a47487711e5,"""TypeError: unsupported format string passed to NoneType.__format__"" while running type inference in version 2.12.x
### Steps to reproduce

I have no concise reproducer. Exception happens every time I run pylint on some internal code, with astroid 2.12.10 and 2.12.12 (debian bookworm). It does _not_ happen with earlier versions of astroid (not with version 2.9). The pylinted code itself is ""valid"", it runs in production here.

### Current behavior

When running pylint on some code, I get this exception:
```
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/pylint/utils/ast_walker.py"", line 90, in walk
    callback(astroid)
  File ""/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py"", line 183, in visit_functiondef
    inferred = _safe_infer_call_result(node, node)
  File ""/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py"", line 42, in _safe_infer_call_result
    value = next(inferit)
  File ""/usr/lib/python3/dist-packages/astroid/nodes/scoped_nodes/scoped_nodes.py"", line 1749, in infer_call_result
    yield from returnnode.value.infer(context)
  File ""/usr/lib/python3/dist-packages/astroid/nodes/node_ng.py"", line 159, in infer
    results = list(self._explicit_inference(self, context, **kwargs))
  File ""/usr/lib/python3/dist-packages/astroid/inference_tip.py"", line 45, in _inference_tip_cached
    result = _cache[func, node] = list(func(*args, **kwargs))
  File ""/usr/lib/python3/dist-packages/astroid/brain/brain_builtin_inference.py"", line 956, in _infer_str_format_call
    formatted_string = format_template.format(*pos_values, **keyword_values)
TypeError: unsupported format string passed to NoneType.__format__
```

### Expected behavior

TypeError exception should not happen

### `python -c ""from astroid import __pkginfo__; print(__pkginfo__.version)""` output

2.12.10,
2.12.12
","Hi @crosser, thanks for the report.

> I have no concise reproducer. 

We might be able to help you distill one.

`pylint` produces a crash report, and shows the link in your terminal, like this:
```shell
************* Module a
a.py:1:0: F0002: a.py: Fatal error while checking 'a.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/Users/.../Library/Caches/pylint/pylint-crash-2022-10-29-08-48-25.txt'. (astroid-error)
```
The offending file is at the top of the crash report. If the code is too long, or contains sensitive information, you can use the knowledge that the crash happened in `_infer_str_format_call` to look for calls to `.format()` on strings. You should be able to then just provide us those calls--and enough surrounding code to rebuild the objects you provided to `format()`. 

Doing this would be a tremendous help!
> `pylint` produces a crash report, and shows the link in your terminal, like this:

No, not really, it does not. I am attaching a (censored) stderr from running the test. The line in the source code that apparently triggers the problem is pretty innocuous:

```
    @property
    def vnet_id(self):  # <---- this is the line 266 that is mentioned in the ""Exception on node"" message
        if ...:
```
There is very similar property definition right before this one, that does not trigger the problem.

[pyerr.txt](https://github.com/PyCQA/astroid/files/9900190/pyerr.txt)

Pylint command was `python3 -m pylint --jobs=0 --rcfile=test/style/pylint.conf <project-dir>`

```
$ pylint --version
pylint 2.15.5
astroid 2.12.12
Python 3.10.8 (main, Oct 24 2022, 10:07:16) [GCC 12.2.0]
```

edit:
> enough surrounding code to rebuild the objects you provided to format().

_I_ did not provide any objects to `format()`, astroid did...
Thanks for providing the traceback.

> No, not really, it does not. I am attaching a (censored) stderr from running the test. 

I see now that it's because you're invoking pylint from a unittest, so your test is managing the output.

> The line in the source code that apparently triggers the problem is pretty innocuous:

The deeper failure is on the call in line 268, not the function def on line 266. Is there anything you can sanitize and tell us about line 268? Thanks again for providing the help.
> I see now that it's because you're invoking pylint from a unittest, so your test is managing the output.

When I run pylint by hand

```
pylint --jobs=0 --rcfile=test/style/pylint.conf <module-name> | tee /tmp/pyerr.txt
```
there is still no ""Fatal error while checking ..."" message in the output

> > The line in the source code that apparently triggers the problem is pretty innocuous:
> 
> The deeper failure is on the call in line 268, not the function def on line 266. Is there anything you can sanitize and tell us about line 268? Thanks again for providing the help.

Oh yes, there is a `something.format()` in that line! But the ""something"" is a literal string:
```
    @property
    def vnet_id(self):
        if self.backend == ""something"":
            return ""{:04x}{:04x}n{:d}"".format(  # <---- this is line 268
                self.<some-attr>, self.<another-attr>, self.<third-attr>
            )
        if self.backend == ""somethingelse"":
            return ""h{:08}n{:d}"".format(self.<more-attr>, self.<and more>)
        return None
```

Thanks, that was very helpful. Here is a reproducer:
```python
x = ""{:c}"".format(None)
```",2022-11-12T19:21:34Z,"<patch>
diff --git a/astroid/brain/brain_builtin_inference.py b/astroid/brain/brain_builtin_inference.py
--- a/astroid/brain/brain_builtin_inference.py
+++ b/astroid/brain/brain_builtin_inference.py
@@ -954,8 +954,10 @@ def _infer_str_format_call(
 
     try:
         formatted_string = format_template.format(*pos_values, **keyword_values)
-    except (IndexError, KeyError):
-        # If there is an IndexError there are too few arguments to interpolate
+    except (IndexError, KeyError, TypeError, ValueError):
+        # IndexError: there are too few arguments to interpolate
+        # TypeError: Unsupported format string
+        # ValueError: Unknown format code
         return iter([util.Uninferable])
 
     return iter([nodes.const_factory(formatted_string)])

</patch>","diff --git a/tests/unittest_brain_builtin.py b/tests/unittest_brain_builtin.py
--- a/tests/unittest_brain_builtin.py
+++ b/tests/unittest_brain_builtin.py
@@ -103,6 +103,12 @@ def test_string_format(self, format_string: str) -> None:
             """"""
             ""My name is {fname}, I'm {age}"".format(fsname = ""Daniel"", age = 12)
             """""",
+            """"""
+            ""My unicode character is {:c}"".format(None)
+            """""",
+            """"""
+            ""My hex format is {:4x}"".format('1')
+            """""",
         ],
     )
     def test_string_format_uninferable(self, format_string: str) -> None:
",2.13,"[""tests/unittest_brain_builtin.py::TestStringNodes::test_string_format_uninferable[\\n""]","[""tests/unittest_brain_builtin.py::BuiltinsTest::test_infer_property"", ""tests/unittest_brain_builtin.py::TestStringNodes::test_string_format[empty-indexes]"", ""tests/unittest_brain_builtin.py::TestStringNodes::test_string_format[numbered-indexes]"", ""tests/unittest_brain_builtin.py::TestStringNodes::test_string_format[named-indexes]"", ""tests/unittest_brain_builtin.py::TestStringNodes::test_string_format[numbered-indexes-from-positional]"", ""tests/unittest_brain_builtin.py::TestStringNodes::test_string_format[named-indexes-from-keyword]"", ""tests/unittest_brain_builtin.py::TestStringNodes::test_string_format[mixed-indexes-from-mixed]"", ""tests/unittest_brain_builtin.py::TestStringNodes::test_string_format[empty-indexes-on-variable]"", ""tests/unittest_brain_builtin.py::TestStringNodes::test_string_format_uninferable[\""I"", ""tests/unittest_brain_builtin.py::TestStringNodes::test_string_format_with_specs""]",fe058bff95745371df5796286d33677c21137847
pylint-dev__astroid-1268,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
'AsStringVisitor' object has no attribute 'visit_unknown'
```python
>>> import astroid
>>> astroid.nodes.Unknown().as_string()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py"", line 609, in as_string
    return AsStringVisitor()(self)
  File ""/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/as_string.py"", line 56, in __call__
    return node.accept(self).replace(DOC_NEWLINE, ""\n"")
  File ""/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py"", line 220, in accept
    func = getattr(visitor, ""visit_"" + self.__class__.__name__.lower())
AttributeError: 'AsStringVisitor' object has no attribute 'visit_unknown'
>>> 
```
### `python -c ""from astroid import __pkginfo__; print(__pkginfo__.version)""` output

2.8.6-dev0

</issue>
<code>
[start of README.rst]
1 Astroid
2 =======
3 
4 .. image:: https://coveralls.io/repos/github/PyCQA/astroid/badge.svg?branch=main
5     :target: https://coveralls.io/github/PyCQA/astroid?branch=main
6     :alt: Coverage badge from coveralls.io
7 
8 .. image:: https://readthedocs.org/projects/astroid/badge/?version=latest
9     :target: http://astroid.readthedocs.io/en/latest/?badge=latest
10     :alt: Documentation Status
11 
12 .. image:: https://img.shields.io/badge/code%20style-black-000000.svg
13     :target: https://github.com/ambv/black
14 
15 .. image:: https://results.pre-commit.ci/badge/github/PyCQA/astroid/main.svg
16    :target: https://results.pre-commit.ci/latest/github/PyCQA/astroid/main
17    :alt: pre-commit.ci status
18 
19 .. |tidelift_logo| image:: https://raw.githubusercontent.com/PyCQA/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png
20    :width: 75
21    :height: 60
22    :alt: Tidelift
23 
24 .. list-table::
25    :widths: 10 100
26 
27    * - |tidelift_logo|
28      - Professional support for astroid is available as part of the
29        `Tidelift Subscription`_.  Tidelift gives software development teams a single source for
30        purchasing and maintaining their software, with professional grade assurances
31        from the experts who know it best, while seamlessly integrating with existing
32        tools.
33 
34 .. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme
35 
36 
37 
38 What's this?
39 ------------
40 
41 The aim of this module is to provide a common base representation of
42 python source code. It is currently the library powering pylint's capabilities.
43 
44 It provides a compatible representation which comes from the `_ast`
45 module.  It rebuilds the tree generated by the builtin _ast module by
46 recursively walking down the AST and building an extended ast. The new
47 node classes have additional methods and attributes for different
48 usages. They include some support for static inference and local name
49 scopes. Furthermore, astroid can also build partial trees by inspecting living
50 objects.
51 
52 
53 Installation
54 ------------
55 
56 Extract the tarball, jump into the created directory and run::
57 
58     pip install .
59 
60 
61 If you want to do an editable installation, you can run::
62 
63     pip install -e .
64 
65 
66 If you have any questions, please mail the code-quality@python.org
67 mailing list for support. See
68 http://mail.python.org/mailman/listinfo/code-quality for subscription
69 information and archives.
70 
71 Documentation
72 -------------
73 http://astroid.readthedocs.io/en/latest/
74 
75 
76 Python Versions
77 ---------------
78 
79 astroid 2.0 is currently available for Python 3 only. If you want Python 2
80 support, use an older version of astroid (though note that these versions
81 are no longer supported).
82 
83 Test
84 ----
85 
86 Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use
87 either `tox` or `pytest`::
88 
89     tox
90     pytest astroid
91 
[end of README.rst]
[start of astroid/nodes/as_string.py]
1 # Copyright (c) 2009-2011, 2013-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>
2 # Copyright (c) 2010 Daniel Harding <dharding@gmail.com>
3 # Copyright (c) 2013-2016, 2018-2020 Claudiu Popa <pcmanticore@gmail.com>
4 # Copyright (c) 2013-2014 Google, Inc.
5 # Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>
6 # Copyright (c) 2016 Jared Garst <jgarst@users.noreply.github.com>
7 # Copyright (c) 2016 Jakub Wilk <jwilk@jwilk.net>
8 # Copyright (c) 2017, 2019 Åukasz Rogalski <rogalski.91@gmail.com>
9 # Copyright (c) 2017 rr- <rr-@sakuya.pl>
10 # Copyright (c) 2018 Serhiy Storchaka <storchaka@gmail.com>
11 # Copyright (c) 2018 Ville SkyttÃ¤ <ville.skytta@iki.fi>
12 # Copyright (c) 2018 brendanator <brendan.maginnis@gmail.com>
13 # Copyright (c) 2018 Nick Drozd <nicholasdrozd@gmail.com>
14 # Copyright (c) 2019 Alex Hall <alex.mojaki@gmail.com>
15 # Copyright (c) 2019 Hugo van Kemenade <hugovk@users.noreply.github.com>
16 # Copyright (c) 2021 DaniÃ«l van Noord <13665637+DanielNoord@users.noreply.github.com>
17 # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>
18 # Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>
19 # Copyright (c) 2021 pre-commit-ci[bot] <bot@noreply.github.com>
20 
21 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html
22 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE
23 
24 """"""This module renders Astroid nodes as string""""""
25 from typing import TYPE_CHECKING, List
26 
27 if TYPE_CHECKING:
28     from astroid.nodes.node_classes import (
29         Match,
30         MatchAs,
31         MatchCase,
32         MatchClass,
33         MatchMapping,
34         MatchOr,
35         MatchSequence,
36         MatchSingleton,
37         MatchStar,
38         MatchValue,
39     )
40 
41 # pylint: disable=unused-argument
42 
43 DOC_NEWLINE = ""\0""
44 
45 
46 # Visitor pattern require argument all the time and is not better with staticmethod
47 # noinspection PyUnusedLocal,PyMethodMayBeStatic
48 class AsStringVisitor:
49     """"""Visitor to render an Astroid node as a valid python code string""""""
50 
51     def __init__(self, indent=""    ""):
52         self.indent = indent
53 
54     def __call__(self, node):
55         """"""Makes this visitor behave as a simple function""""""
56         return node.accept(self).replace(DOC_NEWLINE, ""\n"")
57 
58     def _docs_dedent(self, doc):
59         """"""Stop newlines in docs being indented by self._stmt_list""""""
60         return '\n{}""""""{}""""""'.format(self.indent, doc.replace(""\n"", DOC_NEWLINE))
61 
62     def _stmt_list(self, stmts, indent=True):
63         """"""return a list of nodes to string""""""
64         stmts = ""\n"".join(nstr for nstr in [n.accept(self) for n in stmts] if nstr)
65         if indent:
66             return self.indent + stmts.replace(""\n"", ""\n"" + self.indent)
67 
68         return stmts
69 
70     def _precedence_parens(self, node, child, is_left=True):
71         """"""Wrap child in parens only if required to keep same semantics""""""
72         if self._should_wrap(node, child, is_left):
73             return f""({child.accept(self)})""
74 
75         return child.accept(self)
76 
77     def _should_wrap(self, node, child, is_left):
78         """"""Wrap child if:
79         - it has lower precedence
80         - same precedence with position opposite to associativity direction
81         """"""
82         node_precedence = node.op_precedence()
83         child_precedence = child.op_precedence()
84 
85         if node_precedence > child_precedence:
86             # 3 * (4 + 5)
87             return True
88 
89         if (
90             node_precedence == child_precedence
91             and is_left != node.op_left_associative()
92         ):
93             # 3 - (4 - 5)
94             # (2**3)**4
95             return True
96 
97         return False
98 
99     # visit_<node> methods ###########################################
100 
101     def visit_await(self, node):
102         return f""await {node.value.accept(self)}""
103 
104     def visit_asyncwith(self, node):
105         return f""async {self.visit_with(node)}""
106 
107     def visit_asyncfor(self, node):
108         return f""async {self.visit_for(node)}""
109 
110     def visit_arguments(self, node):
111         """"""return an astroid.Function node as string""""""
112         return node.format_args()
113 
114     def visit_assignattr(self, node):
115         """"""return an astroid.AssAttr node as string""""""
116         return self.visit_attribute(node)
117 
118     def visit_assert(self, node):
119         """"""return an astroid.Assert node as string""""""
120         if node.fail:
121             return f""assert {node.test.accept(self)}, {node.fail.accept(self)}""
122         return f""assert {node.test.accept(self)}""
123 
124     def visit_assignname(self, node):
125         """"""return an astroid.AssName node as string""""""
126         return node.name
127 
128     def visit_assign(self, node):
129         """"""return an astroid.Assign node as string""""""
130         lhs = "" = "".join(n.accept(self) for n in node.targets)
131         return f""{lhs} = {node.value.accept(self)}""
132 
133     def visit_augassign(self, node):
134         """"""return an astroid.AugAssign node as string""""""
135         return f""{node.target.accept(self)} {node.op} {node.value.accept(self)}""
136 
137     def visit_annassign(self, node):
138         """"""Return an astroid.AugAssign node as string""""""
139 
140         target = node.target.accept(self)
141         annotation = node.annotation.accept(self)
142         if node.value is None:
143             return f""{target}: {annotation}""
144         return f""{target}: {annotation} = {node.value.accept(self)}""
145 
146     def visit_binop(self, node):
147         """"""return an astroid.BinOp node as string""""""
148         left = self._precedence_parens(node, node.left)
149         right = self._precedence_parens(node, node.right, is_left=False)
150         if node.op == ""**"":
151             return f""{left}{node.op}{right}""
152 
153         return f""{left} {node.op} {right}""
154 
155     def visit_boolop(self, node):
156         """"""return an astroid.BoolOp node as string""""""
157         values = [f""{self._precedence_parens(node, n)}"" for n in node.values]
158         return (f"" {node.op} "").join(values)
159 
160     def visit_break(self, node):
161         """"""return an astroid.Break node as string""""""
162         return ""break""
163 
164     def visit_call(self, node):
165         """"""return an astroid.Call node as string""""""
166         expr_str = self._precedence_parens(node, node.func)
167         args = [arg.accept(self) for arg in node.args]
168         if node.keywords:
169             keywords = [kwarg.accept(self) for kwarg in node.keywords]
170         else:
171             keywords = []
172 
173         args.extend(keywords)
174         return f""{expr_str}({', '.join(args)})""
175 
176     def visit_classdef(self, node):
177         """"""return an astroid.ClassDef node as string""""""
178         decorate = node.decorators.accept(self) if node.decorators else """"
179         args = [n.accept(self) for n in node.bases]
180         if node._metaclass and not node.has_metaclass_hack():
181             args.append(""metaclass="" + node._metaclass.accept(self))
182         args += [n.accept(self) for n in node.keywords]
183         args = f""({', '.join(args)})"" if args else """"
184         docs = self._docs_dedent(node.doc) if node.doc else """"
185         return ""\n\n{}class {}{}:{}\n{}\n"".format(
186             decorate, node.name, args, docs, self._stmt_list(node.body)
187         )
188 
189     def visit_compare(self, node):
190         """"""return an astroid.Compare node as string""""""
191         rhs_str = "" "".join(
192             f""{op} {self._precedence_parens(node, expr, is_left=False)}""
193             for op, expr in node.ops
194         )
195         return f""{self._precedence_parens(node, node.left)} {rhs_str}""
196 
197     def visit_comprehension(self, node):
198         """"""return an astroid.Comprehension node as string""""""
199         ifs = """".join(f"" if {n.accept(self)}"" for n in node.ifs)
200         generated = f""for {node.target.accept(self)} in {node.iter.accept(self)}{ifs}""
201         return f""{'async ' if node.is_async else ''}{generated}""
202 
203     def visit_const(self, node):
204         """"""return an astroid.Const node as string""""""
205         if node.value is Ellipsis:
206             return ""...""
207         return repr(node.value)
208 
209     def visit_continue(self, node):
210         """"""return an astroid.Continue node as string""""""
211         return ""continue""
212 
213     def visit_delete(self, node):  # XXX check if correct
214         """"""return an astroid.Delete node as string""""""
215         return f""del {', '.join(child.accept(self) for child in node.targets)}""
216 
217     def visit_delattr(self, node):
218         """"""return an astroid.DelAttr node as string""""""
219         return self.visit_attribute(node)
220 
221     def visit_delname(self, node):
222         """"""return an astroid.DelName node as string""""""
223         return node.name
224 
225     def visit_decorators(self, node):
226         """"""return an astroid.Decorators node as string""""""
227         return ""@%s\n"" % ""\n@"".join(item.accept(self) for item in node.nodes)
228 
229     def visit_dict(self, node):
230         """"""return an astroid.Dict node as string""""""
231         return ""{%s}"" % "", "".join(self._visit_dict(node))
232 
233     def _visit_dict(self, node):
234         for key, value in node.items:
235             key = key.accept(self)
236             value = value.accept(self)
237             if key == ""**"":
238                 # It can only be a DictUnpack node.
239                 yield key + value
240             else:
241                 yield f""{key}: {value}""
242 
243     def visit_dictunpack(self, node):
244         return ""**""
245 
246     def visit_dictcomp(self, node):
247         """"""return an astroid.DictComp node as string""""""
248         return ""{{{}: {} {}}}"".format(
249             node.key.accept(self),
250             node.value.accept(self),
251             "" "".join(n.accept(self) for n in node.generators),
252         )
253 
254     def visit_expr(self, node):
255         """"""return an astroid.Discard node as string""""""
256         return node.value.accept(self)
257 
258     def visit_emptynode(self, node):
259         """"""dummy method for visiting an Empty node""""""
260         return """"
261 
262     def visit_excepthandler(self, node):
263         if node.type:
264             if node.name:
265                 excs = f""except {node.type.accept(self)} as {node.name.accept(self)}""
266             else:
267                 excs = f""except {node.type.accept(self)}""
268         else:
269             excs = ""except""
270         return f""{excs}:\n{self._stmt_list(node.body)}""
271 
272     def visit_empty(self, node):
273         """"""return an Empty node as string""""""
274         return """"
275 
276     def visit_for(self, node):
277         """"""return an astroid.For node as string""""""
278         fors = ""for {} in {}:\n{}"".format(
279             node.target.accept(self), node.iter.accept(self), self._stmt_list(node.body)
280         )
281         if node.orelse:
282             fors = f""{fors}\nelse:\n{self._stmt_list(node.orelse)}""
283         return fors
284 
285     def visit_importfrom(self, node):
286         """"""return an astroid.ImportFrom node as string""""""
287         return ""from {} import {}"".format(
288             ""."" * (node.level or 0) + node.modname, _import_string(node.names)
289         )
290 
291     def visit_joinedstr(self, node):
292         string = """".join(
293             # Use repr on the string literal parts
294             # to get proper escapes, e.g. \n, \\, \""
295             # But strip the quotes off the ends
296             # (they will always be one character: ' or "")
297             repr(value.value)[1:-1]
298             # Literal braces must be doubled to escape them
299             .replace(""{"", ""{{"").replace(""}"", ""}}"")
300             # Each value in values is either a string literal (Const)
301             # or a FormattedValue
302             if type(value).__name__ == ""Const"" else value.accept(self)
303             for value in node.values
304         )
305 
306         # Try to find surrounding quotes that don't appear at all in the string.
307         # Because the formatted values inside {} can't contain backslash (\)
308         # using a triple quote is sometimes necessary
309         for quote in (""'"", '""', '""""""', ""'''""):
310             if quote not in string:
311                 break
312 
313         return ""f"" + quote + string + quote
314 
315     def visit_formattedvalue(self, node):
316         result = node.value.accept(self)
317         if node.conversion and node.conversion >= 0:
318             # e.g. if node.conversion == 114: result += ""!r""
319             result += ""!"" + chr(node.conversion)
320         if node.format_spec:
321             # The format spec is itself a JoinedString, i.e. an f-string
322             # We strip the f and quotes of the ends
323             result += "":"" + node.format_spec.accept(self)[2:-1]
324         return ""{%s}"" % result
325 
326     def handle_functiondef(self, node, keyword):
327         """"""return a (possibly async) function definition node as string""""""
328         decorate = node.decorators.accept(self) if node.decorators else """"
329         docs = self._docs_dedent(node.doc) if node.doc else """"
330         trailer = "":""
331         if node.returns:
332             return_annotation = "" -> "" + node.returns.as_string()
333             trailer = return_annotation + "":""
334         def_format = ""\n%s%s %s(%s)%s%s\n%s""
335         return def_format % (
336             decorate,
337             keyword,
338             node.name,
339             node.args.accept(self),
340             trailer,
341             docs,
342             self._stmt_list(node.body),
343         )
344 
345     def visit_functiondef(self, node):
346         """"""return an astroid.FunctionDef node as string""""""
347         return self.handle_functiondef(node, ""def"")
348 
349     def visit_asyncfunctiondef(self, node):
350         """"""return an astroid.AsyncFunction node as string""""""
351         return self.handle_functiondef(node, ""async def"")
352 
353     def visit_generatorexp(self, node):
354         """"""return an astroid.GeneratorExp node as string""""""
355         return ""({} {})"".format(
356             node.elt.accept(self), "" "".join(n.accept(self) for n in node.generators)
357         )
358 
359     def visit_attribute(self, node):
360         """"""return an astroid.Getattr node as string""""""
361         left = self._precedence_parens(node, node.expr)
362         if left.isdigit():
363             left = f""({left})""
364         return f""{left}.{node.attrname}""
365 
366     def visit_global(self, node):
367         """"""return an astroid.Global node as string""""""
368         return f""global {', '.join(node.names)}""
369 
370     def visit_if(self, node):
371         """"""return an astroid.If node as string""""""
372         ifs = [f""if {node.test.accept(self)}:\n{self._stmt_list(node.body)}""]
373         if node.has_elif_block():
374             ifs.append(f""el{self._stmt_list(node.orelse, indent=False)}"")
375         elif node.orelse:
376             ifs.append(f""else:\n{self._stmt_list(node.orelse)}"")
377         return ""\n"".join(ifs)
378 
379     def visit_ifexp(self, node):
380         """"""return an astroid.IfExp node as string""""""
381         return ""{} if {} else {}"".format(
382             self._precedence_parens(node, node.body, is_left=True),
383             self._precedence_parens(node, node.test, is_left=True),
384             self._precedence_parens(node, node.orelse, is_left=False),
385         )
386 
387     def visit_import(self, node):
388         """"""return an astroid.Import node as string""""""
389         return f""import {_import_string(node.names)}""
390 
391     def visit_keyword(self, node):
392         """"""return an astroid.Keyword node as string""""""
393         if node.arg is None:
394             return f""**{node.value.accept(self)}""
395         return f""{node.arg}={node.value.accept(self)}""
396 
397     def visit_lambda(self, node):
398         """"""return an astroid.Lambda node as string""""""
399         args = node.args.accept(self)
400         body = node.body.accept(self)
401         if args:
402             return f""lambda {args}: {body}""
403 
404         return f""lambda: {body}""
405 
406     def visit_list(self, node):
407         """"""return an astroid.List node as string""""""
408         return f""[{', '.join(child.accept(self) for child in node.elts)}]""
409 
410     def visit_listcomp(self, node):
411         """"""return an astroid.ListComp node as string""""""
412         return ""[{} {}]"".format(
413             node.elt.accept(self), "" "".join(n.accept(self) for n in node.generators)
414         )
415 
416     def visit_module(self, node):
417         """"""return an astroid.Module node as string""""""
418         docs = f'""""""{node.doc}""""""\n\n' if node.doc else """"
419         return docs + ""\n"".join(n.accept(self) for n in node.body) + ""\n\n""
420 
421     def visit_name(self, node):
422         """"""return an astroid.Name node as string""""""
423         return node.name
424 
425     def visit_namedexpr(self, node):
426         """"""Return an assignment expression node as string""""""
427         target = node.target.accept(self)
428         value = node.value.accept(self)
429         return f""{target} := {value}""
430 
431     def visit_nonlocal(self, node):
432         """"""return an astroid.Nonlocal node as string""""""
433         return f""nonlocal {', '.join(node.names)}""
434 
435     def visit_pass(self, node):
436         """"""return an astroid.Pass node as string""""""
437         return ""pass""
438 
439     def visit_raise(self, node):
440         """"""return an astroid.Raise node as string""""""
441         if node.exc:
442             if node.cause:
443                 return f""raise {node.exc.accept(self)} from {node.cause.accept(self)}""
444             return f""raise {node.exc.accept(self)}""
445         return ""raise""
446 
447     def visit_return(self, node):
448         """"""return an astroid.Return node as string""""""
449         if node.is_tuple_return() and len(node.value.elts) > 1:
450             elts = [child.accept(self) for child in node.value.elts]
451             return f""return {', '.join(elts)}""
452 
453         if node.value:
454             return f""return {node.value.accept(self)}""
455 
456         return ""return""
457 
458     def visit_set(self, node):
459         """"""return an astroid.Set node as string""""""
460         return ""{%s}"" % "", "".join(child.accept(self) for child in node.elts)
461 
462     def visit_setcomp(self, node):
463         """"""return an astroid.SetComp node as string""""""
464         return ""{{{} {}}}"".format(
465             node.elt.accept(self), "" "".join(n.accept(self) for n in node.generators)
466         )
467 
468     def visit_slice(self, node):
469         """"""return an astroid.Slice node as string""""""
470         lower = node.lower.accept(self) if node.lower else """"
471         upper = node.upper.accept(self) if node.upper else """"
472         step = node.step.accept(self) if node.step else """"
473         if step:
474             return f""{lower}:{upper}:{step}""
475         return f""{lower}:{upper}""
476 
477     def visit_subscript(self, node):
478         """"""return an astroid.Subscript node as string""""""
479         idx = node.slice
480         if idx.__class__.__name__.lower() == ""index"":
481             idx = idx.value
482         idxstr = idx.accept(self)
483         if idx.__class__.__name__.lower() == ""tuple"" and idx.elts:
484             # Remove parenthesis in tuple and extended slice.
485             # a[(::1, 1:)] is not valid syntax.
486             idxstr = idxstr[1:-1]
487         return f""{self._precedence_parens(node, node.value)}[{idxstr}]""
488 
489     def visit_tryexcept(self, node):
490         """"""return an astroid.TryExcept node as string""""""
491         trys = [f""try:\n{self._stmt_list(node.body)}""]
492         for handler in node.handlers:
493             trys.append(handler.accept(self))
494         if node.orelse:
495             trys.append(f""else:\n{self._stmt_list(node.orelse)}"")
496         return ""\n"".join(trys)
497 
498     def visit_tryfinally(self, node):
499         """"""return an astroid.TryFinally node as string""""""
500         return ""try:\n{}\nfinally:\n{}"".format(
501             self._stmt_list(node.body), self._stmt_list(node.finalbody)
502         )
503 
504     def visit_tuple(self, node):
505         """"""return an astroid.Tuple node as string""""""
506         if len(node.elts) == 1:
507             return f""({node.elts[0].accept(self)}, )""
508         return f""({', '.join(child.accept(self) for child in node.elts)})""
509 
510     def visit_unaryop(self, node):
511         """"""return an astroid.UnaryOp node as string""""""
512         if node.op == ""not"":
513             operator = ""not ""
514         else:
515             operator = node.op
516         return f""{operator}{self._precedence_parens(node, node.operand)}""
517 
518     def visit_while(self, node):
519         """"""return an astroid.While node as string""""""
520         whiles = f""while {node.test.accept(self)}:\n{self._stmt_list(node.body)}""
521         if node.orelse:
522             whiles = f""{whiles}\nelse:\n{self._stmt_list(node.orelse)}""
523         return whiles
524 
525     def visit_with(self, node):  # 'with' without 'as' is possible
526         """"""return an astroid.With node as string""""""
527         items = "", "".join(
528             f""{expr.accept(self)}"" + (v and f"" as {v.accept(self)}"" or """")
529             for expr, v in node.items
530         )
531         return f""with {items}:\n{self._stmt_list(node.body)}""
532 
533     def visit_yield(self, node):
534         """"""yield an ast.Yield node as string""""""
535         yi_val = ("" "" + node.value.accept(self)) if node.value else """"
536         expr = ""yield"" + yi_val
537         if node.parent.is_statement:
538             return expr
539 
540         return f""({expr})""
541 
542     def visit_yieldfrom(self, node):
543         """"""Return an astroid.YieldFrom node as string.""""""
544         yi_val = ("" "" + node.value.accept(self)) if node.value else """"
545         expr = ""yield from"" + yi_val
546         if node.parent.is_statement:
547             return expr
548 
549         return f""({expr})""
550 
551     def visit_starred(self, node):
552         """"""return Starred node as string""""""
553         return ""*"" + node.value.accept(self)
554 
555     def visit_match(self, node: ""Match"") -> str:
556         """"""Return an astroid.Match node as string.""""""
557         return f""match {node.subject.accept(self)}:\n{self._stmt_list(node.cases)}""
558 
559     def visit_matchcase(self, node: ""MatchCase"") -> str:
560         """"""Return an astroid.MatchCase node as string.""""""
561         guard_str = f"" if {node.guard.accept(self)}"" if node.guard else """"
562         return (
563             f""case {node.pattern.accept(self)}{guard_str}:\n""
564             f""{self._stmt_list(node.body)}""
565         )
566 
567     def visit_matchvalue(self, node: ""MatchValue"") -> str:
568         """"""Return an astroid.MatchValue node as string.""""""
569         return node.value.accept(self)
570 
571     @staticmethod
572     def visit_matchsingleton(node: ""MatchSingleton"") -> str:
573         """"""Return an astroid.MatchSingleton node as string.""""""
574         return str(node.value)
575 
576     def visit_matchsequence(self, node: ""MatchSequence"") -> str:
577         """"""Return an astroid.MatchSequence node as string.""""""
578         if node.patterns is None:
579             return ""[]""
580         return f""[{', '.join(p.accept(self) for p in node.patterns)}]""
581 
582     def visit_matchmapping(self, node: ""MatchMapping"") -> str:
583         """"""Return an astroid.MatchMapping node as string.""""""
584         mapping_strings: List[str] = []
585         if node.keys and node.patterns:
586             mapping_strings.extend(
587                 f""{key.accept(self)}: {p.accept(self)}""
588                 for key, p in zip(node.keys, node.patterns)
589             )
590         if node.rest:
591             mapping_strings.append(f""**{node.rest.accept(self)}"")
592         return f""{'{'}{', '.join(mapping_strings)}{'}'}""
593 
594     def visit_matchclass(self, node: ""MatchClass"") -> str:
595         """"""Return an astroid.MatchClass node as string.""""""
596         if node.cls is None:
597             raise Exception(f""{node} does not have a 'cls' node"")
598         class_strings: List[str] = []
599         if node.patterns:
600             class_strings.extend(p.accept(self) for p in node.patterns)
601         if node.kwd_attrs and node.kwd_patterns:
602             for attr, pattern in zip(node.kwd_attrs, node.kwd_patterns):
603                 class_strings.append(f""{attr}={pattern.accept(self)}"")
604         return f""{node.cls.accept(self)}({', '.join(class_strings)})""
605 
606     def visit_matchstar(self, node: ""MatchStar"") -> str:
607         """"""Return an astroid.MatchStar node as string.""""""
608         return f""*{node.name.accept(self) if node.name else '_'}""
609 
610     def visit_matchas(self, node: ""MatchAs"") -> str:
611         """"""Return an astroid.MatchAs node as string.""""""
612         # pylint: disable=import-outside-toplevel
613         # Prevent circular dependency
614         from astroid.nodes.node_classes import MatchClass, MatchMapping, MatchSequence
615 
616         if isinstance(node.parent, (MatchSequence, MatchMapping, MatchClass)):
617             return node.name.accept(self) if node.name else ""_""
618         return (
619             f""{node.pattern.accept(self) if node.pattern else '_'}""
620             f""{f' as {node.name.accept(self)}' if node.name else ''}""
621         )
622 
623     def visit_matchor(self, node: ""MatchOr"") -> str:
624         """"""Return an astroid.MatchOr node as string.""""""
625         if node.patterns is None:
626             raise Exception(f""{node} does not have pattern nodes"")
627         return "" | "".join(p.accept(self) for p in node.patterns)
628 
629     # These aren't for real AST nodes, but for inference objects.
630 
631     def visit_frozenset(self, node):
632         return node.parent.accept(self)
633 
634     def visit_super(self, node):
635         return node.parent.accept(self)
636 
637     def visit_uninferable(self, node):
638         return str(node)
639 
640     def visit_property(self, node):
641         return node.function.accept(self)
642 
643     def visit_evaluatedobject(self, node):
644         return node.original.accept(self)
645 
646 
647 def _import_string(names):
648     """"""return a list of (name, asname) formatted as a string""""""
649     _names = []
650     for name, asname in names:
651         if asname is not None:
652             _names.append(f""{name} as {asname}"")
653         else:
654             _names.append(name)
655     return "", "".join(_names)
656 
657 
658 # This sets the default indent to 4 spaces.
659 to_code = AsStringVisitor(""    "")
660 
[end of astroid/nodes/as_string.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pylint-dev/astroid,ce5cbce5ba11cdc2f8139ade66feea1e181a7944,"'AsStringVisitor' object has no attribute 'visit_unknown'
```python
>>> import astroid
>>> astroid.nodes.Unknown().as_string()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py"", line 609, in as_string
    return AsStringVisitor()(self)
  File ""/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/as_string.py"", line 56, in __call__
    return node.accept(self).replace(DOC_NEWLINE, ""\n"")
  File ""/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py"", line 220, in accept
    func = getattr(visitor, ""visit_"" + self.__class__.__name__.lower())
AttributeError: 'AsStringVisitor' object has no attribute 'visit_unknown'
>>> 
```
### `python -c ""from astroid import __pkginfo__; print(__pkginfo__.version)""` output

2.8.6-dev0
","Thank you for opening the issue.
I don't believe `Unknown().as_string()` is ever called regularly. AFAIK it's only used during inference. What should the string representation of an `Unknown` node be? So not sure this needs to be addressed.
Probably just `'Unknown'`.
It's mostly only a problem when we do something like this:

```python
inferred = infer(node)
if inferred is not Uninferable:
    if inferred.as_string().contains(some_value):
        ...
```
So for the most part, as long as it doesn't crash we're good.",2021-11-21T16:15:23Z,"<patch>
diff --git a/astroid/nodes/as_string.py b/astroid/nodes/as_string.py
--- a/astroid/nodes/as_string.py
+++ b/astroid/nodes/as_string.py
@@ -36,6 +36,7 @@
         MatchSingleton,
         MatchStar,
         MatchValue,
+        Unknown,
     )
 
 # pylint: disable=unused-argument
@@ -643,6 +644,9 @@ def visit_property(self, node):
     def visit_evaluatedobject(self, node):
         return node.original.accept(self)
 
+    def visit_unknown(self, node: ""Unknown"") -> str:
+        return str(node)
+
 
 def _import_string(names):
     """"""return a list of (name, asname) formatted as a string""""""

</patch>","diff --git a/tests/unittest_nodes.py b/tests/unittest_nodes.py
--- a/tests/unittest_nodes.py
+++ b/tests/unittest_nodes.py
@@ -306,6 +306,11 @@ def test_f_strings(self):
         ast = abuilder.string_build(code)
         self.assertEqual(ast.as_string().strip(), code.strip())
 
+    @staticmethod
+    def test_as_string_unknown() -> None:
+        assert nodes.Unknown().as_string() == ""Unknown.Unknown()""
+        assert nodes.Unknown(lineno=1, col_offset=0).as_string() == ""Unknown.Unknown()""
+
 
 class _NodeTest(unittest.TestCase):
     """"""test transformation of If Node""""""
",2.9,"[""tests/unittest_nodes.py::AsStringTest::test_as_string_unknown""]","[""tests/unittest_nodes.py::AsStringTest::test_3k_annotations_and_metaclass"", ""tests/unittest_nodes.py::AsStringTest::test_3k_as_string"", ""tests/unittest_nodes.py::AsStringTest::test_as_string"", ""tests/unittest_nodes.py::AsStringTest::test_as_string_for_list_containing_uninferable"", ""tests/unittest_nodes.py::AsStringTest::test_class_def"", ""tests/unittest_nodes.py::AsStringTest::test_ellipsis"", ""tests/unittest_nodes.py::AsStringTest::test_f_strings"", ""tests/unittest_nodes.py::AsStringTest::test_frozenset_as_string"", ""tests/unittest_nodes.py::AsStringTest::test_func_signature_issue_185"", ""tests/unittest_nodes.py::AsStringTest::test_int_attribute"", ""tests/unittest_nodes.py::AsStringTest::test_module2_as_string"", ""tests/unittest_nodes.py::AsStringTest::test_module_as_string"", ""tests/unittest_nodes.py::AsStringTest::test_operator_precedence"", ""tests/unittest_nodes.py::AsStringTest::test_slice_and_subscripts"", ""tests/unittest_nodes.py::AsStringTest::test_slices"", ""tests/unittest_nodes.py::AsStringTest::test_tuple_as_string"", ""tests/unittest_nodes.py::AsStringTest::test_varargs_kwargs_as_string"", ""tests/unittest_nodes.py::IfNodeTest::test_block_range"", ""tests/unittest_nodes.py::IfNodeTest::test_if_elif_else_node"", ""tests/unittest_nodes.py::IfNodeTest::test_if_sys_guard"", ""tests/unittest_nodes.py::IfNodeTest::test_if_typing_guard"", ""tests/unittest_nodes.py::TryExceptNodeTest::test_block_range"", ""tests/unittest_nodes.py::TryFinallyNodeTest::test_block_range"", ""tests/unittest_nodes.py::TryExceptFinallyNodeTest::test_block_range"", ""tests/unittest_nodes.py::ImportNodeTest::test_absolute_import"", ""tests/unittest_nodes.py::ImportNodeTest::test_as_string"", ""tests/unittest_nodes.py::ImportNodeTest::test_bad_import_inference"", ""tests/unittest_nodes.py::ImportNodeTest::test_conditional"", ""tests/unittest_nodes.py::ImportNodeTest::test_conditional_import"", ""tests/unittest_nodes.py::ImportNodeTest::test_from_self_resolve"", ""tests/unittest_nodes.py::ImportNodeTest::test_import_self_resolve"", ""tests/unittest_nodes.py::ImportNodeTest::test_more_absolute_import"", ""tests/unittest_nodes.py::ImportNodeTest::test_real_name"", ""tests/unittest_nodes.py::CmpNodeTest::test_as_string"", ""tests/unittest_nodes.py::ConstNodeTest::test_bool"", ""tests/unittest_nodes.py::ConstNodeTest::test_complex"", ""tests/unittest_nodes.py::ConstNodeTest::test_copy"", ""tests/unittest_nodes.py::ConstNodeTest::test_float"", ""tests/unittest_nodes.py::ConstNodeTest::test_int"", ""tests/unittest_nodes.py::ConstNodeTest::test_none"", ""tests/unittest_nodes.py::ConstNodeTest::test_str"", ""tests/unittest_nodes.py::ConstNodeTest::test_str_kind"", ""tests/unittest_nodes.py::ConstNodeTest::test_unicode"", ""tests/unittest_nodes.py::NameNodeTest::test_assign_to_true"", ""tests/unittest_nodes.py::TestNamedExprNode::test_frame"", ""tests/unittest_nodes.py::TestNamedExprNode::test_scope"", ""tests/unittest_nodes.py::AnnAssignNodeTest::test_as_string"", ""tests/unittest_nodes.py::AnnAssignNodeTest::test_complex"", ""tests/unittest_nodes.py::AnnAssignNodeTest::test_primitive"", ""tests/unittest_nodes.py::AnnAssignNodeTest::test_primitive_without_initial_value"", ""tests/unittest_nodes.py::ArgumentsNodeTC::test_kwoargs"", ""tests/unittest_nodes.py::ArgumentsNodeTC::test_positional_only"", ""tests/unittest_nodes.py::UnboundMethodNodeTest::test_no_super_getattr"", ""tests/unittest_nodes.py::BoundMethodNodeTest::test_is_property"", ""tests/unittest_nodes.py::AliasesTest::test_aliases"", ""tests/unittest_nodes.py::Python35AsyncTest::test_async_await_keywords"", ""tests/unittest_nodes.py::Python35AsyncTest::test_asyncfor_as_string"", ""tests/unittest_nodes.py::Python35AsyncTest::test_asyncwith_as_string"", ""tests/unittest_nodes.py::Python35AsyncTest::test_await_as_string"", ""tests/unittest_nodes.py::Python35AsyncTest::test_decorated_async_def_as_string"", ""tests/unittest_nodes.py::ContextTest::test_list_del"", ""tests/unittest_nodes.py::ContextTest::test_list_load"", ""tests/unittest_nodes.py::ContextTest::test_list_store"", ""tests/unittest_nodes.py::ContextTest::test_starred_load"", ""tests/unittest_nodes.py::ContextTest::test_starred_store"", ""tests/unittest_nodes.py::ContextTest::test_subscript_del"", ""tests/unittest_nodes.py::ContextTest::test_subscript_load"", ""tests/unittest_nodes.py::ContextTest::test_subscript_store"", ""tests/unittest_nodes.py::ContextTest::test_tuple_load"", ""tests/unittest_nodes.py::ContextTest::test_tuple_store"", ""tests/unittest_nodes.py::test_unknown"", ""tests/unittest_nodes.py::test_type_comments_with"", ""tests/unittest_nodes.py::test_type_comments_for"", ""tests/unittest_nodes.py::test_type_coments_assign"", ""tests/unittest_nodes.py::test_type_comments_invalid_expression"", ""tests/unittest_nodes.py::test_type_comments_invalid_function_comments"", ""tests/unittest_nodes.py::test_type_comments_function"", ""tests/unittest_nodes.py::test_type_comments_arguments"", ""tests/unittest_nodes.py::test_type_comments_posonly_arguments"", ""tests/unittest_nodes.py::test_correct_function_type_comment_parent"", ""tests/unittest_nodes.py::test_is_generator_for_yield_assignments"", ""tests/unittest_nodes.py::test_f_string_correct_line_numbering"", ""tests/unittest_nodes.py::test_assignment_expression"", ""tests/unittest_nodes.py::test_assignment_expression_in_functiondef"", ""tests/unittest_nodes.py::test_get_doc"", ""tests/unittest_nodes.py::test_parse_fstring_debug_mode"", ""tests/unittest_nodes.py::test_parse_type_comments_with_proper_parent"", ""tests/unittest_nodes.py::test_const_itered"", ""tests/unittest_nodes.py::test_is_generator_for_yield_in_while"", ""tests/unittest_nodes.py::test_is_generator_for_yield_in_if"", ""tests/unittest_nodes.py::test_is_generator_for_yield_in_aug_assign""]",0d1211558670cfefd95b39984b8d5f7f34837f32
pyvista__pyvista-4315,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
Rectilinear grid does not allow Sequences as inputs
### Describe the bug, what's wrong, and what you expected.

Rectilinear grid gives an error when `Sequence`s are passed in, but `ndarray` are ok.

### Steps to reproduce the bug.

This doesn't work
```python
import pyvista as pv
pv.RectilinearGrid([0, 1], [0, 1], [0, 1])
```

This works
```py
import pyvista as pv
import numpy as np
pv.RectilinearGrid(np.ndarray([0, 1]), np.ndarray([0, 1]), np.ndarray([0, 1]))
```
### System Information

```shell
--------------------------------------------------------------------------------
  Date: Wed Apr 19 20:15:10 2023 UTC

                OS : Linux
            CPU(s) : 2
           Machine : x86_64
      Architecture : 64bit
       Environment : IPython
        GPU Vendor : Mesa/X.org
      GPU Renderer : llvmpipe (LLVM 11.0.1, 256 bits)
       GPU Version : 4.5 (Core Profile) Mesa 20.3.5

  Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]

           pyvista : 0.38.5
               vtk : 9.2.6
             numpy : 1.24.2
           imageio : 2.27.0
            scooby : 0.7.1
             pooch : v1.7.0
        matplotlib : 3.7.1
           IPython : 8.12.0
--------------------------------------------------------------------------------
```


### Screenshots

_No response_

</issue>
<code>
[start of README.rst]
1 #######
2 PyVista
3 #######
4 
5 .. image:: https://github.com/pyvista/pyvista/raw/main/doc/source/_static/pyvista_banner_small.png
6    :target: https://docs.pyvista.org/examples/index.html
7    :alt: pyvista
8 
9 
10 .. |zenodo| image:: https://zenodo.org/badge/92974124.svg
11    :target: https://zenodo.org/badge/latestdoi/92974124
12 
13 .. |joss| image:: http://joss.theoj.org/papers/10.21105/joss.01450/status.svg
14    :target: https://doi.org/10.21105/joss.01450
15 
16 .. |pypi| image:: https://img.shields.io/pypi/v/pyvista.svg?logo=python&logoColor=white
17    :target: https://pypi.org/project/pyvista/
18 
19 .. |conda| image:: https://img.shields.io/conda/vn/conda-forge/pyvista.svg?logo=conda-forge&logoColor=white
20    :target: https://anaconda.org/conda-forge/pyvista
21 
22 .. |GH-CI| image:: https://github.com/pyvista/pyvista/actions/workflows/testing-and-deployment.yml/badge.svg
23    :target: https://github.com/pyvista/pyvista/actions/workflows/testing-and-deployment.yml
24 
25 .. |codecov| image:: https://codecov.io/gh/pyvista/pyvista/branch/main/graph/badge.svg
26    :target: https://codecov.io/gh/pyvista/pyvista
27 
28 .. |codacy| image:: https://app.codacy.com/project/badge/Grade/779ac6aed37548839384acfc0c1aab44
29    :target: https://www.codacy.com/gh/pyvista/pyvista/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=pyvista/pyvista&amp;utm_campaign=Badge_Grade
30 
31 .. |MIT| image:: https://img.shields.io/badge/License-MIT-yellow.svg
32    :target: https://opensource.org/licenses/MIT
33 
34 .. |slack| image:: https://img.shields.io/badge/Slack-pyvista-green.svg?logo=slack
35    :target: http://slack.pyvista.org
36 
37 .. |PyPIact| image:: https://img.shields.io/pypi/dm/pyvista.svg?label=PyPI%20downloads
38    :target: https://pypi.org/project/pyvista/
39 
40 .. |condaact| image:: https://img.shields.io/conda/dn/conda-forge/pyvista.svg?label=Conda%20downloads
41    :target: https://anaconda.org/conda-forge/pyvista
42 
43 .. |discuss| image:: https://img.shields.io/badge/GitHub-Discussions-green?logo=github
44    :target: https://github.com/pyvista/pyvista/discussions
45 
46 .. |isort| image:: https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat
47   :target: https://timothycrosley.github.io/isort
48   :alt: isort
49 
50 .. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg?style=flat
51   :target: https://github.com/psf/black
52   :alt: black
53 
54 .. |python| image:: https://img.shields.io/badge/python-3.8+-blue.svg
55    :target: https://www.python.org/downloads/
56 
57 .. |NumFOCUS Affiliated| image:: https://img.shields.io/badge/affiliated-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A
58    :target: https://numfocus.org/sponsored-projects/affiliated-projects
59 
60 .. |pre-commit.ci status| image:: https://results.pre-commit.ci/badge/github/pyvista/pyvista/main.svg
61    :target: https://results.pre-commit.ci/latest/github/pyvista/pyvista/main
62 
63 
64 +----------------------+------------------------+-------------+
65 | Deployment           | |pypi|                 | |conda|     |
66 +----------------------+------------------------+-------------+
67 | Build Status         | |GH-CI|                | |python|    |
68 |                      +------------------------+-------------+
69 |                      | |pre-commit.ci status| |             |
70 +----------------------+------------------------+-------------+
71 | Metrics              | |codacy|               | |codecov|   |
72 +----------------------+------------------------+-------------+
73 | Activity             | |PyPIact|              | |condaact|  |
74 +----------------------+------------------------+-------------+
75 | Citation             | |joss|                 | |zenodo|    |
76 +----------------------+------------------------+-------------+
77 | License              | |MIT|                  |             |
78 +----------------------+------------------------+-------------+
79 | Community            | |slack|                | |discuss|   |
80 +----------------------+------------------------+-------------+
81 | Formatter            | |black|                | |isort|     |
82 +----------------------+------------------------+-------------+
83 | Affiliated           | |NumFOCUS Affiliated|                |
84 +----------------------+------------------------+-------------+
85 
86 
87     3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK)
88 
89 PyVista is:
90 
91 * *Pythonic VTK*: a high-level API to the `Visualization Toolkit`_ (VTK)
92 * mesh data structures and filtering methods for spatial datasets
93 * 3D plotting made simple and built for large/complex data geometries
94 
95 .. _Visualization Toolkit: https://vtk.org
96 
97 PyVista is a helper module for the Visualization Toolkit (VTK) that wraps the VTK library
98 through NumPy and direct array access through a variety of methods and classes.
99 This package provides a Pythonic, well-documented interface exposing
100 VTK's powerful visualization backend to facilitate rapid prototyping, analysis,
101 and visual integration of spatially referenced datasets.
102 
103 This module can be used for scientific plotting for presentations and research
104 papers as well as a supporting module for other mesh 3D rendering dependent
105 Python modules; see Connections for a list of projects that leverage
106 PyVista.
107 
108 
109 .. |tweet| image:: https://img.shields.io/twitter/url.svg?style=social&url=http%3A%2F%2Fshields.io
110    :target: https://twitter.com/intent/tweet?text=Check%20out%20this%20project%20for%203D%20visualization%20in%20Python&url=https://github.com/pyvista/pyvista&hashtags=3D,visualization,Python,vtk,mesh,plotting,PyVista
111 
112 Share this project on Twitter: |tweet|
113 
114 
115 PyVista is a NumFOCUS affiliated project
116 
117 .. image:: https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png
118    :target: https://numfocus.org/sponsored-projects/affiliated-projects
119    :alt: NumFOCUS affiliated projects
120    :height: 60px
121 
122 
123 Highlights
124 ==========
125 
126 .. |binder| image:: https://static.mybinder.org/badge_logo.svg
127    :target: https://mybinder.org/v2/gh/pyvista/pyvista-examples/master
128    :alt: Launch on Binder
129 
130 Head over to the `Quick Examples`_ page in the docs to explore our gallery of
131 examples showcasing what PyVista can do. Want to test-drive PyVista?
132 All of the examples from the gallery are live on MyBinder for you to test
133 drive without installing anything locally: |binder|
134 
135 .. _Quick Examples: http://docs.pyvista.org/examples/index.html
136 
137 
138 Overview of Features
139 --------------------
140 
141 * Extensive gallery of examples (see `Quick Examples`_)
142 * Interactive plotting in Jupyter Notebooks with server-side and client-side
143   rendering with `trame`_.
144 * Filtering/plotting tools built for interactivity (see `Widgets`_)
145 * Direct access to mesh analysis and transformation routines (see Filters_)
146 * Intuitive plotting routines with ``matplotlib`` similar syntax (see Plotting_)
147 * Import meshes from many common formats (use ``pyvista.read()``). Support for all formats handled by `meshio`_ is built-in.
148 * Export meshes as VTK, STL, OBJ, or PLY (``mesh.save()``) file types or any formats supported by meshio_ (``pyvista.save_meshio()``)
149 
150 .. _trame: https://github.com/Kitware/trame
151 .. _Widgets: https://docs.pyvista.org/api/plotting/index.html#widget-api
152 .. _Filters: https://docs.pyvista.org/api/core/filters.html
153 .. _Plotting: https://docs.pyvista.org/api/plotting/index.html
154 .. _meshio: https://github.com/nschloe/meshio
155 
156 
157 Documentation
158 =============
159 
160 Refer to the `documentation <http://docs.pyvista.org/>`_ for detailed
161 installation and usage details.
162 
163 For general questions about the project, its applications, or about software
164 usage, please create a discussion in `pyvista/discussions`_
165 where the community can collectively address your questions. You are also
166 welcome to join us on Slack_ or send one of the developers an email.
167 The project support team can be reached at `info@pyvista.org`_.
168 
169 .. _pyvista/discussions: https://github.com/pyvista/pyvista/discussions
170 .. _Slack: http://slack.pyvista.org
171 .. _info@pyvista.org: mailto:info@pyvista.org
172 
173 
174 Installation
175 ============
176 
177 PyVista can be installed from `PyPI <https://pypi.org/project/pyvista/>`_
178 using ``pip`` on Python >= 3.8::
179 
180     pip install pyvista
181 
182 You can also visit `PyPI <https://pypi.org/project/pyvista/>`_,
183 `Anaconda <https://anaconda.org/conda-forge/pyvista>`_, or
184 `GitHub <https://github.com/pyvista/pyvista>`_ to download the source.
185 
186 See the `Installation <http://docs.pyvista.org/getting-started/installation.html#install-ref.>`_
187 for more details regarding optional dependencies or if the installation through pip doesn't work out.
188 
189 
190 Connections
191 ===========
192 
193 PyVista is a powerful tool that researchers can harness to create compelling,
194 integrated visualizations of large datasets in an intuitive, Pythonic manner.
195 
196 Learn more about how PyVista is used across science and engineering disciplines
197 by a diverse community of users on our `Connections page`_.
198 
199 .. _Connections page: https://docs.pyvista.org/getting-started/connections.html
200 
201 
202 Authors
203 =======
204 
205 Please take a look at the `contributors page`_ and the active `list of authors`_
206 to learn more about the developers of PyVista.
207 
208 .. _contributors page: https://github.com/pyvista/pyvista/graphs/contributors/
209 .. _list of authors: https://docs.pyvista.org/getting-started/authors.html#authors
210 
211 
212 Contributing
213 ============
214 
215 .. |Contributor Covenant| image:: https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg
216    :target: CODE_OF_CONDUCT.md
217 
218 .. |codetriage| image:: https://www.codetriage.com/pyvista/pyvista/badges/users.svg
219    :target: https://www.codetriage.com/pyvista/pyvista
220    :alt: Code Triage
221 
222 |Contributor Covenant|
223 |codetriage|
224 
225 We absolutely welcome contributions and we hope that our `Contributing Guide`_
226 will facilitate your ability to make PyVista better. PyVista is mostly
227 maintained on a volunteer basis and thus we need to foster a community that can
228 support user questions and develop new features to make this software a useful
229 tool for all users while encouraging every member of the community to share
230 their ideas. To learn more about contributing to PyVista, please see the
231 `Contributing Guide`_ and our `Code of Conduct`_.
232 
233 .. _Contributing Guide: https://github.com/pyvista/pyvista/blob/main/CONTRIBUTING.rst
234 .. _Code of Conduct: https://github.com/pyvista/pyvista/blob/main/CODE_OF_CONDUCT.md
235 
236 
237 Citing PyVista
238 ==============
239 
240 There is a `paper about PyVista <https://doi.org/10.21105/joss.01450>`_.
241 
242 If you are using PyVista in your scientific research, please help our scientific
243 visibility by citing our work.
244 
245 
246     Sullivan and Kaszynski, (2019). PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK). Journal of Open Source Software, 4(37), 1450, https://doi.org/10.21105/joss.01450
247 
248 
249 BibTex:
250 
251 .. code::
252 
253     @article{sullivan2019pyvista,
254       doi = {10.21105/joss.01450},
255       url = {https://doi.org/10.21105/joss.01450},
256       year = {2019},
257       month = {May},
258       publisher = {The Open Journal},
259       volume = {4},
260       number = {37},
261       pages = {1450},
262       author = {Bane Sullivan and Alexander Kaszynski},
263       title = {{PyVista}: {3D} plotting and mesh analysis through a streamlined interface for the {Visualization Toolkit} ({VTK})},
264       journal = {Journal of Open Source Software}
265     }
266 
[end of README.rst]
[start of pyvista/core/grid.py]
1 """"""Sub-classes for vtk.vtkRectilinearGrid and vtk.vtkImageData.""""""
2 from functools import wraps
3 import pathlib
4 from typing import Sequence, Tuple, Union
5 import warnings
6 
7 import numpy as np
8 
9 import pyvista
10 from pyvista import _vtk
11 from pyvista.core.dataset import DataSet
12 from pyvista.core.filters import RectilinearGridFilters, UniformGridFilters, _get_output
13 from pyvista.utilities import abstract_class, assert_empty_kwargs
14 import pyvista.utilities.helpers as helpers
15 from pyvista.utilities.misc import PyVistaDeprecationWarning, raise_has_duplicates
16 
17 
18 @abstract_class
19 class Grid(DataSet):
20     """"""A class full of common methods for non-pointset grids.""""""
21 
22     def __init__(self, *args, **kwargs):
23         """"""Initialize the grid.""""""
24         super().__init__()
25 
26     @property
27     def dimensions(self) -> Tuple[int, int, int]:
28         """"""Return the grid's dimensions.
29 
30         These are effectively the number of points along each of the
31         three dataset axes.
32 
33         Examples
34         --------
35         Create a uniform grid with dimensions ``(1, 2, 3)``.
36 
37         >>> import pyvista
38         >>> grid = pyvista.UniformGrid(dimensions=(2, 3, 4))
39         >>> grid.dimensions
40         (2, 3, 4)
41         >>> grid.plot(show_edges=True)
42 
43         Set the dimensions to ``(3, 4, 5)``
44 
45         >>> grid.dimensions = (3, 4, 5)
46         >>> grid.plot(show_edges=True)
47 
48         """"""
49         return self.GetDimensions()
50 
51     @dimensions.setter
52     def dimensions(self, dims: Sequence[int]):
53         """"""Set the dataset dimensions.""""""
54         self.SetDimensions(*dims)
55         self.Modified()
56 
57     def _get_attrs(self):
58         """"""Return the representation methods (internal helper).""""""
59         attrs = DataSet._get_attrs(self)
60         attrs.append((""Dimensions"", self.dimensions, ""{:d}, {:d}, {:d}""))
61         return attrs
62 
63 
64 class RectilinearGrid(_vtk.vtkRectilinearGrid, Grid, RectilinearGridFilters):
65     """"""Dataset with variable spacing in the three coordinate directions.
66 
67     Can be initialized in several ways:
68 
69     * Create empty grid
70     * Initialize from a ``vtk.vtkRectilinearGrid`` object
71     * Initialize directly from the point arrays
72 
73     Parameters
74     ----------
75     uinput : str, pathlib.Path, vtk.vtkRectilinearGrid, numpy.ndarray, optional
76         Filename, dataset, or array to initialize the rectilinear grid from. If a
77         filename is passed, pyvista will attempt to load it as a
78         :class:`RectilinearGrid`. If passed a ``vtk.vtkRectilinearGrid``, it
79         will be wrapped. If a :class:`numpy.ndarray` is passed, this will be
80         loaded as the x range.
81 
82     y : numpy.ndarray, optional
83         Coordinates of the points in y direction. If this is passed, ``uinput``
84         must be a :class:`numpy.ndarray`.
85 
86     z : numpy.ndarray, optional
87         Coordinates of the points in z direction. If this is passed, ``uinput``
88         and ``y`` must be a :class:`numpy.ndarray`.
89 
90     check_duplicates : bool, optional
91         Check for duplications in any arrays that are passed. Defaults to
92         ``False``. If ``True``, an error is raised if there are any duplicate
93         values in any of the array-valued input arguments.
94 
95     deep : bool, optional
96         Whether to deep copy a ``vtk.vtkRectilinearGrid`` object.
97         Default is ``False``.  Keyword only.
98 
99     Examples
100     --------
101     >>> import pyvista
102     >>> import vtk
103     >>> import numpy as np
104 
105     Create an empty grid.
106 
107     >>> grid = pyvista.RectilinearGrid()
108 
109     Initialize from a vtk.vtkRectilinearGrid object
110 
111     >>> vtkgrid = vtk.vtkRectilinearGrid()
112     >>> grid = pyvista.RectilinearGrid(vtkgrid)
113 
114     Create from NumPy arrays.
115 
116     >>> xrng = np.arange(-10, 10, 2)
117     >>> yrng = np.arange(-10, 10, 5)
118     >>> zrng = np.arange(-10, 10, 1)
119     >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)
120     >>> grid.plot(show_edges=True)
121 
122     """"""
123 
124     _WRITERS = {'.vtk': _vtk.vtkRectilinearGridWriter, '.vtr': _vtk.vtkXMLRectilinearGridWriter}
125 
126     def __init__(self, *args, check_duplicates=False, deep=False, **kwargs):
127         """"""Initialize the rectilinear grid.""""""
128         super().__init__()
129 
130         if len(args) == 1:
131             if isinstance(args[0], _vtk.vtkRectilinearGrid):
132                 if deep:
133                     self.deep_copy(args[0])
134                 else:
135                     self.shallow_copy(args[0])
136             elif isinstance(args[0], (str, pathlib.Path)):
137                 self._from_file(args[0], **kwargs)
138             elif isinstance(args[0], np.ndarray):
139                 self._from_arrays(args[0], None, None, check_duplicates)
140             else:
141                 raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')
142 
143         elif len(args) == 3 or len(args) == 2:
144             arg0_is_arr = isinstance(args[0], np.ndarray)
145             arg1_is_arr = isinstance(args[1], np.ndarray)
146             if len(args) == 3:
147                 arg2_is_arr = isinstance(args[2], np.ndarray)
148             else:
149                 arg2_is_arr = False
150 
151             if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):
152                 self._from_arrays(args[0], args[1], args[2], check_duplicates)
153             elif all([arg0_is_arr, arg1_is_arr]):
154                 self._from_arrays(args[0], args[1], None, check_duplicates)
155             else:
156                 raise TypeError(""Arguments not understood by `RectilinearGrid`."")
157 
158     def __repr__(self):
159         """"""Return the default representation.""""""
160         return DataSet.__repr__(self)
161 
162     def __str__(self):
163         """"""Return the str representation.""""""
164         return DataSet.__str__(self)
165 
166     def _update_dimensions(self):
167         """"""Update the dimensions if coordinates have changed.""""""
168         return self.SetDimensions(len(self.x), len(self.y), len(self.z))
169 
170     def _from_arrays(
171         self, x: np.ndarray, y: np.ndarray, z: np.ndarray, check_duplicates: bool = False
172     ):
173         """"""Create VTK rectilinear grid directly from numpy arrays.
174 
175         Each array gives the uniques coordinates of the mesh along each axial
176         direction. To help ensure you are using this correctly, we take the unique
177         values of each argument.
178 
179         Parameters
180         ----------
181         x : numpy.ndarray
182             Coordinates of the points in x direction.
183 
184         y : numpy.ndarray
185             Coordinates of the points in y direction.
186 
187         z : numpy.ndarray
188             Coordinates of the points in z direction.
189 
190         check_duplicates : bool, optional
191             Check for duplications in any arrays that are passed.
192 
193         """"""
194         # Set the coordinates along each axial direction
195         # Must at least be an x array
196         if check_duplicates:
197             raise_has_duplicates(x)
198 
199         # edges are shown as triangles if x is not floating point
200         if not np.issubdtype(x.dtype, np.floating):
201             x = x.astype(float)
202         self.SetXCoordinates(helpers.convert_array(x.ravel()))
203         if y is not None:
204             if check_duplicates:
205                 raise_has_duplicates(y)
206             if not np.issubdtype(y.dtype, np.floating):
207                 y = y.astype(float)
208             self.SetYCoordinates(helpers.convert_array(y.ravel()))
209         if z is not None:
210             if check_duplicates:
211                 raise_has_duplicates(z)
212             if not np.issubdtype(z.dtype, np.floating):
213                 z = z.astype(float)
214             self.SetZCoordinates(helpers.convert_array(z.ravel()))
215         # Ensure dimensions are properly set
216         self._update_dimensions()
217 
218     @property
219     def meshgrid(self) -> list:
220         """"""Return a meshgrid of numpy arrays for this mesh.
221 
222         This simply returns a :func:`numpy.meshgrid` of the
223         coordinates for this mesh in ``ij`` indexing. These are a copy
224         of the points of this mesh.
225 
226         """"""
227         return np.meshgrid(self.x, self.y, self.z, indexing='ij')
228 
229     @property  # type: ignore
230     def points(self) -> np.ndarray:  # type: ignore
231         """"""Return a copy of the points as an n by 3 numpy array.
232 
233         Notes
234         -----
235         Points of a :class:`pyvista.RectilinearGrid` cannot be
236         set. Set point coordinates with :attr:`RectilinearGrid.x`,
237         :attr:`RectilinearGrid.y`, or :attr:`RectilinearGrid.z`.
238 
239         Examples
240         --------
241         >>> import numpy as np
242         >>> import pyvista
243         >>> xrng = np.arange(-10, 10, 10, dtype=float)
244         >>> yrng = np.arange(-10, 10, 10, dtype=float)
245         >>> zrng = np.arange(-10, 10, 10, dtype=float)
246         >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)
247         >>> grid.points
248         array([[-10., -10., -10.],
249                [  0., -10., -10.],
250                [-10.,   0., -10.],
251                [  0.,   0., -10.],
252                [-10., -10.,   0.],
253                [  0., -10.,   0.],
254                [-10.,   0.,   0.],
255                [  0.,   0.,   0.]])
256 
257         """"""
258         xx, yy, zz = self.meshgrid
259         return np.c_[xx.ravel(order='F'), yy.ravel(order='F'), zz.ravel(order='F')]
260 
261     @points.setter
262     def points(self, points):
263         """"""Raise an AttributeError.
264 
265         This setter overrides the base class's setter to ensure a user
266         does not attempt to set them.
267         """"""
268         raise AttributeError(
269             ""The points cannot be set. The points of ""
270             ""`RectilinearGrid` are defined in each axial direction. Please ""
271             ""use the `x`, `y`, and `z` setters individually.""
272         )
273 
274     @property
275     def x(self) -> np.ndarray:
276         """"""Return or set the coordinates along the X-direction.
277 
278         Examples
279         --------
280         Return the x coordinates of a RectilinearGrid.
281 
282         >>> import numpy as np
283         >>> import pyvista
284         >>> xrng = np.arange(-10, 10, 10, dtype=float)
285         >>> yrng = np.arange(-10, 10, 10, dtype=float)
286         >>> zrng = np.arange(-10, 10, 10, dtype=float)
287         >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)
288         >>> grid.x
289         array([-10.,   0.])
290 
291         Set the x coordinates of a RectilinearGrid.
292 
293         >>> grid.x = [-10.0, 0.0, 10.0]
294         >>> grid.x
295         array([-10.,   0.,  10.])
296 
297         """"""
298         return helpers.convert_array(self.GetXCoordinates())
299 
300     @x.setter
301     def x(self, coords: Sequence):
302         """"""Set the coordinates along the X-direction.""""""
303         self.SetXCoordinates(helpers.convert_array(coords))
304         self._update_dimensions()
305         self.Modified()
306 
307     @property
308     def y(self) -> np.ndarray:
309         """"""Return or set the coordinates along the Y-direction.
310 
311         Examples
312         --------
313         Return the y coordinates of a RectilinearGrid.
314 
315         >>> import numpy as np
316         >>> import pyvista
317         >>> xrng = np.arange(-10, 10, 10, dtype=float)
318         >>> yrng = np.arange(-10, 10, 10, dtype=float)
319         >>> zrng = np.arange(-10, 10, 10, dtype=float)
320         >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)
321         >>> grid.y
322         array([-10.,   0.])
323 
324         Set the y coordinates of a RectilinearGrid.
325 
326         >>> grid.y = [-10.0, 0.0, 10.0]
327         >>> grid.y
328         array([-10.,   0.,  10.])
329 
330         """"""
331         return helpers.convert_array(self.GetYCoordinates())
332 
333     @y.setter
334     def y(self, coords: Sequence):
335         """"""Set the coordinates along the Y-direction.""""""
336         self.SetYCoordinates(helpers.convert_array(coords))
337         self._update_dimensions()
338         self.Modified()
339 
340     @property
341     def z(self) -> np.ndarray:
342         """"""Return or set the coordinates along the Z-direction.
343 
344         Examples
345         --------
346         Return the z coordinates of a RectilinearGrid.
347 
348         >>> import numpy as np
349         >>> import pyvista
350         >>> xrng = np.arange(-10, 10, 10, dtype=float)
351         >>> yrng = np.arange(-10, 10, 10, dtype=float)
352         >>> zrng = np.arange(-10, 10, 10, dtype=float)
353         >>> grid = pyvista.RectilinearGrid(xrng, yrng, zrng)
354         >>> grid.z
355         array([-10.,   0.])
356 
357         Set the z coordinates of a RectilinearGrid.
358 
359         >>> grid.z = [-10.0, 0.0, 10.0]
360         >>> grid.z
361         array([-10.,   0.,  10.])
362 
363         """"""
364         return helpers.convert_array(self.GetZCoordinates())
365 
366     @z.setter
367     def z(self, coords: Sequence):
368         """"""Set the coordinates along the Z-direction.""""""
369         self.SetZCoordinates(helpers.convert_array(coords))
370         self._update_dimensions()
371         self.Modified()
372 
373     @Grid.dimensions.setter  # type: ignore
374     def dimensions(self, dims):
375         """"""Do not let the dimensions of the RectilinearGrid be set.""""""
376         raise AttributeError(
377             ""The dimensions of a `RectilinearGrid` are implicitly ""
378             ""defined and thus cannot be set.""
379         )
380 
381     def cast_to_structured_grid(self) -> 'pyvista.StructuredGrid':
382         """"""Cast this rectilinear grid to a structured grid.
383 
384         Returns
385         -------
386         pyvista.StructuredGrid
387             This grid as a structured grid.
388 
389         """"""
390         alg = _vtk.vtkRectilinearGridToPointSet()
391         alg.SetInputData(self)
392         alg.Update()
393         return _get_output(alg)
394 
395 
396 class UniformGrid(_vtk.vtkImageData, Grid, UniformGridFilters):
397     """"""Models datasets with uniform spacing in the three coordinate directions.
398 
399     Can be initialized in one of several ways:
400 
401     - Create empty grid
402     - Initialize from a vtk.vtkImageData object
403     - Initialize based on dimensions, cell spacing, and origin.
404 
405     .. versionchanged:: 0.33.0
406         First argument must now be either a path or
407         ``vtk.vtkImageData``. Use keyword arguments to specify the
408         dimensions, spacing, and origin of the uniform grid.
409 
410     .. versionchanged:: 0.37.0
411         The ``dims`` parameter has been renamed to ``dimensions``.
412 
413     Parameters
414     ----------
415     uinput : str, vtk.vtkImageData, pyvista.UniformGrid, optional
416         Filename or dataset to initialize the uniform grid from.  If
417         set, remainder of arguments are ignored.
418 
419     dimensions : sequence[int], optional
420         Dimensions of the uniform grid.
421 
422     spacing : sequence[float], default: (1.0, 1.0, 1.0)
423         Spacing of the uniform grid in each dimension. Must be positive.
424 
425     origin : sequence[float], default: (0.0, 0.0, 0.0)
426         Origin of the uniform grid.
427 
428     deep : bool, default: False
429         Whether to deep copy a ``vtk.vtkImageData`` object.  Keyword only.
430 
431     Examples
432     --------
433     Create an empty UniformGrid.
434 
435     >>> import pyvista
436     >>> grid = pyvista.UniformGrid()
437 
438     Initialize from a ``vtk.vtkImageData`` object.
439 
440     >>> import vtk
441     >>> vtkgrid = vtk.vtkImageData()
442     >>> grid = pyvista.UniformGrid(vtkgrid)
443 
444     Initialize using just the grid dimensions and default
445     spacing and origin. These must be keyword arguments.
446 
447     >>> grid = pyvista.UniformGrid(dimensions=(10, 10, 10))
448 
449     Initialize using dimensions and spacing.
450 
451     >>> grid = pyvista.UniformGrid(
452     ...     dimensions=(10, 10, 10),
453     ...     spacing=(2, 1, 5),
454     ... )
455 
456     Initialize using dimensions, spacing, and an origin.
457 
458     >>> grid = pyvista.UniformGrid(
459     ...     dimensions=(10, 10, 10),
460     ...     spacing=(2, 1, 5),
461     ...     origin=(10, 35, 50),
462     ... )
463 
464     Initialize from another UniformGrid.
465 
466     >>> grid = pyvista.UniformGrid(
467     ...     dimensions=(10, 10, 10),
468     ...     spacing=(2, 1, 5),
469     ...     origin=(10, 35, 50),
470     ... )
471     >>> grid_from_grid = pyvista.UniformGrid(grid)
472     >>> grid_from_grid == grid
473     True
474 
475     """"""
476 
477     _WRITERS = {'.vtk': _vtk.vtkDataSetWriter, '.vti': _vtk.vtkXMLImageDataWriter}
478 
479     def __init__(
480         self,
481         uinput=None,
482         *args,
483         dimensions=None,
484         spacing=(1.0, 1.0, 1.0),
485         origin=(0.0, 0.0, 0.0),
486         deep=False,
487         **kwargs,
488     ):
489         """"""Initialize the uniform grid.""""""
490         super().__init__()
491 
492         # permit old behavior
493         if isinstance(uinput, Sequence) and not isinstance(uinput, str):
494             # Deprecated on v0.37.0, estimated removal on v0.40.0
495             warnings.warn(
496                 ""Behavior of pyvista.UniformGrid has changed. First argument must be ""
497                 ""either a ``vtk.vtkImageData`` or path."",
498                 PyVistaDeprecationWarning,
499             )
500             dimensions = uinput
501             uinput = None
502 
503         if dimensions is None and 'dims' in kwargs:
504             dimensions = kwargs.pop('dims')
505             # Deprecated on v0.37.0, estimated removal on v0.40.0
506             warnings.warn(
507                 '`dims` argument is deprecated. Please use `dimensions`.', PyVistaDeprecationWarning
508             )
509         assert_empty_kwargs(**kwargs)
510 
511         if args:
512             # Deprecated on v0.37.0, estimated removal on v0.40.0
513             warnings.warn(
514                 ""Behavior of pyvista.UniformGrid has changed. Use keyword arguments ""
515                 ""to specify dimensions, spacing, and origin. For example:\n\n""
516                 ""    >>> grid = pyvista.UniformGrid(\n""
517                 ""    ...     dimensions=(10, 10, 10),\n""
518                 ""    ...     spacing=(2, 1, 5),\n""
519                 ""    ...     origin=(10, 35, 50),\n""
520                 ""    ... )\n"",
521                 PyVistaDeprecationWarning,
522             )
523             origin = args[0]
524             if len(args) > 1:
525                 spacing = args[1]
526             if len(args) > 2:
527                 raise ValueError(
528                     ""Too many additional arguments specified for UniformGrid. ""
529                     f""Accepts at most 2, and {len(args)} have been input.""
530                 )
531 
532         # first argument must be either vtkImageData or a path
533         if uinput is not None:
534             if isinstance(uinput, _vtk.vtkImageData):
535                 if deep:
536                     self.deep_copy(uinput)
537                 else:
538                     self.shallow_copy(uinput)
539             elif isinstance(uinput, (str, pathlib.Path)):
540                 self._from_file(uinput)
541             else:
542                 raise TypeError(
543                     ""First argument, ``uinput`` must be either ``vtk.vtkImageData`` ""
544                     f""or a path, not {type(uinput)}.  Use keyword arguments to ""
545                     ""specify dimensions, spacing, and origin. For example:\n\n""
546                     ""    >>> grid = pyvista.UniformGrid(\n""
547                     ""    ...     dimensions=(10, 10, 10),\n""
548                     ""    ...     spacing=(2, 1, 5),\n""
549                     ""    ...     origin=(10, 35, 50),\n""
550                     ""    ... )\n""
551                 )
552         elif dimensions is not None:
553             self._from_specs(dimensions, spacing, origin)
554 
555     def __repr__(self):
556         """"""Return the default representation.""""""
557         return DataSet.__repr__(self)
558 
559     def __str__(self):
560         """"""Return the default str representation.""""""
561         return DataSet.__str__(self)
562 
563     def _from_specs(self, dims: Sequence[int], spacing=(1.0, 1.0, 1.0), origin=(0.0, 0.0, 0.0)):
564         """"""Create VTK image data directly from numpy arrays.
565 
566         A uniform grid is defined by the point spacings for each axis
567         (uniform along each individual axis) and the number of points on each axis.
568         These are relative to a specified origin (default is ``(0.0, 0.0, 0.0)``).
569 
570         Parameters
571         ----------
572         dims : tuple(int)
573             Length 3 tuple of ints specifying how many points along each axis.
574 
575         spacing : sequence[float], default: (1.0, 1.0, 1.0)
576             Length 3 tuple of floats/ints specifying the point spacings
577             for each axis. Must be positive.
578 
579         origin : sequence[float], default: (0.0, 0.0, 0.0)
580             Length 3 tuple of floats/ints specifying minimum value for each axis.
581 
582         """"""
583         xn, yn, zn = dims[0], dims[1], dims[2]
584         xo, yo, zo = origin[0], origin[1], origin[2]
585         self.SetDimensions(xn, yn, zn)
586         self.SetOrigin(xo, yo, zo)
587         self.spacing = (spacing[0], spacing[1], spacing[2])
588 
589     @property  # type: ignore
590     def points(self) -> np.ndarray:  # type: ignore
591         """"""Build a copy of the implicitly defined points as a numpy array.
592 
593         Notes
594         -----
595         The ``points`` for a :class:`pyvista.UniformGrid` cannot be set.
596 
597         Examples
598         --------
599         >>> import pyvista
600         >>> grid = pyvista.UniformGrid(dimensions=(2, 2, 2))
601         >>> grid.points
602         array([[0., 0., 0.],
603                [1., 0., 0.],
604                [0., 1., 0.],
605                [1., 1., 0.],
606                [0., 0., 1.],
607                [1., 0., 1.],
608                [0., 1., 1.],
609                [1., 1., 1.]])
610 
611         """"""
612         # Get grid dimensions
613         nx, ny, nz = self.dimensions
614         nx -= 1
615         ny -= 1
616         nz -= 1
617         # get the points and convert to spacings
618         dx, dy, dz = self.spacing
619         # Now make the cell arrays
620         ox, oy, oz = np.array(self.origin) + np.array(self.extent[::2])  # type: ignore
621         x = np.insert(np.cumsum(np.full(nx, dx)), 0, 0.0) + ox
622         y = np.insert(np.cumsum(np.full(ny, dy)), 0, 0.0) + oy
623         z = np.insert(np.cumsum(np.full(nz, dz)), 0, 0.0) + oz
624         xx, yy, zz = np.meshgrid(x, y, z, indexing='ij')
625         return np.c_[xx.ravel(order='F'), yy.ravel(order='F'), zz.ravel(order='F')]
626 
627     @points.setter
628     def points(self, points):
629         """"""Points cannot be set.
630 
631         This setter overrides the base class's setter to ensure a user does not
632         attempt to set them. See https://github.com/pyvista/pyvista/issues/713.
633 
634         """"""
635         raise AttributeError(
636             ""The points cannot be set. The points of ""
637             ""`UniformGrid`/`vtkImageData` are implicitly defined by the ""
638             ""`origin`, `spacing`, and `dimensions` of the grid.""
639         )
640 
641     @property
642     def x(self) -> np.ndarray:
643         """"""Return all the X points.
644 
645         Examples
646         --------
647         >>> import pyvista
648         >>> grid = pyvista.UniformGrid(dimensions=(2, 2, 2))
649         >>> grid.x
650         array([0., 1., 0., 1., 0., 1., 0., 1.])
651 
652         """"""
653         return self.points[:, 0]
654 
655     @property
656     def y(self) -> np.ndarray:
657         """"""Return all the Y points.
658 
659         Examples
660         --------
661         >>> import pyvista
662         >>> grid = pyvista.UniformGrid(dimensions=(2, 2, 2))
663         >>> grid.y
664         array([0., 0., 1., 1., 0., 0., 1., 1.])
665 
666         """"""
667         return self.points[:, 1]
668 
669     @property
670     def z(self) -> np.ndarray:
671         """"""Return all the Z points.
672 
673         Examples
674         --------
675         >>> import pyvista
676         >>> grid = pyvista.UniformGrid(dimensions=(2, 2, 2))
677         >>> grid.z
678         array([0., 0., 0., 0., 1., 1., 1., 1.])
679 
680         """"""
681         return self.points[:, 2]
682 
683     @property
684     def origin(self) -> Tuple[float]:
685         """"""Return the origin of the grid (bottom southwest corner).
686 
687         Examples
688         --------
689         >>> import pyvista
690         >>> grid = pyvista.UniformGrid(dimensions=(5, 5, 5))
691         >>> grid.origin
692         (0.0, 0.0, 0.0)
693 
694         Show how the origin is in the bottom ""southwest"" corner of the
695         UniformGrid.
696 
697         >>> pl = pyvista.Plotter()
698         >>> _ = pl.add_mesh(grid, show_edges=True)
699         >>> _ = pl.add_axes_at_origin(ylabel=None)
700         >>> pl.camera_position = 'xz'
701         >>> pl.show()
702 
703         Set the origin to ``(1, 1, 1)`` and show how this shifts the
704         UniformGrid.
705 
706         >>> grid.origin = (1, 1, 1)
707         >>> pl = pyvista.Plotter()
708         >>> _ = pl.add_mesh(grid, show_edges=True)
709         >>> _ = pl.add_axes_at_origin(ylabel=None)
710         >>> pl.camera_position = 'xz'
711         >>> pl.show()
712 
713         """"""
714         return self.GetOrigin()
715 
716     @origin.setter
717     def origin(self, origin: Sequence[Union[float, int]]):
718         """"""Set the origin.""""""
719         self.SetOrigin(origin[0], origin[1], origin[2])
720         self.Modified()
721 
722     @property
723     def spacing(self) -> Tuple[float, float, float]:
724         """"""Return or set the spacing for each axial direction.
725 
726         Notes
727         -----
728         Spacing must be non-negative. While VTK accepts negative
729         spacing, this results in unexpected behavior. See:
730         https://github.com/pyvista/pyvista/issues/1967
731 
732         Examples
733         --------
734         Create a 5 x 5 x 5 uniform grid.
735 
736         >>> import pyvista
737         >>> grid = pyvista.UniformGrid(dimensions=(5, 5, 5))
738         >>> grid.spacing
739         (1.0, 1.0, 1.0)
740         >>> grid.plot(show_edges=True)
741 
742         Modify the spacing to ``(1, 2, 3)``
743 
744         >>> grid.spacing = (1, 2, 3)
745         >>> grid.plot(show_edges=True)
746 
747         """"""
748         return self.GetSpacing()
749 
750     @spacing.setter
751     def spacing(self, spacing: Sequence[Union[float, int]]):
752         """"""Set spacing.""""""
753         if min(spacing) < 0:
754             raise ValueError(f""Spacing must be non-negative, got {spacing}"")
755         self.SetSpacing(*spacing)
756         self.Modified()
757 
758     def _get_attrs(self):
759         """"""Return the representation methods (internal helper).""""""
760         attrs = Grid._get_attrs(self)
761         fmt = ""{}, {}, {}"".format(*[pyvista.FLOAT_FORMAT] * 3)
762         attrs.append((""Spacing"", self.spacing, fmt))
763         return attrs
764 
765     def cast_to_structured_grid(self) -> 'pyvista.StructuredGrid':
766         """"""Cast this uniform grid to a structured grid.
767 
768         Returns
769         -------
770         pyvista.StructuredGrid
771             This grid as a structured grid.
772 
773         """"""
774         alg = _vtk.vtkImageToStructuredGrid()
775         alg.SetInputData(self)
776         alg.Update()
777         return _get_output(alg)
778 
779     def cast_to_rectilinear_grid(self) -> 'RectilinearGrid':
780         """"""Cast this uniform grid to a rectilinear grid.
781 
782         Returns
783         -------
784         pyvista.RectilinearGrid
785             This uniform grid as a rectilinear grid.
786 
787         """"""
788 
789         def gen_coords(i):
790             coords = (
791                 np.cumsum(np.insert(np.full(self.dimensions[i] - 1, self.spacing[i]), 0, 0))
792                 + self.origin[i]
793             )
794             return coords
795 
796         xcoords = gen_coords(0)
797         ycoords = gen_coords(1)
798         zcoords = gen_coords(2)
799         grid = pyvista.RectilinearGrid(xcoords, ycoords, zcoords)
800         grid.point_data.update(self.point_data)
801         grid.cell_data.update(self.cell_data)
802         grid.field_data.update(self.field_data)
803         grid.copy_meta_from(self, deep=True)
804         return grid
805 
806     @property
807     def extent(self) -> tuple:
808         """"""Return or set the extent of the UniformGrid.
809 
810         The extent is simply the first and last indices for each of the three axes.
811 
812         Examples
813         --------
814         Create a ``UniformGrid`` and show its extent.
815 
816         >>> import pyvista
817         >>> grid = pyvista.UniformGrid(dimensions=(10, 10, 10))
818         >>> grid.extent
819         (0, 9, 0, 9, 0, 9)
820 
821         >>> grid.extent = (2, 5, 2, 5, 2, 5)
822         >>> grid.extent
823         (2, 5, 2, 5, 2, 5)
824 
825         Note how this also modifies the grid bounds and dimensions. Since we
826         use default spacing of 1 here, the bounds match the extent exactly.
827 
828         >>> grid.bounds
829         (2.0, 5.0, 2.0, 5.0, 2.0, 5.0)
830         >>> grid.dimensions
831         (4, 4, 4)
832 
833         """"""
834         return self.GetExtent()
835 
836     @extent.setter
837     def extent(self, new_extent: Sequence[int]):
838         """"""Set the extent of the UniformGrid.""""""
839         if len(new_extent) != 6:
840             raise ValueError('Extent must be a vector of 6 values.')
841         self.SetExtent(new_extent)
842 
843     @wraps(RectilinearGridFilters.to_tetrahedra)
844     def to_tetrahedra(self, *args, **kwargs):
845         """"""Cast to a rectangular grid and then convert to tetrahedra.""""""
846         return self.cast_to_rectilinear_grid().to_tetrahedra(*args, **kwargs)
847 
[end of pyvista/core/grid.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pyvista/pyvista,db6ee8dd4a747b8864caae36c5d05883976a3ae5,"Rectilinear grid does not allow Sequences as inputs
### Describe the bug, what's wrong, and what you expected.

Rectilinear grid gives an error when `Sequence`s are passed in, but `ndarray` are ok.

### Steps to reproduce the bug.

This doesn't work
```python
import pyvista as pv
pv.RectilinearGrid([0, 1], [0, 1], [0, 1])
```

This works
```py
import pyvista as pv
import numpy as np
pv.RectilinearGrid(np.ndarray([0, 1]), np.ndarray([0, 1]), np.ndarray([0, 1]))
```
### System Information

```shell
--------------------------------------------------------------------------------
  Date: Wed Apr 19 20:15:10 2023 UTC

                OS : Linux
            CPU(s) : 2
           Machine : x86_64
      Architecture : 64bit
       Environment : IPython
        GPU Vendor : Mesa/X.org
      GPU Renderer : llvmpipe (LLVM 11.0.1, 256 bits)
       GPU Version : 4.5 (Core Profile) Mesa 20.3.5

  Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]

           pyvista : 0.38.5
               vtk : 9.2.6
             numpy : 1.24.2
           imageio : 2.27.0
            scooby : 0.7.1
             pooch : v1.7.0
        matplotlib : 3.7.1
           IPython : 8.12.0
--------------------------------------------------------------------------------
```


### Screenshots

_No response_
",,2023-04-21T13:47:31Z,"<patch>
diff --git a/pyvista/core/grid.py b/pyvista/core/grid.py
--- a/pyvista/core/grid.py
+++ b/pyvista/core/grid.py
@@ -135,23 +135,30 @@ def __init__(self, *args, check_duplicates=False, deep=False, **kwargs):
                     self.shallow_copy(args[0])
             elif isinstance(args[0], (str, pathlib.Path)):
                 self._from_file(args[0], **kwargs)
-            elif isinstance(args[0], np.ndarray):
-                self._from_arrays(args[0], None, None, check_duplicates)
+            elif isinstance(args[0], (np.ndarray, Sequence)):
+                self._from_arrays(np.asanyarray(args[0]), None, None, check_duplicates)
             else:
                 raise TypeError(f'Type ({type(args[0])}) not understood by `RectilinearGrid`')
 
         elif len(args) == 3 or len(args) == 2:
-            arg0_is_arr = isinstance(args[0], np.ndarray)
-            arg1_is_arr = isinstance(args[1], np.ndarray)
+            arg0_is_arr = isinstance(args[0], (np.ndarray, Sequence))
+            arg1_is_arr = isinstance(args[1], (np.ndarray, Sequence))
             if len(args) == 3:
-                arg2_is_arr = isinstance(args[2], np.ndarray)
+                arg2_is_arr = isinstance(args[2], (np.ndarray, Sequence))
             else:
                 arg2_is_arr = False
 
             if all([arg0_is_arr, arg1_is_arr, arg2_is_arr]):
-                self._from_arrays(args[0], args[1], args[2], check_duplicates)
+                self._from_arrays(
+                    np.asanyarray(args[0]),
+                    np.asanyarray(args[1]),
+                    np.asanyarray(args[2]),
+                    check_duplicates,
+                )
             elif all([arg0_is_arr, arg1_is_arr]):
-                self._from_arrays(args[0], args[1], None, check_duplicates)
+                self._from_arrays(
+                    np.asanyarray(args[0]), np.asanyarray(args[1]), None, check_duplicates
+                )
             else:
                 raise TypeError(""Arguments not understood by `RectilinearGrid`."")
 

</patch>","diff --git a/tests/test_grid.py b/tests/test_grid.py
--- a/tests/test_grid.py
+++ b/tests/test_grid.py
@@ -735,6 +735,21 @@ def test_create_rectilinear_grid_from_specs():
     assert grid.n_cells == 9 * 3 * 19
     assert grid.n_points == 10 * 4 * 20
     assert grid.bounds == (-10.0, 8.0, -10.0, 5.0, -10.0, 9.0)
+
+    # with Sequence
+    xrng = [0, 1]
+    yrng = [0, 1, 2]
+    zrng = [0, 1, 2, 3]
+    grid = pyvista.RectilinearGrid(xrng)
+    assert grid.n_cells == 1
+    assert grid.n_points == 2
+    grid = pyvista.RectilinearGrid(xrng, yrng)
+    assert grid.n_cells == 2
+    assert grid.n_points == 6
+    grid = pyvista.RectilinearGrid(xrng, yrng, zrng)
+    assert grid.n_cells == 6
+    assert grid.n_points == 24
+
     # 2D example
     cell_spacings = np.array([1.0, 1.0, 2.0, 2.0, 5.0, 10.0])
     x_coordinates = np.cumsum(cell_spacings)
",0.39,"[""tests/test_grid.py::test_create_rectilinear_grid_from_specs""]","[""tests/test_grid.py::test_volume"", ""tests/test_grid.py::test_init_from_polydata"", ""tests/test_grid.py::test_init_from_structured"", ""tests/test_grid.py::test_init_from_unstructured"", ""tests/test_grid.py::test_init_from_numpy_arrays"", ""tests/test_grid.py::test_init_bad_input"", ""tests/test_grid.py::test_init_from_arrays[False]"", ""tests/test_grid.py::test_init_from_arrays[True]"", ""tests/test_grid.py::test_init_from_dict[False-False]"", ""tests/test_grid.py::test_init_from_dict[False-True]"", ""tests/test_grid.py::test_init_from_dict[True-False]"", ""tests/test_grid.py::test_init_from_dict[True-True]"", ""tests/test_grid.py::test_init_polyhedron"", ""tests/test_grid.py::test_cells_dict_hexbeam_file"", ""tests/test_grid.py::test_cells_dict_variable_length"", ""tests/test_grid.py::test_cells_dict_empty_grid"", ""tests/test_grid.py::test_cells_dict_alternating_cells"", ""tests/test_grid.py::test_destructor"", ""tests/test_grid.py::test_surface_indices"", ""tests/test_grid.py::test_extract_feature_edges"", ""tests/test_grid.py::test_triangulate_inplace"", ""tests/test_grid.py::test_save[.vtu-True]"", ""tests/test_grid.py::test_save[.vtu-False]"", ""tests/test_grid.py::test_save[.vtk-True]"", ""tests/test_grid.py::test_save[.vtk-False]"", ""tests/test_grid.py::test_pathlib_read_write"", ""tests/test_grid.py::test_init_bad_filename"", ""tests/test_grid.py::test_save_bad_extension"", ""tests/test_grid.py::test_linear_copy"", ""tests/test_grid.py::test_linear_copy_surf_elem"", ""tests/test_grid.py::test_extract_cells[True]"", ""tests/test_grid.py::test_extract_cells[False]"", ""tests/test_grid.py::test_merge"", ""tests/test_grid.py::test_merge_not_main"", ""tests/test_grid.py::test_merge_list"", ""tests/test_grid.py::test_merge_invalid"", ""tests/test_grid.py::test_init_structured_raise"", ""tests/test_grid.py::test_init_structured"", ""tests/test_grid.py::test_no_copy_polydata_init"", ""tests/test_grid.py::test_no_copy_polydata_points_setter"", ""tests/test_grid.py::test_no_copy_structured_mesh_init"", ""tests/test_grid.py::test_no_copy_structured_mesh_points_setter"", ""tests/test_grid.py::test_no_copy_pointset_init"", ""tests/test_grid.py::test_no_copy_pointset_points_setter"", ""tests/test_grid.py::test_no_copy_unstructured_grid_points_setter"", ""tests/test_grid.py::test_no_copy_rectilinear_grid"", ""tests/test_grid.py::test_grid_repr"", ""tests/test_grid.py::test_slice_structured"", ""tests/test_grid.py::test_invalid_init_structured"", ""tests/test_grid.py::test_save_structured[.vtk-True]"", ""tests/test_grid.py::test_save_structured[.vtk-False]"", ""tests/test_grid.py::test_save_structured[.vts-True]"", ""tests/test_grid.py::test_save_structured[.vts-False]"", ""tests/test_grid.py::test_load_structured_bad_filename"", ""tests/test_grid.py::test_instantiate_by_filename"", ""tests/test_grid.py::test_create_rectilinear_after_init"", ""tests/test_grid.py::test_create_rectilinear_grid_from_file"", ""tests/test_grid.py::test_read_rectilinear_grid_from_file"", ""tests/test_grid.py::test_read_rectilinear_grid_from_pathlib"", ""tests/test_grid.py::test_raise_rectilinear_grid_non_unique"", ""tests/test_grid.py::test_cast_rectilinear_grid"", ""tests/test_grid.py::test_create_uniform_grid_from_specs"", ""tests/test_grid.py::test_uniform_grid_invald_args"", ""tests/test_grid.py::test_uniform_setters"", ""tests/test_grid.py::test_create_uniform_grid_from_file"", ""tests/test_grid.py::test_read_uniform_grid_from_file"", ""tests/test_grid.py::test_read_uniform_grid_from_pathlib"", ""tests/test_grid.py::test_cast_uniform_to_structured"", ""tests/test_grid.py::test_cast_uniform_to_rectilinear"", ""tests/test_grid.py::test_uniform_grid_to_tetrahedra"", ""tests/test_grid.py::test_fft_and_rfft"", ""tests/test_grid.py::test_fft_low_pass"", ""tests/test_grid.py::test_fft_high_pass"", ""tests/test_grid.py::test_save_rectilinear[.vtk-True]"", ""tests/test_grid.py::test_save_rectilinear[.vtk-False]"", ""tests/test_grid.py::test_save_rectilinear[.vtr-True]"", ""tests/test_grid.py::test_save_rectilinear[.vtr-False]"", ""tests/test_grid.py::test_save_uniform[.vtk-True]"", ""tests/test_grid.py::test_save_uniform[.vtk-False]"", ""tests/test_grid.py::test_save_uniform[.vti-True]"", ""tests/test_grid.py::test_save_uniform[.vti-False]"", ""tests/test_grid.py::test_grid_points"", ""tests/test_grid.py::test_grid_extract_selection_points"", ""tests/test_grid.py::test_gaussian_smooth"", ""tests/test_grid.py::test_remove_cells[ind0]"", ""tests/test_grid.py::test_remove_cells[ind1]"", ""tests/test_grid.py::test_remove_cells[ind2]"", ""tests/test_grid.py::test_remove_cells_not_inplace[ind0]"", ""tests/test_grid.py::test_remove_cells_not_inplace[ind1]"", ""tests/test_grid.py::test_remove_cells_not_inplace[ind2]"", ""tests/test_grid.py::test_remove_cells_invalid"", ""tests/test_grid.py::test_hide_cells[ind0]"", ""tests/test_grid.py::test_hide_cells[ind1]"", ""tests/test_grid.py::test_hide_cells[ind2]"", ""tests/test_grid.py::test_hide_points[ind0]"", ""tests/test_grid.py::test_hide_points[ind1]"", ""tests/test_grid.py::test_hide_points[ind2]"", ""tests/test_grid.py::test_set_extent"", ""tests/test_grid.py::test_UnstructuredGrid_cast_to_explicit_structured_grid"", ""tests/test_grid.py::test_ExplicitStructuredGrid_init"", ""tests/test_grid.py::test_ExplicitStructuredGrid_cast_to_unstructured_grid"", ""tests/test_grid.py::test_ExplicitStructuredGrid_save"", ""tests/test_grid.py::test_ExplicitStructuredGrid_hide_cells"", ""tests/test_grid.py::test_ExplicitStructuredGrid_show_cells"", ""tests/test_grid.py::test_ExplicitStructuredGrid_dimensions"", ""tests/test_grid.py::test_ExplicitStructuredGrid_visible_bounds"", ""tests/test_grid.py::test_ExplicitStructuredGrid_cell_id"", ""tests/test_grid.py::test_ExplicitStructuredGrid_cell_coords"", ""tests/test_grid.py::test_ExplicitStructuredGrid_neighbors"", ""tests/test_grid.py::test_ExplicitStructuredGrid_compute_connectivity"", ""tests/test_grid.py::test_ExplicitStructuredGrid_compute_connections"", ""tests/test_grid.py::test_ExplicitStructuredGrid_raise_init"", ""tests/test_grid.py::test_copy_no_copy_wrap_object"", ""tests/test_grid.py::test_copy_no_copy_wrap_object_vtk9""]",4c2d1aed10b1600d520271beba8579c71433e808
pydicom__pydicom-1694,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
Dataset.to_json_dict can still generate exceptions when suppress_invalid_tags=True
**Describe the bug**
I'm using `Dataset.to_json_dict(suppress_invalid_tags=True)` and can live with losing invalid tags.  Unfortunately, I can still trigger an exception with something like  `2.0` in an `IS` field.

**Expected behavior**
to_json_dict shouldn't throw an error about an invalid tag when `suppress_invalid_tags` is enabled.

My thought was simply to move the `data_element = self[key]` into the try/catch block that's right after it.

**Steps To Reproduce**

Traceback:
```
  File ""dicom.py"", line 143, in create_dict
    json_ds = ds.to_json_dict(suppress_invalid_tags=True)
  File ""/usr/lib/python3/dist-packages/pydicom/dataset.py"", line 2495, in to_json_dict
    data_element = self[key]
  File ""/usr/lib/python3/dist-packages/pydicom/dataset.py"", line 939, in __getitem__
    self[tag] = DataElement_from_raw(elem, character_set, self)
  File ""/usr/lib/python3/dist-packages/pydicom/dataelem.py"", line 859, in DataElement_from_raw
    value = convert_value(vr, raw, encoding)
  File ""/usr/lib/python3/dist-packages/pydicom/values.py"", line 771, in convert_value
    return converter(byte_string, is_little_endian, num_format)
  File ""/usr/lib/python3/dist-packages/pydicom/values.py"", line 348, in convert_IS_string
    return MultiString(num_string, valtype=pydicom.valuerep.IS)
  File ""/usr/lib/python3/dist-packages/pydicom/valuerep.py"", line 1213, in MultiString
    return valtype(splitup[0])
  File ""/usr/lib/python3/dist-packages/pydicom/valuerep.py"", line 1131, in __new__
    raise TypeError(""Could not convert value to integer without loss"")
TypeError: Could not convert value to integer without loss
```

**Your environment**
python 3.7, pydicom 2.3



</issue>
<code>
[start of README.md]
1 [![unit-tests](https://github.com/pydicom/pydicom/workflows/unit-tests/badge.svg)](https://github.com/pydicom/pydicom/actions?query=workflow%3Aunit-tests)
2 [![type-hints](https://github.com/pydicom/pydicom/workflows/type-hints/badge.svg)](https://github.com/pydicom/pydicom/actions?query=workflow%3Atype-hints)
3 [![doc-build](https://circleci.com/gh/pydicom/pydicom/tree/master.svg?style=shield)](https://circleci.com/gh/pydicom/pydicom/tree/master)
4 [![test-coverage](https://codecov.io/gh/pydicom/pydicom/branch/master/graph/badge.svg)](https://codecov.io/gh/pydicom/pydicom)
5 [![Python version](https://img.shields.io/pypi/pyversions/pydicom.svg)](https://img.shields.io/pypi/pyversions/pydicom.svg)
6 [![PyPI version](https://badge.fury.io/py/pydicom.svg)](https://badge.fury.io/py/pydicom)
7 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6394735.svg)](https://doi.org/10.5281/zenodo.6394735)
8 [![Gitter](https://badges.gitter.im/pydicom/Lobby.svg)](https://gitter.im/pydicom/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
9 
10 # *pydicom*
11 
12 *pydicom* is a pure Python package for working with [DICOM](https://www.dicomstandard.org/) files. It lets you read, modify and write DICOM data in an easy ""pythonic"" way.
13 
14 As a pure Python package, *pydicom* can run anywhere Python runs without any other requirements, although if you're working with *Pixel Data* then we recommend you also install [NumPy](http://www.numpy.org).
15 
16 If you're looking for a Python library for DICOM networking then you might be interested in another of our projects: [pynetdicom](https://github.com/pydicom/pynetdicom).
17 
18 ## Installation
19 
20 Using [pip](https://pip.pypa.io/en/stable/):
21 ```
22 pip install pydicom
23 ```
24 Using [conda](https://docs.conda.io/en/latest/):
25 ```
26 conda install -c conda-forge pydicom
27 ```
28 
29 For more information, including installation instructions for the development version, see the [installation guide](https://pydicom.github.io/pydicom/stable/tutorials/installation.html).
30 
31 
32 ## Documentation
33 
34 The *pydicom* [user guide](https://pydicom.github.io/pydicom/stable/old/pydicom_user_guide.html), [tutorials](https://pydicom.github.io/pydicom/stable/tutorials/index.html), [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) and [API reference](https://pydicom.github.io/pydicom/stable/reference/index.html) documentation is available for both the [current release](https://pydicom.github.io/pydicom/stable) and the [development version](https://pydicom.github.io/pydicom/dev) on GitHub Pages.
35 
36 ## *Pixel Data*
37 
38 Compressed and uncompressed *Pixel Data* is always available to
39 be read, changed and written as [bytes](https://docs.python.org/3/library/stdtypes.html#bytes-objects):
40 ```python
41 >>> from pydicom import dcmread
42 >>> from pydicom.data import get_testdata_file
43 >>> path = get_testdata_file(""CT_small.dcm"")
44 >>> ds = dcmread(path)
45 >>> type(ds.PixelData)
46 <class 'bytes'>
47 >>> len(ds.PixelData)
48 32768
49 >>> ds.PixelData[:2]
50 b'\xaf\x00'
51 
52 ```
53 
54 If [NumPy](http://www.numpy.org) is installed, *Pixel Data* can be converted to an [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) using the [Dataset.pixel_array](https://pydicom.github.io/pydicom/stable/reference/generated/pydicom.dataset.Dataset.html#pydicom.dataset.Dataset.pixel_array) property:
55 
56 ```python
57 >>> arr = ds.pixel_array
58 >>> arr.shape
59 (128, 128)
60 >>> arr
61 array([[175, 180, 166, ..., 203, 207, 216],
62        [186, 183, 157, ..., 181, 190, 239],
63        [184, 180, 171, ..., 152, 164, 235],
64        ...,
65        [906, 910, 923, ..., 922, 929, 927],
66        [914, 954, 938, ..., 942, 925, 905],
67        [959, 955, 916, ..., 911, 904, 909]], dtype=int16)
68 ```
69 ### Compressed *Pixel Data*
70 #### JPEG, JPEG-LS and JPEG 2000
71 Converting JPEG compressed *Pixel Data* to an ``ndarray`` requires installing one or more additional Python libraries. For information on which libraries are required, see the [pixel data handler documentation](https://pydicom.github.io/pydicom/stable/old/image_data_handlers.html#guide-compressed).
72 
73 Compressing data into one of the JPEG formats is not currently supported.
74 
75 #### RLE
76 Encoding and decoding RLE *Pixel Data* only requires NumPy, however it can
77 be quite slow. You may want to consider [installing one or more additional
78 Python libraries](https://pydicom.github.io/pydicom/stable/old/image_data_compression.html) to speed up the process.
79 
80 ## Examples
81 More [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) are available in the documentation.
82 
83 **Change a patient's ID**
84 ```python
85 from pydicom import dcmread
86 
87 ds = dcmread(""/path/to/file.dcm"")
88 # Edit the (0010,0020) 'Patient ID' element
89 ds.PatientID = ""12345678""
90 ds.save_as(""/path/to/file_updated.dcm"")
91 ```
92 
93 **Display the Pixel Data**
94 
95 With [NumPy](http://www.numpy.org) and [matplotlib](https://matplotlib.org/)
96 ```python
97 import matplotlib.pyplot as plt
98 from pydicom import dcmread
99 from pydicom.data import get_testdata_file
100 
101 # The path to a pydicom test dataset
102 path = get_testdata_file(""CT_small.dcm"")
103 ds = dcmread(path)
104 # `arr` is a numpy.ndarray
105 arr = ds.pixel_array
106 
107 plt.imshow(arr, cmap=""gray"")
108 plt.show()
109 ```
110 
111 ## Contributing
112 
113 To contribute to *pydicom*, read our [contribution guide](https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md).
114 
115 To contribute an example or extension of *pydicom* that doesn't belong with the core software, see our contribution repository:
116 [contrib-pydicom](https://www.github.com/pydicom/contrib-pydicom).
117 
[end of README.md]
[start of pydicom/dataset.py]
1 # Copyright 2008-2021 pydicom authors. See LICENSE file for details.
2 """"""Define the Dataset and FileDataset classes.
3 
4 The Dataset class represents the DICOM Dataset while the FileDataset class
5 adds extra functionality to Dataset when data is read from or written to file.
6 
7 Overview of DICOM object model
8 ------------------------------
9 Dataset (dict subclass)
10   Contains DataElement instances, each of which has a tag, VR, VM and value.
11     The DataElement value can be:
12         * A single value, such as a number, string, etc. (i.e. VM = 1)
13         * A list of numbers, strings, etc. (i.e. VM > 1)
14         * A Sequence (list subclass), where each item is a Dataset which
15             contains its own DataElements, and so on in a recursive manner.
16 """"""
17 import copy
18 from bisect import bisect_left
19 import io
20 from importlib.util import find_spec as have_package
21 import inspect  # for __dir__
22 from itertools import takewhile
23 import json
24 import os
25 import os.path
26 import re
27 from types import TracebackType
28 from typing import (
29     Optional, Tuple, Union, List, Any, cast, Dict, ValuesView,
30     Iterator, BinaryIO, AnyStr, Callable, TypeVar, Type, overload,
31     MutableSequence, MutableMapping, AbstractSet
32 )
33 import warnings
34 import weakref
35 
36 from pydicom.filebase import DicomFileLike
37 
38 try:
39     import numpy
40 except ImportError:
41     pass
42 
43 import pydicom  # for dcmwrite
44 from pydicom import jsonrep, config
45 from pydicom._version import __version_info__
46 from pydicom.charset import default_encoding, convert_encodings
47 from pydicom.config import logger
48 from pydicom.datadict import (
49     dictionary_VR, tag_for_keyword, keyword_for_tag, repeater_has_keyword
50 )
51 from pydicom.dataelem import DataElement, DataElement_from_raw, RawDataElement
52 from pydicom.encaps import encapsulate, encapsulate_extended
53 from pydicom.fileutil import path_from_pathlike, PathType
54 from pydicom.pixel_data_handlers.util import (
55     convert_color_space, reshape_pixel_array, get_image_pixel_ids
56 )
57 from pydicom.tag import Tag, BaseTag, tag_in_exception, TagType
58 from pydicom.uid import (
59     ExplicitVRLittleEndian, ImplicitVRLittleEndian, ExplicitVRBigEndian,
60     RLELossless, PYDICOM_IMPLEMENTATION_UID, UID
61 )
62 from pydicom.valuerep import VR as VR_, AMBIGUOUS_VR
63 from pydicom.waveforms import numpy_handler as wave_handler
64 
65 
66 class PrivateBlock:
67     """"""Helper class for a private block in the :class:`Dataset`.
68 
69     .. versionadded:: 1.3
70 
71     See the DICOM Standard, Part 5,
72     :dcm:`Section 7.8.1<part05/sect_7.8.html#sect_7.8.1>` - Private Data
73     Element Tags
74 
75     Attributes
76     ----------
77     group : int
78         The private group where the private block is located as a 32-bit
79         :class:`int`.
80     private_creator : str
81         The private creator string related to the block.
82     dataset : Dataset
83         The parent dataset.
84     block_start : int
85         The start element of the private block as a 32-bit :class:`int`. Note
86         that the 2 low order hex digits of the element are always 0.
87     """"""
88 
89     def __init__(
90         self,
91         key: Tuple[int, str],
92         dataset: ""Dataset"",
93         private_creator_element: int
94     ) -> None:
95         """"""Initializes an object corresponding to a private tag block.
96 
97         Parameters
98         ----------
99         key : tuple
100             The private (tag group, creator) as ``(int, str)``. The group
101             must be an odd number.
102         dataset : Dataset
103             The parent :class:`Dataset`.
104         private_creator_element : int
105             The element of the private creator tag as a 32-bit :class:`int`.
106         """"""
107         self.group = key[0]
108         self.private_creator = key[1]
109         self.dataset = dataset
110         self.block_start = private_creator_element << 8
111 
112     def get_tag(self, element_offset: int) -> BaseTag:
113         """"""Return the private tag ID for the given `element_offset`.
114 
115         Parameters
116         ----------
117         element_offset : int
118             The lower 16 bits (e.g. 2 hex numbers) of the element tag.
119 
120         Returns
121         -------
122             The tag ID defined by the private block location and the
123             given element offset.
124 
125         Raises
126         ------
127         ValueError
128             If `element_offset` is too large.
129         """"""
130         if element_offset > 0xff:
131             raise ValueError('Element offset must be less than 256')
132         return Tag(self.group, self.block_start + element_offset)
133 
134     def __contains__(self, element_offset: int) -> bool:
135         """"""Return ``True`` if the tag with given `element_offset` is in
136         the parent :class:`Dataset`.
137         """"""
138         return self.get_tag(element_offset) in self.dataset
139 
140     def __getitem__(self, element_offset: int) -> DataElement:
141         """"""Return the data element in the parent dataset for the given element
142         offset.
143 
144         Parameters
145         ----------
146         element_offset : int
147             The lower 16 bits (e.g. 2 hex numbers) of the element tag.
148 
149         Returns
150         -------
151             The data element of the tag in the parent dataset defined by the
152             private block location and the given element offset.
153 
154         Raises
155         ------
156         ValueError
157             If `element_offset` is too large.
158         KeyError
159             If no data element exists at that offset.
160         """"""
161         return self.dataset.__getitem__(self.get_tag(element_offset))
162 
163     def __delitem__(self, element_offset: int) -> None:
164         """"""Delete the tag with the given `element_offset` from the dataset.
165 
166         Parameters
167         ----------
168         element_offset : int
169             The lower 16 bits (e.g. 2 hex numbers) of the element tag
170             to be deleted.
171 
172         Raises
173         ------
174         ValueError
175             If `element_offset` is too large.
176         KeyError
177             If no data element exists at that offset.
178         """"""
179         del self.dataset[self.get_tag(element_offset)]
180 
181     def add_new(self, element_offset: int, VR: str, value: object) -> None:
182         """"""Add a private element to the parent :class:`Dataset`.
183 
184         Adds the private tag with the given `VR` and `value` to the parent
185         :class:`Dataset` at the tag ID defined by the private block and the
186         given `element_offset`.
187 
188         Parameters
189         ----------
190         element_offset : int
191             The lower 16 bits (e.g. 2 hex numbers) of the element tag
192             to be added.
193         VR : str
194             The 2 character DICOM value representation.
195         value
196             The value of the data element. See :meth:`Dataset.add_new()`
197             for a description.
198         """"""
199         tag = self.get_tag(element_offset)
200         self.dataset.add_new(tag, VR, value)
201         self.dataset[tag].private_creator = self.private_creator
202 
203 
204 def _dict_equal(
205     a: ""Dataset"", b: Any, exclude: Optional[List[str]] = None
206 ) -> bool:
207     """"""Common method for Dataset.__eq__ and FileDataset.__eq__
208 
209     Uses .keys() as needed because Dataset iter return items not keys
210     `exclude` is used in FileDataset__eq__ ds.__dict__ compare, which
211     would also compare the wrapped _dict member (entire dataset) again.
212     """"""
213     return (len(a) == len(b) and
214             all(key in b for key in a.keys()) and
215             all(a[key] == b[key] for key in a.keys()
216                 if exclude is None or key not in exclude)
217             )
218 
219 
220 _DatasetValue = Union[DataElement, RawDataElement]
221 _DatasetType = Union[""Dataset"", MutableMapping[BaseTag, _DatasetValue]]
222 
223 
224 class Dataset:
225     """"""A DICOM dataset as a mutable mapping of DICOM Data Elements.
226 
227     Examples
228     --------
229     Add an element to the :class:`Dataset` (for elements in the DICOM
230     dictionary):
231 
232     >>> ds = Dataset()
233     >>> ds.PatientName = ""CITIZEN^Joan""
234     >>> ds.add_new(0x00100020, 'LO', '12345')
235     >>> ds[0x0010, 0x0030] = DataElement(0x00100030, 'DA', '20010101')
236 
237     Add a sequence element to the :class:`Dataset`
238 
239     >>> ds.BeamSequence = [Dataset(), Dataset(), Dataset()]
240     >>> ds.BeamSequence[0].Manufacturer = ""Linac, co.""
241     >>> ds.BeamSequence[1].Manufacturer = ""Linac and Sons, co.""
242     >>> ds.BeamSequence[2].Manufacturer = ""Linac and Daughters, co.""
243 
244     Add private elements to the :class:`Dataset`
245 
246     >>> block = ds.private_block(0x0041, 'My Creator', create=True)
247     >>> block.add_new(0x01, 'LO', '12345')
248 
249     Updating and retrieving element values:
250 
251     >>> ds.PatientName = ""CITIZEN^Joan""
252     >>> ds.PatientName
253     'CITIZEN^Joan'
254     >>> ds.PatientName = ""CITIZEN^John""
255     >>> ds.PatientName
256     'CITIZEN^John'
257 
258     Retrieving an element's value from a Sequence:
259 
260     >>> ds.BeamSequence[0].Manufacturer
261     'Linac, co.'
262     >>> ds.BeamSequence[1].Manufacturer
263     'Linac and Sons, co.'
264 
265     Accessing the :class:`~pydicom.dataelem.DataElement` items:
266 
267     >>> elem = ds['PatientName']
268     >>> elem
269     (0010, 0010) Patient's Name                      PN: 'CITIZEN^John'
270     >>> elem = ds[0x00100010]
271     >>> elem
272     (0010, 0010) Patient's Name                      PN: 'CITIZEN^John'
273     >>> elem = ds.data_element('PatientName')
274     >>> elem
275     (0010, 0010) Patient's Name                      PN: 'CITIZEN^John'
276 
277     Accessing a private :class:`~pydicom.dataelem.DataElement`
278     item:
279 
280     >>> block = ds.private_block(0x0041, 'My Creator')
281     >>> elem = block[0x01]
282     >>> elem
283     (0041, 1001) Private tag data                    LO: '12345'
284     >>> elem.value
285     '12345'
286 
287     Alternatively:
288 
289     >>> ds.get_private_item(0x0041, 0x01, 'My Creator').value
290     '12345'
291 
292     Deleting an element from the :class:`Dataset`
293 
294     >>> del ds.PatientID
295     >>> del ds.BeamSequence[1].Manufacturer
296     >>> del ds.BeamSequence[2]
297 
298     Deleting a private element from the :class:`Dataset`
299 
300     >>> block = ds.private_block(0x0041, 'My Creator')
301     >>> if 0x01 in block:
302     ...     del block[0x01]
303 
304     Determining if an element is present in the :class:`Dataset`
305 
306     >>> 'PatientName' in ds
307     True
308     >>> 'PatientID' in ds
309     False
310     >>> (0x0010, 0x0030) in ds
311     True
312     >>> 'Manufacturer' in ds.BeamSequence[0]
313     True
314 
315     Iterating through the top level of a :class:`Dataset` only (excluding
316     Sequences):
317 
318     >>> for elem in ds:
319     ...    print(elem)
320     (0010, 0010) Patient's Name                      PN: 'CITIZEN^John'
321 
322     Iterating through the entire :class:`Dataset` (including Sequences):
323 
324     >>> for elem in ds.iterall():
325     ...     print(elem)
326     (0010, 0010) Patient's Name                      PN: 'CITIZEN^John'
327 
328     Recursively iterate through a :class:`Dataset` (including Sequences):
329 
330     >>> def recurse(ds):
331     ...     for elem in ds:
332     ...         if elem.VR == 'SQ':
333     ...             [recurse(item) for item in elem.value]
334     ...         else:
335     ...             # Do something useful with each DataElement
336 
337     Converting the :class:`Dataset` to and from JSON:
338 
339     >>> ds = Dataset()
340     >>> ds.PatientName = ""Some^Name""
341     >>> jsonmodel = ds.to_json()
342     >>> ds2 = Dataset()
343     >>> ds2.from_json(jsonmodel)
344     (0010, 0010) Patient's Name                      PN: 'Some^Name'
345 
346     Attributes
347     ----------
348     default_element_format : str
349         The default formatting for string display.
350     default_sequence_element_format : str
351         The default formatting for string display of sequences.
352     indent_chars : str
353         For string display, the characters used to indent nested Sequences.
354         Default is ``""   ""``.
355     is_little_endian : bool
356         Shall be set before writing with ``write_like_original=False``.
357         The :class:`Dataset` (excluding the pixel data) will be written using
358         the given endianness.
359     is_implicit_VR : bool
360         Shall be set before writing with ``write_like_original=False``.
361         The :class:`Dataset` will be written using the transfer syntax with
362         the given VR handling, e.g *Little Endian Implicit VR* if ``True``,
363         and *Little Endian Explicit VR* or *Big Endian Explicit VR* (depending
364         on ``Dataset.is_little_endian``) if ``False``.
365     """"""
366     indent_chars = ""   ""
367 
368     def __init__(self, *args: _DatasetType, **kwargs: Any) -> None:
369         """"""Create a new :class:`Dataset` instance.""""""
370         self._parent_encoding: List[str] = kwargs.get(
371             'parent_encoding', default_encoding
372         )
373 
374         self._dict: MutableMapping[BaseTag, _DatasetValue]
375         if not args:
376             self._dict = {}
377         elif isinstance(args[0], Dataset):
378             self._dict = args[0]._dict
379         else:
380             self._dict = args[0]
381 
382         self.is_decompressed = False
383 
384         # the following read_XXX attributes are used internally to store
385         # the properties of the dataset after read from a file
386         # set depending on the endianness of the read dataset
387         self.read_little_endian: Optional[bool] = None
388         # set depending on the VR handling of the read dataset
389         self.read_implicit_vr: Optional[bool] = None
390         # The dataset's original character set encoding
391         self.read_encoding: Union[None, str, MutableSequence[str]] = None
392 
393         self.is_little_endian: Optional[bool] = None
394         self.is_implicit_VR: Optional[bool] = None
395 
396         # True if the dataset is a sequence item with undefined length
397         self.is_undefined_length_sequence_item = False
398 
399         # the parent data set, if this dataset is a sequence item
400         self.parent: ""Optional[weakref.ReferenceType[Dataset]]"" = None
401 
402         # known private creator blocks
403         self._private_blocks: Dict[Tuple[int, str], PrivateBlock] = {}
404 
405         self._pixel_array: Optional[""numpy.ndarray""] = None
406         self._pixel_id: Dict[str, int] = {}
407 
408         self.file_meta: FileMetaDataset
409 
410     def __enter__(self) -> ""Dataset"":
411         """"""Method invoked on entry to a with statement.""""""
412         return self
413 
414     def __exit__(
415         self,
416         exc_type: Optional[Type[BaseException]],
417         exc_val: Optional[BaseException],
418         exc_tb: Optional[TracebackType]
419     ) -> Optional[bool]:
420         """"""Method invoked on exit from a with statement.""""""
421         # Returning anything other than True will re-raise any exceptions
422         return None
423 
424     def add(self, data_element: DataElement) -> None:
425         """"""Add an element to the :class:`Dataset`.
426 
427         Equivalent to ``ds[data_element.tag] = data_element``
428 
429         Parameters
430         ----------
431         data_element : dataelem.DataElement
432             The :class:`~pydicom.dataelem.DataElement` to add.
433         """"""
434         self[data_element.tag] = data_element
435 
436     def add_new(self, tag: TagType, VR: str, value: Any) -> None:
437         """"""Create a new element and add it to the :class:`Dataset`.
438 
439         Parameters
440         ----------
441         tag
442             The DICOM (group, element) tag in any form accepted by
443             :func:`~pydicom.tag.Tag` such as ``[0x0010, 0x0010]``,
444             ``(0x10, 0x10)``, ``0x00100010``, etc.
445         VR : str
446             The 2 character DICOM value representation (see DICOM Standard,
447             Part 5, :dcm:`Section 6.2<part05/sect_6.2.html>`).
448         value
449             The value of the data element. One of the following:
450 
451             * a single string or number
452             * a :class:`list` or :class:`tuple` with all strings or all numbers
453             * a multi-value string with backslash separator
454             * for a sequence element, an empty :class:`list` or ``list`` of
455               :class:`Dataset`
456         """"""
457         self.add(DataElement(tag, VR, value))
458 
459     def __array__(self) -> ""numpy.ndarray"":
460         """"""Support accessing the dataset from a numpy array.""""""
461         return numpy.asarray(self._dict)
462 
463     def data_element(self, name: str) -> Optional[DataElement]:
464         """"""Return the element corresponding to the element keyword `name`.
465 
466         Parameters
467         ----------
468         name : str
469             A DICOM element keyword.
470 
471         Returns
472         -------
473         dataelem.DataElement or None
474             For the given DICOM element `keyword`, return the corresponding
475             :class:`~pydicom.dataelem.DataElement` if present, ``None``
476             otherwise.
477         """"""
478         tag = tag_for_keyword(name)
479         # Test against None as (0000,0000) is a possible tag
480         if tag is not None:
481             return self[tag]
482         return None
483 
484     def __contains__(self, name: TagType) -> bool:
485         """"""Simulate dict.__contains__() to handle DICOM keywords.
486 
487         Examples
488         --------
489 
490         >>> ds = Dataset()
491         >>> ds.SliceLocation = '2'
492         >>> 'SliceLocation' in ds
493         True
494 
495         Parameters
496         ----------
497         name : str or int or 2-tuple
498             The element keyword or tag to search for.
499 
500         Returns
501         -------
502         bool
503             ``True`` if the corresponding element is in the :class:`Dataset`,
504             ``False`` otherwise.
505         """"""
506         try:
507             return Tag(name) in self._dict
508         except Exception as exc:
509             msg = (
510                 f""Invalid value '{name}' used with the 'in' operator: must be ""
511                 ""an element tag as a 2-tuple or int, or an element keyword""
512             )
513             if isinstance(exc, OverflowError):
514                 msg = (
515                     ""Invalid element tag value used with the 'in' operator: ""
516                     ""tags have a maximum value of (0xFFFF, 0xFFFF)""
517                 )
518 
519             if config.INVALID_KEY_BEHAVIOR == ""WARN"":
520                 warnings.warn(msg)
521             elif config.INVALID_KEY_BEHAVIOR == ""RAISE"":
522                 raise ValueError(msg) from exc
523 
524         return False
525 
526     def decode(self) -> None:
527         """"""Apply character set decoding to the elements in the
528         :class:`Dataset`.
529 
530         See DICOM Standard, Part 5,
531         :dcm:`Section 6.1.1<part05/chapter_6.html#sect_6.1.1>`.
532         """"""
533         # Find specific character set. 'ISO_IR 6' is default
534         # May be multi-valued, but let pydicom.charset handle all logic on that
535         dicom_character_set = self._character_set
536 
537         # Shortcut to the decode function in pydicom.charset
538         decode_data_element = pydicom.charset.decode_element
539 
540         # Callback for walk(), to decode the chr strings if necessary
541         # This simply calls the pydicom.charset.decode_element function
542         def decode_callback(ds: ""Dataset"", data_element: DataElement) -> None:
543             """"""Callback to decode `data_element`.""""""
544             if data_element.VR == VR_.SQ:
545                 for dset in data_element.value:
546                     dset._parent_encoding = dicom_character_set
547                     dset.decode()
548             else:
549                 decode_data_element(data_element, dicom_character_set)
550 
551         self.walk(decode_callback, recursive=False)
552 
553     def copy(self) -> ""Dataset"":
554         """"""Return a shallow copy of the dataset.""""""
555         return copy.copy(self)
556 
557     def __delattr__(self, name: str) -> None:
558         """"""Intercept requests to delete an attribute by `name`.
559 
560         Examples
561         --------
562 
563         >>> ds = Dataset()
564         >>> ds.PatientName = 'foo'
565         >>> ds.some_attribute = True
566 
567         If `name` is a DICOM keyword - delete the corresponding
568         :class:`~pydicom.dataelem.DataElement`
569 
570         >>> del ds.PatientName
571         >>> 'PatientName' in ds
572         False
573 
574         If `name` is another attribute - delete it
575 
576         >>> del ds.some_attribute
577         >>> hasattr(ds, 'some_attribute')
578         False
579 
580         Parameters
581         ----------
582         name : str
583             The keyword for the DICOM element or the class attribute to delete.
584         """"""
585         # First check if a valid DICOM keyword and if we have that data element
586         tag = cast(BaseTag, tag_for_keyword(name))
587         if tag is not None and tag in self._dict:
588             del self._dict[tag]
589         # If not a DICOM name in this dataset, check for regular instance name
590         #   can't do delete directly, that will call __delattr__ again
591         elif name in self.__dict__:
592             del self.__dict__[name]
593         # Not found, raise an error in same style as python does
594         else:
595             raise AttributeError(name)
596 
597     def __delitem__(self, key: Union[slice, BaseTag, TagType]) -> None:
598         """"""Intercept requests to delete an attribute by key.
599 
600         Examples
601         --------
602         Indexing using :class:`~pydicom.dataelem.DataElement` tag
603 
604         >>> ds = Dataset()
605         >>> ds.CommandGroupLength = 100
606         >>> ds.PatientName = 'CITIZEN^Jan'
607         >>> del ds[0x00000000]
608         >>> ds
609         (0010, 0010) Patient's Name                      PN: 'CITIZEN^Jan'
610 
611         Slicing using :class:`~pydicom.dataelem.DataElement` tag
612 
613         >>> ds = Dataset()
614         >>> ds.CommandGroupLength = 100
615         >>> ds.SOPInstanceUID = '1.2.3'
616         >>> ds.PatientName = 'CITIZEN^Jan'
617         >>> del ds[:0x00100000]
618         >>> ds
619         (0010, 0010) Patient's Name                      PN: 'CITIZEN^Jan'
620 
621         Parameters
622         ----------
623         key
624             The key for the attribute to be deleted. If a ``slice`` is used
625             then the tags matching the slice conditions will be deleted.
626         """"""
627         # If passed a slice, delete the corresponding DataElements
628         if isinstance(key, slice):
629             for tag in self._slice_dataset(key.start, key.stop, key.step):
630                 del self._dict[tag]
631                 # invalidate private blocks in case a private creator is
632                 # deleted - will be re-created on next access
633                 if self._private_blocks and BaseTag(tag).is_private_creator:
634                     self._private_blocks = {}
635         elif isinstance(key, BaseTag):
636             del self._dict[key]
637             if self._private_blocks and key.is_private_creator:
638                 self._private_blocks = {}
639         else:
640             # If not a standard tag, than convert to Tag and try again
641             tag = Tag(key)
642             del self._dict[tag]
643             if self._private_blocks and tag.is_private_creator:
644                 self._private_blocks = {}
645 
646     def __dir__(self) -> List[str]:
647         """"""Return a list of methods, properties, attributes and element
648         keywords available in the :class:`Dataset`.
649 
650         List of attributes is used, for example, in auto-completion in editors
651         or command-line environments.
652         """"""
653         names = set(super().__dir__())
654         keywords = set(self.dir())
655 
656         return sorted(names | keywords)
657 
658     def dir(self, *filters: str) -> List[str]:
659         """"""Return an alphabetical list of element keywords in the
660         :class:`Dataset`.
661 
662         Intended mainly for use in interactive Python sessions. Only lists the
663         element keywords in the current level of the :class:`Dataset` (i.e.
664         the contents of any sequence elements are ignored).
665 
666         Parameters
667         ----------
668         filters : str
669             Zero or more string arguments to the function. Used for
670             case-insensitive match to any part of the DICOM keyword.
671 
672         Returns
673         -------
674         list of str
675             The matching element keywords in the dataset. If no
676             filters are used then all element keywords are returned.
677         """"""
678         allnames = [keyword_for_tag(tag) for tag in self._dict.keys()]
679         # remove blanks - tags without valid names (e.g. private tags)
680         allnames = [x for x in allnames if x]
681         # Store found names in a dict, so duplicate names appear only once
682         matches = {}
683         for filter_ in filters:
684             filter_ = filter_.lower()
685             match = [x for x in allnames if x.lower().find(filter_) != -1]
686             matches.update({x: 1 for x in match})
687 
688         if filters:
689             return sorted(matches.keys())
690 
691         return sorted(allnames)
692 
693     def __eq__(self, other: Any) -> bool:
694         """"""Compare `self` and `other` for equality.
695 
696         Returns
697         -------
698         bool
699             The result if `self` and `other` are the same class
700         NotImplemented
701             If `other` is not the same class as `self` then returning
702             :class:`NotImplemented` delegates the result to
703             ``superclass.__eq__(subclass)``.
704         """"""
705         # When comparing against self this will be faster
706         if other is self:
707             return True
708 
709         if isinstance(other, self.__class__):
710             return _dict_equal(self, other)
711 
712         return NotImplemented
713 
714     @overload
715     def get(self, key: str, default: Optional[Any] = None) -> Any:
716         pass  # pragma: no cover
717 
718     @overload
719     def get(
720         self,
721         key: Union[int, Tuple[int, int], BaseTag],
722         default: Optional[Any] = None
723     ) -> DataElement:
724         pass  # pragma: no cover
725 
726     def get(
727         self,
728         key: Union[str, Union[int, Tuple[int, int], BaseTag]],
729         default: Optional[Any] = None
730     ) -> Union[Any, DataElement]:
731         """"""Simulate ``dict.get()`` to handle element tags and keywords.
732 
733         Parameters
734         ----------
735         key : str or int or Tuple[int, int] or BaseTag
736             The element keyword or tag or the class attribute name to get.
737         default : obj or None, optional
738             If the element or class attribute is not present, return
739             `default` (default ``None``).
740 
741         Returns
742         -------
743         value
744             If `key` is the keyword for an element in the :class:`Dataset`
745             then return the element's value.
746         dataelem.DataElement
747             If `key` is a tag for a element in the :class:`Dataset` then
748             return the :class:`~pydicom.dataelem.DataElement`
749             instance.
750         value
751             If `key` is a class attribute then return its value.
752         """"""
753         if isinstance(key, str):
754             try:
755                 return getattr(self, key)
756             except AttributeError:
757                 return default
758 
759         # is not a string, try to make it into a tag and then hand it
760         # off to the underlying dict
761         try:
762             key = Tag(key)
763         except Exception as exc:
764             raise TypeError(""Dataset.get key must be a string or tag"") from exc
765 
766         try:
767             return self.__getitem__(key)
768         except KeyError:
769             return default
770 
771     def items(self) -> AbstractSet[Tuple[BaseTag, _DatasetValue]]:
772         """"""Return the :class:`Dataset` items to simulate :meth:`dict.items`.
773 
774         Returns
775         -------
776         dict_items
777             The top-level (:class:`~pydicom.tag.BaseTag`,
778             :class:`~pydicom.dataelem.DataElement`) items for the
779             :class:`Dataset`.
780         """"""
781         return self._dict.items()
782 
783     def keys(self) -> AbstractSet[BaseTag]:
784         """"""Return the :class:`Dataset` keys to simulate :meth:`dict.keys`.
785 
786         Returns
787         -------
788         dict_keys
789             The :class:`~pydicom.tag.BaseTag` of all the elements in
790             the :class:`Dataset`.
791         """"""
792         return self._dict.keys()
793 
794     def values(self) -> ValuesView[_DatasetValue]:
795         """"""Return the :class:`Dataset` values to simulate :meth:`dict.values`.
796 
797         Returns
798         -------
799         dict_keys
800             The :class:`DataElements<pydicom.dataelem.DataElement>` that make
801             up the values of the :class:`Dataset`.
802         """"""
803         return self._dict.values()
804 
805     def __getattr__(self, name: str) -> Any:
806         """"""Intercept requests for :class:`Dataset` attribute names.
807 
808         If `name` matches a DICOM keyword, return the value for the
809         element with the corresponding tag.
810 
811         Parameters
812         ----------
813         name : str
814             An element keyword or a class attribute name.
815 
816         Returns
817         -------
818         value
819               If `name` matches a DICOM keyword, returns the corresponding
820               element's value. Otherwise returns the class attribute's
821               value (if present).
822         """"""
823         tag = tag_for_keyword(name)
824         if tag is not None:  # `name` isn't a DICOM element keyword
825             tag = Tag(tag)
826             if tag in self._dict:  # DICOM DataElement not in the Dataset
827                 return self[tag].value
828 
829         # no tag or tag not contained in the dataset
830         if name == '_dict':
831             # special handling for contained dict, needed for pickle
832             return {}
833         # Try the base class attribute getter (fix for issue 332)
834         return object.__getattribute__(self, name)
835 
836     @property
837     def _character_set(self) -> List[str]:
838         """"""The character set used to encode text values.""""""
839         char_set = self.get(BaseTag(0x00080005), None)
840         if not char_set:
841             return self._parent_encoding
842 
843         return convert_encodings(char_set.value)
844 
845     @overload
846     def __getitem__(self, key: slice) -> ""Dataset"":
847         pass  # pragma: no cover
848 
849     @overload
850     def __getitem__(self, key: TagType) -> DataElement:
851         pass  # pragma: no cover
852 
853     def __getitem__(
854         self, key: Union[slice, TagType]
855     ) -> Union[""Dataset"", DataElement]:
856         """"""Operator for ``Dataset[key]`` request.
857 
858         Any deferred data elements will be read in and an attempt will be made
859         to correct any elements with ambiguous VRs.
860 
861         Examples
862         --------
863         Indexing using :class:`~pydicom.dataelem.DataElement` tag
864 
865         >>> ds = Dataset()
866         >>> ds.SOPInstanceUID = '1.2.3'
867         >>> ds.PatientName = 'CITIZEN^Jan'
868         >>> ds.PatientID = '12345'
869         >>> ds[0x00100010].value
870         'CITIZEN^Jan'
871 
872         Slicing using element tags; all group ``0x0010`` elements in
873         the  dataset
874 
875         >>> ds[0x00100000:0x00110000]
876         (0010, 0010) Patient's Name                      PN: 'CITIZEN^Jan'
877         (0010, 0020) Patient ID                          LO: '12345'
878 
879         All group ``0x0002`` elements in the dataset
880 
881         >>> ds[(0x0002, 0x0000):(0x0003, 0x0000)]
882         <BLANKLINE>
883 
884         Parameters
885         ----------
886         key
887             The DICOM (group, element) tag in any form accepted by
888             :func:`~pydicom.tag.Tag` such as ``[0x0010, 0x0010]``,
889             ``(0x10, 0x10)``, ``0x00100010``, etc. May also be a :class:`slice`
890             made up of DICOM tags.
891 
892         Returns
893         -------
894         dataelem.DataElement or Dataset
895             If a single DICOM element tag is used then returns the
896             corresponding :class:`~pydicom.dataelem.DataElement`.
897             If a :class:`slice` is used then returns a :class:`Dataset` object
898             containing the corresponding
899             :class:`DataElements<pydicom.dataelem.DataElement>`.
900         """"""
901         # If passed a slice, return a Dataset containing the corresponding
902         #   DataElements
903         if isinstance(key, slice):
904             return self._dataset_slice(key)
905 
906         if isinstance(key, BaseTag):
907             tag = key
908         else:
909             try:
910                 tag = Tag(key)
911             except Exception as exc:
912                 raise KeyError(f""'{key}'"") from exc
913 
914         elem = self._dict[tag]
915         if isinstance(elem, DataElement):
916             if elem.VR == VR_.SQ and elem.value:
917                 # let a sequence know its parent dataset, as sequence items
918                 # may need parent dataset tags to resolve ambiguous tags
919                 elem.value.parent = self
920             return elem
921 
922         if isinstance(elem, RawDataElement):
923             # If a deferred read, then go get the value now
924             if elem.value is None and elem.length != 0:
925                 from pydicom.filereader import read_deferred_data_element
926 
927                 elem = read_deferred_data_element(
928                     self.fileobj_type,
929                     self.filename,
930                     self.timestamp,
931                     elem
932                 )
933 
934             if tag != BaseTag(0x00080005):
935                 character_set = self.read_encoding or self._character_set
936             else:
937                 character_set = default_encoding
938             # Not converted from raw form read from file yet; do so now
939             self[tag] = DataElement_from_raw(elem, character_set, self)
940 
941             # If the Element has an ambiguous VR, try to correct it
942             if self[tag].VR in AMBIGUOUS_VR:
943                 from pydicom.filewriter import correct_ambiguous_vr_element
944                 self[tag] = correct_ambiguous_vr_element(
945                     self[tag], self, elem[6]
946                 )
947 
948         return cast(DataElement, self._dict.get(tag))
949 
950     def private_block(
951         self, group: int, private_creator: str, create: bool = False
952     ) -> PrivateBlock:
953         """"""Return the block for the given tag `group` and `private_creator`.
954 
955         .. versionadded:: 1.3
956 
957         If `create` is ``True`` and the `private_creator` does not exist,
958         the private creator tag is added.
959 
960         Notes
961         -----
962         We ignore the unrealistic case that no free block is available.
963 
964         Parameters
965         ----------
966         group : int
967             The group of the private tag to be found as a 32-bit :class:`int`.
968             Must be an odd number (e.g. a private group).
969         private_creator : str
970             The private creator string associated with the tag.
971         create : bool, optional
972             If ``True`` and `private_creator` does not exist, a new private
973             creator tag is added at the next free block. If ``False``
974             (the default) and `private_creator` does not exist,
975             :class:`KeyError` is raised instead.
976 
977         Returns
978         -------
979         PrivateBlock
980             The existing or newly created private block.
981 
982         Raises
983         ------
984         ValueError
985             If `group` doesn't belong to a private tag or `private_creator`
986             is empty.
987         KeyError
988             If the private creator tag is not found in the given group and
989             the `create` parameter is ``False``.
990         """"""
991         def new_block(element: int) -> PrivateBlock:
992             block = PrivateBlock(key, self, element)
993             self._private_blocks[key] = block
994             return block
995 
996         key = (group, private_creator)
997         if key in self._private_blocks:
998             return self._private_blocks[key]
999 
1000         if not private_creator:
1001             raise ValueError('Private creator must have a value')
1002 
1003         if group % 2 == 0:
1004             raise ValueError(
1005                 'Tag must be private if private creator is given')
1006 
1007         # find block with matching private creator
1008         block = self[(group, 0x10):(group, 0x100)]  # type: ignore[misc]
1009         data_el = next(
1010             (
1011                 elem for elem in block if elem.value == private_creator
1012             ),
1013             None
1014         )
1015         if data_el is not None:
1016             return new_block(data_el.tag.element)
1017 
1018         if not create:
1019             # not found and shall not be created - raise
1020             raise KeyError(
1021                 ""Private creator '{}' not found"".format(private_creator))
1022 
1023         # private creator not existing - find first unused private block
1024         # and add the private creator
1025         first_free_el = next(
1026             el for el in range(0x10, 0x100)
1027             if Tag(group, el) not in self._dict
1028         )
1029         self.add_new(Tag(group, first_free_el), 'LO', private_creator)
1030         return new_block(first_free_el)
1031 
1032     def private_creators(self, group: int) -> List[str]:
1033         """"""Return a list of private creator names in the given group.
1034 
1035         .. versionadded:: 1.3
1036 
1037         Examples
1038         --------
1039         This can be used to check if a given private creator exists in
1040         the group of the dataset:
1041 
1042         >>> ds = Dataset()
1043         >>> if 'My Creator' in ds.private_creators(0x0041):
1044         ...     block = ds.private_block(0x0041, 'My Creator')
1045 
1046         Parameters
1047         ----------
1048         group : int
1049             The private group as a 32-bit :class:`int`. Must be an odd number.
1050 
1051         Returns
1052         -------
1053         list of str
1054             All private creator names for private blocks in the group.
1055 
1056         Raises
1057         ------
1058         ValueError
1059             If `group` is not a private group.
1060         """"""
1061         if group % 2 == 0:
1062             raise ValueError('Group must be an odd number')
1063 
1064         block = self[(group, 0x10):(group, 0x100)]  # type: ignore[misc]
1065         return [x.value for x in block]
1066 
1067     def get_private_item(
1068         self, group: int, element_offset: int, private_creator: str
1069     ) -> DataElement:
1070         """"""Return the data element for the given private tag `group`.
1071 
1072         .. versionadded:: 1.3
1073 
1074         This is analogous to ``Dataset.__getitem__()``, but only for private
1075         tags. This allows to find the private tag for the correct private
1076         creator without the need to add the tag to the private dictionary
1077         first.
1078 
1079         Parameters
1080         ----------
1081         group : int
1082             The private tag group where the item is located as a 32-bit int.
1083         element_offset : int
1084             The lower 16 bits (e.g. 2 hex numbers) of the element tag.
1085         private_creator : str
1086             The private creator for the tag. Must match the private creator
1087             for the tag to be returned.
1088 
1089         Returns
1090         -------
1091         dataelem.DataElement
1092             The corresponding element.
1093 
1094         Raises
1095         ------
1096         ValueError
1097             If `group` is not part of a private tag or `private_creator` is
1098             empty.
1099         KeyError
1100             If the private creator tag is not found in the given group.
1101             If the private tag is not found.
1102         """"""
1103         block = self.private_block(group, private_creator)
1104         return self.__getitem__(block.get_tag(element_offset))
1105 
1106     @overload
1107     def get_item(self, key: slice) -> ""Dataset"":
1108         pass  # pragma: no cover
1109 
1110     @overload
1111     def get_item(self, key: TagType) -> DataElement:
1112         pass  # pragma: no cover
1113 
1114     def get_item(
1115         self, key: Union[slice, TagType]
1116     ) -> Union[""Dataset"", DataElement, RawDataElement, None]:
1117         """"""Return the raw data element if possible.
1118 
1119         It will be raw if the user has never accessed the value, or set their
1120         own value. Note if the data element is a deferred-read element,
1121         then it is read and converted before being returned.
1122 
1123         Parameters
1124         ----------
1125         key
1126             The DICOM (group, element) tag in any form accepted by
1127             :func:`~pydicom.tag.Tag` such as ``[0x0010, 0x0010]``,
1128             ``(0x10, 0x10)``, ``0x00100010``, etc. May also be a :class:`slice`
1129             made up of DICOM tags.
1130 
1131         Returns
1132         -------
1133         dataelem.DataElement
1134             The corresponding element.
1135         """"""
1136         if isinstance(key, slice):
1137             return self._dataset_slice(key)
1138 
1139         elem = self._dict.get(Tag(key))
1140         # If a deferred read, return using __getitem__ to read and convert it
1141         if isinstance(elem, RawDataElement) and elem.value is None:
1142             return self[key]
1143 
1144         return elem
1145 
1146     def _dataset_slice(self, slce: slice) -> ""Dataset"":
1147         """"""Return a slice that has the same properties as the original dataset.
1148 
1149         That includes properties related to endianness and VR handling,
1150         and the specific character set. No element conversion is done, e.g.
1151         elements of type ``RawDataElement`` are kept.
1152         """"""
1153         tags = self._slice_dataset(slce.start, slce.stop, slce.step)
1154         ds = Dataset({tag: self.get_item(tag) for tag in tags})
1155         ds.is_little_endian = self.is_little_endian
1156         ds.is_implicit_VR = self.is_implicit_VR
1157         ds.set_original_encoding(
1158             self.read_implicit_vr, self.read_little_endian, self.read_encoding
1159         )
1160         return ds
1161 
1162     @property
1163     def is_original_encoding(self) -> bool:
1164         """"""Return ``True`` if the encoding to be used for writing is set and
1165         is the same as that used to originally encode the  :class:`Dataset`.
1166 
1167         .. versionadded:: 1.1
1168 
1169         This includes properties related to endianness, VR handling and the
1170         (0008,0005) *Specific Character Set*.
1171         """"""
1172         return (
1173             self.is_implicit_VR is not None
1174             and self.is_little_endian is not None
1175             and self.read_implicit_vr == self.is_implicit_VR
1176             and self.read_little_endian == self.is_little_endian
1177             and self.read_encoding == self._character_set
1178         )
1179 
1180     def set_original_encoding(
1181         self,
1182         is_implicit_vr: Optional[bool],
1183         is_little_endian: Optional[bool],
1184         character_encoding: Union[None, str, MutableSequence[str]]
1185     ) -> None:
1186         """"""Set the values for the original transfer syntax and encoding.
1187 
1188         .. versionadded:: 1.2
1189 
1190         Can be used for a :class:`Dataset` with raw data elements to enable
1191         optimized writing (e.g. without decoding the data elements).
1192         """"""
1193         self.read_implicit_vr = is_implicit_vr
1194         self.read_little_endian = is_little_endian
1195         self.read_encoding = character_encoding
1196 
1197     def group_dataset(self, group: int) -> ""Dataset"":
1198         """"""Return a :class:`Dataset` containing only elements of a certain
1199         group.
1200 
1201         Parameters
1202         ----------
1203         group : int
1204             The group part of a DICOM (group, element) tag.
1205 
1206         Returns
1207         -------
1208         Dataset
1209             A :class:`Dataset` containing elements of the group specified.
1210         """"""
1211         return self[(group, 0x0000):(group + 1, 0x0000)]  # type: ignore[misc]
1212 
1213     def __iter__(self) -> Iterator[DataElement]:
1214         """"""Iterate through the top-level of the Dataset, yielding DataElements.
1215 
1216         Examples
1217         --------
1218 
1219         >>> ds = Dataset()
1220         >>> for elem in ds:
1221         ...     print(elem)
1222 
1223         The :class:`DataElements<pydicom.dataelem.DataElement>` are returned in
1224         increasing tag value order. Sequence items are returned as a single
1225         :class:`~pydicom.dataelem.DataElement`, so it is up
1226         to the calling code to recurse into the Sequence items if desired.
1227 
1228         Yields
1229         ------
1230         dataelem.DataElement
1231             The :class:`Dataset`'s
1232             :class:`DataElements<pydicom.dataelem.DataElement>`, sorted by
1233             increasing tag order.
1234         """"""
1235         # Note this is different than the underlying dict class,
1236         #        which returns the key of the key:value mapping.
1237         #   Here the value is returned (but data_element.tag has the key)
1238         taglist = sorted(self._dict.keys())
1239         for tag in taglist:
1240             yield self[tag]
1241 
1242     def elements(self) -> Iterator[DataElement]:
1243         """"""Yield the top-level elements of the :class:`Dataset`.
1244 
1245         .. versionadded:: 1.1
1246 
1247         Examples
1248         --------
1249 
1250         >>> ds = Dataset()
1251         >>> for elem in ds.elements():
1252         ...     print(elem)
1253 
1254         The elements are returned in the same way as in
1255         ``Dataset.__getitem__()``.
1256 
1257         Yields
1258         ------
1259         dataelem.DataElement or dataelem.RawDataElement
1260             The unconverted elements sorted by increasing tag order.
1261         """"""
1262         taglist = sorted(self._dict.keys())
1263         for tag in taglist:
1264             yield self.get_item(tag)
1265 
1266     def __len__(self) -> int:
1267         """"""Return the number of elements in the top level of the dataset.""""""
1268         return len(self._dict)
1269 
1270     def __ne__(self, other: Any) -> bool:
1271         """"""Compare `self` and `other` for inequality.""""""
1272         return not self == other
1273 
1274     def clear(self) -> None:
1275         """"""Delete all the elements from the :class:`Dataset`.""""""
1276         self._dict.clear()
1277 
1278     def pop(self, key: Union[BaseTag, TagType], *args: Any) -> _DatasetValue:
1279         """"""Emulate :meth:`dict.pop` with support for tags and keywords.
1280 
1281         Removes the element for `key` if it exists and returns it,
1282         otherwise returns a default value if given or raises :class:`KeyError`.
1283 
1284         Parameters
1285         ----------
1286         key : int or str or 2-tuple
1287 
1288             * If :class:`tuple` - the group and element number of the DICOM tag
1289             * If :class:`int` - the combined group/element number
1290             * If :class:`str` - the DICOM keyword of the tag
1291 
1292         *args : zero or one argument
1293             Defines the behavior if no tag exists for `key`: if given,
1294             it defines the return value, if not given, :class:`KeyError` is
1295             raised
1296 
1297         Returns
1298         -------
1299         RawDataElement or DataElement
1300             The element for `key` if it exists, or the default value if given.
1301 
1302         Raises
1303         ------
1304         KeyError
1305             If the `key` is not a valid tag or keyword.
1306             If the tag does not exist and no default is given.
1307         """"""
1308         try:
1309             key = Tag(key)
1310         except Exception:
1311             pass
1312 
1313         return self._dict.pop(cast(BaseTag, key), *args)
1314 
1315     def popitem(self) -> Tuple[BaseTag, _DatasetValue]:
1316         """"""Emulate :meth:`dict.popitem`.
1317 
1318         Returns
1319         -------
1320         tuple of (BaseTag, DataElement)
1321         """"""
1322         return self._dict.popitem()
1323 
1324     def setdefault(
1325         self, key: TagType, default: Optional[Any] = None
1326     ) -> DataElement:
1327         """"""Emulate :meth:`dict.setdefault` with support for tags and keywords.
1328 
1329         Examples
1330         --------
1331 
1332         >>> ds = Dataset()
1333         >>> elem = ds.setdefault((0x0010, 0x0010), ""Test"")
1334         >>> elem
1335         (0010, 0010) Patient's Name                      PN: 'Test'
1336         >>> elem.value
1337         'Test'
1338         >>> elem = ds.setdefault('PatientSex',
1339         ...     DataElement(0x00100040, 'CS', 'F'))
1340         >>> elem.value
1341         'F'
1342 
1343         Parameters
1344         ----------
1345         key : int, str or 2-tuple of int
1346 
1347             * If :class:`tuple` - the group and element number of the DICOM tag
1348             * If :class:`int` - the combined group/element number
1349             * If :class:`str` - the DICOM keyword of the tag
1350         default : pydicom.dataelem.DataElement or object, optional
1351             The :class:`~pydicom.dataelem.DataElement` to use with `key`, or
1352             the value of the :class:`~pydicom.dataelem.DataElement` to use with
1353             `key` (default ``None``).
1354 
1355         Returns
1356         -------
1357         pydicom.dataelem.DataElement or object
1358             The :class:`~pydicom.dataelem.DataElement` for `key`.
1359 
1360         Raises
1361         ------
1362         ValueError
1363             If `key` is not convertible to a valid tag or a known element
1364             keyword.
1365         KeyError
1366             If :attr:`~pydicom.config.settings.reading_validation_mode` is
1367              ``RAISE`` and `key` is an unknown non-private tag.
1368         """"""
1369         tag = Tag(key)
1370         if tag in self:
1371             return self[tag]
1372 
1373         vr: Union[str, VR_]
1374         if not isinstance(default, DataElement):
1375             if tag.is_private:
1376                 vr = VR_.UN
1377             else:
1378                 try:
1379                     vr = dictionary_VR(tag)
1380                 except KeyError:
1381                     if (config.settings.writing_validation_mode ==
1382                             config.RAISE):
1383                         raise KeyError(f""Unknown DICOM tag {tag}"")
1384 
1385                     vr = VR_.UN
1386                     warnings.warn(
1387                         f""Unknown DICOM tag {tag} - setting VR to 'UN'""
1388                     )
1389 
1390             default = DataElement(tag, vr, default)
1391 
1392         self[key] = default
1393 
1394         return default
1395 
1396     def convert_pixel_data(self, handler_name: str = '') -> None:
1397         """"""Convert pixel data to a :class:`numpy.ndarray` internally.
1398 
1399         Parameters
1400         ----------
1401         handler_name : str, optional
1402             The name of the pixel handler that shall be used to
1403             decode the data. Supported names are: ``'gdcm'``,
1404             ``'pillow'``, ``'jpeg_ls'``, ``'rle'``, ``'numpy'`` and
1405             ``'pylibjpeg'``. If not used (the default), a matching handler is
1406             used from the handlers configured in
1407             :attr:`~pydicom.config.pixel_data_handlers`.
1408 
1409         Returns
1410         -------
1411         None
1412             Converted pixel data is stored internally in the dataset.
1413 
1414         Raises
1415         ------
1416         ValueError
1417             If `handler_name` is not a valid handler name.
1418         NotImplementedError
1419             If the given handler or any handler, if none given, is unable to
1420             decompress pixel data with the current transfer syntax
1421         RuntimeError
1422             If the given handler, or the handler that has been selected if
1423             none given, is not available.
1424 
1425         Notes
1426         -----
1427         If the pixel data is in a compressed image format, the data is
1428         decompressed and any related data elements are changed accordingly.
1429         """"""
1430         # Check if already have converted to a NumPy array
1431         # Also check if pixel data has changed. If so, get new NumPy array
1432         already_have = True
1433         if not hasattr(self, ""_pixel_array""):
1434             already_have = False
1435         elif self._pixel_id != get_image_pixel_ids(self):
1436             already_have = False
1437 
1438         if already_have:
1439             return
1440 
1441         if handler_name:
1442             self._convert_pixel_data_using_handler(handler_name)
1443         else:
1444             self._convert_pixel_data_without_handler()
1445 
1446     def _convert_pixel_data_using_handler(self, name: str) -> None:
1447         """"""Convert the pixel data using handler with the given name.
1448         See :meth:`~Dataset.convert_pixel_data` for more information.
1449         """"""
1450         # handle some variations in name
1451         handler_name = name.lower()
1452         if not handler_name.endswith('_handler'):
1453             handler_name += '_handler'
1454         if handler_name == 'numpy_handler':
1455             handler_name = 'np_handler'
1456         if handler_name == 'jpeg_ls_handler':
1457             # the name in config differs from the actual handler name
1458             # we allow both
1459             handler_name = 'jpegls_handler'
1460         if not hasattr(pydicom.config, handler_name):
1461             raise ValueError(f""'{name}' is not a known handler name"")
1462 
1463         handler = getattr(pydicom.config, handler_name)
1464 
1465         tsyntax = self.file_meta.TransferSyntaxUID
1466         if not handler.supports_transfer_syntax(tsyntax):
1467             raise NotImplementedError(
1468                 ""Unable to decode pixel data with a transfer syntax UID""
1469                 f"" of '{tsyntax}' ({tsyntax.name}) using the pixel data ""
1470                 f""handler '{name}'. Please see the pydicom documentation for ""
1471                 ""information on supported transfer syntaxes.""
1472             )
1473         if not handler.is_available():
1474             raise RuntimeError(
1475                 f""The pixel data handler '{name}' is not available on your ""
1476                 ""system. Please refer to the pydicom documentation for ""
1477                 ""information on installing needed packages.""
1478             )
1479         # if the conversion fails, the exception is propagated up
1480         self._do_pixel_data_conversion(handler)
1481 
1482     def _convert_pixel_data_without_handler(self) -> None:
1483         """"""Convert the pixel data using the first matching handler.
1484         See :meth:`~Dataset.convert_pixel_data` for more information.
1485         """"""
1486         # Find all possible handlers that support the transfer syntax
1487         ts = self.file_meta.TransferSyntaxUID
1488         possible_handlers = [
1489             hh for hh in pydicom.config.pixel_data_handlers
1490             if hh is not None
1491             and hh.supports_transfer_syntax(ts)
1492         ]
1493 
1494         # No handlers support the transfer syntax
1495         if not possible_handlers:
1496             raise NotImplementedError(
1497                 ""Unable to decode pixel data with a transfer syntax UID of ""
1498                 f""'{ts}' ({ts.name}) as there are no pixel data ""
1499                 ""handlers available that support it. Please see the pydicom ""
1500                 ""documentation for information on supported transfer syntaxes ""
1501             )
1502 
1503         # Handlers that both support the transfer syntax and have their
1504         #   dependencies met
1505         available_handlers = [
1506             hh for hh in possible_handlers
1507             if hh.is_available()
1508         ]
1509 
1510         # There are handlers that support the transfer syntax but none of them
1511         #   can be used as missing dependencies
1512         if not available_handlers:
1513             # For each of the possible handlers we want to find which
1514             #   dependencies are missing
1515             msg = (
1516                 ""The following handlers are available to decode the pixel ""
1517                 ""data however they are missing required dependencies: ""
1518             )
1519             pkg_msg = []
1520             for hh in possible_handlers:
1521                 hh_deps = hh.DEPENDENCIES
1522                 # Missing packages
1523                 missing = [dd for dd in hh_deps if have_package(dd) is None]
1524                 # Package names
1525                 names = [hh_deps[name][1] for name in missing]
1526                 pkg_msg.append(
1527                     f""{hh.HANDLER_NAME} ""
1528                     f""(req. {', '.join(names)})""
1529                 )
1530 
1531             raise RuntimeError(msg + ', '.join(pkg_msg))
1532 
1533         last_exception = None
1534         for handler in available_handlers:
1535             try:
1536                 self._do_pixel_data_conversion(handler)
1537                 return
1538             except Exception as exc:
1539                 logger.debug(
1540                     ""Exception raised by pixel data handler"", exc_info=exc
1541                 )
1542                 last_exception = exc
1543 
1544         # The only way to get to this point is if we failed to get the pixel
1545         #   array because all suitable handlers raised exceptions
1546         self._pixel_array = None
1547         self._pixel_id = {}
1548 
1549         logger.info(
1550             ""Unable to decode the pixel data using the following handlers: {}.""
1551             ""Please see the list of supported Transfer Syntaxes in the ""
1552             ""pydicom documentation for alternative packages that might ""
1553             ""be able to decode the data""
1554             .format("", "".join([str(hh) for hh in available_handlers]))
1555         )
1556         raise last_exception  # type: ignore[misc]
1557 
1558     def _do_pixel_data_conversion(self, handler: Any) -> None:
1559         """"""Do the actual data conversion using the given handler.""""""
1560 
1561         # Use the handler to get a 1D numpy array of the pixel data
1562         # Will raise an exception if no pixel data element
1563         arr = handler.get_pixeldata(self)
1564         self._pixel_array = reshape_pixel_array(self, arr)
1565 
1566         # Some handler/transfer syntax combinations may need to
1567         #   convert the color space from YCbCr to RGB
1568         if handler.needs_to_convert_to_RGB(self):
1569             self._pixel_array = convert_color_space(
1570                 self._pixel_array, 'YBR_FULL', 'RGB'
1571             )
1572 
1573         self._pixel_id = get_image_pixel_ids(self)
1574 
1575     def compress(
1576         self,
1577         transfer_syntax_uid: str,
1578         arr: Optional[""numpy.ndarray""] = None,
1579         encoding_plugin: str = '',
1580         decoding_plugin: str = '',
1581         encapsulate_ext: bool = False,
1582         **kwargs: Any,
1583     ) -> None:
1584         """"""Compress and update an uncompressed dataset in-place with the
1585         resulting :dcm:`encapsulated<part05/sect_A.4.html>` pixel data.
1586 
1587         .. versionadded:: 2.2
1588 
1589         The dataset must already have the following
1590         :dcm:`Image Pixel<part03/sect_C.7.6.3.html>` module elements present
1591         with correct values that correspond to the resulting compressed
1592         pixel data:
1593 
1594         * (0028,0002) *Samples per Pixel*
1595         * (0028,0004) *Photometric Interpretation*
1596         * (0028,0008) *Number of Frames* (if more than 1 frame will be present)
1597         * (0028,0010) *Rows*
1598         * (0028,0011) *Columns*
1599         * (0028,0100) *Bits Allocated*
1600         * (0028,0101) *Bits Stored*
1601         * (0028,0103) *Pixel Representation*
1602 
1603         This method will add the file meta dataset if none is present and add
1604         or modify the following elements:
1605 
1606         * (0002,0010) *Transfer Syntax UID*
1607         * (7FE0,0010) *Pixel Data*
1608 
1609         If *Samples per Pixel* is greater than 1 then the following element
1610         will also be added:
1611 
1612         * (0028,0006) *Planar Configuration*
1613 
1614         If the compressed pixel data is too large for encapsulation using a
1615         basic offset table then an :dcm:`extended offset table
1616         <part03/sect_C.7.6.3.html>` will also be used, in which case the
1617         following elements will also be added:
1618 
1619         * (7FE0,0001) *Extended Offset Table*
1620         * (7FE0,0002) *Extended Offset Table Lengths*
1621 
1622         **Supported Transfer Syntax UIDs**
1623 
1624         +----------------------+----------+----------------------------------+
1625         | UID                  | Plugins  | Encoding Guide                   |
1626         +======================+==========+==================================+
1627         | *RLE Lossless* -     |pydicom,  | :doc:`RLE Lossless               |
1628         | 1.2.840.10008.1.2.5  |pylibjpeg,| </guides/encoding/rle_lossless>` |
1629         |                      |gdcm      |                                  |
1630         +----------------------+----------+----------------------------------+
1631 
1632         Examples
1633         --------
1634 
1635         Compress the existing uncompressed *Pixel Data* in place:
1636 
1637         >>> from pydicom.data import get_testdata_file
1638         >>> from pydicom.uid import RLELossless
1639         >>> ds = get_testdata_file(""CT_small.dcm"", read=True)
1640         >>> ds.compress(RLELossless)
1641         >>> ds.save_as(""CT_small_rle.dcm"")
1642 
1643         Parameters
1644         ----------
1645         transfer_syntax_uid : pydicom.uid.UID
1646             The UID of the :dcm:`transfer syntax<part05/chapter_10.html>` to
1647             use when compressing the pixel data.
1648         arr : numpy.ndarray, optional
1649             Compress the uncompressed pixel data in `arr` and use it
1650             to set the *Pixel Data*. If `arr` is not used then the
1651             existing *Pixel Data* in the dataset will be compressed instead.
1652             The :attr:`~numpy.ndarray.shape`, :class:`~numpy.dtype` and
1653             contents of the array should match the dataset.
1654         encoding_plugin : str, optional
1655             Use the `encoding_plugin` to compress the pixel data. See the
1656             :doc:`user guide </old/image_data_compression>` for a list of
1657             plugins available for each UID and their dependencies. If not
1658             specified then all available plugins will be tried (default).
1659         decoding_plugin : str, optional
1660             Placeholder for future functionality.
1661         encapsulate_ext : bool, optional
1662             If ``True`` then force the addition of an extended offset table.
1663             If ``False`` (default) then an extended offset table
1664             will be added if needed for large amounts of compressed *Pixel
1665             Data*, otherwise just the basic offset table will be used.
1666         **kwargs
1667             Optional keyword parameters for the encoding plugin may also be
1668             present. See the :doc:`encoding plugins options
1669             </guides/encoding/encoder_plugin_options>` for more information.
1670         """"""
1671         from pydicom.encoders import get_encoder
1672 
1673         uid = UID(transfer_syntax_uid)
1674 
1675         # Raises NotImplementedError if `uid` is not supported
1676         encoder = get_encoder(uid)
1677         if not encoder.is_available:
1678             missing = ""\n"".join(
1679                 [f""    {s}"" for s in encoder.missing_dependencies]
1680             )
1681             raise RuntimeError(
1682                 f""The '{uid.name}' encoder is unavailable because its ""
1683                 f""encoding plugins are missing dependencies:\n""
1684                 f""{missing}""
1685             )
1686 
1687         if arr is None:
1688             # Encode the current *Pixel Data*
1689             frame_iterator = encoder.iter_encode(
1690                 self,
1691                 encoding_plugin=encoding_plugin,
1692                 decoding_plugin=decoding_plugin,
1693                 **kwargs
1694             )
1695         else:
1696             # Encode from an uncompressed pixel data array
1697             kwargs.update(encoder.kwargs_from_ds(self))
1698             frame_iterator = encoder.iter_encode(
1699                 arr,
1700                 encoding_plugin=encoding_plugin,
1701                 **kwargs
1702             )
1703 
1704         # Encode!
1705         encoded = [f for f in frame_iterator]
1706 
1707         # Encapsulate the encoded *Pixel Data*
1708         nr_frames = getattr(self, ""NumberOfFrames"", 1) or 1
1709         total = (nr_frames - 1) * 8 + sum([len(f) for f in encoded[:-1]])
1710         if encapsulate_ext or total > 2**32 - 1:
1711             (self.PixelData,
1712              self.ExtendedOffsetTable,
1713              self.ExtendedOffsetTableLengths) = encapsulate_extended(encoded)
1714         else:
1715             self.PixelData = encapsulate(encoded)
1716 
1717         # PS3.5 Annex A.4 - encapsulated pixel data uses undefined length
1718         self['PixelData'].is_undefined_length = True
1719 
1720         # PS3.5 Annex A.4 - encapsulated datasets use explicit VR little endian
1721         self.is_implicit_VR = False
1722         self.is_little_endian = True
1723 
1724         # Set the correct *Transfer Syntax UID*
1725         if not hasattr(self, 'file_meta'):
1726             self.file_meta = FileMetaDataset()
1727 
1728         self.file_meta.TransferSyntaxUID = uid
1729 
1730         # Add or update any other required elements
1731         if self.SamplesPerPixel > 1:
1732             self.PlanarConfiguration: int = 1 if uid == RLELossless else 0
1733 
1734     def decompress(self, handler_name: str = '') -> None:
1735         """"""Decompresses *Pixel Data* and modifies the :class:`Dataset`
1736         in-place.
1737 
1738         .. versionadded:: 1.4
1739 
1740             The `handler_name` keyword argument was added
1741 
1742         If not a compressed transfer syntax, then pixel data is converted
1743         to a :class:`numpy.ndarray` internally, but not returned.
1744 
1745         If compressed pixel data, then is decompressed using an image handler,
1746         and internal state is updated appropriately:
1747 
1748         - ``Dataset.file_meta.TransferSyntaxUID`` is updated to non-compressed
1749           form
1750         - :attr:`~pydicom.dataelem.DataElement.is_undefined_length`
1751           is ``False`` for the (7FE0,0010) *Pixel Data* element.
1752 
1753         .. versionchanged:: 1.4
1754 
1755             The `handler_name` keyword argument was added
1756 
1757         Parameters
1758         ----------
1759         handler_name : str, optional
1760             The name of the pixel handler that shall be used to
1761             decode the data. Supported names are: ``'gdcm'``,
1762             ``'pillow'``, ``'jpeg_ls'``, ``'rle'``, ``'numpy'`` and
1763             ``'pylibjpeg'``.
1764             If not used (the default), a matching handler is used from the
1765             handlers configured in :attr:`~pydicom.config.pixel_data_handlers`.
1766 
1767         Returns
1768         -------
1769         None
1770 
1771         Raises
1772         ------
1773         NotImplementedError
1774             If the pixel data was originally compressed but file is not
1775             *Explicit VR Little Endian* as required by the DICOM Standard.
1776         """"""
1777         self.convert_pixel_data(handler_name)
1778         self.is_decompressed = True
1779         # May have been undefined length pixel data, but won't be now
1780         if 'PixelData' in self:
1781             self[0x7fe00010].is_undefined_length = False
1782 
1783         # Make sure correct Transfer Syntax is set
1784         # According to the dicom standard PS3.5 section A.4,
1785         # all compressed files must have been explicit VR, little endian
1786         # First check if was a compressed file
1787         if (
1788             hasattr(self, 'file_meta')
1789             and self.file_meta.TransferSyntaxUID.is_compressed
1790         ):
1791             # Check that current file as read does match expected
1792             if not self.is_little_endian or self.is_implicit_VR:
1793                 msg = (""Current dataset does not match expected ExplicitVR ""
1794                        ""LittleEndian transfer syntax from a compressed ""
1795                        ""transfer syntax"")
1796                 raise NotImplementedError(msg)
1797 
1798             # All is as expected, updated the Transfer Syntax
1799             self.file_meta.TransferSyntaxUID = ExplicitVRLittleEndian
1800 
1801     def overlay_array(self, group: int) -> ""numpy.ndarray"":
1802         """"""Return the *Overlay Data* in `group` as a :class:`numpy.ndarray`.
1803 
1804         .. versionadded:: 1.4
1805 
1806         Parameters
1807         ----------
1808         group : int
1809             The group number of the overlay data.
1810 
1811         Returns
1812         -------
1813         numpy.ndarray
1814             The (`group`,3000) *Overlay Data* converted to a
1815             :class:`numpy.ndarray`.
1816         """"""
1817         if group < 0x6000 or group > 0x60FF:
1818             raise ValueError(
1819                 ""The group part of the 'Overlay Data' element tag must be ""
1820                 ""between 0x6000 and 0x60FF (inclusive)""
1821             )
1822 
1823         from pydicom.config import overlay_data_handlers
1824 
1825         available_handlers = [
1826             hh for hh in overlay_data_handlers
1827             if hh.is_available()
1828         ]
1829         if not available_handlers:
1830             # For each of the handlers we want to find which
1831             #   dependencies are missing
1832             msg = (
1833                 ""The following handlers are available to decode the overlay ""
1834                 ""data however they are missing required dependencies: ""
1835             )
1836             pkg_msg = []
1837             for hh in overlay_data_handlers:
1838                 hh_deps = hh.DEPENDENCIES
1839                 # Missing packages
1840                 missing = [dd for dd in hh_deps if have_package(dd) is None]
1841                 # Package names
1842                 names = [hh_deps[name][1] for name in missing]
1843                 pkg_msg.append(
1844                     f""{hh.HANDLER_NAME} ""
1845                     f""(req. {', '.join(names)})""
1846                 )
1847 
1848             raise RuntimeError(msg + ', '.join(pkg_msg))
1849 
1850         last_exception = None
1851         for handler in available_handlers:
1852             try:
1853                 # Use the handler to get an ndarray of the pixel data
1854                 func = handler.get_overlay_array
1855                 return cast(""numpy.ndarray"", func(self, group))
1856             except Exception as exc:
1857                 logger.debug(
1858                     ""Exception raised by overlay data handler"", exc_info=exc
1859                 )
1860                 last_exception = exc
1861 
1862         logger.info(
1863             ""Unable to decode the overlay data using the following handlers: ""
1864             ""{}. Please see the list of supported Transfer Syntaxes in the ""
1865             ""pydicom documentation for alternative packages that might ""
1866             ""be able to decode the data""
1867             .format("", "".join([str(hh) for hh in available_handlers]))
1868         )
1869 
1870         raise last_exception  # type: ignore[misc]
1871 
1872     @property
1873     def pixel_array(self) -> ""numpy.ndarray"":
1874         """"""Return the pixel data as a :class:`numpy.ndarray`.
1875 
1876         .. versionchanged:: 1.4
1877 
1878             Added support for *Float Pixel Data* and *Double Float Pixel Data*
1879 
1880         Returns
1881         -------
1882         numpy.ndarray
1883             The (7FE0,0008) *Float Pixel Data*, (7FE0,0009) *Double Float
1884             Pixel Data* or (7FE0,0010) *Pixel Data* converted to a
1885             :class:`numpy.ndarray`.
1886         """"""
1887         self.convert_pixel_data()
1888         return cast(""numpy.ndarray"", self._pixel_array)
1889 
1890     def waveform_array(self, index: int) -> ""numpy.ndarray"":
1891         """"""Return an :class:`~numpy.ndarray` for the multiplex group at
1892         `index` in the (5400,0100) *Waveform Sequence*.
1893 
1894         .. versionadded:: 2.1
1895 
1896         Parameters
1897         ----------
1898         index : int
1899             The index of the multiplex group to return the array for.
1900 
1901         Returns
1902         ------
1903         numpy.ndarray
1904             The *Waveform Data* for the multiplex group as an
1905             :class:`~numpy.ndarray` with shape (samples, channels). If
1906             (003A,0210) *Channel Sensitivity* is present
1907             then the values will be in the units specified by the (003A,0211)
1908             *Channel Sensitivity Units Sequence*.
1909 
1910         See Also
1911         --------
1912         :func:`~pydicom.waveforms.numpy_handler.generate_multiplex`
1913         :func:`~pydicom.waveforms.numpy_handler.multiplex_array`
1914         """"""
1915         if not wave_handler.is_available():
1916             raise RuntimeError(""The waveform data handler requires numpy"")
1917 
1918         return wave_handler.multiplex_array(self, index, as_raw=False)
1919 
1920     # Format strings spec'd according to python string formatting options
1921     #    See http://docs.python.org/library/stdtypes.html#string-formatting-operations # noqa
1922     default_element_format = ""%(tag)s %(name)-35.35s %(VR)s: %(repval)s""
1923     default_sequence_element_format = ""%(tag)s %(name)-35.35s %(VR)s: %(repval)s""  # noqa
1924 
1925     def formatted_lines(
1926         self,
1927         element_format: str = default_element_format,
1928         sequence_element_format: str = default_sequence_element_format,
1929         indent_format: Optional[str] = None
1930     ) -> Iterator[str]:
1931         """"""Iterate through the :class:`Dataset` yielding formatted :class:`str`
1932         for each element.
1933 
1934         Parameters
1935         ----------
1936         element_format : str
1937             The string format to use for non-sequence elements. Formatting uses
1938             the attributes of
1939             :class:`~pydicom.dataelem.DataElement`. Default is
1940             ``""%(tag)s %(name)-35.35s %(VR)s: %(repval)s""``.
1941         sequence_element_format : str
1942             The string format to use for sequence elements. Formatting uses
1943             the attributes of
1944             :class:`~pydicom.dataelem.DataElement`. Default is
1945             ``""%(tag)s %(name)-35.35s %(VR)s: %(repval)s""``
1946         indent_format : str or None
1947             Placeholder for future functionality.
1948 
1949         Yields
1950         ------
1951         str
1952             A string representation of an element.
1953         """"""
1954         exclusion = (
1955             'from_json', 'to_json', 'to_json_dict', 'clear', 'description',
1956             'validate',
1957         )
1958         for elem in self.iterall():
1959             # Get all the attributes possible for this data element (e.g.
1960             #   gets descriptive text name too)
1961             # This is the dictionary of names that can be used in the format
1962             #   string
1963             elem_dict = {
1964                 attr: (
1965                     getattr(elem, attr)() if callable(getattr(elem, attr))
1966                     else getattr(elem, attr)
1967                 )
1968                 for attr in dir(elem) if not attr.startswith(""_"")
1969                 and attr not in exclusion
1970             }
1971             if elem.VR == VR_.SQ:
1972                 yield sequence_element_format % elem_dict
1973             else:
1974                 yield element_format % elem_dict
1975 
1976     def _pretty_str(
1977         self, indent: int = 0, top_level_only: bool = False
1978     ) -> str:
1979         """"""Return a string of the DataElements in the Dataset, with indented
1980         levels.
1981 
1982         This private method is called by the ``__str__()`` method for handling
1983         print statements or ``str(dataset)``, and the ``__repr__()`` method.
1984         It is also used by ``top()``, therefore the `top_level_only` flag.
1985         This function recurses, with increasing indentation levels.
1986 
1987         ..versionchanged:: 2.0
1988 
1989             The file meta information is returned in its own section,
1990             if :data:`~pydicom.config.show_file_meta` is ``True`` (default)
1991 
1992         Parameters
1993         ----------
1994         indent : int, optional
1995             The indent level offset (default ``0``).
1996         top_level_only : bool, optional
1997             When True, only create a string for the top level elements, i.e.
1998             exclude elements within any Sequences (default ``False``).
1999 
2000         Returns
2001         -------
2002         str
2003             A string representation of the Dataset.
2004         """"""
2005         strings = []
2006         indent_str = self.indent_chars * indent
2007         nextindent_str = self.indent_chars * (indent + 1)
2008 
2009         # Display file meta, if configured to do so, and have a non-empty one
2010         if (
2011             hasattr(self, ""file_meta"") and self.file_meta
2012             and pydicom.config.show_file_meta
2013         ):
2014             strings.append(f""{'Dataset.file_meta ':-<49}"")
2015             for elem in self.file_meta:
2016                 with tag_in_exception(elem.tag):
2017                     strings.append(indent_str + repr(elem))
2018             strings.append(f""{'':-<49}"")
2019 
2020         for elem in self:
2021             with tag_in_exception(elem.tag):
2022                 if elem.VR == VR_.SQ:  # a sequence
2023                     strings.append(
2024                         f""{indent_str}{str(elem.tag)}  {elem.name}  ""
2025                         f""{len(elem.value)} item(s) ---- ""
2026                     )
2027                     if not top_level_only:
2028                         for dataset in elem.value:
2029                             strings.append(dataset._pretty_str(indent + 1))
2030                             strings.append(nextindent_str + ""---------"")
2031                 else:
2032                     strings.append(indent_str + repr(elem))
2033         return ""\n"".join(strings)
2034 
2035     def remove_private_tags(self) -> None:
2036         """"""Remove all private elements from the :class:`Dataset`.""""""
2037 
2038         def remove_callback(dataset: ""Dataset"", elem: DataElement) -> None:
2039             """"""Internal method to use as callback to walk() method.""""""
2040             if elem.tag.is_private:
2041                 # can't del self[tag] - won't be right dataset on recursion
2042                 del dataset[elem.tag]
2043 
2044         self.walk(remove_callback)
2045 
2046     def save_as(
2047         self,
2048         filename: Union[str, ""os.PathLike[AnyStr]"", BinaryIO],
2049         write_like_original: bool = True
2050     ) -> None:
2051         """"""Write the :class:`Dataset` to `filename`.
2052 
2053         Wrapper for pydicom.filewriter.dcmwrite, passing this dataset to it.
2054         See documentation for that function for details.
2055 
2056         See Also
2057         --------
2058         pydicom.filewriter.dcmwrite
2059             Write a DICOM file from a :class:`FileDataset` instance.
2060         """"""
2061         pydicom.dcmwrite(filename, self, write_like_original)
2062 
2063     def ensure_file_meta(self) -> None:
2064         """"""Create an empty ``Dataset.file_meta`` if none exists.
2065 
2066         .. versionadded:: 1.2
2067         """"""
2068         # Changed in v2.0 so does not re-assign self.file_meta with getattr()
2069         if not hasattr(self, ""file_meta""):
2070             self.file_meta = FileMetaDataset()
2071 
2072     def fix_meta_info(self, enforce_standard: bool = True) -> None:
2073         """"""Ensure the file meta info exists and has the correct values
2074         for transfer syntax and media storage UIDs.
2075 
2076         .. versionadded:: 1.2
2077 
2078         .. warning::
2079 
2080             The transfer syntax for ``is_implicit_VR = False`` and
2081             ``is_little_endian = True`` is ambiguous and will therefore not
2082             be set.
2083 
2084         Parameters
2085         ----------
2086         enforce_standard : bool, optional
2087             If ``True``, a check for incorrect and missing elements is
2088             performed (see :func:`~validate_file_meta`).
2089         """"""
2090         self.ensure_file_meta()
2091 
2092         if self.is_little_endian and self.is_implicit_VR:
2093             self.file_meta.TransferSyntaxUID = ImplicitVRLittleEndian
2094         elif not self.is_little_endian and not self.is_implicit_VR:
2095             self.file_meta.TransferSyntaxUID = ExplicitVRBigEndian
2096         elif not self.is_little_endian and self.is_implicit_VR:
2097             raise NotImplementedError(""Implicit VR Big Endian is not a ""
2098                                       ""supported Transfer Syntax."")
2099 
2100         if 'SOPClassUID' in self:
2101             self.file_meta.MediaStorageSOPClassUID = self.SOPClassUID
2102         if 'SOPInstanceUID' in self:
2103             self.file_meta.MediaStorageSOPInstanceUID = self.SOPInstanceUID
2104         if enforce_standard:
2105             validate_file_meta(self.file_meta, enforce_standard=True)
2106 
2107     def __setattr__(self, name: str, value: Any) -> None:
2108         """"""Intercept any attempts to set a value for an instance attribute.
2109 
2110         If name is a DICOM keyword, set the corresponding tag and DataElement.
2111         Else, set an instance (python) attribute as any other class would do.
2112 
2113         Parameters
2114         ----------
2115         name : str
2116             The keyword for the element you wish to add/change. If
2117             `name` is not a DICOM element keyword then this will be the
2118             name of the attribute to be added/changed.
2119         value
2120             The value for the attribute to be added/changed.
2121         """"""
2122         tag = tag_for_keyword(name)
2123         if tag is not None:  # successfully mapped name to a tag
2124             if tag not in self:
2125                 # don't have this tag yet->create the data_element instance
2126                 vr = dictionary_VR(tag)
2127                 data_element = DataElement(tag, vr, value)
2128                 if vr == VR_.SQ:
2129                     # let a sequence know its parent dataset to pass it
2130                     # to its items, who may need parent dataset tags
2131                     # to resolve ambiguous tags
2132                     data_element.parent = self
2133             else:
2134                 # already have this data_element, just changing its value
2135                 data_element = self[tag]
2136                 data_element.value = value
2137             # Now have data_element - store it in this dict
2138             self[tag] = data_element
2139         elif repeater_has_keyword(name):
2140             # Check if `name` is repeaters element
2141             raise ValueError(
2142                 f""'{name}' is a DICOM repeating group element and must be ""
2143                 ""added using the add() or add_new() methods.""
2144             )
2145         elif name == ""file_meta"":
2146             self._set_file_meta(value)
2147         else:
2148             # Warn if `name` is camel case but not a keyword
2149             if _RE_CAMEL_CASE.match(name):
2150                 msg = (
2151                     f""Camel case attribute '{name}' used which is not in the ""
2152                     ""element keyword data dictionary""
2153                 )
2154                 if config.INVALID_KEYWORD_BEHAVIOR == ""WARN"":
2155                     warnings.warn(msg)
2156                 elif config.INVALID_KEYWORD_BEHAVIOR == ""RAISE"":
2157                     raise ValueError(msg)
2158 
2159             # name not in dicom dictionary - setting a non-dicom instance
2160             # attribute
2161             # XXX note if user mis-spells a dicom data_element - no error!!!
2162             object.__setattr__(self, name, value)
2163 
2164     def _set_file_meta(self, value: Optional[""Dataset""]) -> None:
2165         if value is not None and not isinstance(value, FileMetaDataset):
2166             if config._use_future:
2167                 raise TypeError(
2168                     ""Pydicom Future: Dataset.file_meta must be an instance ""
2169                     ""of FileMetaDataset""
2170                 )
2171 
2172             FileMetaDataset.validate(value)
2173             warnings.warn(
2174                 ""Starting in pydicom 3.0, Dataset.file_meta must be a ""
2175                 ""FileMetaDataset class instance"",
2176                 DeprecationWarning
2177             )
2178 
2179         self.__dict__[""file_meta""] = value
2180 
2181     def __setitem__(
2182         self, key: Union[slice, TagType], elem: _DatasetValue
2183     ) -> None:
2184         """"""Operator for ``Dataset[key] = elem``.
2185 
2186         Parameters
2187         ----------
2188         key : int or Tuple[int, int] or str
2189             The tag for the element to be added to the :class:`Dataset`.
2190         elem : dataelem.DataElement or dataelem.RawDataElement
2191             The element to add to the :class:`Dataset`.
2192 
2193         Raises
2194         ------
2195         NotImplementedError
2196             If `key` is a :class:`slice`.
2197         ValueError
2198             If the `key` value doesn't match the corresponding
2199             :attr:`DataElement.tag<pydicom.dataelem.tag>`.
2200         """"""
2201         if isinstance(key, slice):
2202             raise NotImplementedError(
2203                 'Slicing is not supported when setting Dataset items'
2204             )
2205 
2206         try:
2207             key = Tag(key)
2208         except Exception as exc:
2209             raise ValueError(
2210                 f""Unable to convert the key '{key}' to an element tag""
2211             ) from exc
2212 
2213         if not isinstance(elem, (DataElement, RawDataElement)):
2214             raise TypeError(""Dataset items must be 'DataElement' instances"")
2215 
2216         if isinstance(elem.tag, BaseTag):
2217             elem_tag = elem.tag
2218         else:
2219             elem_tag = Tag(elem.tag)
2220 
2221         if key != elem_tag:
2222             raise ValueError(
2223                 f""The key '{key}' doesn't match the 'DataElement' tag ""
2224                 f""'{elem_tag}'""
2225             )
2226 
2227         if elem_tag.is_private:
2228             # See PS 3.5-2008 section 7.8.1 (p. 44) for how blocks are reserved
2229             logger.debug(f""Setting private tag {elem_tag}"")
2230             private_block = elem_tag.element >> 8
2231             private_creator_tag = Tag(elem_tag.group, private_block)
2232             if private_creator_tag in self and elem_tag != private_creator_tag:
2233                 if isinstance(elem, RawDataElement):
2234                     elem = DataElement_from_raw(
2235                         elem, self._character_set, self
2236                     )
2237                 elem.private_creator = self[private_creator_tag].value
2238 
2239         self._dict[elem_tag] = elem
2240 
2241     def _slice_dataset(
2242         self,
2243         start: Optional[TagType],
2244         stop: Optional[TagType],
2245         step: Optional[int]
2246     ) -> List[BaseTag]:
2247         """"""Return the element tags in the Dataset that match the slice.
2248 
2249         Parameters
2250         ----------
2251         start : int or 2-tuple of int or None
2252             The slice's starting element tag value, in any format accepted by
2253             :func:`~pydicom.tag.Tag`.
2254         stop : int or 2-tuple of int or None
2255             The slice's stopping element tag value, in any format accepted by
2256             :func:`~pydicom.tag.Tag`.
2257         step : int or None
2258             The slice's step size.
2259 
2260         Returns
2261         ------
2262         list of BaseTag
2263             The tags in the :class:`Dataset` that meet the conditions of the
2264             slice.
2265         """"""
2266         # Check the starting/stopping Tags are valid when used
2267         if start is not None:
2268             start = Tag(start)
2269         if stop is not None:
2270             stop = Tag(stop)
2271 
2272         all_tags = sorted(self._dict.keys())
2273         # If the Dataset is empty, return an empty list
2274         if not all_tags:
2275             return []
2276 
2277         # Special case the common situations:
2278         #   - start and/or stop are None
2279         #   - step is 1
2280 
2281         if start is None:
2282             if stop is None:
2283                 # For step=1 avoid copying the list
2284                 return all_tags if step == 1 else all_tags[::step]
2285             else:  # Have a stop value, get values until that point
2286                 step1_list = list(takewhile(lambda x: x < stop, all_tags))
2287                 return step1_list if step == 1 else step1_list[::step]
2288 
2289         # Have a non-None start value.  Find its index
2290         i_start = bisect_left(all_tags, start)
2291         if stop is None:
2292             return all_tags[i_start::step]
2293 
2294         i_stop = bisect_left(all_tags, stop)
2295         return all_tags[i_start:i_stop:step]
2296 
2297     def __str__(self) -> str:
2298         """"""Handle str(dataset).
2299 
2300         ..versionchanged:: 2.0
2301 
2302             The file meta information was added in its own section,
2303             if :data:`pydicom.config.show_file_meta` is ``True``
2304 
2305         """"""
2306         return self._pretty_str()
2307 
2308     def top(self) -> str:
2309         """"""Return a :class:`str` representation of the top level elements. """"""
2310         return self._pretty_str(top_level_only=True)
2311 
2312     def trait_names(self) -> List[str]:
2313         """"""Return a :class:`list` of valid names for auto-completion code.
2314 
2315         Used in IPython, so that data element names can be found and offered
2316         for autocompletion on the IPython command line.
2317         """"""
2318         return dir(self)
2319 
2320     def update(self, d: _DatasetType) -> None:
2321         """"""Extend :meth:`dict.update` to handle DICOM tags and keywords.
2322 
2323         Parameters
2324         ----------
2325         d : dict or Dataset
2326             The :class:`dict` or :class:`Dataset` to use when updating the
2327             current object.
2328         """"""
2329         for key, value in list(d.items()):
2330             if isinstance(key, str):
2331                 setattr(self, key, value)
2332             else:
2333                 self[Tag(cast(int, key))] = value
2334 
2335     def iterall(self) -> Iterator[DataElement]:
2336         """"""Iterate through the :class:`Dataset`, yielding all the elements.
2337 
2338         Unlike ``iter(Dataset)``, this *does* recurse into sequences,
2339         and so yields all elements as if dataset were ""flattened"".
2340 
2341         Yields
2342         ------
2343         dataelem.DataElement
2344         """"""
2345         for elem in self:
2346             yield elem
2347             if elem.VR == VR_.SQ:
2348                 for ds in elem.value:
2349                     yield from ds.iterall()
2350 
2351     def walk(
2352         self,
2353         callback: Callable[[""Dataset"", DataElement], None],
2354         recursive: bool = True
2355     ) -> None:
2356         """"""Iterate through the :class:`Dataset's<Dataset>` elements and run
2357         `callback` on each.
2358 
2359         Visit all elements in the :class:`Dataset`, possibly recursing into
2360         sequences and their items. The `callback` function is called for each
2361         :class:`~pydicom.dataelem.DataElement` (including elements
2362         with a VR of 'SQ'). Can be used to perform an operation on certain
2363         types of elements.
2364 
2365         For example,
2366         :meth:`~Dataset.remove_private_tags` finds all elements with private
2367         tags and deletes them.
2368 
2369         The elements will be returned in order of increasing tag number within
2370         their current :class:`Dataset`.
2371 
2372         Parameters
2373         ----------
2374         callback
2375             A callable function that takes two arguments:
2376 
2377             * a :class:`Dataset`
2378             * a :class:`~pydicom.dataelem.DataElement` belonging
2379               to that :class:`Dataset`
2380 
2381         recursive : bool, optional
2382             Flag to indicate whether to recurse into sequences (default
2383             ``True``).
2384         """"""
2385         taglist = sorted(self._dict.keys())
2386         for tag in taglist:
2387 
2388             with tag_in_exception(tag):
2389                 data_element = self[tag]
2390                 callback(self, data_element)  # self = this Dataset
2391                 # 'tag in self' below needed in case callback deleted
2392                 # data_element
2393                 if recursive and tag in self and data_element.VR == VR_.SQ:
2394                     sequence = data_element.value
2395                     for dataset in sequence:
2396                         dataset.walk(callback)
2397 
2398     @classmethod
2399     def from_json(
2400         cls: Type[""Dataset""],
2401         json_dataset: Union[Dict[str, Any], str, bytes, bytearray],
2402         bulk_data_uri_handler: Optional[
2403             Union[
2404                 Callable[[str, str, str], Union[None, str, int, float, bytes]],
2405                 Callable[[str], Union[None, str, int, float, bytes]]
2406             ]
2407         ] = None
2408     ) -> ""Dataset"":
2409         """"""Return a :class:`Dataset` from a DICOM JSON Model object.
2410 
2411         .. versionadded:: 1.3
2412 
2413         See the DICOM Standard, Part 18, :dcm:`Annex F<part18/chapter_F.html>`.
2414 
2415         Parameters
2416         ----------
2417         json_dataset : dict, str, bytes or bytearray
2418             :class:`dict`, :class:`str`, :class:`bytes` or :class:`bytearray`
2419             representing a DICOM Data Set formatted based on the :dcm:`DICOM
2420             JSON Model<part18/chapter_F.html>`.
2421         bulk_data_uri_handler : callable, optional
2422             Callable function that accepts either the tag, vr and
2423             ""BulkDataURI"" value or just the ""BulkDataURI"" value of the JSON
2424             representation of a data element and returns the actual value of
2425             that data element (retrieved via DICOMweb WADO-RS). If no
2426             `bulk_data_uri_handler` is specified (default) then the
2427             corresponding element will have an ""empty"" value such as
2428             ``""""``, ``b""""`` or ``None`` depending on the `vr` (i.e. the
2429             Value Multiplicity will be 0).
2430 
2431         Returns
2432         -------
2433         Dataset
2434         """"""
2435         if isinstance(json_dataset, (str, bytes, bytearray)):
2436             json_dataset = cast(Dict[str, Any], json.loads(json_dataset))
2437 
2438         dataset = cls()
2439         for tag, mapping in json_dataset.items():
2440             # `tag` is an element tag in uppercase hex format as a str
2441             # `mapping` is Dict[str, Any] and should have keys 'vr' and at most
2442             #   one of ('Value', 'BulkDataURI', 'InlineBinary') but may have
2443             #   none of those if the element's VM is 0
2444             vr = mapping['vr']
2445             unique_value_keys = tuple(
2446                 set(mapping.keys()) & set(jsonrep.JSON_VALUE_KEYS)
2447             )
2448             if len(unique_value_keys) == 0:
2449                 value_key = None
2450                 value = ['']
2451             else:
2452                 value_key = unique_value_keys[0]
2453                 value = mapping[value_key]
2454             data_element = DataElement.from_json(
2455                 cls, tag, vr, value, value_key, bulk_data_uri_handler
2456             )
2457             dataset.add(data_element)
2458         return dataset
2459 
2460     def to_json_dict(
2461         self,
2462         bulk_data_threshold: int = 1024,
2463         bulk_data_element_handler: Optional[Callable[[DataElement], str]] = None,  # noqa
2464         suppress_invalid_tags: bool = False,
2465     ) -> Dict[str, Any]:
2466         """"""Return a dictionary representation of the :class:`Dataset`
2467         conforming to the DICOM JSON Model as described in the DICOM
2468         Standard, Part 18, :dcm:`Annex F<part18/chapter_F.html>`.
2469 
2470         .. versionadded:: 1.4
2471 
2472         Parameters
2473         ----------
2474         bulk_data_threshold : int, optional
2475             Threshold for the length of a base64-encoded binary data element
2476             above which the element should be considered bulk data and the
2477             value provided as a URI rather than included inline (default:
2478             ``1024``). Ignored if no bulk data handler is given.
2479         bulk_data_element_handler : callable, optional
2480             Callable function that accepts a bulk data element and returns a
2481             JSON representation of the data element (dictionary including the
2482             ""vr"" key and either the ""InlineBinary"" or the ""BulkDataURI"" key).
2483         suppress_invalid_tags : bool, optional
2484             Flag to specify if errors while serializing tags should be logged
2485             and the tag dropped or if the error should be bubbled up.
2486 
2487         Returns
2488         -------
2489         dict
2490             :class:`Dataset` representation based on the DICOM JSON Model.
2491         """"""
2492         json_dataset = {}
2493         for key in self.keys():
2494             json_key = '{:08X}'.format(key)
2495             data_element = self[key]
2496             try:
2497                 json_dataset[json_key] = data_element.to_json_dict(
2498                     bulk_data_element_handler=bulk_data_element_handler,
2499                     bulk_data_threshold=bulk_data_threshold
2500                 )
2501             except Exception as exc:
2502                 logger.error(f""Error while processing tag {json_key}"")
2503                 if not suppress_invalid_tags:
2504                     raise exc
2505 
2506         return json_dataset
2507 
2508     def to_json(
2509         self,
2510         bulk_data_threshold: int = 1024,
2511         bulk_data_element_handler: Optional[Callable[[DataElement], str]] = None,  # noqa
2512         dump_handler: Optional[Callable[[Dict[str, Any]], str]] = None,
2513         suppress_invalid_tags: bool = False,
2514     ) -> str:
2515         """"""Return a JSON representation of the :class:`Dataset`.
2516 
2517         .. versionadded:: 1.3
2518 
2519         See the DICOM Standard, Part 18, :dcm:`Annex F<part18/chapter_F.html>`.
2520 
2521         Parameters
2522         ----------
2523         bulk_data_threshold : int, optional
2524             Threshold for the length of a base64-encoded binary data element
2525             above which the element should be considered bulk data and the
2526             value provided as a URI rather than included inline (default:
2527             ``1024``). Ignored if no bulk data handler is given.
2528         bulk_data_element_handler : callable, optional
2529             Callable function that accepts a bulk data element and returns a
2530             JSON representation of the data element (dictionary including the
2531             ""vr"" key and either the ""InlineBinary"" or the ""BulkDataURI"" key).
2532         dump_handler : callable, optional
2533             Callable function that accepts a :class:`dict` and returns the
2534             serialized (dumped) JSON string (by default uses
2535             :func:`json.dumps`).
2536 
2537             .. note:
2538 
2539                 Make sure to use a dump handler that sorts the keys (see
2540                 example below) to create DICOM-conformant JSON.
2541         suppress_invalid_tags : bool, optional
2542             Flag to specify if errors while serializing tags should be logged
2543             and the tag dropped or if the error should be bubbled up.
2544 
2545         Returns
2546         -------
2547         str
2548             :class:`Dataset` serialized into a string based on the DICOM JSON
2549             Model.
2550 
2551         Examples
2552         --------
2553         >>> def my_json_dumps(data):
2554         ...     return json.dumps(data, indent=4, sort_keys=True)
2555         >>> ds.to_json(dump_handler=my_json_dumps)
2556         """"""
2557         if dump_handler is None:
2558             def json_dump(d: Any) -> str:
2559                 return json.dumps(d, sort_keys=True)
2560 
2561             dump_handler = json_dump
2562 
2563         return dump_handler(
2564             self.to_json_dict(
2565                 bulk_data_threshold,
2566                 bulk_data_element_handler,
2567                 suppress_invalid_tags=suppress_invalid_tags
2568             )
2569         )
2570 
2571     def __getstate__(self) -> Dict[str, Any]:
2572         # pickle cannot handle weakref - remove parent
2573         d = self.__dict__.copy()
2574         del d['parent']
2575         return d
2576 
2577     def __setstate__(self, state: Dict[str, Any]) -> None:
2578         self.__dict__.update(state)
2579         # re-add parent - it will be set to the parent dataset on demand
2580         # if the dataset is in a sequence
2581         self.__dict__['parent'] = None
2582 
2583     __repr__ = __str__
2584 
2585 
2586 _FileDataset = TypeVar(""_FileDataset"", bound=""FileDataset"")
2587 
2588 
2589 class FileDataset(Dataset):
2590     """"""An extension of :class:`Dataset` to make reading and writing to
2591     file-like easier.
2592 
2593     Attributes
2594     ----------
2595     preamble : str or bytes or None
2596         The optional DICOM preamble prepended to the :class:`FileDataset`, if
2597         available.
2598     file_meta : FileMetaDataset or None
2599         The Dataset's file meta information as a :class:`FileMetaDataset`,
2600         if available (``None`` if not present).
2601         Consists of group ``0x0002`` elements.
2602     filename : str or None
2603         The filename that the :class:`FileDataset` was read from (if read from
2604         file) or ``None`` if the filename is not available (if read from a
2605         :class:`io.BytesIO` or  similar).
2606     fileobj_type
2607         The object type of the file-like the :class:`FileDataset` was read
2608         from.
2609     is_implicit_VR : bool
2610         ``True`` if the dataset encoding is implicit VR, ``False`` otherwise.
2611     is_little_endian : bool
2612         ``True`` if the dataset encoding is little endian byte ordering,
2613         ``False`` otherwise.
2614     timestamp : float or None
2615         The modification time of the file the :class:`FileDataset` was read
2616         from, ``None`` if the modification time is not available.
2617     """"""
2618 
2619     def __init__(
2620         self,
2621         filename_or_obj: Union[PathType, BinaryIO, DicomFileLike],
2622         dataset: _DatasetType,
2623         preamble: Optional[bytes] = None,
2624         file_meta: Optional[""FileMetaDataset""] = None,
2625         is_implicit_VR: bool = True,
2626         is_little_endian: bool = True
2627     ) -> None:
2628         """"""Initialize a :class:`FileDataset` read from a DICOM file.
2629 
2630         Parameters
2631         ----------
2632         filename_or_obj : str or PathLike or BytesIO or None
2633             Full path and filename to the file, memory buffer object, or
2634             ``None`` if is a :class:`io.BytesIO`.
2635         dataset : Dataset or dict
2636             Some form of dictionary, usually a :class:`Dataset` returned from
2637             :func:`~pydicom.filereader.dcmread`.
2638         preamble : bytes or str, optional
2639             The 128-byte DICOM preamble.
2640         file_meta : FileMetaDataset, optional
2641             The file meta :class:`FileMetaDataset`, such as the one returned by
2642             :func:`~pydicom.filereader.read_file_meta_info`, or an empty
2643             :class:`FileMetaDataset` if no file meta information is in the
2644             file.
2645         is_implicit_VR : bool, optional
2646             ``True`` (default) if implicit VR transfer syntax used; ``False``
2647             if explicit VR.
2648         is_little_endian : bool
2649             ``True`` (default) if little-endian transfer syntax used; ``False``
2650             if big-endian.
2651         """"""
2652         Dataset.__init__(self, dataset)
2653         self.preamble = preamble
2654         self.file_meta: ""FileMetaDataset"" = (
2655             file_meta if file_meta is not None else FileMetaDataset()
2656         )
2657         self.is_implicit_VR: bool = is_implicit_VR
2658         self.is_little_endian: bool = is_little_endian
2659 
2660         filename: Optional[str] = None
2661         filename_or_obj = path_from_pathlike(filename_or_obj)
2662         self.fileobj_type: Any = None
2663         self.filename: Union[PathType, BinaryIO] = """"
2664 
2665         if isinstance(filename_or_obj, str):
2666             filename = filename_or_obj
2667             self.fileobj_type = open
2668         elif isinstance(filename_or_obj, io.BufferedReader):
2669             filename = filename_or_obj.name
2670             # This is the appropriate constructor for io.BufferedReader
2671             self.fileobj_type = open
2672         else:
2673             # use __class__ python <2.7?;
2674             # http://docs.python.org/reference/datamodel.html
2675             self.fileobj_type = filename_or_obj.__class__
2676             if hasattr(filename_or_obj, ""name""):
2677                 filename = filename_or_obj.name
2678             elif hasattr(filename_or_obj, ""filename""):
2679                 filename = (
2680                     filename_or_obj.filename  # type: ignore[attr-defined]
2681                 )
2682             else:
2683                 # e.g. came from BytesIO or something file-like
2684                 self.filename = filename_or_obj
2685 
2686         self.timestamp = None
2687         if filename:
2688             self.filename = filename
2689             if os.path.exists(filename):
2690                 statinfo = os.stat(filename)
2691                 self.timestamp = statinfo.st_mtime
2692 
2693     def _copy_implementation(self, copy_function: Callable) -> ""FileDataset"":
2694         """"""Implementation of ``__copy__`` and ``__deepcopy__``.
2695         Sets the filename to ``None`` if it isn't a string,
2696         and copies all other attributes using `copy_function`.
2697         """"""
2698         copied = self.__class__(
2699             self.filename, self, self.preamble, self.file_meta,
2700             self.is_implicit_VR, self.is_little_endian
2701         )
2702         filename = self.filename
2703         if filename is not None and not isinstance(filename, str):
2704             warnings.warn(""The 'filename' attribute of the dataset is a ""
2705                           ""file-like object and will be set to None ""
2706                           ""in the copied object"")
2707             self.filename = None  # type: ignore[assignment]
2708         for (k, v) in self.__dict__.items():
2709             copied.__dict__[k] = copy_function(v)
2710 
2711         self.filename = filename
2712 
2713         return copied
2714 
2715     def __copy__(self) -> ""FileDataset"":
2716         """"""Return a shallow copy of the file dataset.
2717         Make sure that the filename is not copied in case it is a file-like
2718         object.
2719 
2720         Returns
2721         -------
2722         FileDataset
2723             A shallow copy of the file data set.
2724         """"""
2725         return self._copy_implementation(copy.copy)
2726 
2727     def __deepcopy__(self, _: Optional[Dict[int, Any]]) -> ""FileDataset"":
2728         """"""Return a deep copy of the file dataset.
2729         Make sure that the filename is not copied in case it is a file-like
2730         object.
2731 
2732         Returns
2733         -------
2734         FileDataset
2735             A deep copy of the file data set.
2736         """"""
2737         return self._copy_implementation(copy.deepcopy)
2738 
2739 
2740 def validate_file_meta(
2741     file_meta: ""FileMetaDataset"", enforce_standard: bool = True
2742 ) -> None:
2743     """"""Validate the *File Meta Information* elements in `file_meta`.
2744 
2745     .. versionchanged:: 1.2
2746 
2747         Moved from :mod:`pydicom.filewriter`.
2748 
2749     Parameters
2750     ----------
2751     file_meta : Dataset
2752         The *File Meta Information* data elements.
2753     enforce_standard : bool, optional
2754         If ``False``, then only a check for invalid elements is performed.
2755         If ``True`` (default), the following elements will be added if not
2756         already present:
2757 
2758         * (0002,0001) *File Meta Information Version*
2759         * (0002,0012) *Implementation Class UID*
2760         * (0002,0013) *Implementation Version Name*
2761 
2762         and the following elements will be checked:
2763 
2764         * (0002,0002) *Media Storage SOP Class UID*
2765         * (0002,0003) *Media Storage SOP Instance UID*
2766         * (0002,0010) *Transfer Syntax UID*
2767 
2768     Raises
2769     ------
2770     ValueError
2771         If `enforce_standard` is ``True`` and any of the checked *File Meta
2772         Information* elements are missing from `file_meta`.
2773     ValueError
2774         If any non-Group 2 Elements are present in `file_meta`.
2775     """"""
2776     # Check that no non-Group 2 Elements are present
2777     for elem in file_meta.elements():
2778         if elem.tag.group != 0x0002:
2779             raise ValueError(""Only File Meta Information Group (0002,eeee) ""
2780                              ""elements must be present in 'file_meta'."")
2781 
2782     if enforce_standard:
2783         if 'FileMetaInformationVersion' not in file_meta:
2784             file_meta.FileMetaInformationVersion = b'\x00\x01'
2785 
2786         if 'ImplementationClassUID' not in file_meta:
2787             file_meta.ImplementationClassUID = UID(PYDICOM_IMPLEMENTATION_UID)
2788 
2789         if 'ImplementationVersionName' not in file_meta:
2790             file_meta.ImplementationVersionName = (
2791                 'PYDICOM ' + ""."".join(str(x) for x in __version_info__))
2792 
2793         # Check that required File Meta Information elements are present
2794         missing = []
2795         for element in [0x0002, 0x0003, 0x0010]:
2796             if Tag(0x0002, element) not in file_meta:
2797                 missing.append(Tag(0x0002, element))
2798         if missing:
2799             msg = (""Missing required File Meta Information elements from ""
2800                    ""'file_meta':\n"")
2801             for tag in missing:
2802                 msg += '\t{0} {1}\n'.format(tag, keyword_for_tag(tag))
2803             raise ValueError(msg[:-1])  # Remove final newline
2804 
2805 
2806 class FileMetaDataset(Dataset):
2807     """"""Contains a collection (dictionary) of group 2 DICOM Data Elements.
2808 
2809     .. versionadded:: 2.0
2810 
2811     Derived from :class:`~pydicom.dataset.Dataset`, but only allows
2812     Group 2 (File Meta Information) data elements
2813     """"""
2814 
2815     def __init__(self, *args: _DatasetType, **kwargs: Any) -> None:
2816         """"""Initialize a FileMetaDataset
2817 
2818         Parameters are as per :class:`Dataset`; this overrides the super class
2819         only to check that all are group 2 data elements
2820 
2821         Raises
2822         ------
2823         ValueError
2824             If any data elements are not group 2.
2825         TypeError
2826             If the passed argument is not a :class:`dict` or :class:`Dataset`
2827         """"""
2828         super().__init__(*args, **kwargs)
2829         FileMetaDataset.validate(self._dict)
2830 
2831         # Set type hints for the possible contents - VR, Type (1|1C|3)
2832         self.FileMetaInformationGroupLength: int  # UL, 1
2833         self.FileMetaInformationVersion: bytes  # OB, 1
2834         self.MediaStorageSOPClassUID: UID  # UI, 1
2835         self.MediaStorageSOPInstanceUID: UID  # UI, 1
2836         self.TransferSyntaxUID: UID  # UI, 1
2837         self.ImplementationClassUID: UID  # UI, 1
2838         self.ImplementationVersionName: Optional[str]  # SH, 3
2839         self.SourceApplicationEntityTitle: Optional[str]  # AE, 3
2840         self.SendingApplicationEntityTitle: Optional[str]  # AE, 3
2841         self.ReceivingApplicationEntityTitle: Optional[str]  # AE, 3
2842         self.SourcePresentationAddress: Optional[str]  # UR, 3
2843         self.ReceivingPresentationAddress: Optional[str]  # UR, 3
2844         self.PrivateInformationCreatorUID: Optional[UID]  # UI, 3
2845         self.PrivateInformation: bytes  # OB, 1C
2846 
2847     @staticmethod
2848     def validate(init_value: _DatasetType) -> None:
2849         """"""Raise errors if initialization value is not acceptable for file_meta
2850 
2851         Parameters
2852         ----------
2853         init_value: dict or Dataset
2854             The tag:data element pairs to initialize a file meta dataset
2855 
2856         Raises
2857         ------
2858         TypeError
2859             If the passed argument is not a :class:`dict` or :class:`Dataset`
2860         ValueError
2861             If any data elements passed are not group 2.
2862         """"""
2863         if init_value is None:
2864             return
2865 
2866         if not isinstance(init_value, (Dataset, dict)):
2867             raise TypeError(
2868                 ""Argument must be a dict or Dataset, not {}"".format(
2869                     type(init_value)
2870                 )
2871             )
2872 
2873         non_group2 = [
2874             Tag(tag) for tag in init_value.keys() if Tag(tag).group != 2
2875         ]
2876         if non_group2:
2877             msg = ""Attempted to set non-group 2 elements: {}""
2878             raise ValueError(msg.format(non_group2))
2879 
2880     def __setitem__(
2881         self, key: Union[slice, TagType], value: _DatasetValue
2882     ) -> None:
2883         """"""Override parent class to only allow setting of group 2 elements.
2884 
2885         Parameters
2886         ----------
2887         key : int or Tuple[int, int] or str
2888             The tag for the element to be added to the Dataset.
2889         value : dataelem.DataElement or dataelem.RawDataElement
2890             The element to add to the :class:`FileMetaDataset`.
2891 
2892         Raises
2893         ------
2894         ValueError
2895             If `key` is not a DICOM Group 2 tag.
2896         """"""
2897 
2898         if isinstance(value.tag, BaseTag):
2899             tag = value.tag
2900         else:
2901             tag = Tag(value.tag)
2902 
2903         if tag.group != 2:
2904             raise ValueError(
2905                 ""Only group 2 data elements are allowed in a FileMetaDataset""
2906             )
2907 
2908         super().__setitem__(key, value)
2909 
2910 
2911 _RE_CAMEL_CASE = re.compile(
2912     # Ensure mix of upper and lowercase and digits, no underscores
2913     # If first character is lowercase ensure at least one uppercase char
2914     ""(?P<start>(^[A-Za-z])((?=.+?[A-Z])[A-Za-z0-9]+)|(^[A-Z])([A-Za-z0-9]+))""
2915     ""(?P<last>[A-Za-z0-9][^_]$)""  # Last character is alphanumeric
2916 )
2917 
[end of pydicom/dataset.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pydicom/pydicom,f8cf45b6c121e5a4bf4a43f71aba3bc64af3db9c,"Dataset.to_json_dict can still generate exceptions when suppress_invalid_tags=True
**Describe the bug**
I'm using `Dataset.to_json_dict(suppress_invalid_tags=True)` and can live with losing invalid tags.  Unfortunately, I can still trigger an exception with something like  `2.0` in an `IS` field.

**Expected behavior**
to_json_dict shouldn't throw an error about an invalid tag when `suppress_invalid_tags` is enabled.

My thought was simply to move the `data_element = self[key]` into the try/catch block that's right after it.

**Steps To Reproduce**

Traceback:
```
  File ""dicom.py"", line 143, in create_dict
    json_ds = ds.to_json_dict(suppress_invalid_tags=True)
  File ""/usr/lib/python3/dist-packages/pydicom/dataset.py"", line 2495, in to_json_dict
    data_element = self[key]
  File ""/usr/lib/python3/dist-packages/pydicom/dataset.py"", line 939, in __getitem__
    self[tag] = DataElement_from_raw(elem, character_set, self)
  File ""/usr/lib/python3/dist-packages/pydicom/dataelem.py"", line 859, in DataElement_from_raw
    value = convert_value(vr, raw, encoding)
  File ""/usr/lib/python3/dist-packages/pydicom/values.py"", line 771, in convert_value
    return converter(byte_string, is_little_endian, num_format)
  File ""/usr/lib/python3/dist-packages/pydicom/values.py"", line 348, in convert_IS_string
    return MultiString(num_string, valtype=pydicom.valuerep.IS)
  File ""/usr/lib/python3/dist-packages/pydicom/valuerep.py"", line 1213, in MultiString
    return valtype(splitup[0])
  File ""/usr/lib/python3/dist-packages/pydicom/valuerep.py"", line 1131, in __new__
    raise TypeError(""Could not convert value to integer without loss"")
TypeError: Could not convert value to integer without loss
```

**Your environment**
python 3.7, pydicom 2.3


",,2022-09-20T18:52:53Z,"<patch>
diff --git a/pydicom/dataset.py b/pydicom/dataset.py
--- a/pydicom/dataset.py
+++ b/pydicom/dataset.py
@@ -2492,8 +2492,8 @@ def to_json_dict(
         json_dataset = {}
         for key in self.keys():
             json_key = '{:08X}'.format(key)
-            data_element = self[key]
             try:
+                data_element = self[key]
                 json_dataset[json_key] = data_element.to_json_dict(
                     bulk_data_element_handler=bulk_data_element_handler,
                     bulk_data_threshold=bulk_data_threshold

</patch>","diff --git a/pydicom/tests/test_json.py b/pydicom/tests/test_json.py
--- a/pydicom/tests/test_json.py
+++ b/pydicom/tests/test_json.py
@@ -7,7 +7,7 @@
 
 from pydicom import dcmread
 from pydicom.data import get_testdata_file
-from pydicom.dataelem import DataElement
+from pydicom.dataelem import DataElement, RawDataElement
 from pydicom.dataset import Dataset
 from pydicom.tag import Tag, BaseTag
 from pydicom.valuerep import PersonName
@@ -284,7 +284,23 @@ def test_suppress_invalid_tags(self, _):
 
         ds_json = ds.to_json_dict(suppress_invalid_tags=True)
 
-        assert ds_json.get(""00100010"") is None
+        assert ""00100010"" not in ds_json
+
+    def test_suppress_invalid_tags_with_failed_dataelement(self):
+        """"""Test tags that raise exceptions don't if suppress_invalid_tags True.
+        """"""
+        ds = Dataset()
+        # we have to add a RawDataElement as creating a DataElement would
+        # already raise an exception
+        ds[0x00082128] = RawDataElement(
+            Tag(0x00082128), 'IS', 4, b'5.25', 0, True, True)
+
+        with pytest.raises(TypeError):
+            ds.to_json_dict()
+
+        ds_json = ds.to_json_dict(suppress_invalid_tags=True)
+
+        assert ""00082128"" not in ds_json
 
 
 class TestSequence:
",2.3,"[""pydicom/tests/test_json.py::TestDataSetToJson::test_suppress_invalid_tags_with_failed_dataelement""]","[""pydicom/tests/test_json.py::TestPersonName::test_json_pn_from_file"", ""pydicom/tests/test_json.py::TestPersonName::test_pn_components_to_json"", ""pydicom/tests/test_json.py::TestPersonName::test_pn_components_from_json"", ""pydicom/tests/test_json.py::TestPersonName::test_empty_value"", ""pydicom/tests/test_json.py::TestPersonName::test_multi_value_to_json"", ""pydicom/tests/test_json.py::TestPersonName::test_dataelem_from_json"", ""pydicom/tests/test_json.py::TestAT::test_to_json"", ""pydicom/tests/test_json.py::TestAT::test_from_json"", ""pydicom/tests/test_json.py::TestAT::test_invalid_value_in_json"", ""pydicom/tests/test_json.py::TestAT::test_invalid_tag_in_json"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_json_from_dicom_file"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_roundtrip"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_dataset_dumphandler"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_dataelement_dumphandler"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_sort_order"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_suppress_invalid_tags"", ""pydicom/tests/test_json.py::TestSequence::test_nested_sequences"", ""pydicom/tests/test_json.py::TestBinary::test_inline_binary"", ""pydicom/tests/test_json.py::TestBinary::test_invalid_inline_binary"", ""pydicom/tests/test_json.py::TestBinary::test_valid_bulkdata_uri"", ""pydicom/tests/test_json.py::TestBinary::test_invalid_bulkdata_uri"", ""pydicom/tests/test_json.py::TestBinary::test_bulk_data_reader_is_called"", ""pydicom/tests/test_json.py::TestBinary::test_bulk_data_reader_is_called_2"", ""pydicom/tests/test_json.py::TestBinary::test_bulk_data_reader_is_called_within_SQ"", ""pydicom/tests/test_json.py::TestNumeric::test_numeric_values"", ""pydicom/tests/test_json.py::TestNumeric::test_numeric_types""]",a8be738418dee0a2b93c241fbd5e0bc82f4b8680
pydicom__pydicom-1413,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
Error : a bytes-like object is required, not 'MultiValue'
Hello,

I am getting following error while updating the tag LongTrianglePointIndexList (0066,0040),
**TypeError: a bytes-like object is required, not 'MultiValue'**

I noticed that the error  gets produced only when the VR is given as ""OL"" , works fine with ""OB"", ""OF"" etc.

sample code (assume 'lineSeq' is the dicom dataset sequence):
```python
import pydicom
import array
data=list(range(1,10))
data=array.array('H', indexData).tostring()  # to convert to unsigned short
lineSeq.add_new(0x00660040, 'OL', data)   
ds.save_as(""mydicom"")
```
outcome: **TypeError: a bytes-like object is required, not 'MultiValue'**

using version - 2.0.0.0

Any help is appreciated.

Thank you

</issue>
<code>
[start of README.md]
1 [![CircleCI](https://circleci.com/gh/pydicom/pydicom/tree/master.svg?style=shield)](https://circleci.com/gh/pydicom/pydicom/tree/master)
2 [![codecov](https://codecov.io/gh/pydicom/pydicom/branch/master/graph/badge.svg)](https://codecov.io/gh/pydicom/pydicom)
3 [![Python version](https://img.shields.io/pypi/pyversions/pydicom.svg)](https://img.shields.io/pypi/pyversions/pydicom.svg)
4 [![PyPI version](https://badge.fury.io/py/pydicom.svg)](https://badge.fury.io/py/pydicom)
5 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4197955.svg)](https://doi.org/10.5281/zenodo.4197955)
6 [![Gitter](https://badges.gitter.im/pydicom/Lobby.svg)](https://gitter.im/pydicom/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
7 
8 # *pydicom*
9 
10 *pydicom* is a pure Python package for working with [DICOM](https://www.dicomstandard.org/) files. It lets you read, modify and write DICOM data in an easy ""pythonic"" way.
11 
12 As a pure Python package, *pydicom* can run anywhere Python runs without any other requirements, although if you're working with *Pixel Data* then we recommend you also install [NumPy](http://www.numpy.org).
13 
14 If you're looking for a Python library for DICOM networking then you might be interested in another of our projects: [pynetdicom](https://github.com/pydicom/pynetdicom).
15 
16 ## Installation
17 
18 Using [pip](https://pip.pypa.io/en/stable/):
19 ```
20 pip install pydicom
21 ```
22 Using [conda](https://docs.conda.io/en/latest/):
23 ```
24 conda install -c conda-forge pydicom
25 ```
26 
27 For more information, including installation instructions for the development version, see the [installation guide](https://pydicom.github.io/pydicom/stable/tutorials/installation.html).
28 
29 
30 ## Documentation
31 
32 The *pydicom* [user guide](https://pydicom.github.io/pydicom/stable/old/pydicom_user_guide.html), [tutorials](https://pydicom.github.io/pydicom/stable/tutorials/index.html), [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) and [API reference](https://pydicom.github.io/pydicom/stable/reference/index.html) documentation is available for both the [current release](https://pydicom.github.io/pydicom/stable) and the [development version](https://pydicom.github.io/pydicom/dev) on GitHub Pages.
33 
34 ## *Pixel Data*
35 
36 Compressed and uncompressed *Pixel Data* is always available to
37 be read, changed and written as [bytes](https://docs.python.org/3/library/stdtypes.html#bytes-objects):
38 ```python
39 >>> from pydicom import dcmread
40 >>> from pydicom.data import get_testdata_file
41 >>> path = get_testdata_file(""CT_small.dcm"")
42 >>> ds = dcmread(path)
43 >>> type(ds.PixelData)
44 <class 'bytes'>
45 >>> len(ds.PixelData)
46 32768
47 >>> ds.PixelData[:2]
48 b'\xaf\x00'
49 
50 ```
51 
52 If [NumPy](http://www.numpy.org) is installed, *Pixel Data* can be converted to an [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) using the [Dataset.pixel_array](https://pydicom.github.io/pydicom/stable/reference/generated/pydicom.dataset.Dataset.html#pydicom.dataset.Dataset.pixel_array) property:
53 
54 ```python
55 >>> arr = ds.pixel_array
56 >>> arr.shape
57 (128, 128)
58 >>> arr
59 array([[175, 180, 166, ..., 203, 207, 216],
60        [186, 183, 157, ..., 181, 190, 239],
61        [184, 180, 171, ..., 152, 164, 235],
62        ...,
63        [906, 910, 923, ..., 922, 929, 927],
64        [914, 954, 938, ..., 942, 925, 905],
65        [959, 955, 916, ..., 911, 904, 909]], dtype=int16)
66 ```
67 ### Compressed *Pixel Data*
68 #### JPEG, JPEG-LS and JPEG 2000
69 Converting JPEG compressed *Pixel Data* to an ``ndarray`` requires installing one or more additional Python libraries. For information on which libraries are required, see the [pixel data handler documentation](https://pydicom.github.io/pydicom/stable/old/image_data_handlers.html#guide-compressed).
70 
71 Compressing data into one of the JPEG formats is not currently supported.
72 
73 #### RLE
74 Encoding and decoding RLE *Pixel Data* only requires NumPy, however it can
75 be quite slow. You may want to consider [installing one or more additional
76 Python libraries](https://pydicom.github.io/pydicom/stable/old/image_data_compression.html) to speed up the process.
77 
78 ## Examples
79 More [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) are available in the documentation.
80 
81 **Change a patient's ID**
82 ```python
83 from pydicom import dcmread
84 
85 ds = dcmread(""/path/to/file.dcm"")
86 # Edit the (0010,0020) 'Patient ID' element
87 ds.PatientID = ""12345678""
88 ds.save_as(""/path/to/file_updated.dcm"")
89 ```
90 
91 **Display the Pixel Data**
92 
93 With [NumPy](http://www.numpy.org) and [matplotlib](https://matplotlib.org/)
94 ```python
95 import matplotlib.pyplot as plt
96 from pydicom import dcmread
97 from pydicom.data import get_testdata_file
98 
99 # The path to a pydicom test dataset
100 path = get_testdata_file(""CT_small.dcm"")
101 ds = dcmread(path)
102 # `arr` is a numpy.ndarray
103 arr = ds.pixel_array
104 
105 plt.imshow(arr, cmap=""gray"")
106 plt.show()
107 ```
108 
109 ## Contributing
110 
111 To contribute to *pydicom*, read our [contribution guide](https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md).
112 
113 To contribute an example or extension of *pydicom* that doesn't belong with the core software, see our contribution repository:
114 [contrib-pydicom](https://www.github.com/pydicom/contrib-pydicom).
115 
[end of README.md]
[start of pydicom/dataelem.py]
1 # Copyright 2008-2018 pydicom authors. See LICENSE file for details.
2 """"""Define the DataElement class.
3 
4 A DataElement has a tag,
5               a value representation (VR),
6               a value multiplicity (VM)
7               and a value.
8 """"""
9 
10 import base64
11 import json
12 from typing import (
13     Optional, Any, Tuple, Callable, Union, TYPE_CHECKING, Dict, TypeVar, Type,
14     List, NamedTuple, MutableSequence, cast
15 )
16 import warnings
17 
18 from pydicom import config  # don't import datetime_conversion directly
19 from pydicom.config import logger
20 from pydicom.datadict import (dictionary_has_tag, dictionary_description,
21                               dictionary_keyword, dictionary_is_retired,
22                               private_dictionary_description, dictionary_VR,
23                               repeater_has_tag, private_dictionary_VR)
24 from pydicom.errors import BytesLengthException
25 from pydicom.jsonrep import JsonDataElementConverter
26 from pydicom.multival import MultiValue
27 from pydicom.tag import Tag, BaseTag
28 from pydicom.uid import UID
29 from pydicom import jsonrep
30 import pydicom.valuerep  # don't import DS directly as can be changed by config
31 from pydicom.valuerep import PersonName
32 
33 if config.have_numpy:
34     import numpy  # type: ignore[import]
35 
36 if TYPE_CHECKING:  # pragma: no cover
37     from pydicom.dataset import Dataset
38 
39 
40 BINARY_VR_VALUES = [
41     'US', 'SS', 'UL', 'SL', 'OW', 'OB', 'OL', 'UN',
42     'OB or OW', 'US or OW', 'US or SS or OW', 'FL', 'FD', 'OF', 'OD'
43 ]
44 
45 
46 def empty_value_for_VR(
47     VR: Optional[str], raw: bool = False
48 ) -> Union[bytes, List[str], str, None, PersonName]:
49     """"""Return the value for an empty element for `VR`.
50 
51     .. versionadded:: 1.4
52 
53     The behavior of this property depends on the setting of
54     :attr:`config.use_none_as_empty_value`. If that is set to ``True``,
55     an empty value is represented by ``None`` (except for VR 'SQ'), otherwise
56     it depends on `VR`. For text VRs (this includes 'AE', 'AS', 'CS', 'DA',
57     'DT', 'LO', 'LT', 'PN', 'SH', 'ST', 'TM', 'UC', 'UI', 'UR' and 'UT') an
58     empty string is used as empty value representation, for all other VRs
59     except 'SQ', ``None``. For empty sequence values (VR 'SQ') an empty list
60     is used in all cases.
61     Note that this is used only if decoding the element - it is always
62     possible to set the value to another empty value representation,
63     which will be preserved during the element object lifetime.
64 
65     Parameters
66     ----------
67     VR : str or None
68         The VR of the corresponding element.
69     raw : bool, optional
70         If ``True``, returns the value for a :class:`RawDataElement`,
71         otherwise for a :class:`DataElement`
72 
73     Returns
74     -------
75     str or bytes or None or list
76         The value a data element with `VR` is assigned on decoding
77         if it is empty.
78     """"""
79     if VR == 'SQ':
80         return b'' if raw else []
81 
82     if config.use_none_as_empty_text_VR_value:
83         return None
84 
85     if VR == 'PN':
86         return b'' if raw else PersonName('')
87 
88     if VR in (
89         'AE', 'AS', 'CS', 'DA', 'DT', 'LO', 'LT', 'SH', 'ST', 'TM',
90         'UC', 'UI', 'UR', 'UT'
91     ):
92         return b'' if raw else ''
93 
94     return None
95 
96 
97 def _is_bytes(val: object) -> bool:
98     """"""Return True only if `val` is of type `bytes`.""""""
99     return isinstance(val, bytes)
100 
101 
102 # double '\' because it is used as escape chr in Python
103 _backslash_str = ""\\""
104 _backslash_byte = b""\\""
105 
106 
107 _DataElement = TypeVar(""_DataElement"", bound=""DataElement"")
108 _Dataset = TypeVar(""_Dataset"", bound=""Dataset"")
109 
110 
111 class DataElement:
112     """"""Contain and manipulate a DICOM Element.
113 
114     Examples
115     --------
116 
117     While its possible to create a new :class:`DataElement` directly and add
118     it to a :class:`~pydicom.dataset.Dataset`:
119 
120     >>> from pydicom import Dataset
121     >>> elem = DataElement(0x00100010, 'PN', 'CITIZEN^Joan')
122     >>> ds = Dataset()
123     >>> ds.add(elem)
124 
125     Its far more convenient to use a :class:`~pydicom.dataset.Dataset`
126     to add a new :class:`DataElement`, as the VR and tag are determined
127     automatically from the DICOM dictionary:
128 
129     >>> ds = Dataset()
130     >>> ds.PatientName = 'CITIZEN^Joan'
131 
132     Empty DataElement objects (e.g. with VM = 0) show an empty string as
133     value for text VRs and `None` for non-text (binary) VRs:
134 
135     >>> ds = Dataset()
136     >>> ds.PatientName = None
137     >>> ds.PatientName
138     ''
139 
140     >>> ds.BitsAllocated = None
141     >>> ds.BitsAllocated
142 
143     >>> str(ds.BitsAllocated)
144     'None'
145 
146     Attributes
147     ----------
148     descripWidth : int
149         For string display, this is the maximum width of the description
150         field (default ``35``).
151     is_undefined_length : bool
152         Indicates whether the length field for the element was ``0xFFFFFFFFL``
153         (ie undefined).
154     maxBytesToDisplay : int
155         For string display, elements with values containing data which is
156         longer than this value will display ``""array of # bytes""``
157         (default ``16``).
158     showVR : bool
159         For string display, include the element's VR just before it's value
160         (default ``True``).
161     tag : pydicom.tag.BaseTag
162         The element's tag.
163     VR : str
164         The element's Value Representation.
165     """"""
166 
167     descripWidth = 35
168     maxBytesToDisplay = 16
169     showVR = True
170     is_raw = False
171 
172     def __init__(
173         self,
174         tag: Union[int, str, Tuple[int, int]],
175         VR: str,
176         value: Any,
177         file_value_tell: Optional[int] = None,
178         is_undefined_length: bool = False,
179         already_converted: bool = False
180     ) -> None:
181         """"""Create a new :class:`DataElement`.
182 
183         Parameters
184         ----------
185         tag : int or str or 2-tuple of int
186             The DICOM (group, element) tag in any form accepted by
187             :func:`~pydicom.tag.Tag` such as ``'PatientName'``,
188             ``(0x10, 0x10)``, ``0x00100010``, etc.
189         VR : str
190             The 2 character DICOM value representation (see DICOM Standard,
191             Part 5, :dcm:`Section 6.2<part05/sect_6.2.html>`).
192         value
193             The value of the data element. One of the following:
194 
195             * a single string value
196             * a number
197             * a :class:`list` or :class:`tuple` with all strings or all numbers
198             * a multi-value string with backslash separator
199         file_value_tell : int, optional
200             The byte offset to the start of the encoded element value.
201         is_undefined_length : bool
202             Used internally to store whether the length field for this element
203             was ``0xFFFFFFFF``, i.e. 'undefined length'. Default is ``False``.
204         already_converted : bool
205             Used to determine whether or not the element's value requires
206             conversion to a value with VM > 1. Default is ``False``.
207         """"""
208         if not isinstance(tag, BaseTag):
209             tag = Tag(tag)
210         self.tag = tag
211 
212         # a known tag shall only have the VR 'UN' if it has a length that
213         # exceeds the size that can be encoded in 16 bit - all other cases
214         # can be seen as an encoding error and can be corrected
215         if (
216             VR == 'UN'
217             and not tag.is_private
218             and config.replace_un_with_known_vr
219             and (is_undefined_length or value is None or len(value) < 0xffff)
220         ):
221             try:
222                 VR = dictionary_VR(tag)
223             except KeyError:
224                 pass
225 
226         self.VR = VR  # Note: you must set VR before setting value
227         if already_converted:
228             self._value = value
229         else:
230             self.value = value  # calls property setter which will convert
231         self.file_tell = file_value_tell
232         self.is_undefined_length = is_undefined_length
233         self.private_creator: Optional[str] = None
234         self.parent: Optional[""Dataset""] = None
235 
236     @classmethod
237     def from_json(
238         cls: Type[_DataElement],
239         dataset_class: Type[_Dataset],
240         tag: Union[BaseTag, int, str],
241         vr: str,
242         value: object,
243         value_key: Union[str, None],
244         bulk_data_uri_handler: Optional[
245             Union[
246                 Callable[[BaseTag, str, str], Any],
247                 Callable[[str], Any]
248             ]
249         ] = None
250     ) -> _DataElement:
251         """"""Return a :class:`DataElement` from JSON.
252 
253         .. versionadded:: 1.3
254 
255         Parameters
256         ----------
257         dataset_class : dataset.Dataset derived class
258             Class used to create sequence items.
259         tag : pydicom.tag.BaseTag, int or str
260             The data element tag.
261         vr : str
262             The data element value representation.
263         value : list
264             The data element's value(s).
265         value_key : str or None
266             Key of the data element that contains the value
267             (options: ``{""Value"", ""InlineBinary"", ""BulkDataURI""}``)
268         bulk_data_uri_handler: callable or None
269             Callable function that accepts either the tag, vr and ""BulkDataURI""
270             or just the ""BulkDataURI"" of the JSON
271             representation of a data element and returns the actual value of
272             that data element (retrieved via DICOMweb WADO-RS)
273 
274         Returns
275         -------
276         DataElement
277         """"""
278         # TODO: test wado-rs retrieve wrapper
279         converter = JsonDataElementConverter(
280             dataset_class, tag, vr, value, value_key, bulk_data_uri_handler
281         )
282         elem_value = converter.get_element_values()
283         try:
284             return cls(tag=tag, value=elem_value, VR=vr)
285         except Exception as exc:
286             raise ValueError(
287                 f""Data element '{tag}' could not be loaded from JSON: ""
288                 f""{elem_value}""
289             ) from exc
290 
291     def to_json_dict(
292         self,
293         bulk_data_element_handler: Optional[Callable[[""DataElement""], str]],
294         bulk_data_threshold: int
295     ) -> Dict[str, Any]:
296         """"""Return a dictionary representation of the :class:`DataElement`
297         conforming to the DICOM JSON Model as described in the DICOM
298         Standard, Part 18, :dcm:`Annex F<part18/chaptr_F.html>`.
299 
300         .. versionadded:: 1.4
301 
302         Parameters
303         ----------
304         bulk_data_element_handler: callable or None
305             Callable that accepts a bulk data element and returns the
306             ""BulkDataURI"" for retrieving the value of the data element
307             via DICOMweb WADO-RS
308         bulk_data_threshold: int
309             Size of base64 encoded data element above which a value will be
310             provided in form of a ""BulkDataURI"" rather than ""InlineBinary"".
311             Ignored if no bulk data handler is given.
312 
313         Returns
314         -------
315         dict
316             Mapping representing a JSON encoded data element
317         """"""
318         json_element: Dict[str, Any] = {'vr': self.VR}
319         if self.VR in jsonrep.BINARY_VR_VALUES:
320             if not self.is_empty:
321                 binary_value = self.value
322                 encoded_value = base64.b64encode(binary_value).decode('utf-8')
323                 if (
324                     bulk_data_element_handler is not None
325                     and len(encoded_value) > bulk_data_threshold
326                 ):
327                     json_element['BulkDataURI'] = (
328                         bulk_data_element_handler(self)
329                     )
330                 else:
331                     logger.info(
332                         f""encode bulk data element '{self.name}' inline""
333                     )
334                     json_element['InlineBinary'] = encoded_value
335         elif self.VR == 'SQ':
336             # recursive call to get sequence item JSON dicts
337             value = [
338                 ds.to_json(
339                     bulk_data_element_handler=bulk_data_element_handler,
340                     bulk_data_threshold=bulk_data_threshold,
341                     dump_handler=lambda d: d
342                 )
343                 for ds in self.value
344             ]
345             json_element['Value'] = value
346         elif self.VR == 'PN':
347             if not self.is_empty:
348                 elem_value = []
349                 if self.VM > 1:
350                     value = self.value
351                 else:
352                     value = [self.value]
353                 for v in value:
354                     comps = {'Alphabetic': v.components[0]}
355                     if len(v.components) > 1:
356                         comps['Ideographic'] = v.components[1]
357                     if len(v.components) > 2:
358                         comps['Phonetic'] = v.components[2]
359                     elem_value.append(comps)
360                 json_element['Value'] = elem_value
361         elif self.VR == 'AT':
362             if not self.is_empty:
363                 value = self.value
364                 if self.VM == 1:
365                     value = [value]
366                 json_element['Value'] = [format(v, '08X') for v in value]
367         else:
368             if not self.is_empty:
369                 if self.VM > 1:
370                     value = self.value
371                 else:
372                     value = [self.value]
373                 json_element['Value'] = [v for v in value]
374         if 'Value' in json_element:
375             json_element['Value'] = jsonrep.convert_to_python_number(
376                 json_element['Value'], self.VR
377             )
378         return json_element
379 
380     def to_json(
381         self,
382         bulk_data_threshold: int = 1024,
383         bulk_data_element_handler: Optional[
384             Callable[[""DataElement""], str]
385         ] = None,
386         dump_handler: Optional[
387             Callable[[Dict[Any, Any]], Dict[str, Any]]
388         ] = None
389     ) -> Dict[str, Any]:
390         """"""Return a JSON representation of the :class:`DataElement`.
391 
392         .. versionadded:: 1.3
393 
394         Parameters
395         ----------
396         bulk_data_element_handler: callable, optional
397             Callable that accepts a bulk data element and returns the
398             ""BulkDataURI"" for retrieving the value of the data element
399             via DICOMweb WADO-RS
400         bulk_data_threshold: int, optional
401             Size of base64 encoded data element above which a value will be
402             provided in form of a ""BulkDataURI"" rather than ""InlineBinary"".
403             Ignored if no bulk data handler is given.
404         dump_handler : callable, optional
405             Callable function that accepts a :class:`dict` and returns the
406             serialized (dumped) JSON string (by default uses
407             :func:`json.dumps`).
408 
409         Returns
410         -------
411         dict
412             Mapping representing a JSON encoded data element
413 
414         See also
415         --------
416         Dataset.to_json
417         """"""
418         if dump_handler is None:
419             def json_dump(d):
420                 return json.dumps(d, sort_keys=True)
421 
422             dump_handler = json_dump
423 
424         return dump_handler(
425             self.to_json_dict(bulk_data_element_handler, bulk_data_threshold)
426         )
427 
428     @property
429     def value(self) -> Any:
430         """"""Return the element's value.""""""
431         return self._value
432 
433     @value.setter
434     def value(self, val: Any) -> None:
435         """"""Convert (if necessary) and set the value of the element.""""""
436         # Check if is a string with multiple values separated by '\'
437         # If so, turn them into a list of separate strings
438         #  Last condition covers 'US or SS' etc
439         if isinstance(val, (str, bytes)) and self.VR not in \
440                 ['UT', 'ST', 'LT', 'FL', 'FD', 'AT', 'OB', 'OW', 'OF', 'SL',
441                  'SQ', 'SS', 'UL', 'OB/OW', 'OW/OB', 'OB or OW',
442                  'OW or OB', 'UN'] and 'US' not in self.VR:
443             try:
444                 if _backslash_str in val:
445                     val = cast(str, val).split(_backslash_str)
446             except TypeError:
447                 if _backslash_byte in val:
448                     val = val.split(_backslash_byte)
449         self._value = self._convert_value(val)
450 
451     @property
452     def VM(self) -> int:
453         """"""Return the value multiplicity of the element as :class:`int`.""""""
454         if self.value is None:
455             return 0
456         if isinstance(self.value, (str, bytes, PersonName)):
457             return 1 if self.value else 0
458         try:
459             iter(self.value)
460         except TypeError:
461             return 1
462         return len(self.value)
463 
464     @property
465     def is_empty(self) -> bool:
466         """"""Return ``True`` if the element has no value.
467 
468         .. versionadded:: 1.4
469         """"""
470         return self.VM == 0
471 
472     @property
473     def empty_value(self) -> Union[bytes, List[str], None, str, PersonName]:
474         """"""Return the value for an empty element.
475 
476         .. versionadded:: 1.4
477 
478         See :func:`empty_value_for_VR` for more information.
479 
480         Returns
481         -------
482         str or None
483             The value this data element is assigned on decoding if it is empty.
484         """"""
485         return empty_value_for_VR(self.VR)
486 
487     def clear(self) -> None:
488         """"""Clears the value, e.g. sets it to the configured empty value.
489 
490         .. versionadded:: 1.4
491 
492         See :func:`empty_value_for_VR`.
493         """"""
494         self._value = self.empty_value
495 
496     def _convert_value(self, val: Any) -> Any:
497         """"""Convert `val` to an appropriate type and return the result.
498 
499         Uses the element's VR in order to determine the conversion method and
500         resulting type.
501         """"""
502         if self.VR == 'SQ':  # a sequence - leave it alone
503             from pydicom.sequence import Sequence
504             if isinstance(val, Sequence):
505                 return val
506             else:
507                 return Sequence(val)
508 
509         # if the value is a list, convert each element
510         try:
511             val.append
512         except AttributeError:  # not a list
513             return self._convert(val)
514         else:
515             return MultiValue(self._convert, val)
516 
517     def _convert(self, val: Any) -> Any:
518         """"""Convert `val` to an appropriate type for the element's VR.""""""
519         # If the value is a byte string and has a VR that can only be encoded
520         # using the default character repertoire, we convert it to a string
521         # here to allow for byte string input in these cases
522         if _is_bytes(val) and self.VR in (
523                 'AE', 'AS', 'CS', 'DA', 'DS', 'DT', 'IS', 'TM', 'UI', 'UR'):
524             val = val.decode()
525 
526         if self.VR == 'IS':
527             return pydicom.valuerep.IS(val)
528         elif self.VR == 'DA' and config.datetime_conversion:
529             return pydicom.valuerep.DA(val)
530         elif self.VR == 'DS':
531             return pydicom.valuerep.DS(val)
532         elif self.VR == 'DT' and config.datetime_conversion:
533             return pydicom.valuerep.DT(val)
534         elif self.VR == 'TM' and config.datetime_conversion:
535             return pydicom.valuerep.TM(val)
536         elif self.VR == ""UI"":
537             return UID(val) if val is not None else None
538         elif self.VR == ""PN"":
539             return PersonName(val)
540         elif self.VR == ""AT"" and (val == 0 or val):
541             return val if isinstance(val, BaseTag) else Tag(val)
542         # Later may need this for PersonName as for UI,
543         #    but needs more thought
544         # elif self.VR == ""PN"":
545         #    return PersonName(val)
546         else:  # is either a string or a type 2 optionally blank string
547             return val  # this means a ""numeric"" value could be empty string """"
548         # except TypeError:
549             # print ""Could not convert value '%s' to VR '%s' in tag %s"" \
550             # % (repr(val), self.VR, self.tag)
551         # except ValueError:
552             # print ""Could not convert value '%s' to VR '%s' in tag %s"" \
553             # % (repr(val), self.VR, self.tag)
554 
555     def __eq__(self, other: Any) -> bool:
556         """"""Compare `self` and `other` for equality.
557 
558         Returns
559         -------
560         bool
561             The result if `self` and `other` are the same class
562         NotImplemented
563             If `other` is not the same class as `self` then returning
564             :class:`NotImplemented` delegates the result to
565             ``superclass.__eq__(subclass)``.
566         """"""
567         # Faster result if same object
568         if other is self:
569             return True
570 
571         if isinstance(other, self.__class__):
572             if self.tag != other.tag or self.VR != other.VR:
573                 return False
574 
575             # tag and VR match, now check the value
576             if config.have_numpy and isinstance(self.value, numpy.ndarray):
577                 return (len(self.value) == len(other.value)
578                         and numpy.allclose(self.value, other.value))
579             else:
580                 return self.value == other.value
581 
582         return NotImplemented
583 
584     def __ne__(self, other: Any) -> bool:
585         """"""Compare `self` and `other` for inequality.""""""
586         return not (self == other)
587 
588     def __str__(self) -> str:
589         """"""Return :class:`str` representation of the element.""""""
590         repVal = self.repval or ''
591         if self.showVR:
592             s = ""%s %-*s %s: %s"" % (str(self.tag), self.descripWidth,
593                                     self.description()[:self.descripWidth],
594                                     self.VR, repVal)
595         else:
596             s = ""%s %-*s %s"" % (str(self.tag), self.descripWidth,
597                                 self.description()[:self.descripWidth], repVal)
598         return s
599 
600     @property
601     def repval(self) -> str:
602         """"""Return a :class:`str` representation of the element's value.""""""
603         long_VRs = {""OB"", ""OD"", ""OF"", ""OW"", ""UN"", ""UT""}
604         if set(self.VR.split("" or "")) & long_VRs:
605             try:
606                 length = len(self.value)
607             except TypeError:
608                 pass
609             else:
610                 if length > self.maxBytesToDisplay:
611                     return ""Array of %d elements"" % length
612         if self.VM > self.maxBytesToDisplay:
613             repVal = ""Array of %d elements"" % self.VM
614         elif isinstance(self.value, UID):
615             repVal = self.value.name
616         else:
617             repVal = repr(self.value)  # will tolerate unicode too
618         return repVal
619 
620     def __getitem__(self, key: int) -> Any:
621         """"""Return the item at `key` if the element's value is indexable.""""""
622         try:
623             return self.value[key]
624         except TypeError:
625             raise TypeError(""DataElement value is unscriptable ""
626                             ""(not a Sequence)"")
627 
628     @property
629     def name(self) -> str:
630         """"""Return the DICOM dictionary name for the element as :class:`str`.
631 
632         For officially registered DICOM Data Elements this will be the *Name*
633         as given in :dcm:`Table 6-1<part06/chapter_6.html#table_6-1>`.
634         For private elements known to *pydicom*
635         this will be the *Name* in the format ``'[name]'``. For unknown
636         private elements this will be ``'Private Creator'``. For unknown
637         elements this will return an empty string ``''``.
638         """"""
639         return self.description()
640 
641     def description(self) -> str:
642         """"""Return the DICOM dictionary name for the element as :class:`str`.""""""
643         if self.tag.is_private:
644             name = ""Private tag data""  # default
645             if self.private_creator:
646                 try:
647                     # If have name from private dictionary, use it, but
648                     #   but put in square brackets so is differentiated,
649                     #   and clear that cannot access it by name
650                     name = private_dictionary_description(
651                         self.tag, self.private_creator)
652                     name = ""[%s]"" % (name)
653                 except KeyError:
654                     pass
655             elif self.tag.element >> 8 == 0:
656                 name = ""Private Creator""
657         elif dictionary_has_tag(self.tag) or repeater_has_tag(self.tag):
658             name = dictionary_description(self.tag)
659 
660         # implied Group Length dicom versions < 3
661         elif self.tag.element == 0:
662             name = ""Group Length""
663         else:
664             name = """"
665         return name
666 
667     @property
668     def is_private(self) -> bool:
669         """"""Return ``True`` if the element's tag is private.
670 
671         .. versionadded:: 2.1
672         """"""
673         return self.tag.is_private
674 
675     @property
676     def is_retired(self) -> bool:
677         """"""Return the element's retired status as :class:`bool`.
678 
679         For officially registered DICOM Data Elements this will be ``True`` if
680         the retired status as given in the DICOM Standard, Part 6,
681         :dcm:`Table 6-1<part06/chapter_6.html#table_6-1>` is 'RET'. For private
682         or unknown elements this will always be ``False``.
683         """"""
684         if dictionary_has_tag(self.tag):
685             return dictionary_is_retired(self.tag)
686 
687         return False
688 
689     @property
690     def keyword(self) -> str:
691         """"""Return the element's keyword (if known) as :class:`str`.
692 
693         For officially registered DICOM Data Elements this will be the
694         *Keyword* as given in
695         :dcm:`Table 6-1<part06/chapter_6.html#table_6-1>`. For private or
696         unknown elements this will return an empty string ``''``.
697         """"""
698         if dictionary_has_tag(self.tag):
699             return dictionary_keyword(self.tag)
700 
701         return ''
702 
703     def __repr__(self) -> str:
704         """"""Return the representation of the element.""""""
705         if self.VR == ""SQ"":
706             return repr(self.value)
707 
708         return str(self)
709 
710 
711 class RawDataElement(NamedTuple):
712     """"""Container for the data from a raw (mostly) undecoded element.""""""
713     tag: BaseTag
714     VR: Optional[str]
715     length: int
716     value: Optional[bytes]
717     value_tell: int
718     is_implicit_VR: bool
719     is_little_endian: bool
720     is_raw: bool = True
721 
722 
723 # The first and third values of the following elements are always US
724 #   even if the VR is SS (PS3.3 C.7.6.3.1.5, C.11.1, C.11.2).
725 # (0028,1101-1103) RGB Palette Color LUT Descriptor
726 # (0028,3002) LUT Descriptor
727 _LUT_DESCRIPTOR_TAGS = (0x00281101, 0x00281102, 0x00281103, 0x00283002)
728 
729 
730 def _private_vr_for_tag(ds: Optional[""Dataset""], tag: BaseTag) -> str:
731     """"""Return the VR for a known private tag, otherwise ""UN"".
732 
733     Parameters
734     ----------
735     ds : Dataset, optional
736         The dataset needed for the private creator lookup.
737         If not given, ""UN"" is returned.
738     tag : BaseTag
739         The private tag to lookup. The caller has to ensure that the
740         tag is private.
741 
742     Returns
743     -------
744     str
745         ""LO"" if the tag is a private creator, the VR of the private tag if
746         found in the private dictionary, or ""UN"".
747     """"""
748     if tag.is_private_creator:
749         return ""LO""
750     # invalid private tags are handled as UN
751     if ds is not None and (tag.element & 0xff00):
752         private_creator_tag = tag.group << 16 | (tag.element >> 8)
753         private_creator = ds.get(private_creator_tag, """")
754         if private_creator:
755             try:
756                 return private_dictionary_VR(tag, private_creator.value)
757             except KeyError:
758                 pass
759     return ""UN""
760 
761 
762 def DataElement_from_raw(
763     raw_data_element: RawDataElement,
764     encoding: Optional[Union[str, MutableSequence[str]]] = None,
765     dataset: Optional[""Dataset""] = None
766 ) -> DataElement:
767     """"""Return a :class:`DataElement` created from `raw_data_element`.
768 
769     Parameters
770     ----------
771     raw_data_element : RawDataElement
772         The raw data to convert to a :class:`DataElement`.
773     encoding : str or list of str, optional
774         The character encoding of the raw data.
775     dataset : Dataset, optional
776         If given, used to resolve the VR for known private tags.
777 
778     Returns
779     -------
780     DataElement
781 
782     Raises
783     ------
784     KeyError
785         If `raw_data_element` belongs to an unknown non-private tag and
786         `config.enforce_valid_values` is set.
787     """"""
788     # XXX buried here to avoid circular import
789     # filereader->Dataset->convert_value->filereader
790     # (for SQ parsing)
791 
792     from pydicom.values import convert_value
793     raw = raw_data_element
794 
795     # If user has hooked into conversion of raw values, call his/her routine
796     if config.data_element_callback:
797         raw = config.data_element_callback(
798             raw_data_element,
799             encoding=encoding,
800             **config.data_element_callback_kwargs
801         )
802 
803     VR = raw.VR
804     if VR is None:  # Can be if was implicit VR
805         try:
806             VR = dictionary_VR(raw.tag)
807         except KeyError:
808             # just read the bytes, no way to know what they mean
809             if raw.tag.is_private:
810                 # for VR for private tags see PS3.5, 6.2.2
811                 VR = _private_vr_for_tag(dataset, raw.tag)
812 
813             # group length tag implied in versions < 3.0
814             elif raw.tag.element == 0:
815                 VR = 'UL'
816             else:
817                 msg = ""Unknown DICOM tag {0:s}"".format(str(raw.tag))
818                 if config.enforce_valid_values:
819                     msg += "" can't look up VR""
820                     raise KeyError(msg)
821                 else:
822                     VR = 'UN'
823                     msg += "" - setting VR to 'UN'""
824                     warnings.warn(msg)
825     elif VR == 'UN' and config.replace_un_with_known_vr:
826         # handle rare case of incorrectly set 'UN' in explicit encoding
827         # see also DataElement.__init__()
828         if raw.tag.is_private:
829             VR = _private_vr_for_tag(dataset, raw.tag)
830         elif raw.value is None or len(raw.value) < 0xffff:
831             try:
832                 VR = dictionary_VR(raw.tag)
833             except KeyError:
834                 pass
835     try:
836         value = convert_value(VR, raw, encoding)
837     except NotImplementedError as e:
838         raise NotImplementedError(""{0:s} in tag {1!r}"".format(str(e), raw.tag))
839     except BytesLengthException as e:
840         message = (f""{e} This occurred while trying to parse ""
841                    f""{raw.tag} according to VR '{VR}'."")
842         if config.convert_wrong_length_to_UN:
843             warnings.warn(f""{message} Setting VR to 'UN'."")
844             VR = ""UN""
845             value = raw.value
846         else:
847             raise BytesLengthException(
848                 f""{message} To replace this error with a warning set ""
849                 ""pydicom.config.convert_wrong_length_to_UN = True.""
850             )
851 
852     if raw.tag in _LUT_DESCRIPTOR_TAGS and value:
853         # We only fix the first value as the third value is 8 or 16
854         try:
855             if value[0] < 0:
856                 value[0] += 65536
857         except TypeError:
858             pass
859 
860     return DataElement(raw.tag, VR, value, raw.value_tell,
861                        raw.length == 0xFFFFFFFF, already_converted=True)
862 
[end of pydicom/dataelem.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pydicom/pydicom,f909c76e31f759246cec3708dadd173c5d6e84b1,"Error : a bytes-like object is required, not 'MultiValue'
Hello,

I am getting following error while updating the tag LongTrianglePointIndexList (0066,0040),
**TypeError: a bytes-like object is required, not 'MultiValue'**

I noticed that the error  gets produced only when the VR is given as ""OL"" , works fine with ""OB"", ""OF"" etc.

sample code (assume 'lineSeq' is the dicom dataset sequence):
```python
import pydicom
import array
data=list(range(1,10))
data=array.array('H', indexData).tostring()  # to convert to unsigned short
lineSeq.add_new(0x00660040, 'OL', data)   
ds.save_as(""mydicom"")
```
outcome: **TypeError: a bytes-like object is required, not 'MultiValue'**

using version - 2.0.0.0

Any help is appreciated.

Thank you
","Also tried following code to get the byte string, but same error.
1. data=array.array('L', indexData).tostring()  # to convert to long -> same error
2. data=array.array('Q', indexData).tostring()  # to convert to long long -> same error


O* VRs should be `bytes`. Use `array.tobytes()` instead of `tostring()`?

Also, in the future if have an issue it's much more helpful if you post the full traceback rather than the error since we can look at it to figure out where in the code the exception is occurring.

It would also help if you posted the version of Python you're using. 

This works fine for me with Python 3.9 and pydicom 2.1.2:
```python
from pydicom import Dataset
import array

arr = array.array('H', range(10))
ds = Dataset()
ds.is_little_endian = True
ds.is_implicit_VR = False
ds.LongTrianglePointIndexList = arr.tobytes()
print(ds[""LongTrianglePointIndexList""].VR)  # 'OL'
ds.save_as('temp.dcm')
```
This also works fine:
```python
ds = Dataset()
ds.add_new(0x00660040, 'OL', arr.tobytes())
```
Thank you for the answer.
Unfortunately the error still persists with above code.
Please find the attached detailed error.
[error.txt](https://github.com/pydicom/pydicom/files/6661451/error.txt)

One more information is that the 'ds' is actually read from a file in the disk (ds=pydicom.read_file(filename)). 
and this byte array is stored under the following sequence
ds[0x0066,0x0002][0][0x0066,0x0013][0][0x0066,0x0028][0][0x0066,0x0040] = arr.tobytes()

pydicom - 2.0.0.0
python - 3.6.4

Thank you.
Could you post a minimal code sample that reproduces the issue please?

If you're using something like this:
`ds[0x0066,0x0002][0][0x0066,0x0013][0][0x0066,0x0028][0][0x0066,0x0040] = arr.tobytes()`

Then you're missing the `.value` assignment:
`ds[0x0066,0x0002][0][0x0066,0x0013][0][0x0066,0x0028][0][0x0066,0x0040].value = arr.tobytes()`
Hello,
above code line I just mentioned to give an idea where the actual data is stored (tree level).

Please find the actual code used below,
```python
import pydicom
from pydicom.sequence import Sequence
from pydicom.dataelem import DataElement
from pydicom.dataset import Dataset

ds = pydicom.read_file(filename)
surfaceSeq= ds[0x0066,0x0002]

#// read existing sequence items in the dataset
seqlist=[]
for n in surfaceSeq:
    seqlist.append(n)

newDs = Dataset()
 
surfaceMeshPrimitiveSq = Dataset()
lineSeq = Dataset()
indexData = list(range(1,100))
indexData = array.array('H', indexData)
indexData = indexData.tobytes()
lineSeq.add_new(0x00660040, 'OL', indexData) 
surfaceMeshPrimitiveSq.add_new(0x00660028, 'SQ', [lineSeq])
newDs.add_new(0x00660013, 'SQ', [surfaceMeshPrimitiveSq])

#add the new sequnce item to the list
seqlist.append(newDs)
ds[0x0066,0x0002] = DataElement(0x00660002,""SQ"",seqlist)
ds.save_as(filename)
```
OK, I can reproduce with:
```python

import array

from pydicom import Dataset
from pydicom.uid import ExplicitVRLittleEndian

ds = Dataset()
ds.file_meta = Dataset()
ds.file_meta.TransferSyntaxUID = ExplicitVRLittleEndian

b = array.array('H', range(100)).tobytes()

ds.LongPrimitivePointIndexList = b
ds.save_as('1421.dcm')
```
And `print(ds)` gives:
```
(0066, 0040) Long Primitive Point Index List     OL: [b'\x00\x00\x01\x00\x02\x00\x03\x00\x04\x00\x05\x00\x06\x00\x07\x00\x08\x00\t\x00\n\x00\x0b\x00\x0c\x00\r\x00\x0e\x00\x0f\x00\x10\x00\x11\x00\x12\x00\x13\x00\x14\x00\x15\x00\x16\x00\x17\x00\x18\x00\x19\x00\x1a\x00\x1b\x00\x1c\x00\x1d\x00\x1e\x00\x1f\x00 \x00!\x00""\x00#\x00$\x00%\x00&\x00\'\x00(\x00)\x00*\x00+\x00,\x00-\x00.\x00/\x000\x001\x002\x003\x004\x005\x006\x007\x008\x009\x00:\x00;\x00<\x00=\x00>\x00?\x00@\x00A\x00B\x00C\x00D\x00E\x00F\x00G\x00H\x00I\x00J\x00K\x00L\x00M\x00N\x00O\x00P\x00Q\x00R\x00S\x00T\x00U\x00V\x00W\x00X\x00Y\x00Z\x00[\x00', b'\x00]\x00^\x00_\x00`\x00a\x00b\x00c\x00']
```
I think this is because the byte value is hitting the hex for the backslash character during assignment. Ouch, that's kinda nasty.",2021-06-16T09:47:08Z,"<patch>
diff --git a/pydicom/dataelem.py b/pydicom/dataelem.py
--- a/pydicom/dataelem.py
+++ b/pydicom/dataelem.py
@@ -433,13 +433,24 @@ def value(self) -> Any:
     @value.setter
     def value(self, val: Any) -> None:
         """"""Convert (if necessary) and set the value of the element.""""""
+        # Ignore backslash characters in these VRs, based on:
+        # * Which str VRs can have backslashes in Part 5, Section 6.2
+        # * All byte VRs
+        exclusions = [
+            'LT', 'OB', 'OD', 'OF', 'OL', 'OV', 'OW', 'ST', 'UN', 'UT',
+            'OB/OW', 'OW/OB', 'OB or OW', 'OW or OB',
+            # Probably not needed
+            'AT', 'FD', 'FL', 'SQ', 'SS', 'SL', 'UL',
+        ]
+
         # Check if is a string with multiple values separated by '\'
         # If so, turn them into a list of separate strings
         #  Last condition covers 'US or SS' etc
-        if isinstance(val, (str, bytes)) and self.VR not in \
-                ['UT', 'ST', 'LT', 'FL', 'FD', 'AT', 'OB', 'OW', 'OF', 'SL',
-                 'SQ', 'SS', 'UL', 'OB/OW', 'OW/OB', 'OB or OW',
-                 'OW or OB', 'UN'] and 'US' not in self.VR:
+        if (
+            isinstance(val, (str, bytes))
+            and self.VR not in exclusions
+            and 'US' not in self.VR
+        ):
             try:
                 if _backslash_str in val:
                     val = cast(str, val).split(_backslash_str)

</patch>","diff --git a/pydicom/tests/test_valuerep.py b/pydicom/tests/test_valuerep.py
--- a/pydicom/tests/test_valuerep.py
+++ b/pydicom/tests/test_valuerep.py
@@ -1546,3 +1546,16 @@ def test_set_value(vr, pytype, vm0, vmN, keyword):
     elem = ds[keyword]
     assert elem.value == list(vmN)
     assert list(vmN) == elem.value
+
+
+@pytest.mark.parametrize(""vr, pytype, vm0, vmN, keyword"", VALUE_REFERENCE)
+def test_assigning_bytes(vr, pytype, vm0, vmN, keyword):
+    """"""Test that byte VRs are excluded from the backslash check.""""""
+    if pytype == bytes:
+        ds = Dataset()
+        value = b""\x00\x01"" + b""\\"" + b""\x02\x03""
+        setattr(ds, keyword, value)
+        elem = ds[keyword]
+        assert elem.VR == vr
+        assert elem.value == value
+        assert elem.VM == 1
",2.1,"[""pydicom/tests/test_valuerep.py::test_assigning_bytes[OD-bytes-vm017-vmN17-DoubleFloatPixelData]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[OL-bytes-vm019-vmN19-TrackPointIndexList]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[OV-bytes-vm020-vmN20-SelectorOVValue]""]","[""pydicom/tests/test_valuerep.py::TestTM::test_pickling"", ""pydicom/tests/test_valuerep.py::TestTM::test_pickling_tm_from_time"", ""pydicom/tests/test_valuerep.py::TestTM::test_str_and_repr"", ""pydicom/tests/test_valuerep.py::TestTM::test_new_empty_str"", ""pydicom/tests/test_valuerep.py::TestTM::test_new_str_conversion"", ""pydicom/tests/test_valuerep.py::TestTM::test_new_obj_conversion"", ""pydicom/tests/test_valuerep.py::TestTM::test_comparison"", ""pydicom/tests/test_valuerep.py::TestTM::test_time_behavior"", ""pydicom/tests/test_valuerep.py::TestDT::test_pickling"", ""pydicom/tests/test_valuerep.py::TestDT::test_pickling_with_timezone"", ""pydicom/tests/test_valuerep.py::TestDT::test_pickling_dt_from_datetime"", ""pydicom/tests/test_valuerep.py::TestDT::test_pickling_dt_from_datetime_with_timezone"", ""pydicom/tests/test_valuerep.py::TestDT::test_new_empty_str"", ""pydicom/tests/test_valuerep.py::TestDT::test_new_obj_conversion"", ""pydicom/tests/test_valuerep.py::TestDT::test_new_str_conversion"", ""pydicom/tests/test_valuerep.py::TestDT::test_str_and_repr"", ""pydicom/tests/test_valuerep.py::TestDT::test_comparison"", ""pydicom/tests/test_valuerep.py::TestDT::test_datetime_behavior"", ""pydicom/tests/test_valuerep.py::TestDA::test_pickling"", ""pydicom/tests/test_valuerep.py::TestDA::test_new_obj_conversion"", ""pydicom/tests/test_valuerep.py::TestDA::test_str_and_repr"", ""pydicom/tests/test_valuerep.py::TestDA::test_comparison"", ""pydicom/tests/test_valuerep.py::TestDA::test_date_behavior"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_valid[1]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_valid[3.14159265358979]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_valid[-1234.456e78]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_valid[1.234E-5]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_valid[1.234E+5]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_valid[+1]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_valid["", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_valid[42"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_invalid[nan]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_invalid[-inf]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_invalid[3.141592653589793]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_invalid[1,000]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_invalid[1"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_invalid[127.0.0.1]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_invalid[1.e]"", ""pydicom/tests/test_valuerep.py::TestIsValidDS::test_invalid[]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[1.0-1.0]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[0.0-0.0]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[-0.0--0.0]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[0.123-0.123]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[-0.321--0.321]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[1e-05-1e-05]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[3.141592653589793-3.14159265358979]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[-3.141592653589793--3.1415926535898]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[5.385940192876374e-07-5.3859401929e-07]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[-5.385940192876374e-07--5.385940193e-07]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[12342534378.125532-12342534378.1255]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[64070869985876.78-64070869985876.8]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_auto_format[1.7976931348623157e+308-1.797693135e+308]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-101]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-100]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[100]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[101]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-16]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-15]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-14]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-13]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-12]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-11]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-10]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-9]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-8]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-7]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-6]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-5]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-4]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-3]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-2]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[-1]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[0]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[1]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[2]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[3]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[4]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[5]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[6]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[7]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[8]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[9]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[10]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[11]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[12]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[13]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[14]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[15]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_pi[16]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-101]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-100]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[100]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[101]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-16]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-15]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-14]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-13]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-12]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-11]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-10]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-9]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-8]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-7]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-6]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-5]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-4]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-3]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-2]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[-1]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[0]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[1]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[2]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[3]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[4]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[5]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[6]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[7]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[8]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[9]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[10]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[11]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[12]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[13]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[14]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[15]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_powers_of_negative_pi[16]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_invalid[nan0]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_invalid[nan1]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_invalid[-inf]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_invalid[inf]"", ""pydicom/tests/test_valuerep.py::TestTruncateFloatForDS::test_wrong_type"", ""pydicom/tests/test_valuerep.py::TestDS::test_empty_value"", ""pydicom/tests/test_valuerep.py::TestDS::test_float_values"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_pickling"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_new_empty"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_str_value"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_str"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_repr"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_DSfloat"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_DSdecimal"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_auto_format[True]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_auto_format[False]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_auto_format_from_invalid_DS"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_auto_format_invalid_string[True]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_auto_format_invalid_string[False]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_auto_format_valid_string[True]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_auto_format_valid_string[False]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_enforce_valid_values_length"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_DSfloat_auto_format"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_enforce_valid_values_value[nan0]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_enforce_valid_values_value[-nan]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_enforce_valid_values_value[inf0]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_enforce_valid_values_value[-inf0]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_enforce_valid_values_value[nan1]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_enforce_valid_values_value[nan2]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_enforce_valid_values_value[-inf1]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_enforce_valid_values_value[inf1]"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_comparison_operators"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_hash"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_pickling"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_float_value"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_new_empty"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_str_value"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_DSfloat"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_DSdecimal"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_repr"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_auto_format[True]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_auto_format[False]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_auto_format_from_invalid_DS"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_auto_format_invalid_string[True]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_auto_format_invalid_string[False]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_enforce_valid_values_value[NaN]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_enforce_valid_values_value[-NaN]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_enforce_valid_values_value[Infinity]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_enforce_valid_values_value[-Infinity]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_enforce_valid_values_value[val4]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_enforce_valid_values_value[val5]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_enforce_valid_values_value[val6]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_enforce_valid_values_value[val7]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_auto_format_valid_string[True]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_auto_format_valid_string[False]"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_DSdecimal_auto_format"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_comparison_operators"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_hash"", ""pydicom/tests/test_valuerep.py::TestIS::test_empty_value"", ""pydicom/tests/test_valuerep.py::TestIS::test_str_value"", ""pydicom/tests/test_valuerep.py::TestIS::test_valid_value"", ""pydicom/tests/test_valuerep.py::TestIS::test_invalid_value"", ""pydicom/tests/test_valuerep.py::TestIS::test_pickling"", ""pydicom/tests/test_valuerep.py::TestIS::test_longint"", ""pydicom/tests/test_valuerep.py::TestIS::test_overflow"", ""pydicom/tests/test_valuerep.py::TestIS::test_str"", ""pydicom/tests/test_valuerep.py::TestIS::test_repr"", ""pydicom/tests/test_valuerep.py::TestIS::test_comparison_operators"", ""pydicom/tests/test_valuerep.py::TestIS::test_hash"", ""pydicom/tests/test_valuerep.py::TestBadValueRead::test_read_bad_value_in_VR_default"", ""pydicom/tests/test_valuerep.py::TestBadValueRead::test_read_bad_value_in_VR_enforce_valid_value"", ""pydicom/tests/test_valuerep.py::TestDecimalString::test_DS_decimal_set"", ""pydicom/tests/test_valuerep.py::TestDecimalString::test_valid_decimal_strings"", ""pydicom/tests/test_valuerep.py::TestDecimalString::test_invalid_decimal_strings"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_last_first"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_copy"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_three_component"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_formatting"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_kr"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_bytes"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_bytes_comp_delimiter"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_bytes_caret_delimiter"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_unicode"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_not_equal"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_encoding_carried"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_hash"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_next"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_iterator"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_contains"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_length"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_kr_from_bytes"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_kr_from_unicode"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_jp_from_bytes"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_jp_from_unicode"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_veterinary"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_with_separator"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_from_named_components_with_separator_from_bytes"", ""pydicom/tests/test_valuerep.py::TestDateTime::test_date"", ""pydicom/tests/test_valuerep.py::TestDateTime::test_date_time"", ""pydicom/tests/test_valuerep.py::TestDateTime::test_time"", ""pydicom/tests/test_valuerep.py::test_person_name_unicode_warns"", ""pydicom/tests/test_valuerep.py::test_set_value[AE-str-vm00-vmN0-Receiver]"", ""pydicom/tests/test_valuerep.py::test_set_value[AS-str-vm01-vmN1-PatientAge]"", ""pydicom/tests/test_valuerep.py::test_set_value[AT-int-vm02-vmN2-OffendingElement]"", ""pydicom/tests/test_valuerep.py::test_set_value[CS-str-vm03-vmN3-QualityControlSubject]"", ""pydicom/tests/test_valuerep.py::test_set_value[DA-str-vm04-vmN4-PatientBirthDate]"", ""pydicom/tests/test_valuerep.py::test_set_value[DS-str-vm05-vmN5-PatientWeight]"", ""pydicom/tests/test_valuerep.py::test_set_value[DS-int-vm06-vmN6-PatientWeight]"", ""pydicom/tests/test_valuerep.py::test_set_value[DS-float-vm07-vmN7-PatientWeight]"", ""pydicom/tests/test_valuerep.py::test_set_value[DT-str-vm08-vmN8-AcquisitionDateTime]"", ""pydicom/tests/test_valuerep.py::test_set_value[FD-float-vm09-vmN9-RealWorldValueLUTData]"", ""pydicom/tests/test_valuerep.py::test_set_value[FL-float-vm010-vmN10-VectorAccuracy]"", ""pydicom/tests/test_valuerep.py::test_set_value[IS-str-vm011-vmN11-BeamNumber]"", ""pydicom/tests/test_valuerep.py::test_set_value[IS-int-vm012-vmN12-BeamNumber]"", ""pydicom/tests/test_valuerep.py::test_set_value[IS-float-vm013-vmN13-BeamNumber]"", ""pydicom/tests/test_valuerep.py::test_set_value[LO-str-vm014-vmN14-DataSetSubtype]"", ""pydicom/tests/test_valuerep.py::test_set_value[LT-str-vm015-vmN15-ExtendedCodeMeaning]"", ""pydicom/tests/test_valuerep.py::test_set_value[OB-bytes-vm016-vmN16-FillPattern]"", ""pydicom/tests/test_valuerep.py::test_set_value[OD-bytes-vm017-vmN17-DoubleFloatPixelData]"", ""pydicom/tests/test_valuerep.py::test_set_value[OF-bytes-vm018-vmN18-UValueData]"", ""pydicom/tests/test_valuerep.py::test_set_value[OL-bytes-vm019-vmN19-TrackPointIndexList]"", ""pydicom/tests/test_valuerep.py::test_set_value[OV-bytes-vm020-vmN20-SelectorOVValue]"", ""pydicom/tests/test_valuerep.py::test_set_value[OW-bytes-vm021-vmN21-TrianglePointIndexList]"", ""pydicom/tests/test_valuerep.py::test_set_value[PN-str-vm022-vmN22-PatientName]"", ""pydicom/tests/test_valuerep.py::test_set_value[SH-str-vm023-vmN23-CodeValue]"", ""pydicom/tests/test_valuerep.py::test_set_value[SL-int-vm024-vmN24-RationalNumeratorValue]"", ""pydicom/tests/test_valuerep.py::test_set_value[SQ-list-vm025-vmN25-BeamSequence]"", ""pydicom/tests/test_valuerep.py::test_set_value[SS-int-vm026-vmN26-SelectorSSValue]"", ""pydicom/tests/test_valuerep.py::test_set_value[ST-str-vm027-vmN27-InstitutionAddress]"", ""pydicom/tests/test_valuerep.py::test_set_value[SV-int-vm028-vmN28-SelectorSVValue]"", ""pydicom/tests/test_valuerep.py::test_set_value[TM-str-vm029-vmN29-StudyTime]"", ""pydicom/tests/test_valuerep.py::test_set_value[UC-str-vm030-vmN30-LongCodeValue]"", ""pydicom/tests/test_valuerep.py::test_set_value[UI-str-vm031-vmN31-SOPClassUID]"", ""pydicom/tests/test_valuerep.py::test_set_value[UL-int-vm032-vmN32-SimpleFrameList]"", ""pydicom/tests/test_valuerep.py::test_set_value[UN-bytes-vm033-vmN33-SelectorUNValue]"", ""pydicom/tests/test_valuerep.py::test_set_value[UR-str-vm034-vmN34-CodingSchemeURL]"", ""pydicom/tests/test_valuerep.py::test_set_value[US-int-vm035-vmN35-SourceAcquisitionBeamNumber]"", ""pydicom/tests/test_valuerep.py::test_set_value[UT-str-vm036-vmN36-StrainAdditionalInformation]"", ""pydicom/tests/test_valuerep.py::test_set_value[UV-int-vm037-vmN37-SelectorUVValue]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[AE-str-vm00-vmN0-Receiver]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[AS-str-vm01-vmN1-PatientAge]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[AT-int-vm02-vmN2-OffendingElement]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[CS-str-vm03-vmN3-QualityControlSubject]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[DA-str-vm04-vmN4-PatientBirthDate]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[DS-str-vm05-vmN5-PatientWeight]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[DS-int-vm06-vmN6-PatientWeight]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[DS-float-vm07-vmN7-PatientWeight]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[DT-str-vm08-vmN8-AcquisitionDateTime]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[FD-float-vm09-vmN9-RealWorldValueLUTData]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[FL-float-vm010-vmN10-VectorAccuracy]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[IS-str-vm011-vmN11-BeamNumber]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[IS-int-vm012-vmN12-BeamNumber]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[IS-float-vm013-vmN13-BeamNumber]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[LO-str-vm014-vmN14-DataSetSubtype]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[LT-str-vm015-vmN15-ExtendedCodeMeaning]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[OB-bytes-vm016-vmN16-FillPattern]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[OF-bytes-vm018-vmN18-UValueData]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[OW-bytes-vm021-vmN21-TrianglePointIndexList]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[PN-str-vm022-vmN22-PatientName]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[SH-str-vm023-vmN23-CodeValue]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[SL-int-vm024-vmN24-RationalNumeratorValue]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[SQ-list-vm025-vmN25-BeamSequence]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[SS-int-vm026-vmN26-SelectorSSValue]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[ST-str-vm027-vmN27-InstitutionAddress]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[SV-int-vm028-vmN28-SelectorSVValue]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[TM-str-vm029-vmN29-StudyTime]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[UC-str-vm030-vmN30-LongCodeValue]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[UI-str-vm031-vmN31-SOPClassUID]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[UL-int-vm032-vmN32-SimpleFrameList]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[UN-bytes-vm033-vmN33-SelectorUNValue]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[UR-str-vm034-vmN34-CodingSchemeURL]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[US-int-vm035-vmN35-SourceAcquisitionBeamNumber]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[UT-str-vm036-vmN36-StrainAdditionalInformation]"", ""pydicom/tests/test_valuerep.py::test_assigning_bytes[UV-int-vm037-vmN37-SelectorUVValue]""]",506ecea8f378dc687d5c504788fc78810a190b7a
pydicom__pydicom-901,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
pydicom should not define handler, formatter and log level.
The `config` module (imported when pydicom is imported) defines a handler and set the log level for the pydicom logger. This should not be the case IMO. It should be the responsibility of the client code of pydicom to configure the logging module to its convenience. Otherwise one end up having multiple logs record as soon as pydicom is imported:

Example:
```
Could not import pillow
2018-03-25 15:27:29,744 :: DEBUG :: pydicom 
  Could not import pillow
Could not import jpeg_ls
2018-03-25 15:27:29,745 :: DEBUG :: pydicom 
  Could not import jpeg_ls
Could not import gdcm
2018-03-25 15:27:29,745 :: DEBUG :: pydicom 
  Could not import gdcm
``` 
Or am I missing something?

</issue>
<code>
[start of README.md]
1 pydicom
2 =======
3 
4 [![Build Status](https://travis-ci.org/pydicom/pydicom.svg?branch=master)](https://travis-ci.org/pydicom/pydicom)
5 [![AppVeyor](https://ci.appveyor.com/api/projects/status/1vjtkr82lumnd3i7?svg=true)](https://ci.appveyor.com/project/glemaitre/pydicom)
6 [![CircleCI](https://circleci.com/gh/pydicom/pydicom/tree/master.svg?style=shield)](https://circleci.com/gh/pydicom/pydicom/tree/master)
7 [![codecov](https://codecov.io/gh/pydicom/pydicom/branch/master/graph/badge.svg)](https://codecov.io/gh/pydicom/pydicom)
8 [![Python version](https://img.shields.io/pypi/pyversions/pydicom.svg)](https://img.shields.io/pypi/pyversions/pydicom.svg)
9 [![PyPI version](https://badge.fury.io/py/pydicom.svg)](https://badge.fury.io/py/pydicom)
10 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3333768.svg)](https://doi.org/10.5281/zenodo.3333768)
11 
12 pydicom is a pure python package for working with [DICOM](http://medical.nema.org/) files.
13 It was made for inspecting and modifying DICOM data in an easy ""pythonic"" way.
14 The modifications can be written again to a new file.
15 
16 As a pure python package, pydicom can run anywhere python runs without any other requirements,
17 although [NumPy](http://www.numpy.org) is needed if manipulating pixel data.
18 
19 pydicom is not a DICOM server, and is not primarily about viewing images.
20 It is designed to let you
21 manipulate data elements in DICOM files with python code.
22 
23 Limitations -- for files with _compressed_ pixel data, pydicom can decompress
24 it (with additional libraries installed) and allow you to manipulate the data,
25 but can only store changed pixel data as uncompressed. Files can always be
26 read and saved (including compressed pixel data that has not been modified),
27 but once decompressed, modified pixel data cannot be compressed again.
28 
29 Documentation
30 -------------
31 
32 pydicom documentation is available on GitHub Pages both for the [development
33  (master) version](https://pydicom.github.io/pydicom/dev) and for the
34 [released version](https://pydicom.github.io/pydicom/stable). The
35 documentation for [the previous 0.9.9 version](https://pydicom.github.io/pydicom/0.9/)
36 is still there for reference.
37 
38 See [Getting Started](https://pydicom.github.io/pydicom/stable/getting_started.html)
39 for installation and basic information, and the
40 [User Guide](https://pydicom.github.io/pydicom/stable/pydicom_user_guide.html)
41 for an overview of how to use the pydicom library.
42 To contribute to pydicom, read our [contribution guide](https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md).
43 To contribute an example or extension of pydicom that does not belong with
44 the core software, see our contribution repository,
45 [contrib-pydicom](https://www.github.com/pydicom/contrib-pydicom).
46 
[end of README.md]
[start of pydicom/config.py]
1 # Copyright 2008-2018 pydicom authors. See LICENSE file for details.
2 """"""Pydicom configuration options.""""""
3 
4 # doc strings following items are picked up by sphinx for documentation
5 
6 import logging
7 
8 # Set the type used to hold DS values
9 #    default False; was decimal-based in pydicom 0.9.7
10 use_DS_decimal = False
11 
12 
13 data_element_callback = None
14 """"""Set data_element_callback to a function to be called from read_dataset
15 every time a RawDataElement has been returned, before it is added
16 to the dataset.
17 """"""
18 
19 data_element_callback_kwargs = {}
20 """"""Set this to use as keyword arguments passed to the data_element_callback
21 function""""""
22 
23 
24 def reset_data_element_callback():
25     global data_element_callback
26     global data_element_callback_kwargs
27     data_element_callback = None
28     data_element_callback_kwargs = {}
29 
30 
31 def DS_decimal(use_Decimal_boolean=True):
32     """"""Set DS class to be derived from Decimal (True) or from float (False)
33     If this function is never called, the default in pydicom >= 0.9.8
34     is for DS to be based on float.
35     """"""
36     use_DS_decimal = use_Decimal_boolean
37     import pydicom.valuerep
38     if use_DS_decimal:
39         pydicom.valuerep.DSclass = pydicom.valuerep.DSdecimal
40     else:
41         pydicom.valuerep.DSclass = pydicom.valuerep.DSfloat
42 
43 
44 # Configuration flags
45 allow_DS_float = False
46 """"""Set allow_float to True to allow DSdecimal instances
47 to be created with floats; otherwise, they must be explicitly
48 converted to strings, with the user explicity setting the
49 precision of digits and rounding. Default: False""""""
50 
51 enforce_valid_values = False
52 """"""Raise errors if any value is not allowed by DICOM standard,
53 e.g. DS strings that are longer than 16 characters;
54 IS strings outside the allowed range.
55 """"""
56 
57 datetime_conversion = False
58 """"""Set datetime_conversion to convert DA, DT and TM
59 data elements to datetime.date, datetime.datetime
60 and datetime.time respectively. Default: False
61 """"""
62 
63 # Logging system and debug function to change logging level
64 logger = logging.getLogger('pydicom')
65 handler = logging.StreamHandler()
66 formatter = logging.Formatter(""%(message)s"")
67 handler.setFormatter(formatter)
68 logger.addHandler(handler)
69 
70 
71 import pydicom.pixel_data_handlers.numpy_handler as np_handler  # noqa
72 import pydicom.pixel_data_handlers.rle_handler as rle_handler  # noqa
73 import pydicom.pixel_data_handlers.pillow_handler as pillow_handler  # noqa
74 import pydicom.pixel_data_handlers.jpeg_ls_handler as jpegls_handler  # noqa
75 import pydicom.pixel_data_handlers.gdcm_handler as gdcm_handler  # noqa
76 
77 pixel_data_handlers = [
78     np_handler,
79     rle_handler,
80     gdcm_handler,
81     pillow_handler,
82     jpegls_handler,
83 ]
84 """"""Handlers for converting (7fe0,0010) Pixel Data.
85 This is an ordered list that the dataset.convert_pixel_data()
86 method will try to extract a correctly sized numpy array from the
87 PixelData element.
88 
89 Handers shall have two methods:
90 
91 def supports_transfer_syntax(ds)
92   This returns True if the handler might support the transfer syntax
93   indicated in the dicom_dataset
94 
95 def get_pixeldata(ds):
96   This shall either throw an exception or return a correctly sized numpy
97   array derived from the PixelData.  Reshaping the array to the correct
98   dimensions is handled outside the image handler
99 
100 The first handler that both announces that it supports the transfer syntax
101 and does not throw an exception, either in getting the data or when the data
102 is reshaped to the correct dimensions, is the handler that will provide the
103 data.
104 
105 If they all fail, the last one to throw an exception gets to see its
106 exception thrown up.
107 
108 If no one throws an exception, but they all refuse to support the transfer
109 syntax, then this fact is announced in a NotImplementedError exception.
110 """"""
111 
112 
113 def debug(debug_on=True):
114     """"""Turn debugging of DICOM file reading and writing on or off.
115     When debugging is on, file location and details about the
116     elements read at that location are logged to the 'pydicom'
117     logger using python's logging module.
118 
119     :param debug_on: True (default) to turn on debugging,
120     False to turn off.
121     """"""
122     global logger, debugging
123     if debug_on:
124         logger.setLevel(logging.DEBUG)
125         debugging = True
126     else:
127         logger.setLevel(logging.WARNING)
128         debugging = False
129 
130 
131 # force level=WARNING, in case logging default is set differently (issue 103)
132 debug(False)
133 
[end of pydicom/config.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pydicom/pydicom,3746878d8edf1cbda6fbcf35eec69f9ba79301ca,"pydicom should not define handler, formatter and log level.
The `config` module (imported when pydicom is imported) defines a handler and set the log level for the pydicom logger. This should not be the case IMO. It should be the responsibility of the client code of pydicom to configure the logging module to its convenience. Otherwise one end up having multiple logs record as soon as pydicom is imported:

Example:
```
Could not import pillow
2018-03-25 15:27:29,744 :: DEBUG :: pydicom 
  Could not import pillow
Could not import jpeg_ls
2018-03-25 15:27:29,745 :: DEBUG :: pydicom 
  Could not import jpeg_ls
Could not import gdcm
2018-03-25 15:27:29,745 :: DEBUG :: pydicom 
  Could not import gdcm
``` 
Or am I missing something?
","In addition, I don't understand what the purpose of the `config.debug` function since the default behavor of the logging module in absence of configuartion seems to already be the one you want.

From https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library:

> If the using application does not use logging, and library code makes logging calls, then (as described in the previous section) events of severity WARNING and greater will be printed to sys.stderr. This is regarded as the best default behaviour.

and

>**It is strongly advised that you do not add any handlers other than NullHandler to your libraryâ€™s loggers.** This is because the configuration of handlers is the prerogative of the application developer who uses your library. The application developer knows their target audience and what handlers are most appropriate for their application: if you add handlers â€˜under the hoodâ€™, you might well interfere with their ability to carry out unit tests and deliver logs which suit their requirements. 

I think you make good points here.  I support changing the logging to comply with python's suggested behavior.

> In addition, I don't understand what the purpose of the config.debug function

One reason is that the core loop in pydicom (data_element_generator in filereader.py) is extremely optimized for speed - it checks the `debugging` flag set by config.debug, to avoid composing messages and doing function calls to logger when not needed.",2019-07-27T00:18:11Z,"<patch>
diff --git a/pydicom/config.py b/pydicom/config.py
--- a/pydicom/config.py
+++ b/pydicom/config.py
@@ -62,10 +62,7 @@ def DS_decimal(use_Decimal_boolean=True):
 
 # Logging system and debug function to change logging level
 logger = logging.getLogger('pydicom')
-handler = logging.StreamHandler()
-formatter = logging.Formatter(""%(message)s"")
-handler.setFormatter(formatter)
-logger.addHandler(handler)
+logger.addHandler(logging.NullHandler())
 
 
 import pydicom.pixel_data_handlers.numpy_handler as np_handler  # noqa
@@ -110,16 +107,29 @@ def get_pixeldata(ds):
 """"""
 
 
-def debug(debug_on=True):
-    """"""Turn debugging of DICOM file reading and writing on or off.
+def debug(debug_on=True, default_handler=True):
+    """"""Turn on/off debugging of DICOM file reading and writing.
+
     When debugging is on, file location and details about the
     elements read at that location are logged to the 'pydicom'
     logger using python's logging module.
 
-    :param debug_on: True (default) to turn on debugging,
-    False to turn off.
+    Parameters
+    ----------
+    debug_on : bool, optional
+        If True (default) then turn on debugging, False to turn off.
+    default_handler : bool, optional
+        If True (default) then use ``logging.StreamHandler()`` as the handler
+        for log messages.
     """"""
     global logger, debugging
+
+    if default_handler:
+        handler = logging.StreamHandler()
+        formatter = logging.Formatter(""%(message)s"")
+        handler.setFormatter(formatter)
+        logger.addHandler(handler)
+
     if debug_on:
         logger.setLevel(logging.DEBUG)
         debugging = True
@@ -129,4 +139,4 @@ def debug(debug_on=True):
 
 
 # force level=WARNING, in case logging default is set differently (issue 103)
-debug(False)
+debug(False, False)

</patch>","diff --git a/pydicom/tests/test_config.py b/pydicom/tests/test_config.py
new file mode 100644
--- /dev/null
+++ b/pydicom/tests/test_config.py
@@ -0,0 +1,107 @@
+# Copyright 2008-2019 pydicom authors. See LICENSE file for details.
+""""""Unit tests for the pydicom.config module.""""""
+
+import logging
+import sys
+
+import pytest
+
+from pydicom import dcmread
+from pydicom.config import debug
+from pydicom.data import get_testdata_files
+
+
+DS_PATH = get_testdata_files(""CT_small.dcm"")[0]
+PYTEST = [int(x) for x in pytest.__version__.split('.')]
+
+
+@pytest.mark.skipif(PYTEST[:2] < [3, 4], reason='no caplog')
+class TestDebug(object):
+    """"""Tests for config.debug().""""""
+    def setup(self):
+        self.logger = logging.getLogger('pydicom')
+
+    def teardown(self):
+        # Reset to just NullHandler
+        self.logger.handlers = [self.logger.handlers[0]]
+
+    def test_default(self, caplog):
+        """"""Test that the default logging handler is a NullHandler.""""""
+        assert 1 == len(self.logger.handlers)
+        assert isinstance(self.logger.handlers[0], logging.NullHandler)
+
+        with caplog.at_level(logging.DEBUG, logger='pydicom'):
+            ds = dcmread(DS_PATH)
+
+            assert ""Call to dcmread()"" not in caplog.text
+            assert ""Reading File Meta Information preamble..."" in caplog.text
+            assert ""Reading File Meta Information prefix..."" in caplog.text
+            assert ""00000080: 'DICM' prefix found"" in caplog.text
+
+    def test_debug_on_handler_null(self, caplog):
+        """"""Test debug(True, False).""""""
+        debug(True, False)
+        assert 1 == len(self.logger.handlers)
+        assert isinstance(self.logger.handlers[0], logging.NullHandler)
+
+        with caplog.at_level(logging.DEBUG, logger='pydicom'):
+            ds = dcmread(DS_PATH)
+
+            assert ""Call to dcmread()"" in caplog.text
+            assert ""Reading File Meta Information preamble..."" in caplog.text
+            assert ""Reading File Meta Information prefix..."" in caplog.text
+            assert ""00000080: 'DICM' prefix found"" in caplog.text
+            msg = (
+                ""00009848: fc ff fc ff 4f 42 00 00 7e 00 00 00    ""
+                ""(fffc, fffc) OB Length: 126""
+            )
+            assert msg in caplog.text
+
+    def test_debug_off_handler_null(self, caplog):
+        """"""Test debug(False, False).""""""
+        debug(False, False)
+        assert 1 == len(self.logger.handlers)
+        assert isinstance(self.logger.handlers[0], logging.NullHandler)
+
+        with caplog.at_level(logging.DEBUG, logger='pydicom'):
+            ds = dcmread(DS_PATH)
+
+            assert ""Call to dcmread()"" not in caplog.text
+            assert ""Reading File Meta Information preamble..."" in caplog.text
+            assert ""Reading File Meta Information prefix..."" in caplog.text
+            assert ""00000080: 'DICM' prefix found"" in caplog.text
+
+    def test_debug_on_handler_stream(self, caplog):
+        """"""Test debug(True, True).""""""
+        debug(True, True)
+        assert 2 == len(self.logger.handlers)
+        assert isinstance(self.logger.handlers[0], logging.NullHandler)
+        assert isinstance(self.logger.handlers[1], logging.StreamHandler)
+
+        with caplog.at_level(logging.DEBUG, logger='pydicom'):
+            ds = dcmread(DS_PATH)
+
+            assert ""Call to dcmread()"" in caplog.text
+            assert ""Reading File Meta Information preamble..."" in caplog.text
+            assert ""Reading File Meta Information prefix..."" in caplog.text
+            assert ""00000080: 'DICM' prefix found"" in caplog.text
+            msg = (
+                ""00009848: fc ff fc ff 4f 42 00 00 7e 00 00 00    ""
+                ""(fffc, fffc) OB Length: 126""
+            )
+            assert msg in caplog.text
+
+    def test_debug_off_handler_stream(self, caplog):
+        """"""Test debug(False, True).""""""
+        debug(False, True)
+        assert 2 == len(self.logger.handlers)
+        assert isinstance(self.logger.handlers[0], logging.NullHandler)
+        assert isinstance(self.logger.handlers[1], logging.StreamHandler)
+
+        with caplog.at_level(logging.DEBUG, logger='pydicom'):
+            ds = dcmread(DS_PATH)
+
+            assert ""Call to dcmread()"" not in caplog.text
+            assert ""Reading File Meta Information preamble..."" in caplog.text
+            assert ""Reading File Meta Information prefix..."" in caplog.text
+            assert ""00000080: 'DICM' prefix found"" in caplog.text
",1.3,"[""pydicom/tests/test_config.py::TestDebug::test_default"", ""pydicom/tests/test_config.py::TestDebug::test_debug_on_handler_null"", ""pydicom/tests/test_config.py::TestDebug::test_debug_off_handler_null"", ""pydicom/tests/test_config.py::TestDebug::test_debug_on_handler_stream"", ""pydicom/tests/test_config.py::TestDebug::test_debug_off_handler_stream""]",[],7241f5d9db0de589b230bb84212fbb643a7c86c3
pydicom__pydicom-1139,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
Make PersonName3 iterable
```python
from pydicom import Dataset

ds = Dataset()
ds.PatientName = 'SomeName'

'S' in ds.PatientName
```
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: argument of type 'PersonName3' is not iterable
```

I'm not really sure if this is intentional or if PN elements should support `str` methods. And yes I know I can `str(ds.PatientName)` but it's a bit silly, especially when I keep having to write exceptions to my element iterators just for PN elements.

</issue>
<code>
[start of README.md]
1 *pydicom*
2 =======
3 
4 [![Build Status](https://travis-ci.org/pydicom/pydicom.svg?branch=master)](https://travis-ci.org/pydicom/pydicom)
5 [![AppVeyor](https://ci.appveyor.com/api/projects/status/1vjtkr82lumnd3i7?svg=true)](https://ci.appveyor.com/project/glemaitre/pydicom)
6 [![CircleCI](https://circleci.com/gh/pydicom/pydicom/tree/master.svg?style=shield)](https://circleci.com/gh/pydicom/pydicom/tree/master)
7 [![codecov](https://codecov.io/gh/pydicom/pydicom/branch/master/graph/badge.svg)](https://codecov.io/gh/pydicom/pydicom)
8 [![Python version](https://img.shields.io/pypi/pyversions/pydicom.svg)](https://img.shields.io/pypi/pyversions/pydicom.svg)
9 [![PyPI version](https://badge.fury.io/py/pydicom.svg)](https://badge.fury.io/py/pydicom)
10 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3891702.svg)](https://doi.org/10.5281/zenodo.3891702)
11 [![Gitter](https://badges.gitter.im/pydicom/Lobby.svg)](https://gitter.im/pydicom/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
12 
13 *pydicom* is a pure python package for working with [DICOM](http://medical.nema.org/) files.
14 It was made for inspecting and modifying DICOM data in an easy ""pythonic"" way.
15 The modifications can be written again to a new file.
16 
17 As a pure python package, *pydicom* can run anywhere python runs without any other requirements,
18 although [NumPy](http://www.numpy.org) is needed if manipulating pixel data.
19 
20 *pydicom* is not a DICOM server, and is not primarily about viewing images.
21 It is designed to let you
22 manipulate data elements in DICOM files with python code.
23 
24 Limitations -- for files with _compressed_ pixel data, *pydicom* can decompress
25 it (with additional libraries installed) and allow you to manipulate the data,
26 but can only store changed pixel data as uncompressed. Files can always be
27 read and saved (including compressed pixel data that has not been modified),
28 but once decompressed, modified pixel data cannot be compressed again.
29 
30 Documentation
31 -------------
32 
33 *pydicom* documentation is available on GitHub Pages both for the [development
34  (master) version](https://pydicom.github.io/pydicom/dev) and for the
35 [released version](https://pydicom.github.io/pydicom/stable). The
36 documentation for [the previous 0.9.9 version](https://pydicom.github.io/pydicom/0.9/)
37 is still there for reference.
38 
39 See [Getting Started](https://pydicom.github.io/pydicom/stable/old/getting_started.html)
40 for installation and basic information, and the
41 [User Guide](https://pydicom.github.io/pydicom/stable/pydicom_user_guide.html)
42 for an overview of how to use the *pydicom* library.
43 To contribute to *pydicom*, read our [contribution guide](https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md).
44 To contribute an example or extension of *pydicom* that does not belong with
45 the core software, see our contribution repository,
46 [contrib-pydicom](https://www.github.com/pydicom/contrib-pydicom).
47 
[end of README.md]
[start of pydicom/valuerep.py]
1 # Copyright 2008-2018 pydicom authors. See LICENSE file for details.
2 """"""Special classes for DICOM value representations (VR)""""""
3 from copy import deepcopy
4 from decimal import Decimal
5 import re
6 
7 from datetime import (date, datetime, time, timedelta, timezone)
8 
9 # don't import datetime_conversion directly
10 from pydicom import config
11 from pydicom.multival import MultiValue
12 
13 # can't import from charset or get circular import
14 default_encoding = ""iso8859""
15 
16 # For reading/writing data elements,
17 # these ones have longer explicit VR format
18 # Taken from PS3.5 Section 7.1.2
19 extra_length_VRs = ('OB', 'OD', 'OF', 'OL', 'OW', 'SQ', 'UC', 'UN', 'UR', 'UT')
20 
21 # VRs that can be affected by character repertoire
22 # in (0008,0005) Specific Character Set
23 # See PS-3.5 (2011), section 6.1.2 Graphic Characters
24 # and PN, but it is handled separately.
25 text_VRs = ('SH', 'LO', 'ST', 'LT', 'UC', 'UT')
26 
27 # Delimiters for text strings and person name that reset the encoding.
28 # See PS3.5, Section 6.1.2.5.3
29 # Note: We use character codes for Python 3
30 # because those are the types yielded if iterating over a byte string.
31 
32 # Characters/Character codes for text VR delimiters: LF, CR, TAB, FF
33 TEXT_VR_DELIMS = {0x0d, 0x0a, 0x09, 0x0c}
34 
35 # Character/Character code for PN delimiter: name part separator '^'
36 # (the component separator '=' is handled separately)
37 PN_DELIMS = {0xe5}
38 
39 
40 class DA(date):
41     """"""Store value for an element with VR **DA** as :class:`datetime.date`.
42 
43     Note that the :class:`datetime.date` base class is immutable.
44     """"""
45     __slots__ = ['original_string']
46 
47     def __getstate__(self):
48         return dict((slot, getattr(self, slot)) for slot in self.__slots__
49                     if hasattr(self, slot))
50 
51     def __setstate__(self, state):
52         for slot, value in state.items():
53             setattr(self, slot, value)
54 
55     def __reduce__(self):
56         return super(DA, self).__reduce__() + (self.__getstate__(),)
57 
58     def __reduce_ex__(self, protocol):
59         return super(DA, self).__reduce__() + (self.__getstate__(),)
60 
61     def __new__(cls, val):
62         """"""Create an instance of DA object.
63 
64         Raise an exception if the string cannot be parsed or the argument
65         is otherwise incompatible.
66 
67         Parameters
68         ----------
69         val : str
70             A string conformant to the DA definition in the DICOM Standard,
71             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.
72         """"""
73         if isinstance(val, str):
74             if len(val) == 8:
75                 year = int(val[0:4])
76                 month = int(val[4:6])
77                 day = int(val[6:8])
78                 val = super(DA, cls).__new__(cls, year, month, day)
79             elif len(val) == 10 and val[4] == '.' and val[7] == '.':
80                 # ACR-NEMA Standard 300, predecessor to DICOM
81                 # for compatibility with a few old pydicom example files
82                 year = int(val[0:4])
83                 month = int(val[5:7])
84                 day = int(val[8:10])
85                 val = super(DA, cls).__new__(cls, year, month, day)
86             elif val == '':
87                 val = None  # empty date
88             else:
89                 try:
90                     val = super(DA, cls).__new__(cls, val)
91                 except TypeError:
92                     raise ValueError(""Cannot convert to datetime: '%s'"" %
93                                      (val))
94         elif isinstance(val, date):
95             val = super(DA, cls).__new__(cls, val.year, val.month, val.day)
96         else:
97             val = super(DA, cls).__new__(cls, val)
98         return val
99 
100     def __init__(self, val):
101         if isinstance(val, str):
102             self.original_string = val
103         elif isinstance(val, DA) and hasattr(val, 'original_string'):
104             self.original_string = val.original_string
105 
106     def __str__(self):
107         if hasattr(self, 'original_string'):
108             return self.original_string
109         else:
110             return super(DA, self).__str__()
111 
112     def __repr__(self):
113         return ""\"""" + str(self) + ""\""""
114 
115 
116 class DT(datetime):
117     """"""Store value for an element with VR **DT** as :class:`datetime.datetime`.
118 
119     Note that the :class:`datetime.datetime` base class is immutable.
120     """"""
121     __slots__ = ['original_string']
122     _regex_dt = re.compile(r""((\d{4,14})(\.(\d{1,6}))?)([+-]\d{4})?"")
123 
124     def __getstate__(self):
125         return dict((slot, getattr(self, slot)) for slot in self.__slots__
126                     if hasattr(self, slot))
127 
128     def __setstate__(self, state):
129         for slot, value in state.items():
130             setattr(self, slot, value)
131 
132     def __reduce__(self):
133         return super(DT, self).__reduce__() + (self.__getstate__(),)
134 
135     def __reduce_ex__(self, protocol):
136         return super(DT, self).__reduce__() + (self.__getstate__(),)
137 
138     @staticmethod
139     def _utc_offset(offset, name):
140         return timezone(timedelta(seconds=offset), name)
141 
142     def __new__(cls, val):
143         """"""Create an instance of DT object.
144 
145         Raise an exception if the string cannot be parsed or the argument
146         is otherwise incompatible.
147 
148         Parameters
149         ----------
150         val : str
151             A string conformant to the DT definition in the DICOM Standard,
152             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.
153         """"""
154         if isinstance(val, str):
155             match = DT._regex_dt.match(val)
156             if match and len(val) <= 26:
157                 dt_match = match.group(2)
158                 year = int(dt_match[0:4])
159                 if len(dt_match) < 6:
160                     month = 1
161                 else:
162                     month = int(dt_match[4:6])
163                 if len(dt_match) < 8:
164                     day = 1
165                 else:
166                     day = int(dt_match[6:8])
167                 if len(dt_match) < 10:
168                     hour = 0
169                 else:
170                     hour = int(dt_match[8:10])
171                 if len(dt_match) < 12:
172                     minute = 0
173                 else:
174                     minute = int(dt_match[10:12])
175                 if len(dt_match) < 14:
176                     second = 0
177                     microsecond = 0
178                 else:
179                     second = int(dt_match[12:14])
180                     ms_match = match.group(4)
181                     if ms_match:
182                         microsecond = int(ms_match.rstrip().ljust(6, '0'))
183                     else:
184                         microsecond = 0
185                 tz_match = match.group(5)
186                 if tz_match:
187                     offset1 = int(tz_match[1:3]) * 60
188                     offset2 = int(tz_match[3:5])
189                     offset = (offset1 + offset2) * 60
190                     if tz_match[0] == '-':
191                         offset = -offset
192                     tzinfo = cls._utc_offset(offset, tz_match)
193                 else:
194                     tzinfo = None
195                 val = super(DT,
196                             cls).__new__(cls, year, month, day, hour, minute,
197                                          second, microsecond, tzinfo)
198             else:
199                 try:
200                     val = super(DT, cls).__new__(cls, val)
201                 except TypeError:
202                     raise ValueError(""Cannot convert to datetime: '%s'"" %
203                                      (val))
204         elif isinstance(val, datetime):
205             val = super(DT, cls).__new__(cls, val.year, val.month, val.day,
206                                          val.hour, val.minute, val.second,
207                                          val.microsecond, val.tzinfo)
208         else:
209             val = super(DT, cls).__new__(cls, val)
210         return val
211 
212     def __init__(self, val):
213         if isinstance(val, str):
214             self.original_string = val
215         elif isinstance(val, DT) and hasattr(val, 'original_string'):
216             self.original_string = val.original_string
217 
218     def __str__(self):
219         if hasattr(self, 'original_string'):
220             return self.original_string
221         else:
222             return super(DT, self).__str__()
223 
224     def __repr__(self):
225         return ""\"""" + str(self) + ""\""""
226 
227 
228 class TM(time):
229     """"""Store value for an element with VR **TM** as :class:`datetime.time`.
230 
231     Note that the :class:`datetime.time` base class is immutable.
232     """"""
233     __slots__ = ['original_string']
234     _regex_tm = re.compile(r""(\d{2,6})(\.(\d{1,6}))?"")
235 
236     def __getstate__(self):
237         return dict((slot, getattr(self, slot)) for slot in self.__slots__
238                     if hasattr(self, slot))
239 
240     def __setstate__(self, state):
241         for slot, value in state.items():
242             setattr(self, slot, value)
243 
244     def __reduce__(self):
245         return super(TM, self).__reduce__() + (self.__getstate__(),)
246 
247     def __reduce_ex__(self, protocol):
248         return super(TM, self).__reduce__() + (self.__getstate__(),)
249 
250     def __new__(cls, val):
251         """"""Create an instance of TM object from a string.
252 
253         Raise an exception if the string cannot be parsed or the argument
254         is otherwise incompatible.
255 
256         Parameters
257         ----------
258         val : str
259             A string conformant to the TM definition in the DICOM Standard,
260             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.
261         """"""
262         if isinstance(val, str):
263             match = TM._regex_tm.match(val)
264             if match and len(val) <= 16:
265                 tm_match = match.group(1)
266                 hour = int(tm_match[0:2])
267                 if len(tm_match) < 4:
268                     minute = 0
269                 else:
270                     minute = int(tm_match[2:4])
271                 if len(tm_match) < 6:
272                     second = 0
273                     microsecond = 0
274                 else:
275                     second = int(tm_match[4:6])
276                     ms_match = match.group(3)
277                     if ms_match:
278                         microsecond = int(ms_match.rstrip().ljust(6, '0'))
279                     else:
280                         microsecond = 0
281                 val = super(TM, cls).__new__(cls, hour, minute, second,
282                                              microsecond)
283             elif val == '':
284                 val = None  # empty time
285             else:
286                 try:
287                     val = super(TM, cls).__new__(cls, val)
288                 except TypeError:
289                     raise ValueError(""Cannot convert to datetime: '%s"" % (val))
290         elif isinstance(val, time):
291             val = super(TM, cls).__new__(cls, val.hour, val.minute, val.second,
292                                          val.microsecond)
293         else:
294             val = super(TM, cls).__new__(cls, val)
295         return val
296 
297     def __init__(self, val):
298         if isinstance(val, str):
299             self.original_string = val
300         elif isinstance(val, TM) and hasattr(val, 'original_string'):
301             self.original_string = val.original_string
302 
303     def __str__(self):
304         if hasattr(self, 'original_string'):
305             return self.original_string
306         else:
307             return super(TM, self).__str__()
308 
309     def __repr__(self):
310         return ""\"""" + str(self) + ""\""""
311 
312 
313 class DSfloat(float):
314     """"""Store value for an element with VR **DS** as :class:`float`.
315 
316     If constructed from an empty string, return the empty string,
317     not an instance of this class.
318 
319     """"""
320     __slots__ = ['original_string']
321 
322     def __getstate__(self):
323         return dict((slot, getattr(self, slot)) for slot in self.__slots__
324                     if hasattr(self, slot))
325 
326     def __setstate__(self, state):
327         for slot, value in state.items():
328             setattr(self, slot, value)
329 
330     def __init__(self, val):
331         """"""Store the original string if one given, for exact write-out of same
332         value later.
333         """"""
334         # ... also if user changes a data element value, then will get
335         # a different object, because float is immutable.
336 
337         has_attribute = hasattr(val, 'original_string')
338         if isinstance(val, str):
339             self.original_string = val
340         elif isinstance(val, (DSfloat, DSdecimal)) and has_attribute:
341             self.original_string = val.original_string
342 
343     def __str__(self):
344         if hasattr(self, 'original_string'):
345             return self.original_string
346 
347         # Issue #937 (Python 3.8 compatibility)
348         return repr(self)[1:-1]
349 
350     def __repr__(self):
351         return '""{}""'.format(super(DSfloat, self).__repr__())
352 
353 
354 class DSdecimal(Decimal):
355     """"""Store value for an element with VR **DS** as :class:`decimal.Decimal`.
356 
357     Notes
358     -----
359     If constructed from an empty string, returns the empty string, not an
360     instance of this class.
361     """"""
362     __slots__ = ['original_string']
363 
364     def __getstate__(self):
365         return dict((slot, getattr(self, slot)) for slot in self.__slots__
366                     if hasattr(self, slot))
367 
368     def __setstate__(self, state):
369         for slot, value in state.items():
370             setattr(self, slot, value)
371 
372     def __new__(cls, val):
373         """"""Create an instance of DS object, or return a blank string if one is
374         passed in, e.g. from a type 2 DICOM blank value.
375 
376         Parameters
377         ----------
378         val : str or numeric
379             A string or a number type which can be converted to a decimal.
380         """"""
381         # Store this value here so that if the input string is actually a valid
382         # string but decimal.Decimal transforms it to an invalid string it will
383         # still be initialized properly
384         enforce_length = config.enforce_valid_values
385         # DICOM allows spaces around the string,
386         # but python doesn't, so clean it
387         if isinstance(val, str):
388             val = val.strip()
389             # If the input string is actually invalid that we relax the valid
390             # value constraint for this particular instance
391             if len(val) <= 16:
392                 enforce_length = False
393         if val == '':
394             return val
395         if isinstance(val, float) and not config.allow_DS_float:
396             msg = (""DS cannot be instantiated with a float value, ""
397                    ""unless config.allow_DS_float is set to True. ""
398                    ""It is recommended to convert to a string instead, ""
399                    ""with the desired number of digits, or use ""
400                    ""Decimal.quantize and pass a Decimal instance."")
401             raise TypeError(msg)
402         if not isinstance(val, Decimal):
403             val = super(DSdecimal, cls).__new__(cls, val)
404         if len(str(val)) > 16 and enforce_length:
405             msg = (""DS value representation must be <= 16 ""
406                    ""characters by DICOM standard. Initialize with ""
407                    ""a smaller string, or set config.enforce_valid_values ""
408                    ""to False to override, or use Decimal.quantize() and ""
409                    ""initialize with a Decimal instance."")
410             raise OverflowError(msg)
411         return val
412 
413     def __init__(self, val):
414         """"""Store the original string if one given, for exact write-out of same
415         value later. E.g. if set ``'1.23e2'``, :class:`~decimal.Decimal` would
416         write ``'123'``, but :class:`DS` will use the original.
417         """"""
418         # ... also if user changes a data element value, then will get
419         # a different Decimal, as Decimal is immutable.
420         if isinstance(val, str):
421             self.original_string = val
422         elif isinstance(val, (DSfloat, DSdecimal)) and hasattr(val, 'original_string'):  # noqa
423             self.original_string = val.original_string
424 
425     def __str__(self):
426         if hasattr(self, 'original_string') and len(self.original_string) <= 16:  # noqa
427             return self.original_string
428         else:
429             return super(DSdecimal, self).__str__()
430 
431     def __repr__(self):
432         return ""\"""" + str(self) + ""\""""
433 
434 
435 # CHOOSE TYPE OF DS
436 if config.use_DS_decimal:
437     DSclass = DSdecimal
438 else:
439     DSclass = DSfloat
440 
441 
442 def DS(val):
443     """"""Factory function for creating DS class instances.
444 
445     Checks for blank string; if so, returns that, else calls :class:`DSfloat`
446     or :class:`DSdecimal` to create the class instance. This avoids overriding
447     ``DSfloat.__new__()`` (which carries a time penalty for large arrays of
448     DS).
449 
450     Similarly the string clean and check can be avoided and :class:`DSfloat`
451     called directly if a string has already been processed.
452     """"""
453     if isinstance(val, str):
454         val = val.strip()
455     if val == '' or val is None:
456         return val
457     return DSclass(val)
458 
459 
460 class IS(int):
461     """"""Store value for an element with VR **IS** as :class:`int`.
462 
463     Stores original integer string for exact rewriting of the string
464     originally read or stored.
465     """"""
466 
467     def __new__(cls, val):
468         """"""Create instance if new integer string""""""
469         if val is None:
470             return val
471         if isinstance(val, str) and val.strip() == '':
472             return ''
473 
474         newval = super(IS, cls).__new__(cls, val)
475 
476         # check if a float or Decimal passed in, then could have lost info,
477         # and will raise error. E.g. IS(Decimal('1')) is ok, but not IS(1.23)
478         if isinstance(val, (float, Decimal)) and newval != val:
479             raise TypeError(""Could not convert value to integer without loss"")
480         # Checks in case underlying int is >32 bits, DICOM does not allow this
481         check_newval = (newval < -2 ** 31 or newval >= 2 ** 31)
482         if check_newval and config.enforce_valid_values:
483             dcm_limit = ""-2**31 to (2**31 - 1) for IS""
484             message = ""Value exceeds DICOM limits of %s"" % (dcm_limit)
485             raise OverflowError(message)
486         return newval
487 
488     def __init__(self, val):
489         # If a string passed, then store it
490         if isinstance(val, str):
491             self.original_string = val
492         elif isinstance(val, IS) and hasattr(val, 'original_string'):
493             self.original_string = val.original_string
494 
495     def __str__(self):
496         if hasattr(self, 'original_string'):
497             return self.original_string
498 
499         # Issue #937 (Python 3.8 compatibility)
500         return repr(self)[1:-1]
501 
502     def __repr__(self):
503         return '""{}""'.format(super(IS, self).__repr__())
504 
505 
506 def MultiString(val, valtype=str):
507     """"""Split a bytestring by delimiters if there are any
508 
509     Parameters
510     ----------
511     val : bytes or str
512         DICOM byte string to split up.
513     valtype
514         Default :class:`str`, but can be e.g. :class:`~pydicom.uid.UID` to
515         overwrite to a specific type.
516 
517     Returns
518     -------
519     valtype or list of valtype
520         The split value as `valtype` or a :class:`list` of `valtype`.
521     """"""
522     # Remove trailing blank used to pad to even length
523     # 2005.05.25: also check for trailing 0, error made
524     # in PET files we are converting
525 
526     while val and (val.endswith(' ') or val.endswith('\x00')):
527         val = val[:-1]
528     splitup = val.split(""\\"")
529 
530     if len(splitup) == 1:
531         val = splitup[0]
532         return valtype(val) if val else val
533     else:
534         return MultiValue(valtype, splitup)
535 
536 
537 def _verify_encodings(encodings):
538     """"""Checks the encoding to ensure proper format""""""
539     if encodings is not None:
540         if not isinstance(encodings, (list, tuple)):
541             return encodings,
542         return tuple(encodings)
543     return encodings
544 
545 
546 def _decode_personname(components, encodings):
547     """"""Return a list of decoded person name components.
548 
549     Parameters
550     ----------
551     components : list of byte string
552         The list of the up to three encoded person name components
553     encodings : list of str
554         The Python encodings uses to decode `components`.
555 
556     Returns
557     -------
558     text type
559         The unicode string representing the person name.
560         If the decoding of some component parts is not possible using the
561         given encodings, they are decoded with the first encoding using
562         replacement characters for bytes that cannot be decoded.
563     """"""
564     from pydicom.charset import decode_string
565 
566     if isinstance(components[0], str):
567         comps = components
568     else:
569         comps = [decode_string(comp, encodings, PN_DELIMS)
570                  for comp in components]
571     # Remove empty elements from the end to avoid trailing '='
572     while len(comps) and not comps[-1]:
573         comps.pop()
574     return tuple(comps)
575 
576 
577 def _encode_personname(components, encodings):
578     """"""Encode a list of text string person name components.
579 
580     Parameters
581     ----------
582     components : list of text type
583         The list of the up to three unicode person name components
584     encodings : list of str
585         The Python encodings uses to encode `components`.
586 
587     Returns
588     -------
589     byte string
590         The byte string that can be written as a PN DICOM tag value.
591         If the encoding of some component parts is not possible using the
592         given encodings, they are encoded with the first encoding using
593         replacement bytes for characters that cannot be encoded.
594     """"""
595     from pydicom.charset import encode_string
596 
597     encoded_comps = []
598     for comp in components:
599         groups = [encode_string(group, encodings)
600                   for group in comp.split('^')]
601         encoded_comps.append(b'^'.join(groups))
602 
603     # Remove empty elements from the end
604     while len(encoded_comps) and not encoded_comps[-1]:
605         encoded_comps.pop()
606     return b'='.join(encoded_comps)
607 
608 
609 class PersonName:
610     def __new__(cls, *args, **kwargs):
611         # Handle None value by returning None instead of a PersonName object
612         if len(args) and args[0] is None:
613             return None
614         return super(PersonName, cls).__new__(cls)
615 
616     def __init__(self, val, encodings=None, original_string=None):
617         if isinstance(val, PersonName):
618             encodings = val.encodings
619             self.original_string = val.original_string
620             self._components = tuple(str(val).split('='))
621         elif isinstance(val, bytes):
622             # this is the raw byte string - decode it on demand
623             self.original_string = val
624             self._components = None
625         else:
626             # handle None `val` as empty string
627             val = val or ''
628 
629             # this is the decoded string - save the original string if
630             # available for easier writing back
631             self.original_string = original_string
632             components = val.split('=')
633             # Remove empty elements from the end to avoid trailing '='
634             while len(components) and not components[-1]:
635                 components.pop()
636             self._components = tuple(components)
637 
638             # if the encoding is not given, leave it as undefined (None)
639         self.encodings = _verify_encodings(encodings)
640         self._dict = {}
641 
642     def _create_dict(self):
643         """"""Creates a dictionary of person name group and component names.
644 
645         Used exclusively for `formatted` for backwards compatibility.
646         """"""
647         if not self._dict:
648             for name in ('family_name', 'given_name', 'middle_name',
649                          'name_prefix', 'name_suffix',
650                          'ideographic', 'phonetic'):
651                 self._dict[name] = getattr(self, name, '')
652 
653     @property
654     def components(self):
655         """"""Returns up to three decoded person name components.
656 
657         .. versionadded:: 1.2
658 
659         The returned components represent the alphabetic, ideographic and
660         phonetic representations as a list of unicode strings.
661         """"""
662         if self._components is None:
663             groups = self.original_string.split(b'=')
664             encodings = self.encodings or [default_encoding]
665             self._components = _decode_personname(groups, encodings)
666 
667         return self._components
668 
669     def _name_part(self, i):
670         try:
671             return self.components[0].split('^')[i]
672         except IndexError:
673             return ''
674 
675     @property
676     def family_name(self):
677         """"""Return the first (family name) group of the alphabetic person name
678         representation as a unicode string
679 
680         .. versionadded:: 1.2
681         """"""
682         return self._name_part(0)
683 
684     @property
685     def given_name(self):
686         """"""Return the second (given name) group of the alphabetic person name
687         representation as a unicode string
688 
689         .. versionadded:: 1.2
690         """"""
691         return self._name_part(1)
692 
693     @property
694     def middle_name(self):
695         """"""Return the third (middle name) group of the alphabetic person name
696         representation as a unicode string
697 
698         .. versionadded:: 1.2
699         """"""
700         return self._name_part(2)
701 
702     @property
703     def name_prefix(self):
704         """"""Return the fourth (name prefix) group of the alphabetic person name
705         representation as a unicode string
706 
707         .. versionadded:: 1.2
708         """"""
709         return self._name_part(3)
710 
711     @property
712     def name_suffix(self):
713         """"""Return the fifth (name suffix) group of the alphabetic person name
714         representation as a unicode string
715 
716         .. versionadded:: 1.2
717         """"""
718         return self._name_part(4)
719 
720     @property
721     def ideographic(self):
722         """"""Return the second (ideographic) person name component as a
723         unicode string
724 
725         .. versionadded:: 1.2
726         """"""
727         try:
728             return self.components[1]
729         except IndexError:
730             return ''
731 
732     @property
733     def phonetic(self):
734         """"""Return the third (phonetic) person name component as a
735         unicode string
736 
737         .. versionadded:: 1.2
738         """"""
739         try:
740             return self.components[2]
741         except IndexError:
742             return ''
743 
744     def __eq__(self, other):
745         return str(self) == other
746 
747     def __ne__(self, other):
748         return not self == other
749 
750     def __str__(self):
751         return '='.join(self.components).__str__()
752 
753     def __repr__(self):
754         return '='.join(self.components).__repr__()
755 
756     def __hash__(self):
757         return hash(self.components)
758 
759     def decode(self, encodings=None):
760         """"""Return the patient name decoded by the given `encodings`.
761 
762         Parameters
763         ----------
764         encodings : list of str
765             The list of encodings used for decoding the byte string. If not
766             given, the initial encodings set in the object are used.
767 
768         Returns
769         -------
770         valuerep.PersonName
771             A person name object that will return the decoded string with
772             the given encodings on demand. If the encodings are not given,
773             the current object is returned.
774         """"""
775         # in the common case (encoding did not change) we decode on demand
776         if encodings is None or encodings == self.encodings:
777             return self
778         # the encoding was unknown or incorrect - create a new
779         # PersonName object with the changed encoding
780         encodings = _verify_encodings(encodings)
781         if self.original_string is None:
782             # if the original encoding was not set, we set it now
783             self.original_string = _encode_personname(
784                 self.components, self.encodings or [default_encoding])
785         return PersonName(self.original_string, encodings)
786 
787     def encode(self, encodings=None):
788         """"""Return the patient name decoded by the given `encodings`.
789 
790         Parameters
791         ----------
792         encodings : list of str
793             The list of encodings used for encoding the unicode string. If
794             not given, the initial encodings set in the object are used.
795 
796         Returns
797         -------
798         bytes
799             The person name encoded with the given encodings as a byte string.
800             If no encoding is given, the original byte string is returned, if
801             available, otherwise each group of the patient name is encoded
802             with the first matching of the given encodings.
803         """"""
804         encodings = _verify_encodings(encodings) or self.encodings
805 
806         # if the encoding is not the original encoding, we have to return
807         # a re-encoded string (without updating the original string)
808         if encodings != self.encodings and self.encodings is not None:
809             return _encode_personname(self.components, encodings)
810         if self.original_string is None:
811             # if the original encoding was not set, we set it now
812             self.original_string = _encode_personname(
813                 self.components, encodings or [default_encoding])
814         return self.original_string
815 
816     def family_comma_given(self):
817         return self.formatted('%(family_name)s, %(given_name)s')
818 
819     def formatted(self, format_str):
820         self._create_dict()
821         return format_str % self._dict
822 
823     def __bool__(self):
824         if self.original_string is None:
825             return (bool(self._components) and
826                     (len(self._components) > 1 or bool(self._components[0])))
827         return bool(self.original_string)
828 
829 
830 # Alias old class names for backwards compat in user code
831 PersonNameUnicode = PersonName = PersonName
832 
[end of pydicom/valuerep.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pydicom/pydicom,b9fb05c177b685bf683f7f57b2d57374eb7d882d,"Make PersonName3 iterable
```python
from pydicom import Dataset

ds = Dataset()
ds.PatientName = 'SomeName'

'S' in ds.PatientName
```
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: argument of type 'PersonName3' is not iterable
```

I'm not really sure if this is intentional or if PN elements should support `str` methods. And yes I know I can `str(ds.PatientName)` but it's a bit silly, especially when I keep having to write exceptions to my element iterators just for PN elements.
","I think it is reasonable to support at least some `str` methods (definitely `__contains__` for the example above), but there are many that don't make a lot of sense in this context though - e.g. `join`, `ljust`, `maketrans`, `splitlines` just to name a few, but I suppose each would either never be actually used or would have no effect.

I have a vague memory that one or more of the `PersonName` classes was at one time subclassed from `str`, or at least that it was discussed... does anyone remember?  Maybe it would be easier now with only Python 3 supported.
`PersonName` was derived from `str` or `unicode` in Python 2, but that caused a number of problems, which is why you switched to `PersonName3` in Python 3, I think. I agree though that it makes sense to implement `str` methods, either by implementing some of them, or generically by adding `__getattr__` that converts it to `str` and applies the attribute to that string. ",2020-06-26T11:47:17Z,"<patch>
diff --git a/pydicom/valuerep.py b/pydicom/valuerep.py
--- a/pydicom/valuerep.py
+++ b/pydicom/valuerep.py
@@ -1,6 +1,5 @@
 # Copyright 2008-2018 pydicom authors. See LICENSE file for details.
 """"""Special classes for DICOM value representations (VR)""""""
-from copy import deepcopy
 from decimal import Decimal
 import re
 
@@ -750,6 +749,25 @@ def __ne__(self, other):
     def __str__(self):
         return '='.join(self.components).__str__()
 
+    def __next__(self):
+        # Get next character or stop iteration
+        if self._i < self._rep_len:
+            c = self._str_rep[self._i]
+            self._i += 1
+            return c
+        else:
+            raise StopIteration
+
+    def __iter__(self):
+        # Get string rep. and length, initialize index counter
+        self._str_rep = self.__str__()
+        self._rep_len = len(self._str_rep)
+        self._i = 0
+        return self
+
+    def __contains__(self, x):
+        return x in self.__str__()
+
     def __repr__(self):
         return '='.join(self.components).__repr__()
 

</patch>","diff --git a/pydicom/tests/test_valuerep.py b/pydicom/tests/test_valuerep.py
--- a/pydicom/tests/test_valuerep.py
+++ b/pydicom/tests/test_valuerep.py
@@ -427,6 +427,62 @@ def test_hash(self):
         )
         assert hash(pn1) == hash(pn2)
 
+    def test_next(self):
+        """"""Test that the next function works on it's own""""""
+        # Test getting the first character
+        pn1 = PersonName(""John^Doe^^Dr"", encodings=default_encoding)
+        pn1_itr = iter(pn1)
+        assert next(pn1_itr) == ""J""
+
+        # Test getting multiple characters
+        pn2 = PersonName(
+            ""Yamada^Tarou=å±±ç”°^å¤ªéƒŽ=ã‚„ã¾ã ^ãŸã‚ã†"", [default_encoding, ""iso2022_jp""]
+        )
+        pn2_itr = iter(pn2)
+        assert next(pn2_itr) == ""Y""
+        assert next(pn2_itr) == ""a""
+
+        # Test getting all characters
+        pn3 = PersonName(""SomeName"")
+        pn3_itr = iter(pn3)
+        assert next(pn3_itr) == ""S""
+        assert next(pn3_itr) == ""o""
+        assert next(pn3_itr) == ""m""
+        assert next(pn3_itr) == ""e""
+        assert next(pn3_itr) == ""N""
+        assert next(pn3_itr) == ""a""
+        assert next(pn3_itr) == ""m""
+        assert next(pn3_itr) == ""e""
+
+        # Attempting to get next characeter should stop the iteration
+        # I.e. next can only start once
+        with pytest.raises(StopIteration):
+            next(pn3_itr)
+
+        # Test that next() doesn't work without instantiating an iterator
+        pn4 = PersonName(""SomeName"")
+        with pytest.raises(AttributeError):
+            next(pn4)
+
+    def test_iterator(self):
+        """"""Test that iterators can be corretly constructed""""""
+        name_str = ""John^Doe^^Dr""
+        pn1 = PersonName(name_str)
+        
+        for i, c in enumerate(pn1):
+            assert name_str[i] == c
+
+        # Ensure that multiple iterators can be created on the same variable
+        for i, c in enumerate(pn1):
+            assert name_str[i] == c
+
+    def test_contains(self):
+        """"""Test that characters can be check if they are within the name""""""
+        pn1 = PersonName(""John^Doe"")
+        assert (""J"" in pn1) == True
+        assert (""o"" in pn1) == True
+        assert (""x"" in pn1) == False
+
 
 class TestDateTime:
     """"""Unit tests for DA, DT, TM conversion to datetime objects""""""
",2.0,"[""pydicom/tests/test_valuerep.py::TestPersonName::test_next"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_iterator"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_contains""]","[""pydicom/tests/test_valuerep.py::TestTM::test_pickling"", ""pydicom/tests/test_valuerep.py::TestDT::test_pickling"", ""pydicom/tests/test_valuerep.py::TestDA::test_pickling"", ""pydicom/tests/test_valuerep.py::TestDS::test_empty_value"", ""pydicom/tests/test_valuerep.py::TestDS::test_float_values"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_pickling"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_str"", ""pydicom/tests/test_valuerep.py::TestDSfloat::test_repr"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_pickling"", ""pydicom/tests/test_valuerep.py::TestDSdecimal::test_float_value"", ""pydicom/tests/test_valuerep.py::TestIS::test_empty_value"", ""pydicom/tests/test_valuerep.py::TestIS::test_valid_value"", ""pydicom/tests/test_valuerep.py::TestIS::test_invalid_value"", ""pydicom/tests/test_valuerep.py::TestIS::test_pickling"", ""pydicom/tests/test_valuerep.py::TestIS::test_longint"", ""pydicom/tests/test_valuerep.py::TestIS::test_overflow"", ""pydicom/tests/test_valuerep.py::TestIS::test_str"", ""pydicom/tests/test_valuerep.py::TestIS::test_repr"", ""pydicom/tests/test_valuerep.py::TestBadValueRead::test_read_bad_value_in_VR_default"", ""pydicom/tests/test_valuerep.py::TestBadValueRead::test_read_bad_value_in_VR_enforce_valid_value"", ""pydicom/tests/test_valuerep.py::TestDecimalString::test_DS_decimal_set"", ""pydicom/tests/test_valuerep.py::TestDecimalString::test_valid_decimal_strings"", ""pydicom/tests/test_valuerep.py::TestDecimalString::test_invalid_decimal_strings"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_last_first"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_copy"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_three_component"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_formatting"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_kr"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_bytes"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_bytes_comp_delimiter"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_bytes_caret_delimiter"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_unicode_jp_from_unicode"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_not_equal"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_encoding_carried"", ""pydicom/tests/test_valuerep.py::TestPersonName::test_hash"", ""pydicom/tests/test_valuerep.py::TestDateTime::test_date"", ""pydicom/tests/test_valuerep.py::TestDateTime::test_date_time"", ""pydicom/tests/test_valuerep.py::TestDateTime::test_time""]",9d69811e539774f296c2f289839147e741251716
pydicom__pydicom-1256,"You will be provided with a partial code base and an issue statement explaining a problem to resolve.
<issue>
from_json does not correctly convert BulkDataURI's in SQ data elements
**Describe the bug**
When a DICOM object contains large data elements in SQ elements and is converted to JSON, those elements are correctly turned into BulkDataURI's. However, when the JSON is converted back to DICOM using from_json, the BulkDataURI's in SQ data elements are not converted back and warnings are thrown.

**Expected behavior**
The BulkDataURI's in SQ data elements get converted back correctly.

**Steps To Reproduce**
Take the `waveform_ecg.dcm` in the test data, convert it to JSON, and then convert the JSON to DICOM

**Your environment**
module       | version
------       | -------
platform     | macOS-10.15.7-x86_64-i386-64bit
Python       | 3.8.2 (v3.8.2:7b3ab5921f, Feb 24 2020, 17:52:18)  [Clang 6.0 (clang-600.0.57)]
pydicom      | 2.1.0
gdcm         | _module not found_
jpeg_ls      | _module not found_
numpy        | _module not found_
PIL          | _module not found_

The problem is in `jsonrep.py` at line 227. I plan on submitting a pull-request today for this.

</issue>
<code>
[start of README.md]
1 [![CircleCI](https://circleci.com/gh/pydicom/pydicom/tree/master.svg?style=shield)](https://circleci.com/gh/pydicom/pydicom/tree/master)
2 [![codecov](https://codecov.io/gh/pydicom/pydicom/branch/master/graph/badge.svg)](https://codecov.io/gh/pydicom/pydicom)
3 [![Python version](https://img.shields.io/pypi/pyversions/pydicom.svg)](https://img.shields.io/pypi/pyversions/pydicom.svg)
4 [![PyPI version](https://badge.fury.io/py/pydicom.svg)](https://badge.fury.io/py/pydicom)
5 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4197955.svg)](https://doi.org/10.5281/zenodo.4197955)
6 [![Gitter](https://badges.gitter.im/pydicom/Lobby.svg)](https://gitter.im/pydicom/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
7 
8 # *pydicom*
9 
10 *pydicom* is a pure Python package for working with [DICOM](https://www.dicomstandard.org/) files. It lets you read, modify and write DICOM data in an easy ""pythonic"" way.
11 
12 As a pure Python package, *pydicom* can run anywhere Python runs without any other requirements, although if you're working with *Pixel Data* then we recommend you also install [NumPy](http://www.numpy.org).
13 
14 If you're looking for a Python library for DICOM networking then you might be interested in another of our projects: [pynetdicom](https://github.com/pydicom/pynetdicom).
15 
16 ## Installation
17 
18 Using [pip](https://pip.pypa.io/en/stable/):
19 ```
20 pip install pydicom
21 ```
22 Using [conda](https://docs.conda.io/en/latest/):
23 ```
24 conda install -c conda-forge pydicom
25 ```
26 
27 For more information, including installation instructions for the development version, see the [installation guide](https://pydicom.github.io/pydicom/stable/tutorials/installation.html).
28 
29 
30 ## Documentation
31 
32 The *pydicom* [user guide](https://pydicom.github.io/pydicom/stable/old/pydicom_user_guide.html), [tutorials](https://pydicom.github.io/pydicom/stable/tutorials/index.html), [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) and [API reference](https://pydicom.github.io/pydicom/stable/reference/index.html) documentation is available for both the [current release](https://pydicom.github.io/pydicom/stable) and the [development version](https://pydicom.github.io/pydicom/dev) on GitHub Pages.
33 
34 ## *Pixel Data*
35 
36 Compressed and uncompressed *Pixel Data* is always available to
37 be read, changed and written as [bytes](https://docs.python.org/3/library/stdtypes.html#bytes-objects):
38 ```python
39 >>> from pydicom import dcmread
40 >>> from pydicom.data import get_testdata_file
41 >>> path = get_testdata_file(""CT_small.dcm"")
42 >>> ds = dcmread(path)
43 >>> type(ds.PixelData)
44 <class 'bytes'>
45 >>> len(ds.PixelData)
46 32768
47 >>> ds.PixelData[:2]
48 b'\xaf\x00'
49 
50 ```
51 
52 If [NumPy](http://www.numpy.org) is installed, *Pixel Data* can be converted to an [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) using the [Dataset.pixel_array](https://pydicom.github.io/pydicom/stable/reference/generated/pydicom.dataset.Dataset.html#pydicom.dataset.Dataset.pixel_array) property:
53 
54 ```python
55 >>> arr = ds.pixel_array
56 >>> arr.shape
57 (128, 128)
58 >>> arr
59 array([[175, 180, 166, ..., 203, 207, 216],
60        [186, 183, 157, ..., 181, 190, 239],
61        [184, 180, 171, ..., 152, 164, 235],
62        ...,
63        [906, 910, 923, ..., 922, 929, 927],
64        [914, 954, 938, ..., 942, 925, 905],
65        [959, 955, 916, ..., 911, 904, 909]], dtype=int16)
66 ```
67 ### Compressed *Pixel Data*
68 #### JPEG, JPEG-LS and JPEG 2000
69 Converting JPEG compressed *Pixel Data* to an ``ndarray`` requires installing one or more additional Python libraries. For information on which libraries are required, see the [pixel data handler documentation](https://pydicom.github.io/pydicom/dev/old/image_data_handlers.html#guide-compressed).
70 
71 Compressing data into one of the JPEG formats is not currently supported.
72 
73 #### RLE
74 RLE encoded *Pixel Data* only requires NumPy, and compression and decompression are both supported.
75 
76 ## Examples
77 More [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) are available in the documentation.
78 
79 **Change a patient's ID**
80 ```python
81 from pydicom import dcmread
82 
83 ds = dcmread(""/path/to/file.dcm"")
84 # Edit the (0010,0020) 'Patient ID' element
85 ds.PatientID = ""12345678""
86 ds.save_as(""/path/to/file_updated.dcm"")
87 ```
88 
89 **Display the Pixel Data**
90 
91 With [NumPy](http://www.numpy.org) and [matplotlib](https://matplotlib.org/)
92 ```python
93 import matplotlib.pyplot as plt
94 from pydicom import dcmread
95 from pydicom.data import get_testdata_file
96 
97 # The path to a pydicom test dataset
98 path = get_testdata_file(""CT_small.dcm"")
99 ds = dcmread(path)
100 # `arr` is a numpy.ndarray
101 arr = ds.pixel_array
102 
103 plt.imshow(arr, cmap=""gray"")
104 plt.show()
105 ```
106 
107 ## Contributing
108 
109 To contribute to *pydicom*, read our [contribution guide](https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md).
110 
111 To contribute an example or extension of *pydicom* that doesn't belong with the core software, see our contribution repository:
112 [contrib-pydicom](https://www.github.com/pydicom/contrib-pydicom).
113 
[end of README.md]
[start of pydicom/jsonrep.py]
1 # Copyright 2008-2019 pydicom authors. See LICENSE file for details.
2 """"""Methods for converting Datasets and DataElements to/from json""""""
3 
4 import base64
5 from inspect import signature
6 import inspect
7 from typing import Callable, Optional, Union
8 import warnings
9 
10 from pydicom.tag import BaseTag
11 
12 # Order of keys is significant!
13 JSON_VALUE_KEYS = ('Value', 'BulkDataURI', 'InlineBinary',)
14 
15 BINARY_VR_VALUES = ['OW', 'OB', 'OD', 'OF', 'OL', 'UN',
16                     'OB or OW', 'US or OW', 'US or SS or OW']
17 VRs_TO_BE_FLOATS = ['DS', 'FL', 'FD', ]
18 VRs_TO_BE_INTS = ['IS', 'SL', 'SS', 'UL', 'US', 'US or SS']
19 
20 
21 def convert_to_python_number(value, vr):
22     """"""Makes sure that values are either ints or floats
23     based on their value representation.
24 
25     .. versionadded:: 1.4
26 
27     Parameters
28     ----------
29     value: Union[Union[str, int, float], List[Union[str, int, float]]]
30         value of data element
31     vr: str
32         value representation of data element
33 
34     Returns
35     -------
36     Union[Union[str, int, float], List[Union[str, int, float]]]
37 
38     """"""
39     if value is None:
40         return None
41     number_type = None
42     if vr in VRs_TO_BE_INTS:
43         number_type = int
44     if vr in VRs_TO_BE_FLOATS:
45         number_type = float
46     if number_type is not None:
47         if isinstance(value, (list, tuple,)):
48             value = [number_type(e) for e in value]
49         else:
50             value = number_type(value)
51     return value
52 
53 
54 class JsonDataElementConverter:
55     """"""Handles conversion between JSON struct and :class:`DataElement`.
56 
57     .. versionadded:: 1.4
58     """"""
59 
60     def __init__(
61         self,
62         dataset_class,
63         tag,
64         vr,
65         value,
66         value_key,
67         bulk_data_uri_handler: Optional[
68             Union[
69                 Callable[[BaseTag, str, str], object],
70                 Callable[[str], object]
71             ]
72         ] = None
73     ):
74         """"""Create a new converter instance.
75 
76         Parameters
77         ----------
78         dataset_class : dataset.Dataset derived class
79             Class used to create sequence items.
80         tag : BaseTag
81             The data element tag or int.
82         vr : str
83             The data element value representation.
84         value : list
85             The data element's value(s).
86         value_key : str or None
87             Key of the data element that contains the value
88             (options: ``{""Value"", ""InlineBinary"", ""BulkDataURI""}``)
89         bulk_data_uri_handler: callable or None
90             Callable function that accepts either the tag, vr and ""BulkDataURI""
91             or just the ""BulkDataURI"" of the JSON
92             representation of a data element and returns the actual value of
93             that data element (retrieved via DICOMweb WADO-RS)
94         """"""
95         self.dataset_class = dataset_class
96         self.tag = tag
97         self.vr = vr
98         self.value = value
99         self.value_key = value_key
100         if (
101             bulk_data_uri_handler and
102             len(signature(bulk_data_uri_handler).parameters) == 1
103         ):
104             def wrapped_bulk_data_handler(tag, vr, value):
105                 return bulk_data_uri_handler(value)
106             self.bulk_data_element_handler = wrapped_bulk_data_handler
107         else:
108             self.bulk_data_element_handler = bulk_data_uri_handler
109 
110     def get_element_values(self):
111         """"""Return a the data element value or list of values.
112 
113         Returns
114         -------
115         str or bytes or int or float or dataset_class
116         or PersonName or list of any of these types
117             The value or value list of the newly created data element.
118         """"""
119         from pydicom.dataelem import empty_value_for_VR
120         if self.value_key == 'Value':
121             if not isinstance(self.value, list):
122                 fmt = '""{}"" of data element ""{}"" must be a list.'
123                 raise TypeError(fmt.format(self.value_key, self.tag))
124             if not self.value:
125                 return empty_value_for_VR(self.vr)
126             element_value = [self.get_regular_element_value(v)
127                              for v in self.value]
128             if len(element_value) == 1 and self.vr != 'SQ':
129                 element_value = element_value[0]
130             return convert_to_python_number(element_value, self.vr)
131 
132         # The value for ""InlineBinary"" shall be encoded as a base64 encoded
133         # string, as shown in PS3.18, Table F.3.1-1, but the example in
134         # PS3.18, Annex F.4 shows the string enclosed in a list.
135         # We support both variants, as the standard is ambiguous here,
136         # and do the same for ""BulkDataURI"".
137         value = self.value
138         if isinstance(value, list):
139             value = value[0]
140 
141         if self.value_key == 'InlineBinary':
142             if not isinstance(value, (str, bytes)):
143                 fmt = '""{}"" of data element ""{}"" must be a bytes-like object.'
144                 raise TypeError(fmt.format(self.value_key, self.tag))
145             return base64.b64decode(value)
146 
147         if self.value_key == 'BulkDataURI':
148             if not isinstance(value, str):
149                 fmt = '""{}"" of data element ""{}"" must be a string.'
150                 raise TypeError(fmt.format(self.value_key, self.tag))
151             if self.bulk_data_element_handler is None:
152                 warnings.warn(
153                     'no bulk data URI handler provided for retrieval '
154                     'of value of data element ""{}""'.format(self.tag)
155                 )
156                 return empty_value_for_VR(self.vr, raw=True)
157             return self.bulk_data_element_handler(self.tag, self.vr, value)
158         return empty_value_for_VR(self.vr)
159 
160     def get_regular_element_value(self, value):
161         """"""Return a the data element value created from a json ""Value"" entry.
162 
163         Parameters
164         ----------
165         value : str or int or float or dict
166             The data element's value from the json entry.
167 
168         Returns
169         -------
170         dataset_class or PersonName
171         or str or int or float
172             A single value of the corresponding :class:`DataElement`.
173         """"""
174         if self.vr == 'SQ':
175             return self.get_sequence_item(value)
176 
177         if self.vr == 'PN':
178             return self.get_pn_element_value(value)
179 
180         if self.vr == 'AT':
181             try:
182                 return int(value, 16)
183             except ValueError:
184                 warnings.warn('Invalid value ""{}"" for AT element - '
185                               'ignoring it'.format(value))
186             return
187         return value
188 
189     def get_sequence_item(self, value):
190         """"""Return a sequence item for the JSON dict `value`.
191 
192         Parameters
193         ----------
194         value : dict or None
195             The sequence item from the JSON entry.
196 
197         Returns
198         -------
199         dataset_class
200             The decoded dataset item.
201 
202         Raises
203         ------
204         KeyError
205             If the ""vr"" key is missing for a contained element
206         """"""
207         ds = self.dataset_class()
208         if value:
209             for key, val in value.items():
210                 if 'vr' not in val:
211                     fmt = 'Data element ""{}"" must have key ""vr"".'
212                     raise KeyError(fmt.format(self.tag))
213                 vr = val['vr']
214                 unique_value_keys = tuple(
215                     set(val.keys()) & set(JSON_VALUE_KEYS)
216                 )
217                 from pydicom import DataElement
218                 from pydicom.dataelem import empty_value_for_VR
219                 if not unique_value_keys:
220                     # data element with no value
221                     elem = DataElement(
222                         tag=int(key, 16),
223                         value=empty_value_for_VR(vr),
224                         VR=vr)
225                 else:
226                     value_key = unique_value_keys[0]
227                     elem = DataElement.from_json(
228                         self.dataset_class, key, vr,
229                         val[value_key], value_key
230                     )
231                 ds.add(elem)
232         return ds
233 
234     def get_pn_element_value(self, value):
235         """"""Return PersonName value from JSON value.
236 
237         Values with VR PN have a special JSON encoding, see the DICOM Standard,
238         Part 18, :dcm:`Annex F.2.2<part18/sect_F.2.2.html>`.
239 
240         Parameters
241         ----------
242         value : dict
243             The person name components in the JSON entry.
244 
245         Returns
246         -------
247         PersonName or str
248             The decoded PersonName object or an empty string.
249         """"""
250         if not isinstance(value, dict):
251             # Some DICOMweb services get this wrong, so we
252             # workaround the issue and warn the user
253             # rather than raising an error.
254             warnings.warn(
255                 'value of data element ""{}"" with VR Person Name (PN) '
256                 'is not formatted correctly'.format(self.tag)
257             )
258             return value
259         else:
260             if 'Phonetic' in value:
261                 comps = ['', '', '']
262             elif 'Ideographic' in value:
263                 comps = ['', '']
264             else:
265                 comps = ['']
266             if 'Alphabetic' in value:
267                 comps[0] = value['Alphabetic']
268             if 'Ideographic' in value:
269                 comps[1] = value['Ideographic']
270             if 'Phonetic' in value:
271                 comps[2] = value['Phonetic']
272             elem_value = '='.join(comps)
273             return elem_value
274 
[end of pydicom/jsonrep.py]
</code>
I need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.
<patch>
--- a/file.py
+++ b/file.py
@@ -1,27 +1,35 @@
 def euclidean(a, b):
-    while b:
-        a, b = b, a % b
-    return a
+    if b == 0:
+        return a
+    return euclidean(b, a % b)
 
 
 def bresenham(x0, y0, x1, y1):
     points = []
     dx = abs(x1 - x0)
     dy = abs(y1 - y0)
-    sx = 1 if x0 < x1 else -1
-    sy = 1 if y0 < y1 else -1
-    err = dx - dy
+    x, y = x0, y0
+    sx = -1 if x0 > x1 else 1
+    sy = -1 if y0 > y1 else 1
 
-    while True:
-        points.append((x0, y0))
-        if x0 == x1 and y0 == y1:
-            break
-        e2 = 2 * err
-        if e2 > -dy:
+    if dx > dy:
+        err = dx / 2.0
+        while x != x1:
+            points.append((x, y))
             err -= dy
-            x0 += sx
-        if e2 < dx:
-            err += dx
-            y0 += sy
+            if err < 0:
+                y += sy
+                err += dx
+            x += sx
+    else:
+        err = dy / 2.0
+        while y != y1:
+            points.append((x, y))
+            err -= dx
+            if err < 0:
+                x += sx
+                err += dy
+            y += sy
 
+    points.append((x, y))
     return points
</patch>

",pydicom/pydicom,49a3da4a3d9c24d7e8427a25048a1c7d5c4f7724,"from_json does not correctly convert BulkDataURI's in SQ data elements
**Describe the bug**
When a DICOM object contains large data elements in SQ elements and is converted to JSON, those elements are correctly turned into BulkDataURI's. However, when the JSON is converted back to DICOM using from_json, the BulkDataURI's in SQ data elements are not converted back and warnings are thrown.

**Expected behavior**
The BulkDataURI's in SQ data elements get converted back correctly.

**Steps To Reproduce**
Take the `waveform_ecg.dcm` in the test data, convert it to JSON, and then convert the JSON to DICOM

**Your environment**
module       | version
------       | -------
platform     | macOS-10.15.7-x86_64-i386-64bit
Python       | 3.8.2 (v3.8.2:7b3ab5921f, Feb 24 2020, 17:52:18)  [Clang 6.0 (clang-600.0.57)]
pydicom      | 2.1.0
gdcm         | _module not found_
jpeg_ls      | _module not found_
numpy        | _module not found_
PIL          | _module not found_

The problem is in `jsonrep.py` at line 227. I plan on submitting a pull-request today for this.
",,2020-11-04T21:13:33Z,"<patch>
diff --git a/pydicom/jsonrep.py b/pydicom/jsonrep.py
--- a/pydicom/jsonrep.py
+++ b/pydicom/jsonrep.py
@@ -226,7 +226,8 @@ def get_sequence_item(self, value):
                     value_key = unique_value_keys[0]
                     elem = DataElement.from_json(
                         self.dataset_class, key, vr,
-                        val[value_key], value_key
+                        val[value_key], value_key,
+                        self.bulk_data_element_handler
                     )
                 ds.add(elem)
         return ds

</patch>","diff --git a/pydicom/tests/test_json.py b/pydicom/tests/test_json.py
--- a/pydicom/tests/test_json.py
+++ b/pydicom/tests/test_json.py
@@ -354,3 +354,25 @@ def bulk_data_reader(tag, vr, value):
         ds = Dataset().from_json(json.dumps(json_data), bulk_data_reader)
 
         assert b'xyzzy' == ds[0x00091002].value
+
+    def test_bulk_data_reader_is_called_within_SQ(self):
+        def bulk_data_reader(_):
+            return b'xyzzy'
+
+        json_data = {
+            ""003a0200"": {
+                ""vr"": ""SQ"", 
+                ""Value"": [
+                    {
+                        ""54001010"": {
+                            ""vr"": ""OW"",
+                            ""BulkDataURI"": ""https://a.dummy.url""
+                        }
+                    }
+                ]
+            }
+        }
+
+        ds = Dataset().from_json(json.dumps(json_data), bulk_data_reader)
+
+        assert b'xyzzy' == ds[0x003a0200].value[0][0x54001010].value
",2.1,"[""pydicom/tests/test_json.py::TestBinary::test_bulk_data_reader_is_called_within_SQ""]","[""pydicom/tests/test_json.py::TestPersonName::test_json_pn_from_file"", ""pydicom/tests/test_json.py::TestPersonName::test_pn_components_to_json"", ""pydicom/tests/test_json.py::TestPersonName::test_pn_components_from_json"", ""pydicom/tests/test_json.py::TestPersonName::test_empty_value"", ""pydicom/tests/test_json.py::TestPersonName::test_multi_value_to_json"", ""pydicom/tests/test_json.py::TestPersonName::test_dataelem_from_json"", ""pydicom/tests/test_json.py::TestAT::test_to_json"", ""pydicom/tests/test_json.py::TestAT::test_from_json"", ""pydicom/tests/test_json.py::TestAT::test_invalid_value_in_json"", ""pydicom/tests/test_json.py::TestAT::test_invalid_tag_in_json"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_json_from_dicom_file"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_roundtrip"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_dataset_dumphandler"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_dataelement_dumphandler"", ""pydicom/tests/test_json.py::TestDataSetToJson::test_sort_order"", ""pydicom/tests/test_json.py::TestSequence::test_nested_sequences"", ""pydicom/tests/test_json.py::TestBinary::test_inline_binary"", ""pydicom/tests/test_json.py::TestBinary::test_invalid_inline_binary"", ""pydicom/tests/test_json.py::TestBinary::test_valid_bulkdata_uri"", ""pydicom/tests/test_json.py::TestBinary::test_invalid_bulkdata_uri"", ""pydicom/tests/test_json.py::TestBinary::test_bulk_data_reader_is_called"", ""pydicom/tests/test_json.py::TestBinary::test_bulk_data_reader_is_called_2""]",506ecea8f378dc687d5c504788fc78810a190b7a
